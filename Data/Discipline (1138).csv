Title,Abstract,Discipline,Link
VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model,"With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",CS,http://arxiv.org/abs/2505.03739v1
AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control,"Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.",CS,http://arxiv.org/abs/2505.03738v1
FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios,"Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/",CS,http://arxiv.org/abs/2505.03730v1
Actor-Critics Can Achieve Optimal Sample Efficiency,"Actor-critic algorithms have become a cornerstone in reinforcement learning (RL), leveraging the strengths of both policy-based and value-based methods. Despite recent progress in understanding their statistical efficiency, no existing work has successfully learned an $\epsilon$-optimal policy with a sample complexity of $O(1/\epsilon^2)$ trajectories with general function approximation when strategic exploration is necessary.   We address this open problem by introducing a novel actor-critic algorithm that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$ regret when the Bellman eluder dimension $d$ does not increase with $T$ at more than a $\log T$ rate.   Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets.   We extend this to the setting of Hybrid RL, showing that initializing the critic with offline data yields sample efficiency gains compared to purely offline or online RL. Further, utilizing access to offline data, we provide a \textit{non-optimistic} provably efficient actor-critic algorithm that only additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy concentrability coefficient and $N_{\text{off}}$ is the number of offline samples. This addresses another open problem in the literature. We further provide numerical experiments to support our theoretical findings.",CS,http://arxiv.org/abs/2505.03710v1
Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid,"Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation.",CS,http://arxiv.org/abs/2505.03694v1
Graph Drawing for LLMs: An Empirical Evaluation,"Our work contributes to the fast-growing literature on the use of Large Language Models (LLMs) to perform graph-related tasks. In particular, we focus on usage scenarios that rely on the visual modality, feeding the model with a drawing of the graph under analysis. We investigate how the model's performance is affected by the chosen layout paradigm, the aesthetics of the drawing, and the prompting technique used for the queries. We formulate three corresponding research questions and present the results of a thorough experimental analysis. Our findings reveal that choosing the right layout paradigm and optimizing the readability of the input drawing from a human perspective can significantly improve the performance of the model on the given task. Moreover, selecting the most effective prompting technique is a challenging yet crucial task for achieving optimal performance.",CS,http://arxiv.org/abs/2505.03678v1
"Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance","In human-agent teams, openly sharing goals is often assumed to enhance planning, collaboration, and effectiveness. However, direct communication of these goals is not always feasible, requiring teammates to infer their partner's intentions through actions. Building on this, we investigate whether an AI agent's ability to share its inferred understanding of a human teammate's goals can improve task performance and perceived collaboration. Through an experiment comparing three conditions-no recognition (NR), viable goals (VG), and viable goals on-demand (VGod) - we find that while goal-sharing information did not yield significant improvements in task performance or overall satisfaction scores, thematic analysis suggests that it supported strategic adaptations and subjective perceptions of collaboration. Cognitive load assessments revealed no additional burden across conditions, highlighting the challenge of balancing informativeness and simplicity in human-agent interactions. These findings highlight the nuanced trade-off of goal-sharing: while it fosters trust and enhances perceived collaboration, it can occasionally hinder objective performance gains.",CS,http://arxiv.org/abs/2505.03674v1
Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time,"This paper proposes an integration of temporal logical reasoning and Partially Observable Markov Decision Processes (POMDPs) to achieve interpretable decision-making under uncertainty with macro-actions. Our method leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus (EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon, significantly reducing inference time while ensuring robust performance. Such macro-actions are learnt via Inductive Logic Programming (ILP) from a few traces of execution (belief-action pairs), thus eliminating the need for manually designed heuristics and requiring only the specification of the POMDP transition model. In the Pocman and Rocksample benchmark scenarios, our learned macro-actions demonstrate increased expressiveness and generality when compared to time-independent heuristics, indeed offering substantial computational efficiency improvements.",CS,http://arxiv.org/abs/2505.03668v1
Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models,"Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are essential for evaluating white matter integrity and structural connectivity in neuroimaging. However, the spatial misalignment between FA maps and tractography atlases hinders their effective integration into predictive models. To address this issue, we propose a CycleGAN based approach for generating FA maps directly from T1-weighted MRI scans, representing the first application of this technique to both healthy and tumour-affected tissues. Our model, trained on unpaired data, produces high fidelity maps, which have been rigorously evaluated using Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in tumour regions. Radiological assessments further underscore the model's potential to enhance clinical workflows by providing an AI-driven alternative that reduces the necessity for additional scans.",CS,http://arxiv.org/abs/2505.03662v1
Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems,"Recommender Systems (RSs) aim to provide personalized recommendations for users. A newly discovered bias, known as sentiment bias, uncovers a common phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users or items with negative reviews deteriorates compared with users or items with positive reviews. Critical users and niche items are disadvantaged by such unfair recommendations. We study this problem from the perspective of counterfactual inference with two stages. At the model training stage, we build a causal graph and model how sentiment influences the final rating score. During the inference stage, we decouple the direct and indirect effects to mitigate the impact of sentiment bias and remove the indirect effect using counterfactual inference. We have conducted extensive experiments, and the results validate that our model can achieve comparable performance on rating prediction for better recommendations and effective mitigation of sentiment bias. To the best of our knowledge, this is the first work to employ counterfactual inference on sentiment bias mitigation in RSs.",CS,http://arxiv.org/abs/2505.03655v1
ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant,"Recent advances in personalized MLLMs enable effective capture of user-specific concepts, supporting both recognition of personalized concepts and contextual captioning. However, humans typically explore and reason over relations among objects and individuals, transcending surface-level information to achieve more personalized and contextual understanding. To this end, existing methods may face three main limitations: Their training data lacks multi-object sets in which relations among objects are learnable. Building on the limited training data, their models overlook the relations between different personalized concepts and fail to reason over them. Their experiments mainly focus on a single personalized concept, where evaluations are limited to recognition and captioning tasks. To address the limitations, we present a new dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard graph prompting methods are designed to align KGs within the model's semantic space. We establish the ReGraP Benchmark, which contains diverse task types: multiple-choice, fill-in-the-blank, True/False, and descriptive questions in both open- and closed-ended settings. The proposed benchmark is designed to evaluate the relational reasoning and knowledge-connection capability of personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and other competitive MLLMs. Results show that the proposed model not only learns personalized knowledge but also performs relational reasoning in responses, achieving the SoTA performance compared with the competitive methods. All the codes and datasets are released at: https://github.com/xyfyyds/ReGraP.",CS,http://arxiv.org/abs/2505.03654v1
Binding threshold units with artificial oscillatory neurons,"Artificial Kuramoto oscillatory neurons were recently introduced as an alternative to threshold units. Empirical evidence suggests that oscillatory units outperform threshold units in several tasks including unsupervised object discovery and certain reasoning problems. The proposed coupling mechanism for these oscillatory neurons is heterogeneous, combining a generalized Kuramoto equation with standard coupling methods used for threshold units. In this research note, we present a theoretical framework that clearly distinguishes oscillatory neurons from threshold units and establishes a coupling mechanism between them. We argue that, from a biological standpoint, oscillatory and threshold units realise distinct aspects of neural coding: roughly, threshold units model intensity of neuron firing, while oscillatory units facilitate information exchange by frequency modulation. To derive interaction between these two types of units, we constrain their dynamics by focusing on dynamical systems that admit Lyapunov functions. For threshold units, this leads to Hopfield associative memory model, and for oscillatory units it yields a specific form of generalized Kuramoto model. The resulting dynamical systems can be naturally coupled to form a Hopfield-Kuramoto associative memory model, which also admits a Lyapunov function. Various forms of coupling are possible. Notably, oscillatory neurons can be employed to implement a low-rank correction to the weight matrix of a Hopfield network. This correction can be viewed either as a form of Hebbian learning or as a popular LoRA method used for fine-tuning of large language models. We demonstrate the practical realization of this particular coupling through illustrative toy experiments.",CS,http://arxiv.org/abs/2505.03648v1
ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders,"Despite the extensive use of deep autoencoders (AEs) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. AE robustness is characterized by the Lipschitz bounds of its components. Existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in AEs. In the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. To address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local Lipschitz bounds by enhancing loss gradient information propagation during attack optimization. We demonstrate through extensive experiments on state-of-the-art AEs that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. As a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.",CS,http://arxiv.org/abs/2505.03646v1
BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems,"Learning-enabled planning and control algorithms are increasingly popular, but they often lack rigorous guarantees of performance or safety. We introduce an algorithm for computing underapproximate backward reachable sets of nonlinear discrete time neural feedback loops. We then use the backward reachable sets to check goal-reaching properties. Our algorithm is based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets through solutions of mixed-integer linear programs. We rigorously analyze the soundness of our algorithm and demonstrate it on a numerical example. Our work expands the class of properties that can be verified for learning-enabled systems.",CS,http://arxiv.org/abs/2505.03643v1
Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability,"Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. We present a computational framework BAM (Boundary Alignment & Manipulation framework) that combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. Our perceptual boundary sampling algorithm generates stimuli along ANN decision boundaries that intrinsically induce significant perceptual variability. The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images. Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants. This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis.",CS,http://arxiv.org/abs/2505.03641v1
Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation,"In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalization capability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework.",CS,http://arxiv.org/abs/2505.03586v1
BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation,"Public deliberation, as in open discussion of issues of public concern, often suffers from scattered and shallow discourse, poor sensemaking, and a disconnect from actionable policy outcomes. This paper introduces BCause, a discussion system leveraging generative AI and human-machine collaboration to transform unstructured dialogue around public issues (such as urban living, policy changes, and current socio-economic transformations) into structured, actionable democratic processes. We present three innovations: (i) importing and transforming unstructured transcripts into argumentative discussions, (ii) geo-deliberated problem-sensing via a Telegram bot for local issue reporting, and (iii) smart reporting with customizable widgets (e.g., summaries, topic modelling, policy recommendations, clustered arguments). The system's human-AI partnership preserves critical human participation to ensure ethical oversight, contextual relevance, and creative synthesis.",CS,http://arxiv.org/abs/2505.03584v1
LlamaFirewall: An open source guardrail system for building secure AI agents,"Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails.",CS,http://arxiv.org/abs/2505.03574v1
OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents,"In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.",CS,http://arxiv.org/abs/2505.03570v1
Real-Time Person Image Synthesis Using a Flow Matching Model,"Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images conditioned on a target pose and a source image. This task plays a key role in various real-world applications, such as sign language video generation, AR/VR, gaming, and live streaming. In these scenarios, real-time PGPIS is critical for providing immediate visual feedback and maintaining user immersion.However, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-fidelity images from diverse and dynamic human poses. Recent diffusion-based methods have shown impressive image quality in PGPIS, but their slow sampling speeds hinder deployment in time-sensitive applications. This latency is particularly problematic in tasks like generating sign language videos during live broadcasts, where rapid image updates are required. Therefore, developing a fast and reliable PGPIS model is a crucial step toward enabling real-time interactive systems. To address this challenge, we propose a generative model based on flow matching (FM). Our approach enables faster, more stable, and more efficient training and sampling. Furthermore, the proposed model supports conditional generation and can operate in latent space, making it especially suitable for real-time PGPIS applications where both speed and quality are critical. We evaluate our proposed method, Real-Time Person Image Synthesis Using a Flow Matching Model (RPFM), on the widely used DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves near-real-time sampling speeds while maintaining performance comparable to the state-of-the-art models. Our methodology trades off a slight, acceptable decrease in generated-image accuracy for over a twofold increase in generation speed, thereby ensuring real-time performance.",CS,http://arxiv.org/abs/2505.03562v1
Ergodic Generative Flows,"Generative Flow Networks (GFNs) were initially introduced on directed acyclic graphs to sample from an unnormalized distribution density. Recent works have extended the theoretical framework for generative methods allowing more flexibility and enhancing application range. However, many challenges remain in training GFNs in continuous settings and for imitation learning (IL), including intractability of flow-matching loss, limited tests of non-acyclic training, and the need for a separate reward model in imitation learning. The present work proposes a family of generative flows called Ergodic Generative Flows (EGFs) which are used to address the aforementioned issues. First, we leverage ergodicity to build simple generative flows with finitely many globally defined transformations (diffeomorphisms) with universality guarantees and tractable flow-matching loss (FM loss). Second, we introduce a new loss involving cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It is designed for IL training without a separate reward model. We evaluate IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning experiments with a target reward, using the FM loss.",CS,http://arxiv.org/abs/2505.03561v1
Rapid AI-based generation of coverage paths for dispensing applications,"Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial role in the design of power electronics and electronic control units. Up to now, this is done manually by experts or by using optimization approaches with a high computational effort. We propose a novel AI-based approach to generate dispense paths for TIM and similar dispensing applications. It is a drop-in replacement for optimization-based approaches. An Artificial Neural Network (ANN) receives the target cooling area as input and directly outputs the dispense path. Our proposed setup does not require labels and we show its feasibility on multiple target areas. The resulting dispense paths can be directly transferred to automated manufacturing equipment and do not exhibit air entrapments. The approach of using an ANN to predict process parameters for a desired target state in real-time could potentially be transferred to other manufacturing processes.",CS,http://arxiv.org/abs/2505.03560v1
Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID,"The personalization of Stable Diffusion for generating professional portraits from amateur photographs is a burgeoning area, with applications in various downstream contexts. This paper investigates the impact of augmentations on improving facial resemblance when using two prominent personalization techniques: DreamBooth and InstantID. Through a series of experiments with diverse subject datasets, we assessed the effectiveness of various augmentation strategies on the generated headshots' fidelity to the original subject. We introduce FaceDistance, a wrapper around FaceNet, to rank the generations based on facial similarity, which aided in our assessment. Ultimately, this research provides insights into the role of augmentations in enhancing facial resemblance in SDXL-generated portraits, informing strategies for their effective deployment in downstream applications.",CS,http://arxiv.org/abs/2505.03557v1
A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning,"Inconsistent outputs and hallucinations from large language models (LLMs) are major obstacles to reliable AI systems. When different proprietary reasoning models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI, are given the same complex request, they often produce divergent results due to variations in training and inference. This paper proposes a novel consensus mechanism, inspired by distributed ledger technology, to validate and converge these outputs, treating each RM as a black-box peer. Building on the Hashgraph consensus algorithm, our approach employs gossip-about-gossip communication and virtual voting to achieve agreement among an ensemble of RMs. We present an architectural design for a prototype system in which RMs iteratively exchange and update their answers, using information from each round to improve accuracy and confidence in subsequent rounds. This approach goes beyond simple majority voting by incorporating the knowledge and cross-verification content of every model. We justify the feasibility of this Hashgraph-inspired consensus for AI ensembles and outline its advantages over traditional ensembling techniques in reducing nonfactual outputs. Preliminary considerations for implementation, evaluation criteria for convergence and accuracy, and potential challenges are discussed. The proposed mechanism demonstrates a promising direction for multi-agent AI systems to self-validate and deliver high-fidelity responses in complex tasks.",CS,http://arxiv.org/abs/2505.03553v1
STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game,"We introduce STORY2GAME, a novel approach to using Large Language Models to generate text-based interactive fiction games that starts by generating a story, populates the world, and builds the code for actions in a game engine that enables the story to play out interactively. Whereas a given set of hard-coded actions can artificially constrain story generation, the ability to generate actions means the story generation process can be more open-ended but still allow for experiences that are grounded in a game state. The key to successful action generation is to use LLM-generated preconditions and effects of actions in the stories as guides for what aspects of the game state must be tracked and changed by the game engine when a player performs an action. We also introduce a technique for dynamically generating new actions to accommodate the player's desire to perform actions that they think of that are not part of the story. Dynamic action generation may require on-the-fly updates to the game engine's state representation and revision of previously generated actions. We evaluate the success rate of action code generation with respect to whether a player can interactively play through the entire generated story.",CS,http://arxiv.org/abs/2505.03547v1
Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks,"Deep learning has substantially advanced the Single Image Super-Resolution (SISR). However, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of ""Universality"" and its associated definitions which extend the traditional notion of ""Generalization"" to encompass the modules' ease of transferability, thus revealing the relationships between module universality and model generalizability. Then we propose the Universality Assessment Equation (UAE), a metric for quantifying how readily a given module could be transplanted across models. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, extreme-industrial imagery and on-device deployments, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity.",CS,http://arxiv.org/abs/2505.03522v1
From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition,"In this paper, we introduce a novel paradigm for reservoir computing (RC) that leverages a pool of cultured biological neurons as the reservoir substrate, creating a biological reservoir computing (BRC). This system operates similarly to an echo state network (ESN), with the key distinction that the neural activity is generated by a network of cultured neurons, rather than being modeled by traditional artificial computational units. The neuronal activity is recorded using a multi-electrode array (MEA), which enables high-throughput recording of neural signals. In our approach, inputs are introduced into the network through a subset of the MEA electrodes, while the remaining electrodes capture the resulting neural activity. This generates a nonlinear mapping of the input data to a high-dimensional biological feature space, where distinguishing between data becomes more efficient and straightforward, allowing a simple linear classifier to perform pattern recognition tasks effectively. To evaluate the performance of our proposed system, we present an experimental study that includes various input patterns, such as positional codes, bars with different orientations, and a digit recognition task. The results demonstrate the feasibility of using biological neural networks to perform tasks traditionally handled by artificial neural networks, paving the way for further exploration of biologically-inspired computing systems, with potential applications in neuromorphic engineering and bio-hybrid computing.",CS,http://arxiv.org/abs/2505.03510v1
Augmenting Human Cognition through Everyday AR,"As spatial computing and multimodal LLMs mature, AR is tending to become an intuitive ""thinking tool,"" embedding semantic and context-aware intelligence directly into everyday environments. This paper explores how always-on AR can seamlessly bridge digital cognition and physical affordances, enabling proactive, context-sensitive interactions that enhance human task performance and understanding.",CS,http://arxiv.org/abs/2505.03492v1
A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM),"Generative models can unintentionally memorize training data, posing significant privacy risks. This paper addresses the memorization phenomenon in time series imputation models, introducing the Loss-Based with Reference Model (LBRM) algorithm. The LBRM method leverages a reference model to enhance the accuracy of membership inference attacks, distinguishing between training and test data. Our contributions are twofold: first, we propose an innovative method to effectively extract and identify memorized training data, significantly improving detection accuracy. On average, without fine-tuning, the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased by approximately 60\%. Second, we validate our approach through membership inference attacks on two types of architectures designed for time series imputation, demonstrating the robustness and versatility of the LBRM approach in different contexts. These results highlight the significant enhancement in detection accuracy provided by the LBRM approach, addressing privacy risks in time series imputation models.",CS,http://arxiv.org/abs/2505.03490v1
am-ELO: A Stable Framework for Arena-based LLM Evaluation,"Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.",CS,http://arxiv.org/abs/2505.03475v1
Blending 3D Geometry and Machine Learning for Multi-View Stereopsis,"Traditional multi-view stereo (MVS) methods primarily depend on photometric and geometric consistency constraints. In contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3D geometry, applying explicit geometric consistency (GC) checks only as a post-processing step, with no impact on the learning process itself. In this work, we introduce GC MVSNet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see Fig. 1). This integrated GC check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other MVS methods. Furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. Extensive experiments demonstrate that our approach achieves a new state of the art on the DTU and BlendedMVS datasets and secures second place on the Tanks and Temples benchmark. To our knowledge, GC MVSNet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. Our code is available.",CS,http://arxiv.org/abs/2505.03470v1
An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation,"Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with two optimized evaluation metrics. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with iterative random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing models first is preferable to the prevalent practice of optimizing sequentially according to the RAG pipeline order.",CS,http://arxiv.org/abs/2505.03452v1
Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis,"The rise of QR code based phishing (""Quishing"") poses a growing cybersecurity threat, as attackers increasingly exploit QR codes to bypass traditional phishing defenses. Existing detection methods predominantly focus on URL analysis, which requires the extraction of the QR code payload, and may inadvertently expose users to malicious content. Moreover, QR codes can encode various types of data beyond URLs, such as Wi-Fi credentials and payment information, making URL-based detection insufficient for broader security concerns. To address these gaps, we propose the first framework for quishing detection that directly analyzes QR code structure and pixel patterns without extracting the embedded content. We generated a dataset of phishing and benign QR codes and we used it to train and evaluate multiple machine learning models, including Logistic Regression, Decision Trees, Random Forest, Naive Bayes, LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of 0.9106, demonstrating the feasibility of QR-centric detection. Through feature importance analysis, we identify key visual indicators of malicious intent and refine our feature set by removing non-informative pixels, improving performance to an AUC of 0.9133 with a reduced feature space. Our findings reveal that the structural features of QR code correlate strongly with phishing risk. This work establishes a foundation for quishing mitigation and highlights the potential of direct QR analysis as a critical layer in modern phishing defenses.",CS,http://arxiv.org/abs/2505.03451v1
Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories,"Centralized and distributed systems are two main approaches to organizing ICT infrastructure, each with its pros and cons. Centralized systems concentrate resources in one location, making management easier but creating single points of failure. Distributed systems, on the other hand, spread resources across multiple nodes, offering better scalability and fault tolerance, but requiring more complex management. The choice between them depends on factors like application needs, scalability, and data sensitivity. Centralized systems suit applications with limited scalability and centralized control, while distributed systems excel in large-scale environments requiring high availability and performance. This paper explores a distributed document repository system developed for the Italian Ministry of Justice, using edge repositories to analyze textual data and metadata, enhancing semantic exploration capabilities.",CS,http://arxiv.org/abs/2505.03443v1
The Steganographic Potentials of Language Models,"The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment.",CS,http://arxiv.org/abs/2505.03439v1
Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents,"Large Language Models (LLMs) represent a landmark achievement in Artificial Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks such as text generation, code completion, and conversational coherence. These capabilities stem from their architecture, which mirrors human procedural memory -- the brain's ability to automate repetitive, pattern-driven tasks through practice. However, as LLMs are increasingly deployed in real-world applications, it becomes impossible to ignore their limitations operating in complex, unpredictable environments. This paper argues that LLMs, while transformative, are fundamentally constrained by their reliance on procedural memory. To create agents capable of navigating ``wicked'' learning environments -- where rules shift, feedback is ambiguous, and novelty is the norm -- we must augment LLMs with semantic memory and associative learning systems. By adopting a modular architecture that decouples these cognitive functions, we can bridge the gap between narrow procedural expertise and the adaptive intelligence required for real-world problem-solving.",CS,http://arxiv.org/abs/2505.03434v1
MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks,"Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.",CS,http://arxiv.org/abs/2505.03427v1
Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications,"Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is availabel at https://anonymous.4open.science/r/CPGG.",CS,http://arxiv.org/abs/2505.03426v1
Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense,"The growing need for Trusted AI (TAI) highlights the importance of interpretability and robustness in machine learning models. However, many existing tools overlook graph data and rarely combine these two aspects into a single solution. Graph Neural Networks (GNNs) have become a popular approach, achieving top results across various tasks. We introduce GNN-AID (Graph Neural Network Analysis, Interpretation, and Defense), an open-source framework designed for graph data to address this gap. Built as a Python library, GNN-AID supports advanced trust methods and architectural layers, allowing users to analyze graph datasets and GNN behavior using attacks, defenses, and interpretability methods.   GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models, and support for any GNNs through customizable interfaces. It also includes a web interface with tools for graph visualization and no-code features like an interactive model builder, simplifying the exploration and analysis of GNNs. The framework also supports MLOps techniques, ensuring reproducibility and result versioning to track and revisit analyses efficiently.   GNN-AID is a flexible tool for developers and researchers. It helps developers create, analyze, and customize graph models, while also providing access to prebuilt datasets and models for quick experimentation. Researchers can use the framework to explore advanced topics on the relationship between interpretability and robustness, test defense strategies, and combine methods to protect against different types of attacks.   We also show how defenses against evasion and poisoning attacks can conflict when applied to graph data, highlighting the complex connections between defense strategies.   GNN-AID is available at \href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}",CS,http://arxiv.org/abs/2505.03424v1
Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation,"This research paper investigates the application of Large Language Models (LLMs) in healthcare, specifically focusing on enhancing medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation (QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By embedding and retrieving context-relevant healthcare information, the system significantly improves response accuracy. QLoRA facilitates notable parameter efficiency and memory optimization, preserving the integrity of medical information through specialized quantization techniques. Our research also shows that our model performs relatively well on various medical benchmarks, indicating that it can be used to make basic medical suggestions. This paper details the system's technical components, including its architecture, quantization methods, and key healthcare applications such as enhanced disease prediction from patient symptoms and medical history, treatment suggestions, and efficient summarization of complex medical reports. We touch on the ethical considerations-patient privacy, data security, and the need for rigorous clinical validation-as well as the practical challenges of integrating such systems into real-world healthcare workflows. Furthermore, the lightweight quantized weights ensure scalability and ease of deployment even in low-resource hospital environments. Finally, the paper concludes with an analysis of the broader impact of LLMs on healthcare and outlines future directions for LLMs in medical settings.",CS,http://arxiv.org/abs/2505.03406v1
DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation,"Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. Existing LRRG approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. However, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. Consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in LRRG. To address this, we develop a novel dynamic difference-aware temporal residual network (DDaTR). In DDaTR, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. The Dynamic Feature Alignment Module (DFAM) is designed to align prior features across modalities for the integrity of prior clinical information. Prompted by the enriched prior features, the dynamic difference-aware module (DDAM) captures favorable difference information by identifying relationships across exams. Furthermore, our DDaTR employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. Extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both RRG and LRRG tasks.",CS,http://arxiv.org/abs/2505.03401v1
Automatic Calibration for Membership Inference Attack on Large Language Models,"Membership Inference Attacks (MIAs) have recently been employed to determine whether a specific text was part of the pre-training data of Large Language Models (LLMs). However, existing methods often misinfer non-members as members, leading to a high false positive rate, or depend on additional reference models for probability calibration, which limits their practicality. To overcome these challenges, we introduce a novel framework called Automatic Calibration Membership Inference Attack (ACMIA), which utilizes a tunable temperature to calibrate output probabilities effectively. This approach is inspired by our theoretical insights into maximum likelihood estimation during the pre-training of LLMs. We introduce ACMIA in three configurations designed to accommodate different levels of model access and increase the probability gap between members and non-members, improving the reliability and robustness of membership inference. Extensive experiments on various open-source LLMs demonstrate that our proposed attack is highly effective, robust, and generalizable, surpassing state-of-the-art baselines across three widely used benchmarks. Our code is available at: \href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.",CS,http://arxiv.org/abs/2505.03392v1
Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant,"Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare.",CS,http://arxiv.org/abs/2505.03380v1
SPAP: Structured Pruning via Alternating Optimization and Penalty Methods,"The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.",CS,http://arxiv.org/abs/2505.03373v1
Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten,"Free play is a fundamental aspect of early childhood education, supporting children's cognitive, social, emotional, and motor development. However, assessing children's development during free play poses significant challenges due to the unstructured and spontaneous nature of the activity. Traditional assessment methods often rely on direct observations by teachers, parents, or researchers, which may fail to capture comprehensive insights from free play and provide timely feedback to educators. This study proposes an innovative approach combining Large Language Models (LLMs) with learning analytics to analyze children's self-narratives of their play experiences. The LLM identifies developmental abilities, while performance scores across different play settings are calculated using learning analytics techniques. We collected 2,224 play narratives from 29 children in a kindergarten, covering four distinct play areas over one semester. According to the evaluation results from eight professionals, the LLM-based approach achieved high accuracy in identifying cognitive, motor, and social abilities, with accuracy exceeding 90% in most domains. Moreover, significant differences in developmental outcomes were observed across play settings, highlighting each area's unique contributions to specific abilities. These findings confirm that the proposed approach is effective in identifying children's development across various free play settings. This study demonstrates the potential of integrating LLMs and learning analytics to provide child-centered insights into developmental trajectories, offering educators valuable data to support personalized learning and enhance early childhood education practices.",CS,http://arxiv.org/abs/2505.03369v1
Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection,"Speech-based AI models are emerging as powerful tools for detecting depression and the presence of Post-traumatic stress disorder (PTSD), offering a non-invasive and cost-effective way to assess mental health. However, these models often struggle with gender bias, which can lead to unfair and inaccurate predictions. In this study, our study addresses this issue by introducing a domain adversarial training approach that explicitly considers gender differences in speech-based depression and PTSD detection. Specifically, we treat different genders as distinct domains and integrate this information into a pretrained speech foundation model. We then validate its effectiveness on the E-DAIC dataset to assess its impact on performance. Experimental results show that our method notably improves detection performance, increasing the F1-score by up to 13.29 percentage points compared to the baseline. This highlights the importance of addressing demographic disparities in AI-driven mental health assessment.",CS,http://arxiv.org/abs/2505.03359v1
Safer Prompts: Reducing IP Risk in Visual Generative AI,"Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.",CS,http://arxiv.org/abs/2505.03338v1
Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs,"Large Language Models (LLMs) have shown promise for generative recommender systems due to their transformative capabilities in user interaction. However, ensuring they do not recommend out-of-domain (OOD) items remains a challenge. We study two distinct methods to address this issue: RecLM-ret, a retrieval-based method, and RecLM-cgen, a constrained generation method. Both methods integrate seamlessly with existing LLMs to ensure in-domain recommendations. Comprehensive experiments on three recommendation datasets demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing LLM-based recommender models in accuracy while eliminating OOD recommendations, making it the preferred method for adoption. Additionally, RecLM-cgen maintains strong generalist capabilities and is a lightweight plug-and-play module for easy integration into LLMs, offering valuable practical benefits for the community. Source code is available at https://github.com/microsoft/RecAI",CS,http://arxiv.org/abs/2505.03336v1
Absolute Zero: Reinforced Self-play Reasoning with Zero Data,"Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",CS,http://arxiv.org/abs/2505.03335v1
"AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning","Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",CS,http://arxiv.org/abs/2505.03332v1
Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning,"Deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with TanDEM-X interferometric SAR data. Such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. In this work, our aim is to exploit the high-resolution capabilities of the TanDEM-X mission to map forests at 6 m. The goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. To cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. A 1 m resolution forest/non-forest reference map over Pennsylvania, USA, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. We select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the Amazon rainforest, where only very few labeled data at high resolution are available. In this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with TanDEM-X data.",CS,http://arxiv.org/abs/2505.03327v1
SD-VSum: A Method and Dataset for Script-Driven Video Summarization,"In this work, we introduce the task of script-driven video summarization, which aims to produce a summary of the full-length video by selecting the parts that are most relevant to a user-provided script outlining the visual content of the desired summary. Following, we extend a recently-introduced large-scale dataset for generic video summarization (VideoXum) by producing natural language descriptions of the different human-annotated summaries that are available per video. In this way we make it compatible with the introduced task, since the available triplets of ``video, summary and summary description'' can be used for training a method that is able to produce different summaries for a given video, driven by the provided script about the content of each summary. Finally, we develop a new network architecture for script-driven video summarization (SD-VSum), that relies on the use of a cross-modal attention mechanism for aligning and fusing information from the visual and text modalities. Our experimental evaluations demonstrate the advanced performance of SD-VSum against state-of-the-art approaches for query-driven and generic (unimodal and multimodal) summarization from the literature, and document its capacity to produce video summaries that are adapted to each user's needs about their content.",CS,http://arxiv.org/abs/2505.03319v1
"Artificial Behavior Intelligence: Technology, Challenges, and Future Directions","Understanding and predicting human behavior has emerged as a core capability in various AI application domains such as autonomous driving, smart healthcare, surveillance systems, and social robotics. This paper defines the technical framework of Artificial Behavior Intelligence (ABI), which comprehensively analyzes and interprets human posture, facial expressions, emotions, behavioral sequences, and contextual cues. It details the essential components of ABI, including pose estimation, face and emotion recognition, sequential behavior analysis, and context-aware modeling. Furthermore, we highlight the transformative potential of recent advances in large-scale pretrained models, such as large language models (LLMs), vision foundation models, and multimodal integration models, in significantly improving the accuracy and interpretability of behavior recognition. Our research team has a strong interest in the ABI domain and is actively conducting research, particularly focusing on the development of intelligent lightweight models capable of efficiently inferring complex human behaviors. This paper identifies several technical challenges that must be addressed to deploy ABI in real-world applications including learning behavioral intelligence from limited data, quantifying uncertainty in complex behavior prediction, and optimizing model structures for low-power, real-time inference. To tackle these challenges, our team is exploring various optimization strategies including lightweight transformers, graph-based recognition architectures, energy-aware loss functions, and multimodal knowledge distillation, while validating their applicability in real-time environments.",CS,http://arxiv.org/abs/2505.03315v1
Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation,"The recent surge in the popularity of diffusion models for image synthesis has attracted new attention to their potential for generation tasks in other domains. However, their applications to symbolic music generation remain largely under-explored because symbolic music is typically represented as sequences of discrete events and standard diffusion models are not well-suited for discrete data. We represent symbolic music as image-like pianorolls, facilitating the use of diffusion models for the generation of symbolic music. Moreover, this study introduces a novel diffusion model that incorporates our proposed Transformer-Mamba block and learnable wavelet transform. Classifier-free guidance is utilised to generate symbolic music with target chords. Our evaluation shows that our method achieves compelling results in terms of music quality and controllability, outperforming the strong baseline in pianoroll generation. Our code is available at https://github.com/jinchengzhanggg/proffusion.",CS,http://arxiv.org/abs/2505.03314v1
Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices,"This paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. Five state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (FLOPs), and model size. Additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on MobileNetV3 Small. Our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest accuracy, while MobileNetV3 offers the best balance between accuracy and efficiency, and SqueezeNet excels in inference speed and compactness. This study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. By addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.",CS,http://arxiv.org/abs/2505.03303v1
Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach,"Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. This method is based on what we call ""capabilities encoding."" The utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. Codes are available at https://github.com/pierreadorni/capabilities-encoding.",CS,http://arxiv.org/abs/2505.03299v1
The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning,"We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at https://midigap.cs.uni-freiburg.de.",CS,http://arxiv.org/abs/2505.03296v1
Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces,"Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.",CS,http://arxiv.org/abs/2505.03295v1
Physics-inspired Energy Transition Neural Network for Sequence Learning,"Recently, the superior performance of Transformers has made them a more robust and scalable solution for sequence modeling than traditional recurrent neural networks (RNNs). However, the effectiveness of Transformer in capturing long-term dependencies is primarily attributed to their comprehensive pair-modeling process rather than inherent inductive biases toward sequence semantics. In this study, we explore the capabilities of pure RNNs and reassess their long-term learning mechanisms. Inspired by the physics energy transition models that track energy changes over time, we propose a effective recurrent structure called the``Physics-inspired Energy Transition Neural Network"" (PETNN). We demonstrate that PETNN's memory mechanism effectively stores information over long-term dependencies. Experimental results indicate that PETNN outperforms transformer-based methods across various sequence tasks. Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower complexity. Our study presents an optimal foundational recurrent architecture and highlights the potential for developing effective recurrent neural networks in fields currently dominated by Transformer.",CS,http://arxiv.org/abs/2505.03281v1
RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation,"Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to identify the most relevant MCP(s) for a given query from an external index before engaging the LLM. Only the selected tool descriptions are passed to the model, drastically reducing prompt size and simplifying decision-making. Experiments, including an MCP stress test, demonstrate RAG-MCP significantly cuts prompt tokens (e.g., by over 50%) and more than triples tool selection accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables scalable and accurate tool integration for LLMs.",CS,http://arxiv.org/abs/2505.03275v1
Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models,"While modern Requirements Engineering (RE) heavily relies on natural language processing and Machine Learning (ML) techniques, their effectiveness is limited by the scarcity of high-quality datasets. This paper introduces Synthline, a Product Line (PL) approach that leverages Large Language Models to systematically generate synthetic RE data for classification-based use cases. Through an empirical evaluation conducted in the context of using ML for the identification of requirements specification defects, we investigated both the diversity of the generated data and its utility for training downstream models. Our analysis reveals that while synthetic datasets exhibit less diversity than real data, they are good enough to serve as viable training resources. Moreover, our evaluation shows that combining synthetic and real data leads to substantial performance improvements. Specifically, hybrid approaches achieve up to 85% improvement in precision and a 2x increase in recall compared to models trained exclusively on real data. These findings demonstrate the potential of PL-based synthetic data generation to address data scarcity in RE. We make both our implementation and generated datasets publicly available to support reproducibility and advancement in the field.",CS,http://arxiv.org/abs/2505.03265v1
Seeing the Abstract: Translating the Abstract Language for Vision Language Models,"Natural language goes beyond dryly describing visual content. It contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. Yet, current research in Vision Language Models (VLMs) has not shed light on abstract-oriented language. Our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. Particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. By analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. However, a critical challenge emerges: current general-purpose or fashion-specific VLMs are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. We propose a training-free and model-agnostic method, Abstract-to-Concrete Translator (ACT), to shift abstract representations towards well-represented concrete ones in the VLM latent space, using pre-trained models and existing multimodal databases. On the text-to-image retrieval task, despite being training-free, ACT outperforms the fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. Moreover, the improvement introduced by ACT is consistent with various VLMs, making it a plug-and-play solution.",CS,http://arxiv.org/abs/2505.03242v1
Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover,"This study introduces an innovative crossover operator named Particle Swarm Optimization-inspired Crossover (PSOX), which is specifically developed for real-coded genetic algorithms. Departing from conventional crossover approaches that only exchange information between individuals within the same generation, PSOX uniquely incorporates guidance from both the current global best solution and historical optimal solutions across multiple generations. This novel mechanism enables the algorithm to maintain population diversity while simultaneously accelerating convergence toward promising regions of the search space. The effectiveness of PSOX is rigorously evaluated through comprehensive experiments on 15 benchmark test functions with diverse characteristics, including unimodal, multimodal, and highly complex landscapes. Comparative analysis against five state-of-the-art crossover operators reveals that PSOX consistently delivers superior performance in terms of solution accuracy, algorithmic stability, and convergence speed, especially when combined with an appropriate mutation strategy. Furthermore, the study provides an in-depth investigation of how different mutation rates influence PSOX's performance, yielding practical guidelines for parameter tuning when addressing optimization problems with varying landscape properties.",CS,http://arxiv.org/abs/2505.03217v1
DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral,"Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.",CS,http://arxiv.org/abs/2505.03214v1
DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations,"Deep learning methods have shown promise in classifying breast cancer histopathology images, but their performance often declines with limited annotated data, a critical challenge in medical imaging due to the high cost and expertise required for annotations.",CS,http://arxiv.org/abs/2505.03204v1
"A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case","Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area.",CS,http://arxiv.org/abs/2505.03196v1
A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive,"With the rise of short video platforms in global communication, embedding steganographic data in audio synchronization streams has emerged as a new covert communication method. To address the limitations of traditional techniques in detecting synchronized steganography, this paper proposes a detection and distributed guidance reconstruction model based on short video ""Yupan"" samples released by China's South Sea Fleet on TikTok. The method integrates sliding spectrum feature extraction and intelligent inference mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is used to extract the main frequency trajectory and construct the synchronization frame detection model (M1), identifying a frame flag ""FFFFFFFFFFFFFFFFFF80"". The subsequent 32-byte payload is decoded by a structured model (M2) to infer distributed guidance commands. Analysis reveals a low-entropy, repetitive byte sequence in the 36 to 45 second audio segment with highly concentrated spectral energy, confirming the presence of synchronization frames. Although plaintext semantics are not restored, the consistency in command field layout suggests features of military communication protocols. The multi-segment splicing model further shows cross-video embedding and centralized decoding capabilities. The proposed framework validates the effectiveness of sliding spectral features for synchronized steganography detection and builds an extensible inference model for covert communication analysis and tactical guidance simulation on open platforms.",CS,http://arxiv.org/abs/2505.03193v1
Patterns and Mechanisms of Contrastive Activation Engineering,"Controlling the behavior of Large Language Models (LLMs) remains a significant challenge due to their inherent complexity and opacity. While techniques like fine-tuning can modify model behavior, they typically require extensive computational resources. Recent work has introduced a class of contrastive activation engineering (CAE) techniques as promising approaches for steering LLM outputs through targeted modifications to their internal representations. Applied at inference-time with zero cost, CAE has the potential to introduce a new paradigm of flexible, task-specific LLM behavior tuning. We analyze the performance of CAE in in-distribution, out-of-distribution settings, evaluate drawbacks, and begin to develop comprehensive guidelines for its effective deployment. We find that 1. CAE is only reliably effective when applied to in-distribution contexts. 2. Increasing the number of samples used to generate steering vectors has diminishing returns at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs that reverses the behavior that is steered for. 4. Steering vectors harm the overall model perplexity. 5. Larger models are more resistant to steering-induced degradation.",CS,http://arxiv.org/abs/2505.03189v1
seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models,"Current self-supervised algorithms mostly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by inducing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm can limit the flexibility of learned representations for downstream adaptation by creating performance trade-offs between invariance-related tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we introduce \emph{seq-JEPA}, a world modeling paradigm based on joint-embedding predictive architecture that leverages architectural inductive biases to resolve this trade-off. Without requiring an additional equivariance predictor or loss term, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to the specified transformations and another invariant to them and suited for tasks such as classification. To do so, our model processes a short sequence of different views (observations) of an input image. Each encoded view is concatenated with embeddings corresponding to the relative transformation (action) producing the next observation in the sequence. A transformer encoder outputs an aggregate representation of this sequence, which is subsequently conditioned on the action leading to the next observation to predict its representation. Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and image classification without sacrificing one for the other. Additionally, our framework excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.",CS,http://arxiv.org/abs/2505.03176v1
RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph,"Comprehending long videos remains a significant challenge for Large Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. To address this limitation, we propose RAVU (Retrieval Augmented Video Understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. We construct a graph representation of the video, capturing both spatial and temporal relationships between entities. This graph serves as a long-term memory, allowing us to track objects and their actions across time. To answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. Our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. Our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other SOTA methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.",CS,http://arxiv.org/abs/2505.03173v1
Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning,"Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal -- an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However because interactions do not have a consensus statistical definition tractable for downstream GCRL, we propose a definition of interactions based on the concept of null counterfactual: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a ""nulling'' operation with a learned model to infer interactions. NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.",CS,http://arxiv.org/abs/2505.03172v1
CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics,"Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both ``with solution'' and ``without solution'' scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at https://github.com/MoonshotAI/CombiBench/.",CS,http://arxiv.org/abs/2505.03171v1
Soft Best-of-n Sampling for Model Alignment,"Best-of-$n$ (BoN) sampling is a practical approach for aligning language model outputs with human preferences without expensive fine-tuning. BoN sampling is performed by generating $n$ responses to a prompt and then selecting the sample that maximizes a reward function. BoN yields high reward values in practice at a distortion cost, as measured by the KL-divergence between the sampled and original distribution. This distortion is coarsely controlled by varying the number of samples: larger $n$ yields a higher reward at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a generalization of BoN that allows for smooth interpolation between the original distribution and reward-maximizing distribution through a temperature parameter $\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$ sampling converges sharply to the optimal tilted distribution at a rate of $O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete outputs, we analyze an additive reward model that reveals the fundamental limitations of blockwise sampling.",CS,http://arxiv.org/abs/2505.03156v1
StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data,"Motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. Cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. Previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. Constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. In this work, we present StableMotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. The core component of our method is the introduction of motion quality indicators, which can be easily annotated through manual labeling or heuristic algorithms and enable training of quality-aware motion generation models on raw motion data with mixed quality. At test time, the model can be prompted to generate high-quality motions using the quality indicators. Our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. We demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying StableMotion to SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. The trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. See https://youtu.be/3Y7MMAH02B4 for more results.",CS,http://arxiv.org/abs/2505.03154v1
Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo),"We introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging (MRI). We express the image volume corresponding to each specific motion phase as the deformation of a single static image template. The main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. The diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. The velocity field at different phases is represented using a low-rank model. The static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. The more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3D cine MRI.",CS,http://arxiv.org/abs/2505.03149v1
Holmes: Automated Fact Check with Large Language Models,"The rise of Internet connectivity has accelerated the spread of disinformation, threatening societal trust, decision-making, and national security. Disinformation has evolved from simple text to complex multimodal forms combining images and text, challenging existing detection methods. Traditional deep learning models struggle to capture the complexity of multimodal disinformation. Inspired by advances in AI, this study explores using Large Language Models (LLMs) for automated disinformation detection. The empirical study shows that (1) LLMs alone cannot reliably assess the truthfulness of claims; (2) providing relevant evidence significantly improves their performance; (3) however, LLMs cannot autonomously search for accurate evidence. To address this, we propose Holmes, an end-to-end framework featuring a novel evidence retrieval method that assists LLMs in collecting high-quality evidence. Our approach uses (1) LLM-powered summarization to extract key information from open sources and (2) a new algorithm and metrics to evaluate evidence quality. Holmes enables LLMs to verify claims and generate justifications effectively. Experiments show Holmes achieves 88.3% accuracy on two open-source datasets and 90.2% in real-time verification tasks. Notably, our improved evidence retrieval boosts fact-checking accuracy by 30.8% over existing methods",CS,http://arxiv.org/abs/2505.03135v1
VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis,"Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.",CS,http://arxiv.org/abs/2505.03132v1
"Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE","Oysters are ecologically and commercially important species that require frequent monitoring to track population demographics (e.g. abundance, growth, mortality). Current methods of monitoring oyster reefs often require destructive sampling methods and extensive manual effort. Therefore, they are suboptimal for small-scale or sensitive environments. A recent alternative, the ODYSSEE model, was developed to use deep learning techniques to identify live oysters using video or images taken in the field of oyster reefs to assess abundance. The validity of this model in identifying live oysters on a reef was compared to expert and non-expert annotators. In addition, we identified potential sources of prediction error. Although the model can make inferences significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm 0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number of live oysters, achieving lower accuracy (63\%) in identifying live oysters compared to experts (74\%) and non-experts (75\%) alike. Image quality was an important factor in determining the accuracy of the model and the annotators. Better quality images improved human accuracy and worsened model accuracy. Although ODYSSEE was not sufficiently accurate, we anticipate that future training on higher-quality images, utilizing additional live imagery, and incorporating additional annotation training classes will greatly improve the model's predictive power based on the results of this analysis. Future research should address methods that improve the detection of living vs. dead oysters.",CS,http://arxiv.org/abs/2505.03108v1
"Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation","Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive ""capability signatures"" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.",CS,http://arxiv.org/abs/2505.03105v1
Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering,"This study explores the application of chaos engineering to enhance the robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in production-like environments under real-world conditions. LLM-MAS can potentially improve a wide range of tasks, from answering questions and generating content to automating customer support and improving decision-making processes. However, LLM-MAS in production or preproduction environments can be vulnerable to emergent errors or disruptions, such as hallucinations, agent failures, and agent communication failures. This study proposes a chaos engineering framework to proactively identify such vulnerabilities in LLM-MAS, assess and build resilience against them, and ensure reliable performance in critical applications.",CS,http://arxiv.org/abs/2505.03096v1
Latent Adaptive Planner for Dynamic Manipulation,"This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.",CS,http://arxiv.org/abs/2505.03077v1
BLAB: Brutally Long Audio Bench,"Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.",CS,http://arxiv.org/abs/2505.03054v1
Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text,"LLM evaluation is challenging even the case of base models. In real world deployments, evaluation is further complicated by the interplay of task specific prompts and experiential context. At scale, bias evaluation is often based on short context, fixed choice benchmarks that can be rapidly evaluated, however, these can lose validity when the LLMs' deployed context differs. Large scale human evaluation is often seen as too intractable and costly. Here we present our journey towards developing a semi-automated bias evaluation framework for free text responses that has human insights at its core. We discuss how we developed an operational definition of bias that helped us automate our pipeline and a methodology for classifying bias beyond multiple choice. We additionally comment on how human evaluation helped us uncover problematic templates in a bias benchmark.",CS,http://arxiv.org/abs/2505.03053v1
MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning,"Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at https://more-model.cs.uni-freiburg.de.",CS,http://arxiv.org/abs/2505.03035v1
"Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes","Independent learners often struggle with sustaining focus and emotional regulation in unstructured or distracting settings. Although some rely on ambient aids such as music, ASMR, or visual backgrounds to support concentration, these tools are rarely integrated into cohesive, learner-centered systems. Moreover, existing educational technologies focus primarily on content adaptation and feedback, overlooking the emotional and sensory context in which learning takes place. Large language models have demonstrated powerful multimodal capabilities including the ability to generate and adapt text, audio, and visual content. Educational research has yet to fully explore their potential in creating personalized audiovisual learning environments. To address this gap, we introduce an AI-powered system that uses LLMs to generate personalized multisensory study environments. Users select or generate customized visual themes (e.g., abstract vs. realistic, static vs. animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs. novel sounds) to create immersive settings aimed at reducing distraction and enhancing emotional stability. Our primary research question investigates how combinations of personalized audiovisual elements affect learner cognitive load and engagement. Using a mixed-methods design that incorporates biometric measures and performance outcomes, this study evaluates the effectiveness of LLM-driven sensory personalization. The findings aim to advance emotionally responsive educational technologies and extend the application of multimodal LLMs into the sensory dimension of self-directed learning.",CS,http://arxiv.org/abs/2505.03033v1
A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts,"Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.   While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.",CS,http://arxiv.org/abs/2505.03025v1
The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI,"Multimodal learning, which integrates diverse data sources such as images, text, and structured data, has proven superior to unimodal counterparts in high-stakes decision-making. However, while performance gains remain the gold standard for evaluating multimodal systems, concerns around bias and robustness are frequently overlooked. In this context, this paper explores two key research questions (RQs): (i) RQ1 examines whether adding a modality con-sistently enhances performance and investigates its role in shaping fairness measures, assessing whether it mitigates or amplifies bias in multimodal models; (ii) RQ2 investigates the impact of missing modalities at inference time, analyzing how multimodal models generalize in terms of both performance and fairness. Our analysis reveals that incorporating new modalities during training consistently enhances the performance of multimodal models, while fairness trends exhibit variability across different evaluation measures and datasets. Additionally, the absence of modalities at inference degrades performance and fairness, raising concerns about its robustness in real-world deployment. We conduct extensive experiments using multimodal healthcare datasets containing images, time series, and structured information to validate our findings.",CS,http://arxiv.org/abs/2505.03020v1
Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis,"While Large Language Models (LLMs) achieve remarkable performance through training on massive datasets, they can exhibit concerning behaviors such as verbatim reproduction of training data rather than true generalization. This memorization phenomenon raises significant concerns about data privacy, intellectual property rights, and the reliability of model evaluations. This paper introduces PEARL, a novel approach for detecting memorization in LLMs. PEARL assesses how sensitive an LLM's performance is to input perturbations, enabling memorization detection without requiring access to the model's internals. We investigate how input perturbations affect the consistency of outputs, enabling us to distinguish between true generalization and memorization. Our findings, following extensive experiments on the Pythia open model, provide a robust framework for identifying when the model simply regurgitates learned information. Applied on the GPT 4o models, the PEARL framework not only identified cases of memorization of classic texts from the Bible or common code from HumanEval but also demonstrated that it can provide supporting evidence that some data, such as from the New York Times news articles, were likely part of the training data of a given model.",CS,http://arxiv.org/abs/2505.03019v1
Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer,"Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. It acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. While CESM offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. To address these limitations, we propose Seg-CycleGAN, a generative deep learning framework for Virtual Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. Building upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. Experiments on the CESM@UCBM dataset demonstrate that Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while maintaining competitive MSE and VIF. Qualitative evaluations further confirm improved lesion fidelity in the generated images. These results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free CESM alternatives.",CS,http://arxiv.org/abs/2505.03018v1
RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale,"We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper",CS,http://arxiv.org/abs/2505.03005v1
Generating Narrated Lecture Videos from Slides with Synchronized Highlights,"Turning static slides into engaging video lectures takes considerable time and effort, requiring presenters to record explanations and visually guide their audience through the material. We introduce an end-to-end system designed to automate this process entirely. Given a slide deck, this system synthesizes a video lecture featuring AI-generated narration synchronized precisely with dynamic visual highlights. These highlights automatically draw attention to the specific concept being discussed, much like an effective presenter would. The core technical contribution is a novel highlight alignment module. This module accurately maps spoken phrases to locations on a given slide using diverse strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at selectable granularities (line or word level) and utilizes timestamp-providing Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's effectiveness through a technical evaluation using a manually annotated slide dataset with 1000 samples, finding that LLM-based alignment achieves high location accuracy (F1 > 92%), significantly outperforming simpler methods, especially on complex, math-heavy content. Furthermore, the calculated generation cost averages under $1 per hour of video, offering potential savings of two orders of magnitude compared to conservative estimates of manual production costs. This combination of high accuracy and extremely low cost positions this approach as a practical and scalable tool for transforming static slides into effective, visually-guided video lectures.",CS,http://arxiv.org/abs/2505.02966v1
Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach,"Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.",CS,http://arxiv.org/abs/2505.02952v1
The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence,"A key challenge in multi-agent AI is modeling social cooperation under realistic behavioral constraints. Many foundational concepts in economics and ethics such as ""trust"" or ""morality"" are often defined informally, without operational criteria or cognitive grounding, which limits their testability and implementation in artificial agents. Drawing on converging empirical evidence from primate behavior, infant cognition, and economic anthropology, we propose a conceptual framework composed of three cognitively minimal mechanisms: individual recognition, reciprocal credence, and cost return sensitivity. This framework reframes trust as a graded cognitive expectation, providing a simulateable basis for reciprocal exchange in artificial agents, and enabling the bottom-up emergence of scalable cooperation and institutional dynamics.",CS,http://arxiv.org/abs/2505.02945v1
The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models,"Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.   We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.   Our results show that by using only a fraction (<1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.",CS,http://arxiv.org/abs/2505.02931v1
Early Prediction of Sepsis: Feature-Aligned Transfer Learning,"Sepsis is a life threatening medical condition that occurs when the body has an extreme response to infection, leading to widespread inflammation, organ failure, and potentially death. Because sepsis can worsen rapidly, early detection is critical to saving lives. However, current diagnostic methods often identify sepsis only after significant damage has already occurred. Our project aims to address this challenge by developing a machine learning based system to predict sepsis in its early stages, giving healthcare providers more time to intervene.   A major problem with existing models is the wide variability in the patient information or features they use, such as heart rate, temperature, and lab results. This inconsistency makes models difficult to compare and limits their ability to work across different hospitals and settings. To solve this, we propose a method called Feature Aligned Transfer Learning (FATL), which identifies and focuses on the most important and commonly reported features across multiple studies, ensuring the model remains consistent and clinically relevant.   Most existing models are trained on narrow patient groups, leading to population bias. FATL addresses this by combining knowledge from models trained on diverse populations, using a weighted approach that reflects each models contribution. This makes the system more generalizable and effective across different patient demographics and clinical environments. FATL offers a practical and scalable solution for early sepsis detection, particularly in hospitals with limited resources, and has the potential to improve patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery.",CS,http://arxiv.org/abs/2505.02889v1
LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery,"Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/",CS,http://arxiv.org/abs/2505.02829v1
Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review,"Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.",CS,http://arxiv.org/abs/2505.02828v1
Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models,"Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.",CS,http://arxiv.org/abs/2505.02824v1
AutoLibra: Agent Metric Induction from Open-Ended Feedback,"Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., ""If you find that the button is disabled, don't click it again"", or ""This agent has too much autonomy to decide what to do on its own"", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: ""coverage"" and ""redundancy"". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.",CS,http://arxiv.org/abs/2505.02820v1
Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing,"Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",CS,http://arxiv.org/abs/2505.02811v1
Multi-Agent System for Comprehensive Soccer Understanding,"Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.",CS,http://arxiv.org/abs/2505.03735v1
Visual Imitation Enables Contextual Humanoid Control,"How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.",CS,http://arxiv.org/abs/2505.03729v1
DISARM++: Beyond scanner-free harmonization,"Harmonization of T1-weighted MR images across different scanners is crucial for ensuring consistency in neuroimaging studies. This study introduces a novel approach to direct image harmonization, moving beyond feature standardization to ensure that extracted features remain inherently reliable for downstream analysis. Our method enables image transfer in two ways: (1) mapping images to a scanner-free space for uniform appearance across all scanners, and (2) transforming images into the domain of a specific scanner used in model training, embedding its unique characteristics. Our approach presents strong generalization capability, even for unseen scanners not included in the training phase. We validated our method using MR images from diverse cohorts, including healthy controls, traveling subjects, and individuals with Alzheimer's disease (AD). The model's effectiveness is tested in multiple applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis prediction (AUC = 0.95). In all cases, our harmonization technique outperforms state-of-the-art methods, showing improvements in both reliability and predictive accuracy. Moreover, our approach eliminates the need for extensive preprocessing steps, such as skull-stripping, which can introduce errors by misclassifying brain and non-brain structures. This makes our method particularly suitable for applications that require full-head analysis, including research on head trauma and cranial deformities. Additionally, our harmonization model does not require retraining for new datasets, allowing smooth integration into various neuroimaging workflows. By ensuring scanner-invariant image quality, our approach provides a robust and efficient solution for improving neuroimaging studies across diverse settings. The code is available at this link.",CS,http://arxiv.org/abs/2505.03715v1
Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning,"Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.",CS,http://arxiv.org/abs/2505.03703v1
Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach,"Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.",CS,http://arxiv.org/abs/2505.03702v1
Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration,"Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.",CS,http://arxiv.org/abs/2505.03692v1
CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting,"Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.",CS,http://arxiv.org/abs/2505.03679v1
Distribution-Conditional Generation: From Class Distribution to Creative Generation,"Text-to-image (T2I) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. Existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. Inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose Distribution-Conditional Generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. Building on this, we propose DisTok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. DisTok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. To enforce distributional consistency, latent vectors sampled from a Gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. The resulting tokens are added to the concept pool for subsequent composition. Extensive experiments demonstrate that DisTok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores.",CS,http://arxiv.org/abs/2505.03667v1
Towards Smart Point-and-Shoot Photography,"Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets.",CS,http://arxiv.org/abs/2505.03638v1
Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision,"Video quality assessment (VQA) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. While recent supervised VQA models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. To bridge this gap, we introduce a self-supervised learning framework for VQA to learn quality assessment capabilities from large-scale, unlabeled web videos. Our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (LMM) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing VQA models and relative quality ranking based on synthetic distortion simulations. Furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. By training on a dataset $10\times$ larger than the existing VQA benchmarks, our model: (1) achieves zero-shot performance on in-domain VQA benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (OOD) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. Extensive experimental results validate the effectiveness of our self-supervised approach in training generalized VQA models. The datasets and code will be publicly released to facilitate future research.",CS,http://arxiv.org/abs/2505.03631v1
Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map,"Synthetic dataset generation in Computer Vision, particularly for industrial applications, is still underexplored. Industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. To address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. Our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. Compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. We introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. Our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. The code is publicly available at https://github.com/covisionlab/diffusion_labeling.",CS,http://arxiv.org/abs/2505.03623v1
PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing,"Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.",CS,http://arxiv.org/abs/2505.03621v1
Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images,"Face anti-spoofing is a critical technology for ensuring the security of face recognition systems. However, its ability to generalize across diverse scenarios remains a significant challenge. In this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. To address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. Our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model's ability to generalize to unseen target domains. Specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. This framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. Moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. Experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images.",CS,http://arxiv.org/abs/2505.03611v1
Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection,"3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.",CS,http://arxiv.org/abs/2505.03610v1
PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model,"Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance.",CS,http://arxiv.org/abs/2505.03603v1
From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction,"Deep learning-based medical image-to-mesh reconstruction has rapidly evolved, enabling the transformation of medical imaging data into three-dimensional mesh models that are critical in computational medicine and in silico trials for advancing our understanding of disease mechanisms, and diagnostic and therapeutic techniques in modern medicine. This survey systematically categorizes existing approaches into four main categories: template models, statistical models, generative models, and implicit models. Each category is analysed in detail, examining their methodological foundations, strengths, limitations, and applicability to different anatomical structures and imaging modalities. We provide an extensive evaluation of these methods across various anatomical applications, from cardiac imaging to neurological studies, supported by quantitative comparisons using standard metrics. Additionally, we compile and analyze major public datasets available for medical mesh reconstruction tasks and discuss commonly used evaluation metrics and loss functions. The survey identifies current challenges in the field, including requirements for topological correctness, geometric accuracy, and multi-modality integration. Finally, we present promising future research directions in this domain. This systematic review aims to serve as a comprehensive reference for researchers and practitioners in medical image analysis and computational medicine.",CS,http://arxiv.org/abs/2505.03599v1
Fixed-Length Dense Fingerprint Representation,"Fixed-length fingerprint representations, which map each fingerprint to a compact and fixed-size feature vector, are computationally efficient and well-suited for large-scale matching. However, designing a robust representation that effectively handles diverse fingerprint modalities, pose variations, and noise interference remains a significant challenge. In this work, we propose a fixed-length dense descriptor of fingerprints, and introduce FLARE-a fingerprint matching framework that integrates the Fixed-Length dense descriptor with pose-based Alignment and Robust Enhancement. This fixed-length representation employs a three-dimensional dense descriptor to effectively capture spatial relationships among fingerprint ridge structures, enabling robust and locally discriminative representations. To ensure consistency within this dense feature space, FLARE incorporates pose-based alignment using complementary estimation methods, along with dual enhancement strategies that refine ridge clarity while preserving the original fingerprint modality. The proposed dense descriptor supports fixed-length representation while maintaining spatial correspondence, enabling fast and accurate similarity computation. Extensive experiments demonstrate that FLARE achieves superior performance across rolled, plain, latent, and contactless fingerprints, significantly outperforming existing methods in cross-modality and low-quality scenarios. Further analysis validates the effectiveness of the dense descriptor design, as well as the impact of alignment and enhancement modules on the accuracy of dense descriptor matching. Experimental results highlight the effectiveness and generalizability of FLARE as a unified and scalable solution for robust fingerprint representation and matching. The implementation and code will be publicly available at https://github.com/Yu-Yy/FLARE.",CS,http://arxiv.org/abs/2505.03597v1
DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes,"The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com/linukc/DyGEnc.",CS,http://arxiv.org/abs/2505.03581v1
Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning,"Recycling textile fibers is critical to reducing the environmental impact of the textile industry. Hyperspectral near-infrared (NIR) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. In this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. We show that optimized convolutional neural networks (CNNs) and autoencoder networks achieve robust generalization under varying conditions. These results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.",CS,http://arxiv.org/abs/2505.03575v1
Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models,"Backgrounds in images play a major role in contributing to spurious correlations among different data points. Owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. In this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. To better illustrate our findings, we propose a synthetic dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images with various backgrounds, object positions, and object sizes. By evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (ROI) to image ratio is small and the object is far from the center of the image. Moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change.",CS,http://arxiv.org/abs/2505.03569v1
Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images,"Text-based pedestrian search (TBPS) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. However, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. To address this, we propose UPD-TBPS, a novel framework comprising three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. PUD leverages visual context decoupling and prototype mining to extract features of the target pedestrian described in the query. It separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. ReID evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the effectiveness of our framework.",CS,http://arxiv.org/abs/2505.03567v1
Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment,"The Equine Facial Action Coding System (EquiFACS) enables the systematic annotation of facial movements through distinct Action Units (AUs). It serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. However, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial AUs is both time-consuming and costly. To address this challenge, automated annotation systems are essential for leveraging existing datasets and improving affective states detection tools. In this work, we study different methods for specific ear AU detection and localization from horse videos. We leverage past works on deep learning-based video feature extraction combined with recurrent neural networks for the video classification task, as well as a classic optical flow based approach. We achieve 87.5% classification accuracy of ear movement presence on a public horse video dataset, demonstrating the potential of our approach. We discuss future directions to develop these systems, with the aim of bridging the gap between automated AU detection and practical applications in equine welfare and veterinary diagnostics. Our code will be made publicly available at https://github.com/jmalves5/read-my-ears.",CS,http://arxiv.org/abs/2505.03554v1
Panoramic Out-of-Distribution Segmentation,"Panoramic imaging enables capturing 360{\deg} images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.",CS,http://arxiv.org/abs/2505.03539v1
RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT,"Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.",CS,http://arxiv.org/abs/2505.03538v1
Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication,"Cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. Existing works have explored the effects of V2V communication impairments on perception precision, but they lack generalization to different levels of impairments. In this work, we propose a joint weighting and denoising framework, Coop-WD, to enhance cooperative perception subject to V2V channel impairments. In this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is proposed to selectively deactivate denoising to reduce processing overhead. Rician fading, non-stationarity, and time-varying distortion are considered. Simulation results demonstrate that the proposed Coop-WD outperforms conventional benchmarks in all types of channels. Qualitative analysis with visual examples further proves the superiority of our proposed method. The proposed Coop-WD-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.",CS,http://arxiv.org/abs/2505.03528v1
Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking,"To reduce the reliance on large-scale annotations, self-supervised RGB-T tracking approaches have garnered significant attention. However, the omission of the object region by erroneous pseudo-label or the introduction of background noise affects the efficiency of modality fusion, while pseudo-label noise triggered by similar object noise can further affect the tracking performance. In this paper, we propose GDSTrack, a novel approach that introduces dynamic graph fusion and temporal diffusion to address the above challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the modalities of neighboring frames, treats them as distractor noise, and leverages the denoising capability of a generative model. Specifically, by constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic adjacency matrix to guide graph attention, focusing on and fusing the object's coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features from neighboring frames as interference, and thus improving robustness against similar-object noise. Extensive experiments conducted on four public RGB-T tracking datasets demonstrate that GDSTrack outperforms the existing state-of-the-art methods. The source code is available at https://github.com/LiShenglana/GDSTrack.",CS,http://arxiv.org/abs/2505.03507v1
MRI motion correction via efficient residual-guided denoising diffusion probabilistic models,"Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly degrade image quality and impair quantitative analysis. Conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model tailored for MRI motion artifact correction. Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. A U-net backbone enhanced with Swin-Transformer blocks conventional attention layers, improving adaptability across resolutions. Training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and normalized mean squared error (NMSE). Results: The proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.",CS,http://arxiv.org/abs/2505.03498v1
UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion,"Background: Brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. Accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. Objective: We propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. Methods: The proposed method utilizes a multi-scale feature fusion (MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale features and capture global contextual information. To enhance the model's robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout) strategy is employed for uncertainty estimation. Results: Extensive experiments demonstrate that the proposed method achieves superior performance on Brain Tumor Segmentation (BraTS) datasets, significantly outperforming various state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are 89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT) segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019 validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for ET, WT, and TC segmentation, respectively. Ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. Conclusion: This study proposed a novel 3D brain tumor segmentation network based on the U-Net architecture. By incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. The code for the proposed method is available at https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.",CS,http://arxiv.org/abs/2505.03494v1
Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD),"Dynamic computed tomography (CT) reconstruction faces significant challenges in addressing motion artifacts, particularly for nonperiodic rapid movements such as cardiac imaging with fast heart rates. Traditional methods struggle with the extreme limited-angle problems inherent in nonperiodic cases. Deep learning methods have improved performance but face generalization challenges. Recent implicit neural representation (INR) techniques show promise through self-supervised deep learning, but have critical limitations: computational inefficiency due to forward-warping modeling, difficulty balancing DVF complexity with anatomical plausibility, and challenges in preserving fine details without additional patient-specific pre-scans. This paper presents a novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It addresses these challenges through four key contributions: (1) backward-warping deformation that enables direct computation of each dynamic voxel with significantly reduced computational cost, (2) diffeomorphism-based DVF regularization that ensures anatomically plausible deformations while maintaining representational capacity, (3) motion-compensated analytical reconstruction that enhances fine details without requiring additional pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate encoding. Through various simulations and practical studies, including digital and physical phantoms and retrospective patient data, we demonstrate the effectiveness of our approach for nonperiodic dynamic CT reconstruction with enhanced details and reduced motion artifacts. The proposed framework enables more accurate dynamic CT reconstruction with potential clinical applications, such as one-beat cardiac reconstruction, cinematic image sequences for functional imaging, and motion artifact reduction in conventional CT scans.",CS,http://arxiv.org/abs/2505.03463v1
Polar Coordinate-Based 2D Pose Prior with Neural Distance Field,"Human pose capture is essential for sports analysis, enabling precise evaluation of athletes' movements. While deep learning-based human pose estimation (HPE) models from RGB videos have achieved impressive performance on public datasets, their effectiveness in real-world sports scenarios is often hindered by motion blur, occlusions, and domain shifts across different pose representations. Fine-tuning these models can partially alleviate such challenges but typically requires large-scale annotated data and still struggles to generalize across diverse sports environments. To address these limitations, we propose a 2D pose prior-guided refinement approach based on Neural Distance Fields (NDF). Unlike existing approaches that rely solely on angular representations of human poses, we introduce a polar coordinate-based representation that explicitly incorporates joint connection lengths, enabling a more accurate correction of erroneous pose estimations. Additionally, we define a novel non-geodesic distance metric that separates angular and radial discrepancies, which we demonstrate is better suited for polar representations than traditional geodesic distances. To mitigate data scarcity, we develop a gradient-based batch-projection augmentation strategy, which synthesizes realistic pose samples through iterative refinement. Our method is evaluated on a long jump dataset, demonstrating its ability to improve 2D pose estimation across multiple pose representations, making it robust across different domains. Experimental results show that our approach enhances pose plausibility while requiring only limited training data. Code is available at: https://github.com/QGAN2019/polar-NDF.",CS,http://arxiv.org/abs/2505.03445v1
Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks,"The rapid advancement of generative image technology has introduced significant security concerns, particularly in the domain of face generation detection. This paper investigates the vulnerabilities of current AI-generated face detection systems. Our study reveals that while existing detection methods often achieve high accuracy under standard conditions, they exhibit limited robustness against adversarial attacks. To address these challenges, we propose an approach that integrates adversarial training to mitigate the impact of adversarial examples. Furthermore, we utilize diffusion inversion and reconstruction to further enhance detection robustness. Experimental results demonstrate that minor adversarial perturbations can easily bypass existing detection systems, but our method significantly improves the robustness of these systems. Additionally, we provide an in-depth analysis of adversarial and benign examples, offering insights into the intrinsic characteristics of AI-generated content. All associated code will be made publicly available in a dedicated repository to facilitate further research and verification.",CS,http://arxiv.org/abs/2505.03435v1
A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution,"The fusion of low-spatial-resolution hyperspectral images (HSIs) with high-spatial-resolution conventional images (e.g., panchromatic or RGB) has played a significant role in recent advancements in HSI super-resolution. However, this fusion process relies on the availability of precise alignment between image pairs, which is often challenging in real-world scenarios. To mitigate this limitation, we propose a single-image super-resolution model called the Fusion-Guided Inception Network (FGIN). Specifically, we first employ a spectral-spatial fusion module to effectively integrate spectral and spatial information at an early stage. Next, an Inception-like hierarchical feature extraction strategy is used to capture multiscale spatial dependencies, followed by a dedicated multi-scale fusion block. To further enhance reconstruction quality, we incorporate an optimized upsampling module that combines bilinear interpolation with depthwise separable convolutions. Experimental evaluations on two publicly available hyperspectral datasets demonstrate the competitive performance of our method.",CS,http://arxiv.org/abs/2505.03431v1
LiftFeat: 3D Geometry-Aware Local Feature Matching,"Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called \textit{LiftFeat}, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.",CS,http://arxiv.org/abs/2505.03422v1
Mitigating Image Captioning Hallucinations in Vision-Language Models,"Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness.",CS,http://arxiv.org/abs/2505.03420v1
Enhancing Target-unspecific Tasks through a Features Matrix,"Recent developments in prompt learning of large vision-language models have significantly improved performance in target-specific tasks. However, these prompt optimizing methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge having strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) regularization approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks, achieving state-of-the-art performance.",CS,http://arxiv.org/abs/2505.03414v1
CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection,"Internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. However, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available X-ray datasets targeting internal defects in components. To address this gap, we construct the first publicly accessible component X-ray anomaly detection (CXR-AD) dataset, comprising real-world X-ray images. The dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. We systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in X-ray imaging, and (3) significant variations in defect scales and morphologies. To evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). Experimental results demonstrate a 29.78% average performance degradation on CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms in handling internal defect detection tasks. To the best of our knowledge, CXR-AD represents the first publicly available X-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.",CS,http://arxiv.org/abs/2505.03412v1
EOPose : Exemplar-based object reposing using Generalized Pose Correspondences,"Reposing objects in images has a myriad of applications, especially for e-commerce where several variants of product images need to be produced quickly. In this work, we leverage the recent advances in unsupervised keypoint correspondence detection between different object images of the same class to propose an end-to-end framework for generic object reposing. Our method, EOPose, takes a target pose-guidance image as input and uses its keypoint correspondence with the source object image to warp and re-render the latter into the target pose using a novel three-step approach. Unlike generative approaches, our method also preserves the fine-grained details of the object such as its exact colors, textures, and brand marks. We also prepare a new dataset of paired objects based on the Objaverse dataset to train and test our network. EOPose produces high-quality reposing output as evidenced by different image quality metrics (PSNR, SSIM and FID). Besides a description of the method and the dataset, the paper also includes detailed ablation and user studies to indicate the efficacy of the proposed method",CS,http://arxiv.org/abs/2505.03394v1
Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples,"Adversarial examples have revealed the vulnerability of deep learning models and raised serious concerns about information security. The transfer-based attack is a hot topic in black-box attacks that are practical to real-world scenarios where the training datasets, parameters, and structure of the target model are unknown to the attacker. However, few methods consider the particularity of class-specific deep models for fine-grained vision tasks, such as face recognition (FR), giving rise to unsatisfactory attacking performance. In this work, we first investigate what in a face exactly contributes to the embedding learning of FR models and find that both decisive and auxiliary facial features are specific to each FR model, which is quite different from the biological mechanism of human visual system. Accordingly we then propose a novel attack method named Attention-aggregated Attack (AAA) to enhance the transferability of adversarial examples against FR, which is inspired by the attention divergence and aims to destroy the facial features that are critical for the decision-making of other FR models by imitating their attentions on the clean face images. Extensive experiments conducted on various FR models validate the superiority and robust effectiveness of the proposed method over existing methods.",CS,http://arxiv.org/abs/2505.03383v1
Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models,"Introduction: Data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. One common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. Methods: We compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in Oxfordshire, United Kingdom and Sichuan, China, respectively, using the Autographer (OMG Life, defunct) wearable camera. Results: We found that the best open-source vision-language model (VLM) and fine-tuned discriminative model (DM) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86, 0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53, 0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study, performance fell across all intensity categories, with median Cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden.",CS,http://arxiv.org/abs/2505.03374v1
3D Surface Reconstruction with Enhanced High-Frequency Details,"Neural implicit 3D reconstruction can reproduce shapes without 3D supervision, and it learns the 3D scene through volume rendering methods and neural implicit representations. Current neural surface reconstruction methods tend to randomly sample the entire image, making it difficult to learn high-frequency details on the surface, and thus the reconstruction results tend to be too smooth. We designed a method (FreNeuS) based on high-frequency information to solve the problem of insufficient surface detail. Specifically, FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in an image and uses the obtained high-frequency information to guide surface detail reconstruction. High-frequency information is first used to guide the dynamic sampling of rays, applying different sampling strategies according to variations in high-frequency regions. To further enhance the focus on surface details, we have designed a high-frequency weighting method that constrains the representation of high-frequency details during the reconstruction process. Qualitative and quantitative results show that our method can reconstruct fine surface details and obtain better surface reconstruction quality compared to existing methods. In addition, our method is more applicable and can be generalized to any NeuS-based work.",CS,http://arxiv.org/abs/2505.03362v1
Interpretable Zero-shot Learning with Infinite Class Concepts,"Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images with intermediate class semantics, like human-annotated concepts or class definitions. An emerging alternative leverages Large-scale Language Models (LLMs) to automatically generate class documents. However, these methods often face challenges with transparency in the classification process and may suffer from the notorious hallucination problem in LLMs, resulting in non-visual class semantics. This paper redefines class semantics in ZSL with a focus on transferability and discriminability, introducing a novel framework called Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach leverages the powerful capabilities of LLMs to dynamically generate an unlimited array of phrase-level class concepts. To address the hallucination challenge, we introduce an entropy-based scoring process that incorporates a ``goodness"" concept selection mechanism, ensuring that only the most transferable and discriminative concepts are selected. Our InfZSL framework not only demonstrates significant improvements on three popular benchmark datasets but also generates highly interpretable, image-grounded concepts. Code will be released upon acceptance.",CS,http://arxiv.org/abs/2505.03361v1
GUAVA: Generalizable Upper Body 3D Gaussian Avatar,"Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.",CS,http://arxiv.org/abs/2505.03351v1
A Vision-Language Model for Focal Liver Lesion Classification,"Accurate classification of focal liver lesions is crucial for diagnosis and treatment in hepatology. However, traditional supervised deep learning models depend on large-scale annotated datasets, which are often limited in medical imaging. Recently, Vision-Language models (VLMs) such as Contrastive Language-Image Pre-training model (CLIP) has been applied to image classifications. Compared to the conventional convolutional neural network (CNN), which classifiers image based on visual information only, VLM leverages multimodal learning with text and images, allowing it to learn effectively even with a limited amount of labeled data. Inspired by CLIP, we pro-pose a Liver-VLM, a model specifically designed for focal liver lesions (FLLs) classification. First, Liver-VLM incorporates class information into the text encoder without introducing additional inference overhead. Second, by calculating the pairwise cosine similarities between image and text embeddings and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively aligns image features with class-level text features. Experimental results on MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve (AUC). Further analysis shows that using a lightweight ResNet18 backbone enhances classification performance, particularly under data-constrained conditions.",CS,http://arxiv.org/abs/2505.03350v1
From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection,"In recent years, language-guided open-world aerial object detection has gained significant attention due to its better alignment with real-world application needs. However, due to limited datasets, most existing language-guided methods primarily focus on vocabulary, which fails to meet the demands of more fine-grained open-world detection. To address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. Centered around an open-source large vision-language model and integrating image-operation-based preprocessing with BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. Using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of current remote sensing grounding data and enabling effective open-set aerial detection. Specifically, MI-OAD contains 163,023 images and 2 million image-caption pairs, approximately 40 times larger than comparable datasets. We also employ state-of-the-art open-set methods from the natural image domain, trained on our proposed dataset, to validate the model's open-set detection capabilities. For instance, when trained on our dataset, Grounding DINO achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs under zero-shot transfer conditions. Both the dataset and the label engine will be released publicly.",CS,http://arxiv.org/abs/2505.03334v1
FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing,"The task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. Recent works based on latent diffusion models (LDM) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-Latin ones (\eg, Chinese), which have complex glyph structures. To address these issues, we present FLUX-Text, a simple and advanced multilingual scene text editing framework based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. To retain the original generative capabilities of FLUX-Fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. Owning to the lightweight design, FLUX-Text is trained only with $100K$ training examples compared to current popular methods trained with 2.9M ones. With no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. Qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity.",CS,http://arxiv.org/abs/2505.03329v1
Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning,"Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.",CS,http://arxiv.org/abs/2505.03318v1
3D Gaussian Splatting Data Compression with Mixture of Priors,"3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.",CS,http://arxiv.org/abs/2505.03310v1
3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation,"Semantic segmentation of 3D LiDAR point clouds, essential for autonomous driving and infrastructure management, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. We introduce a new 3D semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need for direct 3D annotation or reliance on additional modalities such as camera images at inference time. Our approach generates 2D views from LiDAR scans colored by sensor intensity and applies 2D semantic segmentation to these views using a camera-domain pretrained model. The segmented 2D outputs are then back-projected onto the 3D points, with a simple voting-based estimator that merges the labels associated to each 3D point. Our main contribution is a global pipeline for 3D semantic segmentation requiring no prior 3D annotation and not other modality for inference, which can be used for pseudo-label generation. We conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the Unsupervised Domain Adaptation task.",CS,http://arxiv.org/abs/2505.03300v1
Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification,"Visible-infrared person re-identification (VIReID) provides a solution for ReID tasks in 24-hour scenarios; however, significant challenges persist in achieving satisfactory performance due to the substantial discrepancies between visible (VIS) and infrared (IR) modalities. Existing methods inadequately leverage information from different modalities, primarily focusing on digging distinguishing features from modality-shared information while neglecting modality-specific details. To fully utilize differentiated minutiae, we propose a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information. Specifically, the proposed BDLF mines detail and base features through a lossless detail feature extraction module and a complementary base embedding generation mechanism, respectively, supported by a novel correlation restriction method that ensures the features gained by BDLF enrich both detail and base knowledge across VIS and IR features. Comprehensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the effectiveness of BDLF.",CS,http://arxiv.org/abs/2505.03286v1
OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction,"The safe operation of autonomous vehicles (AVs) is highly dependent on their understanding of the surroundings. For this, the task of 3D semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. Recent perception models have used multisensor fusion to perform this task. However, existing multisensor fusion-based approaches focus mainly on using sensor information in the Cartesian coordinate system. This ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. In this paper, we propose OccCylindrical that merges and refines the different modality features under cylindrical coordinates. Our method preserves more fine-grained geometry detail that leads to better performance. Extensive experiments conducted on the nuScenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. The code will be available at: https://github.com/DanielMing123/OccCylindrical",CS,http://arxiv.org/abs/2505.03284v1
DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor,"Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.",CS,http://arxiv.org/abs/2505.03261v1
PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs,"Convolutional neural networks (CNNs) are crucial for computer vision tasks on resource-constrained devices. Quantization effectively compresses these models, reducing storage size and energy cost. However, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. By applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. To this end, we introduce PROM, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. Specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. Additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on ImageNet. Our method advances the Pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on ImageNet. PROM addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.",CS,http://arxiv.org/abs/2505.03254v1
Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data,"Hyperspectral images (HSIs) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. However, the scarcity of labeled HSI data limits the full potential of deep learning, especially for transformer-based architectures that require large-scale training. To address this constraint, we propose Spatial-Frequency Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for hyperspectral data that utilizes the large portion of unlabeled data. Our method introduces a novel dual-domain masking mechanism that operates in both spatial and frequency domains. The input HSI cube is initially divided into non-overlapping patches along the spatial dimension, with each patch comprising the entire spectrum of its corresponding spatial location. In spatial masking, we randomly mask selected patches and train the model to reconstruct the masked inputs using the visible patches. Concurrently, in frequency masking, we remove portions of the frequency components of the input spectra and predict the missing frequencies. By learning to reconstruct these masked components, the transformer-based encoder captures higher-order spectral-spatial correlations. We evaluate our approach on three publicly available HSI classification benchmarks and demonstrate that it achieves state-of-the-art performance. Notably, our model shows rapid convergence during fine-tuning, highlighting the efficiency of our pretraining strategy.",CS,http://arxiv.org/abs/2505.03220v1
PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models,"Advanced diffusion models have made notable progress in text-to-image compositional generation. However, it is still a challenge for existing models to achieve text-image alignment when confronted with complex text prompts. In this work, we highlight two factors that affect this alignment: the quality of the randomly initialized noise and the reliability of the generated controlling mask. We then propose PiCo (Pick-and-Control), a novel training-free approach with two key components to tackle these two factors. First, we develop a noise selection module to assess the quality of the random noise and determine whether the noise is suitable for the target text. A fast sampling strategy is utilized to ensure efficiency in the noise selection stage. Second, we introduce a referring mask module to generate pixel-level masks and to precisely modulate the cross-attention maps. The referring mask is applied to the standard diffusion process to guide the reasonable interaction between text and image features. Extensive experiments have been conducted to verify the effectiveness of PiCo in liberating users from the tedious process of random generation and in enhancing the text-image alignment for diverse text descriptions.",CS,http://arxiv.org/abs/2505.03203v1
CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization,"The inherent synchronization between a speaker's lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 22.0 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.",CS,http://arxiv.org/abs/2505.03186v1
Interactive Instance Annotation with Siamese Networks,"Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.",CS,http://arxiv.org/abs/2505.03184v1
Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets,"Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.",CS,http://arxiv.org/abs/2505.03174v1
Robust Fairness Vision-Language Learning for Medical Image Analysis,"The advent of Vision-Language Models (VLMs) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. However, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. In this paper, we introduce a framework for ensuring robustness and fairness of VLM models. This framework modifies the loss function at training by identifying and adjusting faulty image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing Sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. Experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled AUC.",CS,http://arxiv.org/abs/2505.03153v1
Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control,"Visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. This paper presents a novel approach using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. The methodology significantly enhances image classification performance of standard CNN architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting anomalies by increasing the minority class representation. Experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. The most dramatic improvement was observed in ResNet50V2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. This work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.",CS,http://arxiv.org/abs/2505.03134v1
STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis,"We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.",CS,http://arxiv.org/abs/2505.03123v1
TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion,"Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.",CS,http://arxiv.org/abs/2505.03116v1
Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation,"Accurate MRI-to-CT translation promises the integration of complementary imaging information without the need for additional imaging sessions. Given the practical challenges associated with acquiring paired MRI and CT scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on CT but less distinguishable on MRI, such as bone structures. This limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. To address this challenge, we propose a path- and bone-contour regularized approach for unpaired MRI-to-CT translation. In our method, MRI and CT images are projected to a shared latent space, where the MRI-to-CT mapping is modeled as a continuous flow governed by neural ordinary differential equations. The optimal mapping is obtained by minimizing the transition path length of the flow. To enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from MRI and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. Evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired MRI-to-CT translation approaches, achieving lower overall error rates. Moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. Our code is available at: https://github.com/kennysyp/PaBoT.",CS,http://arxiv.org/abs/2505.03114v1
Image Recognition with Online Lightweight Vision Transformer: A Survey,"The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: https://github.com/ajxklo/Lightweight-VIT",CS,http://arxiv.org/abs/2505.03113v1
Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability,"The diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. Thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., ResNet or GANs) which captures or generates the image semantic information at different layers. This difference inspires us to explore the time-wise diffusion models. We initially investigate the key contributions of the U-Net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. Capitalizing on this discovery, we propose a simple yet effective method-termed ``MaskUNet''- that enhances generation quality with negligible parameter numbers. Our method fully leverages timestep- and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. In zero-shot inference on the COCO dataset, MaskUNet achieves the best FID score and further demonstrates its effectiveness in downstream task evaluations. Project page: https://gudaochangsheng.github.io/MaskUnet-Page/",CS,http://arxiv.org/abs/2505.03097v1
Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera,"Forest inventories rely on accurate measurements of the diameter at breast height (DBH) for ecological monitoring, resource management, and carbon accounting. While LiDAR-based techniques can achieve centimeter-level precision, they are cost-prohibitive and operationally complex. We present a low-cost alternative that only needs a consumer-grade 360 video camera. Our semi-automated pipeline comprises of (i) a dense point cloud reconstruction using Structure from Motion (SfM) photogrammetry software called Agisoft Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based technique to estimate cross section shape and DBH. We introduce an interactive visualization tool for inspecting segmented trees and their estimated DBH. On 61 acquisitions of 43 trees under a variety of conditions, our method attains median absolute relative errors of 5-9% with respect to ""ground-truth"" manual measurements. This is only 2-4% higher than LiDAR-based estimates, while employing a single 360 camera that costs orders of magnitude less, requires minimal setup, and is widely available.",CS,http://arxiv.org/abs/2505.03093v1
Sim2Real Transfer for Vision-Based Grasp Verification,"The verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. Traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. In this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. Our method employs a two-stage architecture; first YOLO-based object detection model to detect and locate the robot's gripper and then a ResNet-based classifier determines the presence of an object. To address the limitations of real-world data capture, we introduce HSR-GraspSynth, a synthetic dataset designed to simulate diverse grasping scenarios. Furthermore, we explore the use of Visual Question Answering capabilities as a zero-shot baseline to which we compare our model. Experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. Code and datasets are publicly available at https://github.com/pauamargant/HSR-GraspSynth .",CS,http://arxiv.org/abs/2505.03046v1
An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices,"Continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. In this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. Leveraging data from 2,023 participants with defined healthy baselines, our LSTM autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. Anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). The model achieved an adjusted F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 = 0.85). Model interpretability was supported by SHAP-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. Together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings.",CS,http://arxiv.org/abs/2505.03039v1
Dual Prompting for Diverse Count-level PET Denoising,"The to-be-denoised positron emission tomography (PET) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. In this work, we resort to the recently flourished prompt learning to achieve generalizable PET denoising with different count levels. Specifically, we propose dual prompts to guide the PET denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential PET denoising knowledge. Then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. The prompts are able to dynamically guide the noise-conditioned denoising process. Therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies. It shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.",CS,http://arxiv.org/abs/2505.03037v1
GIF: Generative Inspiration for Face Recognition at Scale,"Aiming to reduce the computational cost of Softmax in massive label space of Face Recognition (FR) benchmarks, recent studies estimate the output using a subset of identities. Although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. A shared characteristic among available FR methods is the employment of atomic scalar labels during training. Consequently, the input to label matching is through a dot product between the feature vector of the input and the Softmax centroids. Inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. Specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. Then, we train an FR backbone to predict the code for each input instead of its scalar label. As a result, the associated computational cost becomes logarithmic w.r.t. number of identities. We demonstrate the benefits of the proposed method by conducting experiments. In particular, our method outperforms its competitors by 1.52%, and 0.6% at TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF",CS,http://arxiv.org/abs/2505.03012v1
NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results,"This paper presents an overview of the NTIRE 2025 Challenge on UGC Video Enhancement. The challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. The goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. Given the widespread use of UGC on short-form video platforms, this task holds substantial practical importance. The evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. The challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. The outcomes may provide insights into the state-of-the-art in UGC video enhancement and highlight emerging trends and effective strategies in this evolving research area. All data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.",CS,http://arxiv.org/abs/2505.03007v1
Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking,"Spatial Transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. Among the various Spatial Transcriptomics techniques available, Visium has emerged as the most widely adopted. However, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. Additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. To address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. Yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. To bridge this gap, we introduce SpaRED, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. We further propose SpaCKLE, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE substantially improves the results across all the gene expression prediction models. Altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on Spatial Transcriptomics.",CS,http://arxiv.org/abs/2505.02980v1
Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation,"Adversarial attacks have been fairly explored for computer vision and vision-language models. However, the avenue of adversarial attack for the vision language segmentation models (VLSMs) is still under-explored, especially for medical image analysis.   Thus, we have investigated the robustness of VLSMs against adversarial attacks for 2D medical images with different modalities with radiology, photography, and endoscopy. The main idea of this project was to assess the robustness of the fine-tuned VLSMs specially in the medical domain setting to address the high risk scenario.   First, we have fine-tuned pre-trained VLSMs for medical image segmentation with adapters.   Then, we have employed adversarial attacks -- projected gradient descent (PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to determine its robustness against adversaries.   We have reported models' performance decline to analyze the adversaries' impact.   The results exhibit significant drops in the DSC and IoU scores after the introduction of these adversaries. Furthermore, we also explored universal perturbation but were not able to find for the medical images.   \footnote{https://github.com/anjilab/secure-private-ai}",CS,http://arxiv.org/abs/2505.02971v1
Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images,"Neural compression methods are gaining popularity due to their superior rate-distortion performance over traditional methods, even at extremely low bitrates below 0.1 bpp. As deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. In this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. Using this framework, we investigate racial bias in neural compression algorithms by analyzing nine popular models and their variants. Through this investigation, we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. Next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. We then examine the relationship between bias and realism in the decoded images and demonstrate a trade-off across models. Finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy. We additionally show the bias can be attributed to compression model bias and classification model bias. We believe that this work is a first step towards evaluating and eliminating bias in neural image compression models.",CS,http://arxiv.org/abs/2505.02949v1
Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation,"Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.",CS,http://arxiv.org/abs/2505.02836v1
R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning,"Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",CS,http://arxiv.org/abs/2505.02835v1
WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch,"LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.",CS,http://arxiv.org/abs/2505.03733v1
NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation,"We present our system submission for SemEval 2025 Task 5, which focuses on cross-lingual subject classification in the English and German academic domains. Our approach leverages bilingual data during training, employing negative sampling and a margin-based retrieval objective. We demonstrate that a dimension-as-token self-attention mechanism designed with significantly reduced internal dimensions can effectively encode sentence embeddings for subject retrieval. In quantitative evaluation, our system achieved an average recall rate of 32.24% in the general quantitative setting (all subjects), 43.16% and 31.53% of the general qualitative evaluation methods with minimal GPU usage, highlighting their competitive performance. Our results demonstrate that our approach is effective in capturing relevant subject information under resource constraints, although there is still room for improvement.",CS,http://arxiv.org/abs/2505.03711v1
IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages,"The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp",CS,http://arxiv.org/abs/2505.03688v1
Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure,"We explore the potential of ChatGPT (3.5-turbo and 4) to generate conversations focused on self-care strategies for African-American heart failure patients -- a domain with limited specialized datasets. To simulate patient-health educator dialogues, we employed four prompting strategies: domain, African American Vernacular English (AAVE), Social Determinants of Health (SDOH), and SDOH-informed reasoning. Conversations were generated across key self-care domains of food, exercise, and fluid intake, with varying turn lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as age, gender, neighborhood, and socioeconomic status. Our findings show that effective prompt design is essential. While incorporating SDOH and reasoning improves dialogue quality, ChatGPT still lacks the empathy and engagement needed for meaningful healthcare communication.",CS,http://arxiv.org/abs/2505.03675v1
Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval,"Current sparse neural information retrieval (IR) methods, and to a lesser extent more traditional models such as BM25, do not take into account the document collection and the complex interplay between different term weights when representing a single document. In this paper, we show how the Rational Speech Acts (RSA), a linguistics framework used to minimize the number of features to be communicated when identifying an object in a set, can be adapted to the IR case -- and in particular to the high number of potential features (here, tokens). RSA dynamically modulates token-document interactions by considering the influence of other documents in the dataset, better contrasting document representations. Experiments show that incorporating RSA consistently improves multiple sparse retrieval models and achieves state-of-the-art performance on out-of-domain datasets from the BEIR benchmark. https://github.com/arthur-75/Rational-Retrieval-Acts",CS,http://arxiv.org/abs/2505.03676v1
Say It Another Way: A Framework for User-Grounded Paraphrasing,"Small changes in how a prompt is worded can lead to meaningful differences in the behavior of large language models (LLMs), raising concerns about the stability and reliability of their evaluations. While prior work has explored simple formatting changes, these rarely capture the kinds of natural variation seen in real-world language use. We propose a controlled paraphrasing framework based on a taxonomy of minimal linguistic transformations to systematically generate natural prompt variations. Using the BBQ dataset, we validate our method with both human annotations and automated checks, then use it to study how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our analysis shows that even subtle prompt modifications can lead to substantial changes in model behavior. These results highlight the need for robust, paraphrase-aware evaluation protocols.",CS,http://arxiv.org/abs/2505.03563v1
Faster MoE LLM Inference for Extremely Large Models,"Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.",CS,http://arxiv.org/abs/2505.03531v1
BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models,"In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness",CS,http://arxiv.org/abs/2505.03501v1
Sentence Embeddings as an intermediate target in end-to-end summarisation,"Current neural network-based methods to the problem of document summarisation struggle when applied to datasets containing large inputs. In this paper we propose a new approach to the challenge of content-selection when dealing with end-to-end summarisation of user reviews of accommodations. We show that by combining an extractive approach with externally pre-trained sentence level embeddings in an addition to an abstractive summarisation model we can outperform existing methods when this is applied to the task of summarising a large input dataset. We also prove that predicting sentence level embedding of a summary increases the quality of an end-to-end system for loosely aligned source to target corpora, than compared to commonly predicting probability distributions of sentence selection.",CS,http://arxiv.org/abs/2505.03481v1
Evaluation of LLMs on Long-tail Entity Linking in Historical Documents,"Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP) applications, enabling the disambiguation of entity mentions by linking them to their corresponding entries in a reference knowledge base (KB). Thanks to their deep contextual understanding capabilities, LLMs offer a new perspective to tackle EL, promising better results than traditional methods. Despite the impressive generalization capabilities of LLMs, linking less popular, long-tail entities remains challenging as these entities are often underrepresented in training data and knowledge bases. Furthermore, the long-tail EL task is an understudied problem, and limited studies address it with LLMs. In the present work, we assess the performance of two popular LLMs, GPT and LLama3, in a long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated benchmark of sentences from domain-specific historical texts, we quantitatively compare the performance of LLMs in identifying and linking entities to their corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity Linking and Relation Extraction framework. Our preliminary experiments reveal that LLMs perform encouragingly well in long-tail EL, indicating that this technology can be a valuable adjunct in filling the gap between head and long-tail EL.",CS,http://arxiv.org/abs/2505.03473v1
Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models,"Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the ""overthinking"" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought \textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning (\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\% across various benchmarks while substantially reducing model response length by approximately 47.61\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models.",CS,http://arxiv.org/abs/2505.03469v1
Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis,"Explainable disease diagnosis, which leverages patient information (e.g., signs and symptoms) and computational models to generate probable diagnoses and reasonings, offers clear clinical values. However, when clinical notes encompass insufficient evidence for a definite diagnosis, such as the absence of definitive symptoms, diagnostic uncertainty usually arises, increasing the risk of misdiagnosis and adverse outcomes. Although explicitly identifying and explaining diagnostic uncertainties is essential for trustworthy diagnostic systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an uncertainty-aware large language model (LLM) created by fine-tuning open-source LLMs with diagnostic criteria. We formalized the task and assembled richly annotated datasets that capture varying degrees of diagnostic ambiguity. Evaluating ConfiDx on real-world datasets demonstrated that it excelled in identifying diagnostic uncertainties, achieving superior diagnostic performance, and generating trustworthy explanations for diagnoses and uncertainties. To our knowledge, this is the first study to jointly address diagnostic uncertainty recognition and explanation, substantially enhancing the reliability of automatic diagnostic systems.",CS,http://arxiv.org/abs/2505.03467v1
Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation,"Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.",CS,http://arxiv.org/abs/2505.03320v1
Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback,"Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare.",CS,http://arxiv.org/abs/2505.03293v1
SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation,"While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.",CS,http://arxiv.org/abs/2505.03273v1
"Survey of Abstract Meaning Representation: Then, Now, Future","This paper presents a survey of Abstract Meaning Representation (AMR), a semantic representation framework that captures the meaning of sentences through a graph-based structure. AMR represents sentences as rooted, directed acyclic graphs, where nodes correspond to concepts and edges denote relationships, effectively encoding the meaning of complex sentences. This survey investigates AMR and its extensions, focusing on AMR capabilities. It then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by showing traditional, current, and possible futures approaches. It also reviews various applications of AMR including text generation, text classification, and information extraction and information seeking. By analyzing recent developments and challenges in the field, this survey provides insights into future directions for research and the potential impact of AMR on enhancing machine understanding of human language.",CS,http://arxiv.org/abs/2505.03229v1
Improving Model Alignment Through Collective Intelligence of Open-Source LLMS,"Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models finetuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.",CS,http://arxiv.org/abs/2505.03059v1
Teaching Models to Understand (but not Generate) High-risk Data,"Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.",CS,http://arxiv.org/abs/2505.03052v1
Radio: Rate-Distortion Optimization for Large Language Model Compression,"In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a rate-distortion theory perspective and propose a quantization technique based on simple rate-distortion optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user.",CS,http://arxiv.org/abs/2505.03031v1
"UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output","Hallucinations pose a significant challenge for large language models when answering knowledge-intensive queries. As LLMs become more widely adopted, it is crucial not only to detect if hallucinations occur but also to pinpoint exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM: Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes, is a recent effort in this direction. This paper describes the UCSC system submission to the shared Mu-SHROOM task. We introduce a framework that first retrieves relevant context, next identifies false content from the answer, and finally maps them back to spans in the LLM output. The process is further enhanced by automatically optimizing prompts. Our system achieves the highest overall performance, ranking #1 in average position across all languages. We release our code and experiment results.",CS,http://arxiv.org/abs/2505.03030v1
Logits-Constrained Framework with RoBERTa for Ancient Chinese NER,"This paper presents a Logits-Constrained (LC) framework for Ancient Chinese Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our two-stage model integrates GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions. Experiments demonstrate that LC improves performance over traditional CRF and BiLSTM-based approaches, especially in high-label or large-data settings. We also propose a model selection criterion balancing label complexity and dataset size, providing practical guidance for real-world Ancient Chinese NLP tasks.",CS,http://arxiv.org/abs/2505.02983v1
AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation,"Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks.",CS,http://arxiv.org/abs/2505.02830v1
ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations,"We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.",CS,http://arxiv.org/abs/2505.02819v1
When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger,"We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\""odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.",CS,http://arxiv.org/abs/2505.02888v1
"Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models","Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.",CS,http://arxiv.org/abs/2505.02763v1
Using Knowledge Graphs to harvest datasets for efficient CLIP model training,"Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",CS,http://arxiv.org/abs/2505.02746v1
Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play,"A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.",CS,http://arxiv.org/abs/2505.02707v1
Predicting Movie Hits Before They Happen with LLMs,"Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.",CS,http://arxiv.org/abs/2505.02693v1
fastabx: A library for efficient computation of ABX discriminability,"We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.",CS,http://arxiv.org/abs/2505.02692v1
Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models,"Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.",CS,http://arxiv.org/abs/2505.02686v1
A Survey on Progress in LLM Alignment from the Perspective of Reward Design,"The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies.",CS,http://arxiv.org/abs/2505.02666v1
Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset,"Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization.",CS,http://arxiv.org/abs/2505.02656v1
Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning,"Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.",CS,http://arxiv.org/abs/2505.02639v1
LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis,"Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.",CS,http://arxiv.org/abs/2505.02625v1
Automatic Proficiency Assessment in L2 English Learners,"Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation.",CS,http://arxiv.org/abs/2505.02615v1
Ensemble Kalman filter for uncertainty in human language comprehension,"Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities.",CS,http://arxiv.org/abs/2505.02590v1
EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning,"Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.",CS,http://arxiv.org/abs/2505.02579v2
Bielik v3 Small: Technical Report,"We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.",CS,http://arxiv.org/abs/2505.02550v1
Bemba Speech Translation: Exploring a Low-Resource African Language,"This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup.",CS,http://arxiv.org/abs/2505.02518v1
Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda,"In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages.",CS,http://arxiv.org/abs/2505.02463v1
Incentivizing Inclusive Contributions in Model Sharing Markets,"While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.",CS,http://arxiv.org/abs/2505.02462v1
Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs,"One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.",CS,http://arxiv.org/abs/2505.02456v1
Bielik 11B v2 Technical Report,"We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.",CS,http://arxiv.org/abs/2505.02410v1
Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL,"Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.",CS,http://arxiv.org/abs/2505.02391v1
RM-R1: Reward Modeling as Reasoning,"Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.",CS,http://arxiv.org/abs/2505.02387v1
JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings,"Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.",CS,http://arxiv.org/abs/2505.02366v1
SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning,"Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.",CS,http://arxiv.org/abs/2505.02363v1
Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering,"The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.",CS,http://arxiv.org/abs/2505.02311v1
Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques,"Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.",CS,http://arxiv.org/abs/2505.02309v1
Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition,"Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.",CS,http://arxiv.org/abs/2505.02304v1
Demystifying optimized prompts in language models,"Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (``optimized'') prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network.",CS,http://arxiv.org/abs/2505.02273v1
Parameter-Efficient Transformer Embeddings,"Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.",CS,http://arxiv.org/abs/2505.02266v1
Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models,"Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.",CS,http://arxiv.org/abs/2505.02252v1
SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation,"Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.",CS,http://arxiv.org/abs/2505.02235v1
Interpretable Emergent Language Using Inter-Agent Transformers,"This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.",CS,http://arxiv.org/abs/2505.02215v1
DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units,"Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.",CS,http://arxiv.org/abs/2505.02206v1
Exploring new Approaches for Information Retrieval through Natural Language Processing,"This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.",CS,http://arxiv.org/abs/2505.02199v1
Measuring Hong Kong Massive Multi-Task Language Understanding,"Multilingual understanding is crucial for the cross-cultural applicability of Large Language Models (LLMs). However, evaluation benchmarks designed for Hong Kong's unique linguistic landscape, which combines Traditional Chinese script with Cantonese as the spoken form and its cultural context, remain underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language understanding benchmark that evaluates Hong Kong's linguistic competence and socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions across 66 subjects, organized into four categories: Science, Technology, Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To evaluate the multilingual understanding ability of LLMs, 90,550 Mandarin-Cantonese translation tasks were additionally included. We conduct comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs of varying sizes on HKMMLU. The results show that the best-performing model, DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than that of MMLU and CMMLU. This performance gap highlights the need to improve LLMs' capabilities in Hong Kong-specific language and knowledge domains. Furthermore, we investigate how question language, model size, prompting strategies, and question and reasoning token lengths affect model performance. We anticipate that HKMMLU will significantly advance the development of LLMs in multilingual and cross-cultural contexts, thereby enabling broader and more impactful applications.",CS,http://arxiv.org/abs/2505.02177v1
"Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization","As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate ``scaling effects'' - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks.",CS,http://arxiv.org/abs/2505.02172v1
A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking,"Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.",CS,http://arxiv.org/abs/2505.02171v1
Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use,"This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law. Motivated by the increasing prevalence of DMCA takedowns and the lack of accessible legal support for content creators, we propose a structured approach that combines semantic search with legal knowledge graphs and court citation networks to improve retrieval quality and reasoning reliability. Our prototype models legal precedents at the statutory factor level (e.g., purpose, nature, amount, market effect) and incorporates citation-weighted graph representations to prioritize doctrinally authoritative sources. We use Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning. Preliminary testing suggests this method improves doctrinal relevance in the retrieval process, laying groundwork for future evaluation and deployment of LLM-based legal assistance tools.",CS,http://arxiv.org/abs/2505.02164v1
Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents,"Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach.",CS,http://arxiv.org/abs/2505.02156v2
QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach,"Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering ""Write Once, Run Anywhere"" of tensor programs an open question.   We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs.",CS,http://arxiv.org/abs/2505.02146v1
Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study,"Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches.",CS,http://arxiv.org/abs/2505.02142v1
Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data,"Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}",CS,http://arxiv.org/abs/2505.02130v1
Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading,"When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.",CS,http://arxiv.org/abs/2505.02872v1
LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications,"Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios.",CS,http://arxiv.org/abs/2505.02091v1
LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning,"Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval.",CS,http://arxiv.org/abs/2505.02078v1
What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction,"The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.",CS,http://arxiv.org/abs/2505.02072v1
An overview of artificial intelligence in computer-assisted language learning,"Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work.",CS,http://arxiv.org/abs/2505.02032v1
Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs,"Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.",CS,http://arxiv.org/abs/2505.02009v1
LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load,"Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, we used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate our approach, we conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Our results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. Our work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.",CS,http://arxiv.org/abs/2505.01980v1
Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview,"Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or ""worldviews"". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.",CS,http://arxiv.org/abs/2505.01967v1
A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.",CS,http://arxiv.org/abs/2505.01958v1
Explainability by design: an experimental analysis of the legal coding process,"Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.   In this paper we deliver a methodology for \textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.   Finally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \textit{depth} that refers to the length of the paths of legal references.",CS,http://arxiv.org/abs/2505.01944v1
CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation,"Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\% while preserving textual coherence and semantic equivalence to the original claims.",CS,http://arxiv.org/abs/2505.01900v1
Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams,"We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts.",CS,http://arxiv.org/abs/2505.01883v1
"Humans can learn to detect AI-generated texts, or at least learn when they can't","This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.   We used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.   Participants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.   The ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts.",CS,http://arxiv.org/abs/2505.01877v2
Positional Attention for Efficient BERT-Based Named Entity Recognition,"This paper presents a framework for Named Entity Recognition (NER) leveraging the Bidirectional Encoder Representations from Transformers (BERT) model in natural language processing (NLP). NER is a fundamental task in NLP with broad applicability across downstream applications. While BERT has established itself as a state-of-the-art model for entity recognition, fine-tuning it from scratch for each new application is computationally expensive and time-consuming. To address this, we propose a cost-efficient approach that integrates positional attention mechanisms into the entity recognition process and enables effective customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves strong performance with fewer training epochs. This work contributes to the field by offering a practical solution for reducing the training cost of BERT-based NER systems while maintaining high accuracy.",CS,http://arxiv.org/abs/2505.01868v1
Intra-Layer Recurrence in Transformers for Language Modeling,"Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.",CS,http://arxiv.org/abs/2505.01855v1
$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge,"Humans and intelligent animals can effortlessly internalize new information (""news"") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\textit{contexual shadowing effect}$, where training with the news $\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.",CS,http://arxiv.org/abs/2505.01812v1
Accelerating Large Language Model Reasoning via Speculative Search,"Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.",CS,http://arxiv.org/abs/2505.02865v1
Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis,"The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.",CS,http://arxiv.org/abs/2505.01800v1
Differential Privacy for Network Assortativity,"The analysis of network assortativity is of great importance for understanding the structural characteristics of and dynamics upon networks. Often, network assortativity is quantified using the assortativity coefficient that is defined based on the Pearson correlation coefficient between vertex degrees. It is well known that a network may contain sensitive information, such as the number of friends of an individual in a social network (which is abstracted as the degree of vertex.). So, the computation of the assortativity coefficient leads to privacy leakage, which increases the urgent need for privacy-preserving protocol. However, there has been no scheme addressing the concern above.   To bridge this gap, in this work, we are the first to propose approaches based on differential privacy (DP for short). Specifically, we design three DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are designed for settings where each individual only knows his/her direct friends. In contrast, the third algorithm, based on Decentralized DP (DDP), targets scenarios where each individual has a broader view, i.e., also knowing his/her friends' friends. Theoretically, we prove that each algorithm enables an unbiased estimation of the assortativity coefficient of the network. We further evaluate the performance of the proposed algorithms using mean squared error (MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by $Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three algorithms have different assumptions, so each has its applicability scenario. Lastly, we conduct extensive numerical simulations, which demonstrate that the presented approaches are adequate to achieve the estimation of network assortativity under the demand for privacy protection.",CS,http://arxiv.org/abs/2505.03639v1
Empc: Effective Path Prioritization for Symbolic Execution with Path Cover,"Symbolic execution is a powerful program analysis technique that can formally reason the correctness of program behaviors and detect software bugs. It can systematically explore the execution paths of the tested program. But it suffers from an inherent limitation: path explosion. Path explosion occurs when symbolic execution encounters an overwhelming number (exponential to the program size) of paths that need to be symbolically reasoned. It severely impacts the scalability and performance of symbolic execution. To tackle this problem, previous works leverage various heuristics to prioritize paths for symbolic execution. They rank the exponential number of paths using static rules or heuristics and explore the paths with the highest rank. However, in practice, these works often fail to generalize to diverse programs. In this work, we propose a novel and effective path prioritization technique with path cover, named Empc. Our key insight is that not all paths need to be symbolically reasoned. Unlike traditional path prioritization, our approach leverages a small subset of paths as a minimum path cover (MPC) that can cover all code regions of the tested programs. To encourage diversity in path prioritization, we compute multiple MPCs. We then guide the search for symbolic execution on the small number of paths inside multiple MPCs rather than the exponential number of paths. We implement our technique Empc based on KLEE. We conduct a comprehensive evaluation of Empc to investigate its performance in code coverage, bug findings, and runtime overhead. The evaluation shows that Empc can cover 19.6% more basic blocks than KLEE's best search strategy and 24.4% more lines compared to the state-of-the-art work cgs. Empc also finds 24 more security violations than KLEE's best search strategy. Meanwhile, Empc can significantly reduce the memory usage of KLEE by up to 93.5%.",CS,http://arxiv.org/abs/2505.03555v1
SKALD: Scalable K-Anonymisation for Large Datasets,"Data privacy and anonymisation are critical concerns in today's data-driven society, particularly when handling personal and sensitive user data. Regulatory frameworks worldwide recommend privacy-preserving protocols such as k-anonymisation to de-identify releases of tabular data. Available hardware resources provide an upper bound on the maximum size of dataset that can be processed at a time. Large datasets with sizes exceeding this upper bound must be broken up into smaller data chunks for processing. In these cases, standard k-anonymisation tools such as ARX can only operate on a per-chunk basis. This paper proposes SKALD, a novel algorithm for performing k-anonymisation on large datasets with limited RAM. Our SKALD algorithm offers multi-fold performance improvement over standard k-anonymisation methods by extracting and combining sufficient statistics from each chunk during processing to ensure successful k-anonymisation while providing better utility.",CS,http://arxiv.org/abs/2505.03529v1
Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in Voice Authentication Systems,"Voice authentication systems remain susceptible to two major threats: backdoor triggered attacks and targeted data poisoning attacks. This dual vulnerability is critical because conventional solutions typically address each threat type separately, leaving systems exposed to adversaries who can exploit both attacks simultaneously. We propose a unified defense framework that effectively addresses both BTA and TDPA. Our framework integrates a frequency focused detection mechanism that flags covert pitch boosting and sound masking backdoor attacks in near real time, followed by a convolutional neural network that addresses TDPA. This dual layered defense approach utilizes multidimensional acoustic features to isolate anomalous signals without requiring costly model retraining. In particular, our PBSM detection mechanism can seamlessly integrate into existing voice authentication pipelines and scale effectively for large scale deployments. Experimental results on benchmark datasets and their compression with the state of the art algorithm demonstrate that our PBSM detection mechanism outperforms the state of the art. Our framework reduces attack success rates to as low as five to fifteen percent while maintaining a recall rate of up to ninety five percent in recognizing TDPA.",CS,http://arxiv.org/abs/2505.03455v1
Directed Greybox Fuzzing via Large Language Model,"Directed greybox fuzzing (DGF) focuses on efficiently reaching specific program locations or triggering particular behaviors, making it essential for tasks like vulnerability detection and crash reproduction. However, existing methods often suffer from path explosion and randomness in input mutation, leading to inefficiencies in exploring and exploiting target paths. In this paper, we propose HGFuzzer, an automatic framework that leverages the large language model (LLM) to address these challenges. HGFuzzer transforms path constraint problems into targeted code generation tasks, systematically generating test harnesses and reachable inputs to reduce unnecessary exploration paths significantly. Additionally, we implement custom mutators designed specifically for target functions, minimizing randomness and improving the precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world vulnerabilities, successfully triggering 17, including 11 within the first minute, achieving a speedup of at least 24.8x compared to state-of-the-art directed fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown vulnerabilities, all of which were assigned CVE IDs, demonstrating the effectiveness of our approach in identifying real-world vulnerabilities.",CS,http://arxiv.org/abs/2505.03425v1
Elevating Cyber Threat Intelligence against Disinformation Campaigns with LLM-based Concept Extraction and the FakeCTI Dataset,"The swift spread of fake news and disinformation campaigns poses a significant threat to public trust, political stability, and cybersecurity. Traditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level indicators such as domain names and social media handles, are easily evaded by adversaries who frequently modify their online infrastructure. To address these limitations, we introduce a novel CTI framework that focuses on high-level, semantic indicators derived from recurrent narratives and relationships of disinformation campaigns. Our approach extracts structured CTI indicators from unstructured disinformation content, capturing key entities and their contextual dependencies within fake news using Large Language Models (LLMs). We further introduce FakeCTI, the first dataset that systematically links fake news to disinformation campaigns and threat actors. To evaluate the effectiveness of our CTI framework, we analyze multiple fake news attribution techniques, spanning from traditional Natural Language Processing (NLP) to fine-tuned LLMs. This work shifts the focus from low-level artifacts to persistent conceptual structures, establishing a scalable and adaptive approach to tracking and countering disinformation campaigns.",CS,http://arxiv.org/abs/2505.03345v1
A Chaos Driven Metric for Backdoor Attack Detection,"The advancement and adoption of Artificial Intelligence (AI) models across diverse domains have transformed the way we interact with technology. However, it is essential to recognize that while AI models have introduced remarkable advancements, they also present inherent challenges such as their vulnerability to adversarial attacks. The current work proposes a novel defense mechanism against one of the most significant attack vectors of AI models - the backdoor attack via data poisoning of training datasets. In this defense technique, an integrated approach that combines chaos theory with manifold learning is proposed. A novel metric - Precision Matrix Dependency Score (PDS) that is based on the conditional variance of Neurochaos features is formulated. The PDS metric has been successfully evaluated to distinguish poisoned samples from non-poisoned samples across diverse datasets.",CS,http://arxiv.org/abs/2505.03208v1
Bridging Expertise Gaps: The Role of LLMs in Human-AI Collaboration for Cybersecurity,"This study investigates whether large language models (LLMs) can function as intelligent collaborators to bridge expertise gaps in cybersecurity decision-making. We examine two representative tasks-phishing email detection and intrusion detection-that differ in data modality, cognitive complexity, and user familiarity. Through a controlled mixed-methods user study, n = 58 (phishing, n = 34; intrusion, n = 24), we find that human-AI collaboration improves task performance,reducing false positives in phishing detection and false negatives in intrusion detection. A learning effect is also observed when participants transition from collaboration to independent work, suggesting that LLMs can support long-term skill development. Our qualitative analysis shows that interaction dynamics-such as LLM definitiveness, explanation style, and tone-influence user trust, prompting strategies, and decision revision. Users engaged in more analytic questioning and showed greater reliance on LLM feedback in high-complexity settings. These results provide design guidance for building interpretable, adaptive, and trustworthy human-AI teaming systems, and demonstrate that LLMs can meaningfully support non-experts in reasoning through complex cybersecurity problems.",CS,http://arxiv.org/abs/2505.03179v1
An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks,"Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.",CS,http://arxiv.org/abs/2505.03161v1
Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models,"This work evaluates the performance of Cyber Threat Intelligence (CTI) extraction methods in identifying attack techniques from threat reports available on the web using the MITRE ATT&CK framework. We analyse four configurations utilising state-of-the-art tools, including the Threat Report ATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as Llama2. Our findings reveal significant challenges, including class imbalance, overfitting, and domain-specific complexity, which impede accurate technique extraction. To mitigate these issues, we propose a novel two-step pipeline: first, an LLM summarises the reports, and second, a retrained SciBERT model processes a rebalanced dataset augmented with LLM-generated data. This approach achieves an improvement in F1-scores compared to baseline models, with several attack techniques surpassing an F1-score of 0.90. Our contributions enhance the efficiency of web-based CTI systems and support collaborative cybersecurity operations in an interconnected digital landscape, paving the way for future research on integrating human-AI collaboration platforms.",CS,http://arxiv.org/abs/2505.03147v1
Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems,"Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable to adversarial attacks. It is crucial for an IDS to learn to recognize adversarial examples before malicious entities exploit them. In this paper, we generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We validate the generalization and scalability of the adversarial samples to tackle a broad range of real attacks on Industrial Control Systems (ICS). We evaluated the impact by assessing multiple attacks generated using the proposed method. The model trained with adversarial samples detected attacks with 95% accuracy on real-world attack data not used during training. The study was conducted using an operational secure water treatment (SWaT) testbed.",CS,http://arxiv.org/abs/2505.03120v1
Towards a standardized methodology and dataset for evaluating LLM-based digital forensic timeline analysis,"Large language models (LLMs) have seen widespread adoption in many domains including digital forensics. While prior research has largely centered on case studies and examples demonstrating how LLMs can assist forensic investigations, deeper explorations remain limited, i.e., a standardized approach for precise performance evaluations is lacking. Inspired by the NIST Computer Forensic Tool Testing Program, this paper proposes a standardized methodology to quantitatively evaluate the application of LLMs for digital forensic tasks, specifically in timeline analysis. The paper describes the components of the methodology, including the dataset, timeline generation, and ground truth development. Additionally, the paper recommends using BLEU and ROUGE metrics for the quantitative evaluation of LLMs through case studies or tasks involving timeline analysis. Experimental results using ChatGPT demonstrate that the proposed methodology can effectively evaluate LLM-based forensic timeline analysis. Finally, we discuss the limitations of applying LLMs to forensic timeline analysis.",CS,http://arxiv.org/abs/2505.03100v1
Acoustic Side-Channel Attacks on a Computer Mouse,"Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using audio emitted from a computing devices and their peripherals. Attacks targeting keyboards are popular and have been explored in the literature. However, similar attacks targeting other human interface peripherals, such as computer mice, are under-explored. To this end, this paper considers security leakage via acoustic signals emanating from normal mouse usage. We first confirm feasibility of such attacks by showing a proof-of-concept attack that classifies four mouse movements with 97% accuracy in a controlled environment. We then evolve the attack towards discerning twelve unique mouse movements using a smartphone to record the experiment. Using Machine Learning (ML) techniques, the model is trained on an experiment with six participants to be generalizable and discern among twelve movements with 94% accuracy. In addition, we experiment with an attack that detects a user action of closing a full-screen window on a laptop. Achieving an accuracy of 91%, this experiment highlights exploiting audio leakage from computer mouse movements in a realistic scenario.",CS,http://arxiv.org/abs/2505.02725v1
SoK: Stealing Cars Since Remote Keyless Entry Introduction and How to Defend From It,"Remote Keyless Entry (RKE) systems have been the target of thieves since their introduction in automotive industry. Robberies targeting vehicles and their remote entry systems are booming again without a significant advancement from the industrial sector being able to protect against them. Researchers and attackers continuously play cat and mouse to implement new methodologies to exploit weaknesses and defense strategies for RKEs. In this fragment, different attacks and defenses have been discussed in research and industry without proper bridging. In this paper, we provide a Systematization Of Knowledge (SOK) on RKE and Passive Keyless Entry and Start (PKES), focusing on their history and current situation, ranging from legacy systems to modern web-based ones. We provide insight into vehicle manufacturers' technologies and attacks and defense mechanisms involving them. To the best of our knowledge, this is the first comprehensive SOK on RKE systems, and we address specific research questions to understand the evolution and security status of such systems. By identifying the weaknesses RKE still faces, we provide future directions for security researchers and companies to find viable solutions to address old attacks, such as Relay and RollJam, as well as new ones, like API vulnerabilities.",CS,http://arxiv.org/abs/2505.02713v1
Antifragility of RIS-assisted Communication Systems under Jamming Attacks,"Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.",CS,http://arxiv.org/abs/2505.02565v1
Attestable builds: compiling verifiable binaries on untrusted systems using trusted execution environments,"In this paper we present attestable builds, a new paradigm to provide strong source-to-binary correspondence in software artifacts. We tackle the challenge of opaque build pipelines that disconnect the trust between source code, which can be understood and audited, and the final binary artifact, which is difficult to inspect. Our system uses modern trusted execution environments (TEEs) and sandboxed build containers to provide strong guarantees that a given artifact was correctly built from a specific source code snapshot. As such it complements existing approaches like reproducible builds which typically require time-intensive modifications to existing build configurations and dependencies, and require independent parties to continuously build and verify artifacts. In comparison, an attestable build requires only minimal changes to an existing project, and offers nearly instantaneous verification of the correspondence between a given binary and the source code and build pipeline used to construct it. We evaluate it by building open-source software libraries - focusing on projects which are important to the trust chain and those which have proven difficult to be built deterministically. Overall, the overhead (42 seconds start-up latency and 14% increase in build duration) is small in comparison to the overall build time. Importantly, our prototype builds even complex projects such as LLVM Clang without requiring any modifications to their source code and build scripts. Finally, we formally model and verify the attestable build design to demonstrate its security against well-resourced adversaries.",CS,http://arxiv.org/abs/2505.02521v1
Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study,"Background: Large language models (LLMs) are increasingly deployed via open-source and commercial frameworks, enabling individuals and organizations to self-host advanced AI capabilities. However, insecure defaults and misconfigurations often expose LLM services to the public Internet, posing significant security and system engineering risks. Aims: This study aims to unveil the current landscape of public-facing LLM deployments in the wild through a large-scale empirical study, focusing on service prevalence, exposure characteristics, systemic vulnerabilities, and associated risks. Method: We conducted an Internet-wide measurement to identify public-facing LLM deployments across 15 frameworks, discovering 320,102 services. We extracted 158 unique API endpoints, grouped into 12 functional categories based on capabilities and security risks. We further analyzed configurations, authentication practices, and geographic distributions, revealing deployment trends and systemic issues in real-world LLM system engineering. Results: Our study shows that public LLM deployments are rapidly growing but often insecure. Among all endpoints, we observe widespread use of insecure protocols, poor TLS configurations, and unauthenticated access to critical operations. Security risks, including model disclosure, system leakage, and unauthorized access, are pervasive, highlighting the need for secure-by-default frameworks and stronger deployment practices. Conclusions: Public-facing LLM deployments suffer from widespread security and configuration flaws, exposing services to misuse, model theft, resource hijacking, and remote exploitation. Strengthening default security, deployment practices, and operational standards is critical for the growing self-hosted LLM ecosystem.",CS,http://arxiv.org/abs/2505.02502v1
An Efficient Hybrid Key Exchange Mechanism,"We present \textsc{CHOKE}, a novel code-based hybrid key-encapsulation mechanism (KEM) designed to securely and efficiently transmit multiple session keys simultaneously. By encoding $n$ independent session keys with an individually secure linear code and encapsulating each resulting coded symbol using a separate KEM, \textsc{CHOKE} achieves computational individual security -- each key remains secure as long as at least one underlying KEM remains unbroken. Compared to traditional serial or combiner-based hybrid schemes, \textsc{CHOKE} reduces computational and communication costs by an $n$-fold factor. Furthermore, we show that the communication cost of our construction is optimal under the requirement that each KEM must be used at least once.",CS,http://arxiv.org/abs/2505.02499v1
Dynamic Graph-based Fingerprinting of In-browser Cryptomining,"The decentralized and unregulated nature of cryptocurrencies, combined with their monetary value, has made them a vehicle for various illicit activities. One such activity is cryptojacking, an attack that uses stolen computing resources to mine cryptocurrencies without consent for profit. In-browser cryptojacking malware exploits high-performance web technologies like WebAssembly to mine cryptocurrencies directly within the browser without file downloads. Although existing methods for cryptomining detection report high accuracy and low overhead, they are often susceptible to various forms of obfuscation, and due to the limited variety of cryptomining scripts in the wild, standard code obfuscation methods present a natural and appealing solution to avoid detection. To address these limitations, we propose using instruction-level data-flow graphs to detect cryptomining behavior. Data-flow graphs offer detailed structural insights into a program's computations, making them suitable for characterizing proof-of-work algorithms, but they can be difficult to analyze due to their large size and susceptibility to noise and fragmentation under obfuscation. We present two techniques to simplify and compare data-flow graphs: (1) a graph simplification algorithm to reduce the computational burden of processing large and granular data-flow graphs while preserving local substructures; and (2) a subgraph similarity measure, the n-fragment inclusion score, based on fragment inclusion that is robust against noise and obfuscation. Using data-flow graphs as computation fingerprints, our detection framework PoT (Proof-of-Theft) was able to achieve high detection accuracy against standard obfuscations, outperforming existing detection methods. Moreover, PoT uses generic data-flow properties that can be applied to other platforms more susceptible to cryptojacking such as servers and data centers.",CS,http://arxiv.org/abs/2505.02493v1
Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation,"Rust is a promising programming language that focuses on concurrency, usability, and security. It is used in production code by major industry players and got recommended by government bodies. Rust provides strong security guarantees achieved by design utilizing the concepts of ownership and borrowing. However, Rust allows programmers to write unsafe code which is not subject to the strict Rust security policy. Empirical studies show that security issues in practice always involve code written in unsafe Rust.   In this paper, we present the first approach that utilizes selective code coverage feedback to focus the fuzzing efforts on unsafe Rust code. Our approach significantly improves the efficiency when fuzzing Rust programs and does not require additional computational resources while fuzz testing the target. To quantify the impact of partial code instrumentation, we implement our approach by extending the capabilities of the Rust compiler toolchain. We present an automated approach to detect unsafe and safe code components to decide which parts of the program a fuzzer should focus on when running a fuzzing campaign to find vulnerabilities in Rust programs. Our approach is fully compatible with existing fuzzing implementations and does not require complex manual work, thus retaining the existing high usability standard. Focusing on unsafe code, our implementation allows us to generate inputs that trigger more unsafe code locations with statistical significance and therefore is able to detect potential vulnerabilities in a shorter time span while imposing no performance overhead during fuzzing itself.",CS,http://arxiv.org/abs/2505.02464v1
Encrypted Federated Search Using Homomorphic Encryption,"The sharing of information between agencies is effective in dealing with cross-jurisdictional criminal activities; however, such sharing is often restricted due to concerns about data privacy, ownership, and compliance. Towards this end, this work has introduced a privacy-preserving federated search system that allows law enforcement agencies to conduct queries on encrypted criminal databases by utilizing Homomorphic Encryption (HE). The key innovation here is the ability to execute encrypted queries across distributed databases, without the decryption of the data, thus preserving end-to-end confidentiality. In essence, this approach meets stringent privacy requirements in the interests of national security and regulatory compliance. The system incorporates the CKKS and BFV scheme embedded within TenSEAL, with each agency holding its key pair in a centralized key management table. In this federated search, encrypted queries are computed on the server side, and only authorized clients can decrypt the computed results. The matching of agencies is flexible for working in real-time while at the same time being secure and scalable while preserving control over data and the integrity of the process. Experimental results demonstrate the model. This paper also provide the implementation code and other details.",CS,http://arxiv.org/abs/2505.02409v1
"Moneros Decentralized P2P Exchanges: Functionality, Adoption, and Privacy Risks","Privacy-focused cryptocurrencies like Monero remain popular, despite increasing regulatory scrutiny that has led to their delisting from major centralized exchanges. The latter also explains the recent popularity of decentralized exchanges (DEXs) with no centralized ownership structures. These platforms typically leverage peer-to-peer (P2P) networks, promising secure and anonymous asset trading. However, questions of liability remain, and the academic literature lacks comprehensive insights into the functionality, trading activity, and privacy claims of these P2P platforms. In this paper, we provide an early systematization of the current landscape of decentralized peer-to-peer exchanges within the Monero ecosystem. We examine several recently developed DEX platforms, analyzing their popularity, functionality, architectural choices, and potential weaknesses. We further identify and report on a privacy vulnerability in the recently popularized Haveno exchange, demonstrating that certain Haveno trades could be detected, allowing transactions to be linked across the Monero and Bitcoin blockchains. We hope that our findings can nourish the discussion in the research community about more secure designs, and provide insights for regulators.",CS,http://arxiv.org/abs/2505.02392v1
Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models,"Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks.",CS,http://arxiv.org/abs/2505.02362v1
A Slicing-Based Approach for Detecting and Patching Vulnerable Code Clones,"Code cloning is a common practice in software development, but it poses significant security risks by propagating vulnerabilities across cloned segments. To address this challenge, we introduce srcVul, a scalable, precise detection approach that combines program slicing with Locality-Sensitive Hashing to identify vulnerable code clones and recommend patches. srcVul builds a database of vulnerability-related slices by analyzing known vulnerable programs and their corresponding patches, indexing each slice's unique structural characteristics as a vulnerability slicing vector. During clone detection, srcVul efficiently matches slicing vectors from target programs with those in the database, recommending patches upon identifying similarities. Our evaluation of srcVul against three state-of-the-art vulnerable clone detectors demonstrates its accuracy, efficiency, and scalability, achieving 91% precision and 75% recall on established vulnerability databases and open-source repositories. These results highlight srcVul's effectiveness in detecting complex vulnerability patterns across diverse codebases.",CS,http://arxiv.org/abs/2505.02349v1
An End-to-End Model For Logits Based Large Language Models Watermarking,"The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37-39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs.",CS,http://arxiv.org/abs/2505.02344v1
Performance Analysis and Deployment Considerations of Post-Quantum Cryptography for Consumer Electronics,"Quantum computing threatens the security foundations of consumer electronics (CE). Preparing the diverse CE ecosystem, particularly resource-constrained devices, for the post-quantum era requires quantitative understanding of quantum-resistant cryptography (PQC) performance. This paper presents a comprehensive cross-platform performance analysis of leading PQC Key Encapsulation Mechanisms (KEMs) and digital signatures (NIST standards/candidates) compared against classical RSA/ECC. We evaluated execution time, communication costs (key/signature sizes), and memory footprint indicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms (Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes, notably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong balance of computational efficiency and moderate communication/storage overhead, making them highly suitable for many CE applications. In contrast, code-based Classic McEliece imposes significant key size challenges, while hash-based SPHINCS+ offers high security assurance but demands large signature sizes impacting bandwidth and storage. Based on empirical data across platforms and security levels, we provide specific deployment recommendations tailored to different CE scenarios (e.g., wearables, smart home hubs, mobile devices), offering guidance for manufacturers navigating the PQC transition.",CS,http://arxiv.org/abs/2505.02239v1
Risk Assessment and Threat Modeling for safe autonomous driving technology,"This research paper delves into the field of autonomous vehicle technology, examining the vulnerabilities inherent in each component of these transformative vehicles. Autonomous vehicles (AVs) are revolutionizing transportation by seamlessly integrating advanced functionalities such as sensing, perception, planning, decision-making, and control. However, their reliance on interconnected systems and external communication interfaces renders them susceptible to cybersecurity threats.   This research endeavors to develop a comprehensive threat model for AV systems, employing OWASP Threat Dragon and the STRIDE framework. This model categorizes threats into Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service (DoS), and Elevation of Privilege.   A systematic risk assessment is conducted to evaluate vulnerabilities across various AV components, including perception modules, planning systems, control units, and communication interfaces.",CS,http://arxiv.org/abs/2505.02231v1
Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees,"A decision tree is an easy-to-understand tool that has been widely used for classification tasks. On the one hand, due to privacy concerns, there has been an urgent need to create privacy-preserving classifiers that conceal the user's input from the classifier. On the other hand, with the rise of cloud computing, data owners are keen to reduce risk by outsourcing their model, but want security guarantees that third parties cannot steal their decision tree model. To address these issues, Joye and Salehi introduced a theoretical protocol that efficiently evaluates decision trees while maintaining privacy by leveraging their comparison protocol that is resistant to timing attacks. However, their approach was not only inefficient but also prone to side-channel attacks. Therefore, in this paper, we propose a new decision tree inference protocol in which the model is shared and evaluated among multiple entities. We partition our decision tree model by each level to be stored in a new entity we refer to as a ""level-site."" Utilizing this approach, we were able to gain improved average run time for classifier evaluation for a non-complete tree, while also having strong mitigations against side-channel attacks.",CS,http://arxiv.org/abs/2505.02224v1
Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents,"Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.",CS,http://arxiv.org/abs/2505.02077v1
Triple-identity Authentication: The Future of Secure Access,"In a typical authentication process, the local system verifies the user's identity using a stored hash value generated by a cross-system hash algorithm. This article shifts the research focus from traditional password encryption to the establishment of gatekeeping mechanisms for effective interactions between a system and the outside world. Here, we propose a triple-identity authentication system to achieve this goal. Specifically, this local system opens the inner structure of its hash algorithm to all user credentials, including the login name, login password, and authentication password. When a login credential is entered, the local system hashes it and then creates a unique identifier using intermediate hash elements randomly selected from the open algorithm. Importantly, this locally generated unique identifier (rather than the stored hash produced by the open algorithm) is utilized to verify the user's combined identity, which is generated by combining the entered credential with the International Mobile Equipment Identity and the International Mobile Subscriber Identity. The verification process is implemented at each interaction point: the login name field, the login password field, and the server's authentication point. Thus, within the context of this triple-identity authentication system, we establish a robust gatekeeping mechanism for system interactions, ultimately providing a level of security that is equivalent to multi-factor authentication.",CS,http://arxiv.org/abs/2505.02004v1
A Survey on Privacy Risks and Protection in Large Language Models,"Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.",CS,http://arxiv.org/abs/2505.01976v1
"UK Finfluencers: Exploring Content, Reach, and Responsibility","The rise of social media financial influencers (finfluencers) has significantly transformed the personal finance landscape, making financial advice and insights more accessible to a broader and younger audience. By leveraging digital platforms, these influencers have contributed to the democratization of financial literacy. However, the line between education and promotion is often blurred, as many finfluencers lack formal financial qualifications, raising concerns about the accuracy and reliability of the information they share. This study investigates the patterns and behaviours of finfluencers in the UK on TikTok, focusing not on individual actions but on broader trends and the interactions between influencers and their followers. The aim is to identify common engagement patterns and propose guidelines that can help protect the public from potential financial harm. Specifically, the paper contributes a detailed analysis of finfluencer content categorization, sentiment trends, and the prevalence and role of disclaimers, offering empirical insights that inform recommendations for safer and more transparent financial communication on social media.",CS,http://arxiv.org/abs/2505.01941v1
Towards Trustworthy Federated Learning with Untrusted Participants,"Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.",CS,http://arxiv.org/abs/2505.01874v1
An Approach for Handling Missing Attribute Values in Attribute-Based Access Control Policy Mining,"Attribute-Based Access Control (ABAC) enables highly expressive and flexible access decisions by considering a wide range of contextual attributes. ABAC policies use logical expressions that combine these attributes, allowing for precise and context-aware control. Algorithms that mine ABAC policies from legacy access control systems can significantly reduce the costs associated with migrating to ABAC. However, a major challenge in this process is handling incomplete entity information, where some attribute values are missing.   This paper introduces an approach that enhances the policy mining process by predicting or inferring missing attribute values. This is accomplished by employing a contextual clustering technique that groups entities according to their known attributes, which are then used to analyze and refine authorization decisions. By effectively managing incomplete data, our approach provides security administrators with a valuable tool to improve their attribute data and ensure a smoother, more efficient transition to ABAC.",CS,http://arxiv.org/abs/2505.01873v1
PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework,"Federated Learning (FL) enables collaborative model training while preserving data privacy, but its classical cryptographic underpinnings are vulnerable to quantum attacks. This vulnerability is particularly critical in sensitive domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure Blockchain-based Federated Learning), a framework integrating post-quantum cryptography (PQC) with blockchain verification to secure FL against quantum adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly Dilithium) signatures to authenticate model updates and leverage optimized smart contracts for decentralized validation. Extensive evaluations on diverse datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms) with a fixed signature size of 3309 Bytes. Blockchain integration incurs a manageable overhead, with average transaction times around 4.8 s and gas usage per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the cryptographic overhead relative to transaction time remains minimal (around 0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the bottleneck in blockchain-based FL. The system maintains competitive model accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with round times showing sublinear growth with increasing client numbers. Our open-source implementation and reproducible benchmarks validate the feasibility of deploying long-term, quantum-resistant security in practical FL systems.",CS,http://arxiv.org/abs/2505.01866v1
M-ary Precomputation-Based Accelerated Scalar Multiplication Algorithms for Enhanced Elliptic Curve Cryptography,"Efficient scalar multiplication is critical for enhancing the performance of elliptic curve cryptography (ECC), especially in applications requiring large-scale or real-time cryptographic operations. This paper proposes an M-ary precomputation-based scalar multiplication algorithm, aiming to optimize both computational efficiency and memory usage. The method reduces the time complexity from $\Theta(Q \log p)$ to $\Theta\left(\frac{Q \log p}{\log Q}\right)$ and achieves a memory complexity of $\Theta\left(\frac{Q \log p}{\log^2 Q}\right)$. Experiments on ElGamal encryption and NS3-based communication simulations validate its effectiveness. On secp256k1, the proposed method achieves up to a 59\% reduction in encryption time and 30\% memory savings. In network simulations, the binary-optimized variant reduces communication time by 22.1\% on secp384r1 and simulation time by 25.4\% on secp521r1. The results demonstrate the scalability, efficiency, and practical applicability of the proposed algorithm. The source code will be publicly released upon acceptance.",CS,http://arxiv.org/abs/2505.01845v1
Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp,"The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978.",CS,http://arxiv.org/abs/2505.01816v1
Backdoor Attacks Against Patch-based Mixture of Experts,"As Deep Neural Networks (DNNs) continue to require larger amounts of data and computational power, Mixture of Experts (MoE) models have become a popular choice to reduce computational complexity. This popularity increases the importance of considering the security of MoE architectures. Unfortunately, the security of models using a MoE architecture has not yet gained much attention compared to other DNN models. In this work, we investigate the vulnerability of patch-based MoE (pMoE) models for image classification against backdoor attacks. We examine multiple trigger generation methods and Fine-Pruning as a defense. To better understand a pMoE model's vulnerability to backdoor attacks, we investigate which factors affect the model's patch selection. Our work shows that pMoE models are highly susceptible to backdoor attacks. More precisely, we achieve high attack success rates of up to 100% with visible triggers and a 2% poisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally, we show that pruning itself is ineffective as a defense but that fine-tuning can remove the backdoor almost completely. Our results show that fine-tuning the model for five epochs reduces the attack success rate to 2.1% whilst sacrificing 1.4% accuracy.",CS,http://arxiv.org/abs/2505.01811v1
Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning,"The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.",CS,http://arxiv.org/abs/2505.01788v1
Energy-Efficient NTT Sampler for Kyber Benchmarked on FPGA,"Kyber is a lattice-based key encapsulation mechanism selected for standardization by the NIST Post-Quantum Cryptography (PQC) project. A critical component of Kyber's key generation process is the sampling of matrix elements from a uniform distribution over the ring Rq . This step is one of the most computationally intensive tasks in the scheme, significantly impacting performance in low-power embedded systems such as Internet of Things (IoT), wearable devices, wireless sensor networks (WSNs), smart cards, TPMs (Trusted Platform Modules), etc. Existing approaches to this sampling, notably conventional SampleNTT and Parse-SPDM3, rely on rejection sampling. Both algorithms require a large number of random bytes, which needs at least three SHAKE-128 squeezing steps per polynomial. As a result, it causes significant amount of latency and energy. In this work, we propose a novel and efficient sampling algorithm, namely Modified SampleNTT, which substantially educes the average number of bits required from SHAKE-128 to generate elements in Rq - achieving approximately a 33% reduction compared to conventional SampleNTT. Modified SampleNTT achieves 99.16% success in generating a complete polynomial using only two SHAKE-128 squeezes, outperforming both state-of-the-art methods, which never succeed in two squeezes of SHAKE-128. Furthermore, our algorithm maintains the same average rejection rate as existing techniques and passes all standard statistical tests for randomness quality. FPGA implementation on Artix-7 demonstrates a 33.14% reduction in energy, 33.32% lower latency, and 0.28% fewer slices compared to SampleNTT. Our results confirm that Modified SampleNTT is an efficient and practical alternative for uniform polynomial sampling in PQC schemes such as Kyber, especially for low-power security processors.",CS,http://arxiv.org/abs/2505.01782v1
Unified Steganography via Implicit Neural Representation,"Digital steganography is the practice of concealing for encrypted data transmission. Typically, steganography methods embed secret data into cover data to create stega data that incorporates hidden secret data. However, steganography techniques often require designing specific frameworks for each data type, which restricts their generalizability. In this paper, we present U-INR, a novel method for steganography via Implicit Neural Representation (INR). Rather than using the specific framework for each data format, we directly use the neurons of the INR network to represent the secret data and cover data across different data types. To achieve this idea, a private key is shared between the data sender and receivers. Such a private key can be used to determine the position of secret data in INR networks. To effectively leverage this key, we further introduce a key-based selection strategy that can be used to determine the position within the INRs for data storage. Comprehensive experiments across multiple data types, including images, videos, audio, and SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR, emphasizing its potential for improving data security and privacy in various applications.",CS,http://arxiv.org/abs/2505.01749v1
Allocation of Heterogeneous Resources in General Lotto Games,"The allocation of resources plays an important role in the completion of system objectives and tasks, especially in the presence of strategic adversaries. Optimal allocation strategies are becoming increasingly more complex, given that multiple heterogeneous types of resources are at a system planner's disposal. In this paper, we focus on deriving optimal strategies for the allocation of heterogeneous resources in a well-known competitive resource allocation model known as the General Lotto game. In standard formulations, outcomes are determined solely by the players' allocation strategies of a common, single type of resource across multiple contests. In particular, a player wins a contest if it sends more resources than the opponent. Here, we propose a multi-resource extension where the winner of a contest is now determined not only by the amount of resources allocated, but also by the composition of resource types that are allocated. We completely characterize the equilibrium payoffs and strategies for two distinct formulations. The first consists of a weakest-link/best-shot winning rule, and the second considers a winning rule based on a weighted linear combination of the allocated resources. We then consider a scenario where the resource types are costly to purchase, and derive the players' equilibrium investments in each of the resource types.",CS,http://arxiv.org/abs/2505.02860v1
HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning,"As vector databases gain traction in enterprise applications, robust access control has become critical to safeguard sensitive data. Access control in these systems is often implemented through hybrid vector queries, which combine nearest neighbor search on vector data with relational predicates based on user permissions. However, existing approaches face significant trade-offs: creating dedicated indexes for each user minimizes query latency but introduces excessive storage redundancy, while building a single index and applying access control after vector search reduces storage overhead but suffers from poor recall and increased query latency. This paper introduces HoneyBee, a dynamic partitioning framework that bridges the gap between these approaches by leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC, widely adopted in enterprise settings, groups users into roles and assigns permissions to those roles, creating a natural ""thin waist"" in the permission structure that is ideal for partitioning decisions. Specifically, HoneyBee produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling storage overhead. By introducing analytical models for the performance and recall of the vector search, HoneyBee formulates the partitioning strategy as a constrained optimization problem to dynamically balance storage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee reduces storage redundancy compared to role partitioning and achieves up to 6x faster query speeds than row-level security (RLS) with only 1.4x storage increase, offering a practical middle ground for secure and efficient vector search.",CS,http://arxiv.org/abs/2505.01538v1
Disassembly as Weighted Interval Scheduling with Learned Weights,"Disassembly is the first step of a variety of binary analysis and transformation techniques, such as reverse engineering, or binary rewriting. Recent disassembly approaches consist of three phases: an exploration phase, that overapproximates the binary's code; an analysis phase, that assigns weights to candidate instructions or basic blocks; and a conflict resolution phase, that downselects the final set of instructions. We present a disassembly algorithm that generalizes this pattern for a wide range of architectures, namely x86, x64, arm32, and aarch64. Our algorithm presents a novel conflict resolution method that reduces disassembly to weighted interval scheduling.",CS,http://arxiv.org/abs/2505.01536v1
The DCR Delusion: Measuring the Privacy Risk of Synthetic Data,"Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.",CS,http://arxiv.org/abs/2505.01524v1
Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security,"The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption.",CS,http://arxiv.org/abs/2505.01518v1
"Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration","The rapid digitalization of communication systems has elevated Interactive Voice Response (IVR) technologies to become critical interfaces for customer engagement. With Artificial Intelligence (AI) now driving these platforms, ensuring secure, compliant, and ethically designed development practices is more imperative than ever. AI-powered IVRs leverage Natural Language Processing (NLP) and Machine Learning (ML) to personalize interactions, automate service delivery, and optimize user experiences. However, these innovations expose systems to heightened risks, including data privacy breaches, AI decision opacity, and model security vulnerabilities. This paper analyzes the evolution of IVRs from static code-based designs to adaptive AI-driven systems, presenting a cybersecurity-centric perspective. We propose a practical governance framework that embeds agile security principles, compliance with global data legislation, and user-centric ethics. Emphasizing privacy-by-design, adaptive risk modeling, and transparency, the paper argues that ethical AI integration is not a feature but a strategic imperative. Through this multidimensional lens, we highlight how modern IVRs can transition from communication tools to intelligent, secure, and accountable digital frontlines-resilient against emerging threats and aligned with societal expectations.",CS,http://arxiv.org/abs/2505.01514v1
VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models,"The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{https://github.com/SPIN-UMass/VidStamp}",CS,http://arxiv.org/abs/2505.01406v1
Machine Learning for Cyber-Attack Identification from Traffic Flows,"This paper presents our simulation of cyber-attacks and detection strategies on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual machines and the OPNSense firewall, along with traffic dynamics from SUMO and exploitation via the Metasploit framework. We try to answer the research questions: are we able to identify cyber attacks by only analyzing traffic flow patterns. In this research, the cyber attacks are focused particularly when lights are randomly turned all green or red at busy intersections by adversarial attackers. Despite challenges stemming from imbalanced data and overlapping traffic patterns, our best model shows 85\% accuracy when detecting intrusions purely using traffic flow statistics. Key indicators for successful detection included occupancy, jam length, and halting durations.",CS,http://arxiv.org/abs/2505.01489v1
Explainable Machine Learning for Cyberattack Identification from Traffic Flows,"The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems.",CS,http://arxiv.org/abs/2505.01488v1
LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps,"Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme.",CS,http://arxiv.org/abs/2505.01484v1
"Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability","While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.",CS,http://arxiv.org/abs/2505.01328v1
Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams,"Local Differential Privacy (LDP) enables massive data collection and analysis while protecting end users' privacy against untrusted aggregators. It has been applied to various data types (e.g., categorical, numerical, and graph data) and application settings (e.g., static and streaming). Recent findings indicate that LDP protocols can be easily disrupted by poisoning or manipulation attacks, which leverage injected/corrupted fake users to send crafted data conforming to the LDP reports. However, current attacks primarily target static protocols, neglecting the security of LDP protocols in the streaming settings. Our research fills the gap by developing novel fine-grained manipulation attacks to LDP protocols for data streams. By reviewing the attack surfaces in existing algorithms, We introduce a unified attack framework with composable modules, which can manipulate the LDP estimated stream toward a target stream. Our attack framework can adapt to state-of-the-art streaming LDP algorithms with different analytic tasks (e.g., frequency and mean) and LDP models (event-level, user-level, w-event level). We validate our attacks theoretically and through extensive experiments on real-world datasets, and finally explore a possible defense mechanism for mitigating these attacks.",CS,http://arxiv.org/abs/2505.01292v1
Watermark Overwriting Attack on StegaStamp algorithm,"This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS ""Erasing the invisible"" competition.",CS,http://arxiv.org/abs/2505.01474v1
Shuffling Cards When You Are of Very Little Brain: Low Memory Generation of Permutations,"How can we generate a permutation of the numbers $1$ through $n$ so that it is hard to guess the next element given the history so far? The twist is that the generator of the permutation (the ``Dealer"") has limited memory, while the ``Guesser"" has unlimited memory. With unbounded memory (actually $n$ bits suffice), the Dealer can generate a truly random permutation where~$\ln n$ is the expected number of correct guesses.   Our main results are tight bounds for the relationship between the guessing probability and the memory required to generate the permutation. We suggest a method for the Dealer that requires~$m$ bits of storage, constant time for each turn and makes any Guesser pick correctly only $O(n/m+\log m)$ cards in expectation. The method does not require any secrecy from the dealer, i.e. it is ``open book"" or ``whitebox"". On the other hand, we show that this bound is the best possible, even for Dealers with secret memory: For any $m$-bit Dealer there is a (computationally powerful) guesser that makes $\Omega(n/m+\log m)$ correct guesses in expectation.   We also give an $O(n)$ bit memory Dealer that generates perfectly random permutations and operates in constant time per turn.",CS,http://arxiv.org/abs/2505.01287v1
PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC),"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC). The tabulations contain statistics of counts of U.S. persons living in certain types of households, including averages. The article describes the PHSafe algorithm, which is based on adding noise drawn from a discrete Gaussian distribution to the statistics of interest. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.",CS,http://arxiv.org/abs/2505.01254v1
SafeTab-H: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File B (Detailed DHC-B),"This article describes SafeTab-H, a disclosure avoidance algorithm applied to the release of the U.S. Census Bureau's Detailed Demographic and Housing Characteristics File B (Detailed DHC-B) as part of the 2020 Census. The tabulations contain household statistics about household type and tenure iterated by the householder's detailed race, ethnicity, or American Indian and Alaska Native tribe and village at varying levels of geography. We describe the algorithmic strategy which is based on adding noise from a discrete Gaussian distribution and show that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We discuss how the implementation of the SafeTab-H codebase relies on the Tumult Analytics privacy library. We also describe the theoretical expected error properties of the algorithm and explore various aspects of its parameter tuning.",CS,http://arxiv.org/abs/2505.03072v1
SafeTab-P: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File A (Detailed DHC-A),"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the Detailed Demographic and Housing Characteristics File A (Detailed DHC-A) of the 2020 Census. The tabulations contain statistics (counts) of demographic characteristics of the entire population of the United States, crossed with detailed races and ethnicities at varying levels of geography. The article describes the SafeTab-P algorithm, which is based on adding noise drawn to statistics of interest from a discrete Gaussian distribution. A key innovation in SafeTab-P is the ability to adaptively choose how many statistics and at what granularity to release them, depending on the size of a population group. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy (zCDP). We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.",CS,http://arxiv.org/abs/2505.01472v1
Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks,"Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.",CS,http://arxiv.org/abs/2505.01186v1
"LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures","As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.",CS,http://arxiv.org/abs/2505.01177v1
Active Sybil Attack and Efficient Defense Strategy in IPFS DHT,"The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P) storage that relies on Kademlia, a Distributed Hash Table (DHT) structure commonly used in P2P systems for its proved scalability. However, DHTs are known to be vulnerable to Sybil attacks, in which a single entity controls multiple malicious nodes. Recent studies have shown that IPFS is affected by a passive content eclipse attack, leveraging Sybils, in which adversarial nodes hide received indexed information from other peers, making the content appear unavailable. Fortunately, the latest mitigation strategy coupling an attack detection based on statistical tests and a wider publication strategy upon detection was able to circumvent it.   In this work, we present a new active attack, with malicious nodes responding with semantically correct but intentionally false data, exploiting both an optimized placement of Sybils to stay below the detection threshold and an early trigger of the content discovery termination in Kubo, the main IPFS implementation. Our attack achieves to completely eclipse content on the latest Kubo release. When evaluated against the most recent known mitigation, it successfully denies access to the target content in approximately 80\% of lookup attempts.   To address this vulnerability, we propose a new mitigation called SR-DHT-Store, which enables efficient, Sybil-resistant content publication without relying on attack detection but instead on a systematic and precise use of region-based queries, defined by a dynamically computed XOR distance to the target ID. SR-DHT-Store can be combined with other defense mechanisms resulting in a defense strategy that completely mitigates both passive and active Sybil attacks at a lower overhead, while allowing an incremental deployment.",CS,http://arxiv.org/abs/2505.01139v1
Poster: Machine Learning for Vulnerability Detection as Target Oracle in Automated Fuzz Driver Generation,"In vulnerability detection, machine learning has been used as an effective static analysis technique, although it suffers from a significant rate of false positives. Contextually, in vulnerability discovery, fuzzing has been used as an effective dynamic analysis technique, although it requires manually writing fuzz drivers. Fuzz drivers usually target a limited subset of functions in a library that must be chosen according to certain criteria, e.g., the depth of a function, the number of paths. These criteria are verified by components called target oracles. In this work, we propose an automated fuzz driver generation workflow composed of: (1) identifying a likely vulnerable function by leveraging a machine learning for vulnerability detection model as a target oracle, (2) automatically generating fuzz drivers, (3) fuzzing the target function to find bugs which could confirm the vulnerability inferred by the target oracle. We show our method on an existing vulnerability in libgd, with a plan for large-scale evaluation.",CS,http://arxiv.org/abs/2505.01123v1
A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories,"Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.",CS,http://arxiv.org/abs/2505.01067v1
Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation,"Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.",CS,http://arxiv.org/abs/2505.01065v1
Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services,"We propose a capability-based access control method that leverages OAuth 2.0 and Verifiable Credentials (VCs) to share resources in crowdsourced drone services. VCs securely encode claims about entities, offering flexibility. However, standardized protocols for VCs are lacking, limiting their adoption. To address this, we integrate VCs into OAuth 2.0, creating a novel access token. This token encapsulates VCs using JSON Web Tokens (JWT) and employs JWT-based methods for proof of possession. Our method streamlines VC verification with JSON Web Signatures (JWS) requires only minor adjustments to current OAuth 2.0 systems. Furthermore, in order to increase security and efficiency in multi-tenant environments, we provide a novel protocol for VC creation that makes use of the OAuth 2.0 client credentials grant. Using VCs as access tokens enhances OAuth 2.0, supporting long-term use and efficient data management. This system aids bushfire management authorities by ensuring high availability, enhanced privacy, and improved data portability. It supports multi-tenancy, allowing drone operators to control data access policies in a decentralized environment.",CS,http://arxiv.org/abs/2505.01048v1
Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory,"Security vulnerabilities in Windows Active Directory (AD) systems are typically modeled using an attack graph and hardening AD systems involves an iterative workflow: security teams propose an edge to remove, and IT operations teams manually review these fixes before implementing the removal. As verification requires significant manual effort, we formulate an Adaptive Path Removal Problem to minimize the number of steps in this iterative removal process. In our model, a wizard proposes an attack path in each step and presents it as a set of multiple-choice options to the IT admin. The IT admin then selects one edge from the proposed set to remove. This process continues until the target $t$ is disconnected from source $s$ or the number of proposed paths reaches $B$. The model aims to optimize the human effort by minimizing the expected number of interactions between the IT admin and the security wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then propose a set of solutions including an exact algorithm, an approximate algorithm, and several scalable heuristics. Our best heuristic, called DPR, can operate effectively on larger-scale graphs compared to the exact algorithm and consistently outperforms the approximate algorithm across all graphs. We verify the effectiveness of our algorithms on several synthetic AD graphs and an AD attack graph collected from a real organization.",CS,http://arxiv.org/abs/2505.01028v1
Quantum Support Vector Regression for Robust Anomaly Detection,"Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In recent years, Machine Learning (ML) algorithms have emerged as a powerful tool for AD in large-scale data. In this study, we explore the potential of quantum ML approaches, specifically quantum kernel methods, for the application to robust AD. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning and demonstrate that QSVR is highly vulnerable to adversarial attacks and that noise does not improve the adversarial robustness of the model.",CS,http://arxiv.org/abs/2505.01012v1
A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts,"Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.",CS,http://arxiv.org/abs/2505.00977v1
Attack and defense techniques in large language models: A survey and new perspectives,"Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",CS,http://arxiv.org/abs/2505.00976v1
Preserving Privacy and Utility in LLM-Based Product Recommendations,"Large Language Model (LLM)-based recommendation systems leverage powerful language models to generate personalized suggestions by processing user interactions and preferences. Unlike traditional recommendation systems that rely on structured data and collaborative filtering, LLM-based models process textual and contextual information, often using cloud-based infrastructure. This raises privacy concerns, as user data is transmitted to remote servers, increasing the risk of exposure and reducing control over personal information. To address this, we propose a hybrid privacy-preserving recommendation framework which separates sensitive from nonsensitive data and only shares the latter with the cloud to harness LLM-powered recommendations. To restore lost recommendations related to obfuscated sensitive data, we design a de-obfuscation module that reconstructs sensitive recommendations locally. Experiments on real-world e-commerce datasets show that our framework achieves almost the same recommendation utility with a system which shares all data with an LLM, while preserving privacy to a large extend. Compared to obfuscation-only techniques, our approach improves HR@10 scores and category distribution alignment, offering a better balance between privacy and recommendation quality. Furthermore, our method runs efficiently on consumer-grade hardware, making privacy-aware LLM-based recommendation systems practical for real-world use.",CS,http://arxiv.org/abs/2505.00951v1
Addressing Noise and Stochasticity in Fraud Detection for Service Networks,"Fraud detection is crucial in social service networks to maintain user trust and improve service network security. Existing spectral graph-based methods address this challenge by leveraging different graph filters to capture signals with different frequencies in service networks. However, most graph filter-based methods struggle with deriving clean and discriminative graph signals. On the one hand, they overlook the noise in the information propagation process, resulting in degradation of filtering ability. On the other hand, they fail to discriminate the frequency-specific characteristics of graph signals, leading to distortion of signals fusion. To address these issues, we develop a novel spectral graph network based on information bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB splits the original graph into homophilic and heterophilic subgraphs to better capture the signals at different frequencies. For the first limitation, SGNN-IB applies information bottleneck theory to extract key characteristics of encoded representations. For the second limitation, SGNN-IB introduces prototype learning to implement signal fusion, preserving the frequency-specific characteristics of signals. Extensive experiments on three real-world datasets demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.",CS,http://arxiv.org/abs/2505.00946v1
Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations,"The power of adaptivity in algorithms has been intensively studied in diverse areas of theoretical computer science. In this paper, we obtain a number of sharp lower bound results which show that adaptivity provides a significant extra power in cryptanalytic time-space tradeoffs with (possibly unlimited) preprocessing time.   Most notably, we consider the discrete logarithm (DLOG) problem in a generic group of $N$ elements. The classical `baby-step giant-step' algorithm for the problem has time complexity $T=O(\sqrt{N})$, uses $O(\sqrt{N})$ bits of space (up to logarithmic factors in $N$) and achieves constant success probability.   We examine a generalized setting where an algorithm obtains an advice string of $S$ bits and is allowed to make $T$ arbitrary non-adaptive queries that depend on the advice string (but not on the challenge group element).   We show that in this setting, the $T=O(\sqrt{N})$ online time complexity of the baby-step giant-step algorithm cannot be improved, unless the advice string is more than $\Omega(\sqrt{N})$ bits long. This lies in stark contrast with the classical adaptive Pollard's rho algorithm for DLOG, which can exploit preprocessing to obtain the tradeoff curve $ST^2=O(N)$. We obtain similar sharp lower bounds for several other cryptanalytic problems.   To obtain our results, we present a new model that allows analyzing non-adaptive preprocessing algorithms for a wide array of search and decision problems in a unified way. Since previous proof techniques inherently cannot distinguish between adaptive and non-adaptive algorithms for the problems in our model, they cannot be used to obtain our results. Consequently, our proof uses a variant of Shearer's lemma for this setting, due to Barthe, Cordero-Erausquin, Ledoux, and Maurey (2011). This seems to be the first time a variant of Shearer's lemma for permutations is used in an algorithmic context.",CS,http://arxiv.org/abs/2505.00894v1
Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting,"As new project upgrading the blockchain industry, novel forms of attack challenges developers to rethink about the design of their innovations. In the growth stage of the development, Decentralized Autonomous Organizations (DAO) introduces different approaches in managing fund through voting in governance tokens. However, relying on tokens as a weight for voting introduces opportunities for hackers to manipulate voting results through flash loan, allowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to execute through the smart contract. In this research, we learned different defense mechanism against the flash loan attack, and their weakness in accessibility that compromise the security of different blockchain projects. Based on our observation, we propose a new defensing structure and apply it with cases.",CS,http://arxiv.org/abs/2505.00888v2
Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting,"While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.",CS,http://arxiv.org/abs/2505.00881v1
Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme,"This study investigates a duality approach to information leak detection in the generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme. While previous work by Chamon and Kish sampled voltages at zero-current instances, this research explores sampling currents at zero-voltage crossings. The objective is to determine if this dual approach can reveal information leaks in non-equilibrium KLJN systems. Results indicate that the duality method successfully detects information leaks, further supporting the necessity of thermal equilibrium for unconditional security in KLJN systems.",CS,http://arxiv.org/abs/2505.00858v1
TherMod Communication: Low Power or Hot Air?,"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages statistical physics to enable secure communication with zero average power flow in a wired channel. While the original KLJN scheme requires significant power for operation, a recent wireless modification, TherMod, proposed by Basar claims a ""low power"" implementation. This paper critically examines this claim. We explain that the additional components inherent in Basar's wireless adaptation substantially increase power consumption, rendering the ""low power"" assertion inappropriate. Furthermore, we clarify that the security claims of the original KLJN scheme do not directly translate to this wireless adaptation, implying significant security breach. Finally, the scheme looks identical one of the stealth communicators from 2005, which was shown not to be secure.",CS,http://arxiv.org/abs/2505.00849v1
OET: Optimization-based prompt injection Evaluation Toolkit,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.",CS,http://arxiv.org/abs/2505.00843v1
From Texts to Shields: Convergence of Large Language Models and Cybersecurity,"This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.",CS,http://arxiv.org/abs/2505.00841v1
Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models,"Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.   A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.   Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.",CS,http://arxiv.org/abs/2505.00817v1
Enhancing the Cloud Security through Topic Modelling,"Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline.",CS,http://arxiv.org/abs/2505.01463v1
Auditing without Leaks Despite Curiosity,"\textit{Auditing} data accesses helps preserve privacy and ensures accountability by allowing one to determine who accessed (potentially sensitive) information. A prior formal definition of register auditability was based on the values returned by read operations, \emph{without accounting for cases where a reader might learn a value without explicitly reading it or gain knowledge of data access without being an auditor}.   This paper introduces a refined definition of auditability that focuses on when a read operation is \emph{effective}, rather than relying on its completion and return of a value. Furthermore, we formally specify the constraints that \textit{prevent readers from learning values they did not explicitly read or from auditing other readers' accesses.}   Our primary algorithmic contribution is a wait-free implementation of a \emph{multi-writer, multi-reader register} that tracks effective reads while preventing unauthorized audits. The key challenge is ensuring that a read is auditable as soon as it becomes effective, which we achieve by combining value access and access logging into a single atomic operation. Another challenge is recording accesses without exposing them to readers, which we address using a simple encryption technique (one-time pad).   We extend this implementation to an \emph{auditable max register} that tracks the largest value ever written. The implementation deals with the additional challenge posed by the max register semantics, which allows readers to learn prior values without reading them.   The max register, in turn, serves as the foundation for implementing an \emph{auditable snapshot} object and, more generally, \emph{versioned types}. These extensions maintain the strengthened notion of auditability, appropriately adapted from multi-writer, multi-reader registers.",CS,http://arxiv.org/abs/2505.00665v1
Key exchange protocol based on circulant matrix action over congruence-simple semiring,"We present a new key exchange protocol based on circulant matrices acting on matrices over a congruence-simple semiring. We describe how to compute matrices with the necessary properties for the implementation of the protocol. Additionally, we provide an analysis of its computational cost and its security against known attacks.",CS,http://arxiv.org/abs/2505.00664v1
RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks,"Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks.   In this paper, we propose RevealNet, a decentralized framework for attack attribution that orchestrates a fleet of P4-programmable switches to perform traffic correlation. RevealNet builds on a set of correlation primitives inspired by prior work on computing and comparing flow sketches -- compact summaries of flows' key characteristics -- to enable efficient, distributed, in-network traffic correlation. Our evaluation suggests that RevealNet achieves comparable accuracy to centralized attack attribution systems while significantly reducing both the computational complexity and bandwidth overheads imposed by correlation tasks.",CS,http://arxiv.org/abs/2505.00618v1
A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks,"The security of image data in the Internet of Things (IoT) and edge networks is crucial due to the increasing deployment of intelligent systems for real-time decision-making. Traditional encryption algorithms such as AES and RSA are computationally expensive for resource-constrained IoT devices and ineffective for large-volume image data, leading to inefficiencies in privacy-preserving distributed learning applications. To address these concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic Chain Permutation and Confusion mechanisms to enhance security while maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS, which extracts and reorganizes pixels based on high and low edge intensity features for correlation disruption; (2) Chaotic Chain Permutation, which employs a logistic chaotic map with SHA-256-based dynamically updated keys for block-wise permutation; and (3) Chaotic chain Confusion, which utilises dynamically generated chaotic seed matrices for bitwise XOR operations. Extensive security and performance evaluations demonstrate that the proposed scheme significantly reduces pixel correlation -- almost zero, achieves high entropy values close to 8, and resists differential cryptographic attacks. The optimum design of the proposed scheme makes it suitable for real-time deployment in resource-constrained environments.",CS,http://arxiv.org/abs/2505.00593v1
Notes on Univariate Sumcheck,These notes describe an adaptation of the multivariate sumcheck protocol to univariate polynomials interpolated over roots of unity.,CS,http://arxiv.org/abs/2505.00554v1
Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks,"This article describes the process of creating a script and conducting an analytical study of a dataset using the DeepMIMO emulator. An advertorial attack was carried out using the FGSM method to maximize the gradient. A comparison is made of the effectiveness of binary classifiers in the task of detecting distorted data. The dynamics of changes in the quality indicators of the regression model were analyzed in conditions without adversarial attacks, during an adversarial attack and when the distorted data was isolated. It is shown that an adversarial FGSM attack with gradient maximization leads to an increase in the value of the MSE metric by 33% and a decrease in the R2 indicator by 10% on average. The LightGBM binary classifier effectively identifies data with adversarial anomalies with 98% accuracy. Regression machine learning models are susceptible to adversarial attacks, but rapid analysis of network traffic and data transmitted over the network makes it possible to identify malicious activity",CS,http://arxiv.org/abs/2505.00487v1
Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services,"Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder",CS,http://arxiv.org/abs/2505.01460v1
"Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management","This paper proposes a decentralized, blockchain-based system for the publication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate the limitations of the current centralized model primarily overseen by MITRE. The proposed architecture leverages a permissioned blockchain, wherein only authenticated CVE Numbering Authorities (CNAs) are authorized to submit entries. This ensures controlled write access while preserving public transparency. By incorporating smart contracts, the system supports key features such as embargoed disclosures and decentralized governance. We evaluate the proposed model in comparison with existing practices, highlighting its advantages in transparency, trust decentralization, and auditability. A prototype implementation using Hyperledger Fabric is presented to demonstrate the feasibility of the approach, along with a discussion of its implications for the future of vulnerability disclosure.",CS,http://arxiv.org/abs/2505.00480v1
HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment,"Windows operating systems (OS) are ubiquitous in enterprise Information Technology (IT) and operational technology (OT) environments. Due to their widespread adoption and known vulnerabilities, they are often the primary targets of malware and ransomware attacks. With 93% of the ransomware targeting Windows-based systems, there is an urgent need for advanced defensive mechanisms to detect, analyze, and mitigate threats effectively. In this paper, we propose HoneyWin a high-interaction Windows honeypot that mimics an enterprise IT environment. The HoneyWin consists of three Windows 11 endpoints and an enterprise-grade gateway provisioned with comprehensive network traffic capturing, host-based logging, deceptive tokens, endpoint security and real-time alerts capabilities. The HoneyWin has been deployed live in the wild for 34 days and receives more than 5.79 million unsolicited connections, 1.24 million login attempts, 5 and 354 successful logins via remote desktop protocol (RDP) and secure shell (SSH) respectively. The adversary interacted with the deceptive token in one of the RDP sessions and exploited the public-facing endpoint to initiate the Simple Mail Transfer Protocol (SMTP) brute-force bot attack via SSH sessions. The adversary successfully harvested 1,250 SMTP credentials after attempting 151,179 credentials during the attack.",CS,http://arxiv.org/abs/2505.00465v1
Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication,"Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication.",CS,http://arxiv.org/abs/2505.00340v1
PatchFuzz: Patch Fuzzing for JavaScript Engines,"Patch fuzzing is a technique aimed at identifying vulnerabilities that arise from newly patched code. While researchers have made efforts to apply patch fuzzing to testing JavaScript engines with considerable success, these efforts have been limited to using ordinary test cases or publicly available vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of these approaches is hindered by the challenges associated with automating the PoC collection. To address these limitations, we propose an end-to-end sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It automates the collection of PoCs of a broader range of historical vulnerabilities and leverages both the PoCs and their corresponding patches to uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing git commits which intend to fix security bugs. Subsequently, it extracts and processes PoCs from these commits to form the seeds for fuzzing, while utilizing code revisions to focus limited fuzzing resources on the more vulnerable code areas through selective instrumentation. The mutation strategy of PatchFuzz is also optimized to maximize the potential of the PoCs. Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54 bugs across six popular JavaScript engines have been exposed and a total of $62,500 bounties has been received.",CS,http://arxiv.org/abs/2505.00289v1
Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation,"The sharing of external data has become a strong demand of financial institutions, but the privacy issue has led to the difficulty of interconnecting different platforms and the low degree of data openness. To effectively solve the privacy problem of financial data in trans-border flow and sharing, to ensure that the data is available but not visible, to realize the joint portrait of all kinds of heterogeneous data of business organizations in different industries, we propose a Heterogeneous Federated Graph Neural Network (HFGNN) approach. In this method, the distribution of heterogeneous business data of trans-border organizations is taken as subgraphs, and the sharing and circulation process among subgraphs is constructed as a statistically heterogeneous global graph through a central server. Each subgraph learns the corresponding personalized service model through local training to select and update the relevant subset of subgraphs with aggregated parameters, and effectively separates and combines topological and feature information among subgraphs. Finally, our simulation experimental results show that the proposed method has higher accuracy performance and faster convergence speed than existing methods.",CS,http://arxiv.org/abs/2505.00257v1
LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems,"The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.",CS,http://arxiv.org/abs/2505.00240v1
Moral Testing of Autonomous Driving Systems,"Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS.",CS,http://arxiv.org/abs/2505.03683v1
ATRAF-driven IMRaD Methodology: Tradeoff and Risk Analysis of Software Architectures Across Abstraction Levels,"Software architecture research relies on key architectural artifacts -- Software Architectures, Reference Architectures, and Architectural Frameworks -- that underpin the design and analysis of complex systems. Evaluating these artifacts is essential to assess tradeoffs and risks affecting quality attributes such as performance, modifiability, and security. Although methodologies like the Architecture Tradeoff Analysis Method (ATAM) support software architecture evaluation, their industrial focus misaligns with the IMRaD (Introduction, Methods, Results, Discussion) format prevalent in academic research, impeding transparency and reproducibility. Our prior work introduced the Architecture Tradeoff and Risk Analysis Framework (ATRAF), extending ATAM through three methods -- ATRAM, RATRAM, and AFTRAM, addressing all abstraction levels, using a unified, iterative four-phase spiral model. These phases -- Scenario and Requirements Gathering, Architectural Views and Scenario Realization, Attribute-Specific Analyses, and Sensitivity, Tradeoff, and Risk Analysis -- ensure traceability and coherence. This paper presents the ATRAF-driven IMRaD Methodology, a concise method to align ATRAF's phases with IMRaD sections. This methodology enhances the rigor, transparency, and accessibility of software architecture research, enabling systematic reporting of complex evaluations.",CS,http://arxiv.org/abs/2505.03624v1
Qimax: Efficient quantum simulation via GPU-accelerated extended stabilizer formalism,"Simulating Clifford and near-Clifford circuits using the extended stabilizer formalism has become increasingly popular, particularly in quantum error correction. Compared to the state-vector approach, the extended stabilizer formalism can solve the same problems with fewer computational resources, as it operates on stabilizers rather than full state vectors. Most existing studies on near-Clifford circuits focus on balancing the trade-off between the number of ancilla qubits and simulation accuracy, often overlooking performance considerations. Furthermore, in the presence of high-rank stabilizers, performance is limited by the sequential property of the stabilizer formalism. In this work, we introduce a parallelized version of the extended stabilizer formalism, enabling efficient execution on multi-core devices such as GPU. Experimental results demonstrate that, in certain scenarios, our Python-based implementation outperforms state-of-the-art simulators such as Qiskit and Pennylane.",CS,http://arxiv.org/abs/2505.03307v1
Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis,"The field of deep learning has witnessed significant breakthroughs, spanning various applications, and fundamentally transforming current software capabilities. However, alongside these advancements, there have been increasing concerns about reproducing the results of these deep learning methods. This is significant because reproducibility is the foundation of reliability and validity in software development, particularly in the rapidly evolving domain of deep learning. The difficulty of reproducibility may arise due to several reasons, including having differences from the original execution environment, incompatible software libraries, proprietary data and source code, lack of transparency, and the stochastic nature in some software. A study conducted by the Nature journal reveals that more than 70% of researchers failed to reproduce other researchers experiments and over 50% failed to reproduce their own experiments. Irreproducibility of deep learning poses significant challenges for researchers and practitioners. To address these concerns, this paper presents a systematic approach at analyzing and improving the reproducibility of deep learning models by demonstrating these guidelines using a case study. We illustrate the patterns and anti-patterns involved with these guidelines for improving the reproducibility of deep learning models. These guidelines encompass establishing a methodology to replicate the original software environment, implementing end-to-end training and testing algorithms, disclosing architectural designs, and enhancing transparency in data processing and training pipelines. We also conduct a sensitivity analysis to understand the model performance across diverse conditions. By implementing these strategies, we aim to bridge the gap between research and practice, so that innovations in deep learning can be effectively reproduced and deployed within software.",CS,http://arxiv.org/abs/2505.03165v1
An Empirical Study on the Impact of Gender Diversity on Code Quality in AI Systems,"The rapid advancement of AI systems necessitates high-quality, sustainable code to ensure reliability and mitigate risks such as bias and technical debt. However, the underrepresentation of women in software engineering raises concerns about homogeneity in AI development. Studying gender diversity in AI systems is crucial, as diverse perspectives are essential for improving system robustness, reducing bias, and enhancing overall code quality. While prior research has demonstrated the benefits of diversity in general software teams, its specific impact on the code quality of AI systems remains unexplored. This study addresses this gap by examining how gender diversity within AI teams influences project popularity, code quality, and individual contributions. Our study makes three key contributions. First, we analyzed the relationship between team diversity and repository popularity, revealing that diverse AI repositories not only differ significantly from non-diverse ones but also achieve higher popularity and greater community engagement. Second, we explored the effect of diversity on the overall code quality of AI systems and found that diverse repositories tend to have superior code quality compared to non-diverse ones. Finally, our analysis of individual contributions revealed that although female contributors contribute to a smaller proportion of the total code, their contributions demonstrate consistently higher quality than those of their male counterparts. These findings highlight the need to remove barriers to female participation in AI development, as greater diversity can improve the overall quality of AI systems.",CS,http://arxiv.org/abs/2505.03082v1
Testing SSD Firmware with State Data-Aware Fuzzing: Accelerating Coverage in Nondeterministic I/O Environments,"Solid-State Drive (SSD) firmware manages complex internal states, including flash memory maintenance. Due to nondeterministic I/O operations, traditional testing methods struggle to rapidly achieve coverage of firmware code areas that require extensive I/O accumulation. To address this challenge, we propose a state data-aware fuzzing approach that leverages SSD firmware's internal state to guide input generation under nondeterministic I/O conditions and accelerate coverage discovery. Our experiments with an open-source SSD firmware emulator show that the proposed method achieves the same firmware test coverage as a state-of-the-art coverage-based fuzzer (AFL++) while requiring approximately 67% fewer commands, without reducing the number of crashes or hangs detected. Moreover, we extend our experiments by incorporating various I/O commands beyond basic write/read operations to reflect real user scenarios, and we confirm that our strategy remains effective even for multiple types of I/O tests. We further validate the effectiveness of state data-aware fuzzing for firmware testing under I/O environments and suggest that this approach can be extended to other storage firmware or threshold-based embedded systems in the future.",CS,http://arxiv.org/abs/2505.03062v1
Can We Recycle Our Old Models? An Empirical Evaluation of Model Selection Mechanisms for AIOps Solutions,"AIOps (Artificial Intelligence for IT Operations) solutions leverage the tremendous amount of data produced during the operation of large-scale systems and machine learning models to assist software practitioners in their system operations. Existing AIOps solutions usually maintain AIOps models against concept drift through periodical retraining, despite leaving a pile of discarded historical models that may perform well on specific future data. Other prior works propose dynamically selecting models for prediction tasks from a set of candidate models to optimize the model performance. However, there is no prior work in the AIOps area that assesses the use of model selection mechanisms on historical models to improve model performance or robustness. To fill the gap, we evaluate several model selection mechanisms by assessing their capabilities in selecting the optimal AIOps models that were built in the past to make predictions for the target data. We performed a case study on three large-scale public operation datasets: two trace datasets from the cloud computing platforms of Google and Alibaba, and one disk stats dataset from the BackBlaze cloud storage data center. We observe that the model selection mechnisms utilizing temporal adjacency tend to have a better performance and can prevail the periodical retraining approach. Our findings also highlight a performance gap between existing model selection mechnisms and the theoretical upper bound which may motivate future researchers and practitioners in investigating more efficient and effective model selection mechanisms that fit in the context of AIOps.",CS,http://arxiv.org/abs/2505.02961v1
A Unifying Framework to Enable Artificial Intelligence in High Performance Computing Workflows,"Current trends point to a future where large-scale scientific applications are tightly-coupled HPC/AI hybrids. Hence, we urgently need to invest in creating a seamless, scalable framework where HPC and AI/ML can efficiently work together and adapt to novel hardware and vendor libraries without starting from scratch every few years. The current ecosystem and sparsely-connected community are not sufficient to tackle these challenges, and we require a breakthrough catalyst for science similar to what PyTorch enabled for AI.",CS,http://arxiv.org/abs/2505.02738v1
Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for Automated Patch Correctness Assessment,"Automated program repair (APR) aims to automatically repair program errors without human intervention, and recent years have witnessed a growing interest on this research topic. While much progress has been made and techniques originating from different disciplines have been proposed, APR techniques generally suffer from the patch overfitting issue, i.e., the generated patches are not genuinely correct despite they pass the employed tests. To alleviate this issue, many research efforts have been devoted for automated patch correctness assessment (APCA). In particular, with the emergence of large language model (LLM) technology, researchers have employed LLM to assess the patch correctness and have obtained the state-of-the-art performance. The literature on APCA has demonstrated the importance of capturing patch semantic and explicitly considering certain code attributes in predicting patch correctness. However, existing LLM-based methods typically treat code as token sequences and ignore the inherent formal structure for code, making it difficult to capture the deep patch semantics. Moreover, these LLM-based methods also do not explicitly account for enough code attributes. To overcome these drawbacks, we in this paper design a novel patch graph representation named attributed patch semantic graph (APSG), which adequately captures the patch semantic and explicitly reflects important patch attributes. To effectively use graph information in APSG, we accordingly propose a new parameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA. Extensive evaluations have been conducted to evaluate our method, and the results show that compared to the state-of-the-art methods, our method improves accuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.",CS,http://arxiv.org/abs/2505.02629v1
SynQ: An Embedded DSL for Synchronous System Design with Quantitative Types,"System design automation aims to manage the design of embedded systems with ever-increasing complexity. To the success of system design automation, there is still a lack of systematic and formal design process because an entire design process, from a system's specification to its implementation, has to deal with inherent concerns about the systems' different aspects and, consequently, inherent semantic gaps. These gaps make it hard for a design process to be traceable or transparent. Particularly, guaranteeing the correctness of produced implementations becomes the main challenge for a system design process.   SynQ (Synchronous system design with Quantitative types) is an embedded domain specification language (EDSL) targeting the design of systems obeying the perfect synchrony hypothesis. SynQ is based on a component-based design framework and, by design, facilitates semantic coherency by leveraging the quantitative type theory (QTT) and language embedding. SynQ enables a semantically coherent design process, including formal specification and verification, modelling, simulation and code generation. This paper presents SynQ and its underlying formalism and demonstrates its features and potential for semantically coherent system design through a case study.",CS,http://arxiv.org/abs/2505.02883v1
Automating Automotive Software Development: A Synergy of Generative AI and Formal Methods,"As the automotive industry shifts its focus toward software-defined vehicles, the need for faster and reliable software development continues to grow. However, traditional methods show their limitations. The rise of Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), introduces new opportunities to automate automotive software development tasks such as requirement analysis and code generation. However, due to the complexity of automotive systems, where software components must interact with each other seamlessly, challenges remain in software integration and system-level validation. In this paper, we propose to combine GenAI with model-driven engineering to automate automotive software development. Our approach uses LLMs to convert free-text requirements into event chain descriptions and to generate platform-independent software components that realize the required functionality. At the same time, formal models are created based on event chain descriptions to support system validation and the generation of integration code for integrating generated software components in the whole vehicle system through middleware. This approach increases development automation while enabling formal analysis to improve system reliability. As a proof of concept, we used GPT-4o to implement our method and tested it in the CARLA simulation environment with ROS2 middleware. We evaluated the system in a simple Autonomous Emergency Braking scenario.",CS,http://arxiv.org/abs/2505.02500v1
Beyond the model: Key differentiators in large language models and multi-agent services,"With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it has become evident that large language models (LLMs) are no longer the sole defining factor in generative AI. As many now operate at comparable levels of capability, the real race is not about having the biggest model but optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article delves into these critical differentiators that ensure modern AI services are efficient and profitable.",CS,http://arxiv.org/abs/2505.02489v1
"Running a Data Integration Lab in the Context of the EHRI Project: Challenges, Lessons Learnt and Future Directions","Historical study of the Holocaust is commonly hampered by the dispersed and fragmented nature of important archival sources relating to this event. The EHRI project set out to mitigate this problem by building a trans-national network of archives, researchers, and digital practitioners, and one of its main outcomes was the creation of the EHRI Portal, a ""virtual observatory"" that gathers in one centralised platform descriptions of Holocaust-related archival sources from around the world. In order to build the Portal a strong data identification and integration effort was required, culminating in the project's third phase with the creation of the EHRI-3 data integration lab. The focus of the lab was to lower the bar to participation in the EHRI Portal by providing support to institutions in conforming their archival metadata with that required for integration, ultimately opening the process up to smaller institutions (and even so-called ""micro-archives"") without the necessary resources to undertake this process themselves. In this paper we present our experiences from running the data integration lab and discuss some of the challenges (both of a technical and social nature), how we tried to overcome them, and the overall lessons learnt. We envisage this work as an archetype upon which other practitioners seeking to pursue similar data integration activities can build their own efforts.",CS,http://arxiv.org/abs/2505.02455v1
Towards Effective Issue Assignment using Online Machine Learning,"Efficient issue assignment in software development relates to faster resolution time, resources optimization, and reduced development effort. To this end, numerous systems have been developed to automate issue assignment, including AI and machine learning approaches. Most of them, however, often solely focus on a posteriori analyses of textual features (e.g. issue titles, descriptions), disregarding the temporal characteristics of software development. Thus, they fail to adapt as projects and teams evolve, such cases of team evolution, or project phase shifts (e.g. from development to maintenance). To incorporate such cases in the issue assignment process, we propose an Online Machine Learning methodology that adapts to the evolving characteristics of software projects. Our system processes issues as a data stream, dynamically learning from new data and adjusting in real time to changes in team composition and project requirements. We incorporate metadata such as issue descriptions, components and labels and leverage adaptive drift detection mechanisms to identify when model re-evaluation is necessary. Upon assessing our methodology on a set of software projects, we conclude that it can be effective on issue assignment, while meeting the evolving needs of software teams.",CS,http://arxiv.org/abs/2505.02437v1
LAMeD: LLM-generated Annotations for Memory Leak Detection,"Static analysis tools are widely used to detect software bugs and vulnerabilities but often struggle with scalability and efficiency in complex codebases. Traditional approaches rely on manually crafted annotations -- labeling functions as sources or sinks -- to track data flows, e.g., ensuring that allocated memory is eventually freed, and code analysis tools such as CodeQL, Infer, or Cooddy can use function specifications, but manual annotation is laborious and error-prone, especially for large or third-party libraries. We present LAMeD (LLM-generated Annotations for Memory leak Detection), a novel approach that leverages large language models (LLMs) to automatically generate function-specific annotations. When integrated with analyzers such as Cooddy, LAMeD significantly improves memory leak detection and reduces path explosion. We also suggest directions for extending LAMeD to broader code analysis.",CS,http://arxiv.org/abs/2505.02376v1
RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search,"Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash.   Prior works use random testing like fuzzing, but these are not effective in the three-dimensional search space of PID parameters. The expensive dynamic execution of UAV tests further hinders random testing performance.   We address PID parameter misconfiguration by combining the Routh-Hurwitz stability criterion with coordinate search, introducing RouthSearch. Instead of ad-hoc identification, RouthSearch principledly determines valid ranges for three-dimensional PID parameters. We first leverage the Routh-Hurwitz Criterion to identify a theoretical PID parameter boundary, then refine it using efficient coordinate search. The determined valid range can filter misconfigured PID parameters from users during flight and help discover logical bugs in flight control programs.   We evaluated RouthSearch across eight flight modes in PX4 and Ardupilot. Results show RouthSearch determines valid ranges with 92.0% accuracy compared to ground truth. RouthSearch discovers 3,853 PID misconfigurations within 48 hours, while the STOA work PGFuzz discovers only 449 sets, significantly outperforming prior works by 8.58 times. Our method also helped detect three bugs in ArduPilot and PX4.",CS,http://arxiv.org/abs/2505.02357v1
An Empirical Study on the Performance and Energy Usage of Compiled Python Code,"Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.",CS,http://arxiv.org/abs/2505.02346v1
Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling,"The impacts of algorithmic management (AM) on worker well-being have led to increasing calls to regulate AM practices to prevent further worker harms. Yet existing work in aligning software with the law reduces compliance to just one piece of the entire process of regulating AM -- which involves rule operationalization, software use, and enforcement. We interviewed key stakeholders involved in enforcing or complying with workplace scheduling law -- regulators, advocates, defense attorneys, scheduling managers, and workers ($N = 38$). Based on their beliefs and experiences, we describe how scheduling software affects beliefs about and compliance with workplace scheduling law. In so doing, we discuss the challenges and opportunities in designing software as a tool for regulating AM.",CS,http://arxiv.org/abs/2505.02329v1
Refining Fuzzed Crashing Inputs for Better Fault Diagnosis,"We present DiffMin, a technique that refines a fuzzed crashing input to gain greater similarities to given passing inputs to help developers analyze the crashing input to identify the failure-inducing condition and locate buggy code for debugging. DiffMin iteratively applies edit actions to transform a fuzzed input while preserving the crash behavior. Our pilot study with the Magma benchmark demonstrates that DiffMin effectively minimizes the differences between crashing and passing inputs while enhancing the accuracy of spectrum-based fault localization, highlighting its potential as a valuable pre-debugging step after greybox fuzzing.",CS,http://arxiv.org/abs/2505.02305v2
A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm,"The emergence of Large Code Models (LCMs) has transformed software engineering (SE) automation, driving significant advancements in tasks such as code generation, source code documentation, code review, and bug fixing. However, these advancements come with trade-offs: achieving high performance often entails exponential computational costs, reduced interpretability, and an increasing dependence on data-intensive models with hundreds of billions of parameters. In this paper, we propose Neurosymbolic Software Engineering, in short NSE, as a promising paradigm combining neural learning with symbolic (rule-based) reasoning, while strategically introducing a controlled source of chaos to simulate the complex dynamics of real-world software systems. This hybrid methodology aims to enhance efficiency, reliability, and transparency in AI-driven software engineering while introducing controlled randomness to adapt to evolving requirements, unpredictable system behaviors, and non-deterministic execution environments. By redefining the core principles of AI-driven software engineering automation, NSE lays the groundwork for solutions that are more adaptable, transparent, and closely aligned with the evolving demands of modern software development practices.",CS,http://arxiv.org/abs/2505.02275v1
On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles,"Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.",CS,http://arxiv.org/abs/2505.02274v1
Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes,"While large language models (LLMs) are increasingly used for generating parallel scientific code, most current efforts emphasize functional correctness, often overlooking performance and energy considerations. In this work, we propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel code on a target parallel system for a given parallel code as input. Through a multi-stage, iterative pipeline process, LASSI-EE achieved an average energy reduction of 47% across 85% of the 20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the broader potential of LLMs, not only for generating correct code but also for enabling energy-aware programming. We also address key insights and limitations within the framework, offering valuable guidance for future improvements.",CS,http://arxiv.org/abs/2505.02184v1
Proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins,"This volume contains the proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins (ASQAP 2025), which was held in Hamilton, Canada, on May 4th, 2025, as a satellite event of ETAPS 2025. The aim of ASQAP 2025 is to gather experts from academia and industry to explore the potential of digital twin technology in supporting quality assurance in autonomous systems, including concepts such as specification, verification, validation, testing, analysis, and many others.",CS,http://arxiv.org/abs/2505.02873v1
Large Language Models are overconfident and amplify human bias,"Large language models (LLMs) are revolutionizing every aspect of society. They are increasingly used in problem-solving tasks to substitute human assessment and reasoning. LLMs are trained on what humans write and thus prone to learn human biases. One of the most widespread human biases is overconfidence. We examine whether LLMs inherit this bias. We automatically construct reasoning problems with known ground truths, and prompt LLMs to assess the confidence in their answers, closely following similar protocols in human experiments. We find that all five LLMs we study are overconfident: they overestimate the probability that their answer is correct between 20% and 60%. Humans have accuracy similar to the more advanced LLMs, but far lower overconfidence. Although humans and LLMs are similarly biased in questions which they are certain they answered correctly, a key difference emerges between them: LLM bias increases sharply relative to humans if they become less sure that their answers are correct. We also show that LLM input has ambiguous effects on human decision making: LLM input leads to an increase in the accuracy, but it more than doubles the extent of overconfidence in the answers.",CS,http://arxiv.org/abs/2505.02151v1
"Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency","The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",CS,http://arxiv.org/abs/2505.02133v1
Requirements-Based Test Generation: A Comprehensive Survey,"As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirement types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.",CS,http://arxiv.org/abs/2505.02015v1
Testing Database Systems with Large Language Model Synthesized Fragments,"Various automated testing approaches have been proposed for Database Management Systems (DBMSs). Many such approaches generate pairs of equivalent queries to identify bugs that cause DBMSs to compute incorrect results, and have found hundreds of bugs in mature, widely used DBMSs. Most of these approaches are based on manually written SQL generators; however, their bug-finding capabilities remain constrained by the limited set of SQL features supported by the generators. In this work, we propose ShQveL, an approach that augments existing SQL test-case generators by leveraging Large Language Models (LLMs) to synthesize SQL fragments. Our key idea is to systematically incorporate SQL features gained through automated interactions with LLMs into the SQL generators, increasing the features covered while efficiently generating test cases. Specifically, ShQveL uses SQL sketches -- SQL statements with incomplete code segments that LLMs fill -- to integrate LLM-generated content into the generator. We evaluated ShQveL on 5 DBMSs and discovered 55 unique and previously unknown bugs, 50 of which were promptly fixed after our reports.",CS,http://arxiv.org/abs/2505.02012v1
Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach,"UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.",CS,http://arxiv.org/abs/2505.01947v1
One Documentation Does Not Fit All: Case Study of TensorFlow Documentation,"Software documentation guides the proper use of tools or services. With the rapid growth of machine learning libraries, individuals from various fields are incorporating machine learning into their workflows through programming. However, many of these users lack software engineering experience, affecting the usability of the documentation. Traditionally, software developers have created documentation primarily for their peers, making it challenging for others to interpret and effectively use these resources. Moreover, no study has specifically focused on machine learning software documentation or analyzing the backgrounds of developers who rely on such documentation, highlighting a critical gap in understanding how to make these resources more accessible. This study examined customization trends in TensorFlow tutorials and compared these artifacts to analyze content and design differences. We also analyzed Stack Overflow questions related to TensorFlow documentation to understand the types of questions and the backgrounds of the developers asking them. Further, we developed two taxonomies based on the nature and triggers of the questions for machine learning software. Our findings showed no significant differences in the content or the nature of the questions across different tutorials. Our results show that 24.9% of the questions concern errors and exceptions, while 64.3% relate to inadequate and non-generalizable examples in the documentation. Despite efforts to create customized documentation, our analysis indicates that current TensorFlow documentation does not effectively support its target users.",CS,http://arxiv.org/abs/2505.01939v1
Site Reliability Engineering (SRE) and Observations on SRE Process to Make Tasks Easier,"This paper explores Site Reliability Engineering (SRE), a modern approach to maintaining scalable and reliable software systems. It presents observations on how structured SRE processes improve operational efficiency, reduce system downtime, and simplify maintenance. Drawing from real-world implementations, the study outlines key techniques in automation, monitoring, incident management, and deployment strategies. The work also highlights how these practices can be tailored to different environments, offering practical insights for engineers aiming to improve service reliability.",CS,http://arxiv.org/abs/2505.01926v1
ImageR: Enhancing Bug Report Clarity by Screenshots,"In issue-tracking systems, incorporating screenshots significantly enhances the clarity of bug reports, facilitating more efficient communication and expediting issue resolution. However, determining when and what type of visual content to include remains challenging, as not all attachments effectively contribute to problem-solving; studies indicate that 22.5% of images in issue reports fail to aid in resolving the reported issues. To address this, we introduce ImageR, an AI model and tool that analyzes issue reports to assess the potential benefits of including screenshots and recommends the most pertinent types when appropriate. By proactively suggesting relevant visuals, ImageR aims to make issue reports clearer, more informative, and time-efficient. We have curated and publicly shared a dataset comprising 6,235 Bugzilla issues, each meticulously labeled with the type of image attachment, providing a valuable resource for benchmarking and advancing research in image processing within developer communication contexts. To evaluate ImageR, we conducted empirical experiments on a subset of these reports from various Mozilla projects. The tool achieved an F1-score of 0.76 in determining when images are needed, with 75% of users finding its recommendations highly valuable. By minimizing the back-and-forth communication often needed to obtain suitable screenshots, ImageR streamlines the bug reporting process. Furthermore, it guides users in selecting the most effective visual documentation from ten established categories, potentially reducing resolution times and improving the quality of bug documentation. ImageR is open-source, inviting further use and improvement by the community. The labeled dataset offers a rare resource for benchmarking and exploring image processing in the context of developer communication.",CS,http://arxiv.org/abs/2505.01925v1
Certus: A domain specific language for confidence assessment in assurance cases,"Assurance cases (ACs) are prepared to argue that a system has satisfied critical quality attributes. Many methods exist to assess confidence in ACs, including quantitative methods that represent confidence numerically. While quantitative methods are attractive in principle, existing methods suffer from issues related to interpretation, subjectivity, scalability, dialectic reasoning, and trustworthiness, which have limited their adoption. This paper introduces Certus, a domain specific language for quantitative confidence assessment. In Certus, users describe their confidence with fuzzy sets, which allow them to represent their judgment using vague, but linguistically meaningful terminology. Certus includes syntax to specify confidence propagation using expressions that can be easily inspected by users. To demonstrate the concept of the language, Certus is applied to a worked example from the automotive domain.",CS,http://arxiv.org/abs/2505.01894v1
OODTE: A Differential Testing Engine for the ONNX Optimizer,"With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.",CS,http://arxiv.org/abs/2505.01892v1
LogDB: Multivariate Log-based Failure Diagnosis for Distributed Databases (Extended from MultiLog),"Distributed databases, as the core infrastructure software for internet applications, play a critical role in modern cloud services. However, existing distributed databases frequently experience system failures and performance degradation, often leading to significant economic losses. Log data, naturally generated within systems, can effectively reflect internal system states. In practice, operators often manually inspect logs to monitor system behavior and diagnose anomalies, a process that is labor-intensive and costly. Although various log-based failure diagnosis methods have been proposed, they are generally not tailored for database systems and fail to fully exploit the internal characteristics and distributed nature of these systems. To address this gap, we propose LogDB, a log-based failure diagnosis method specifically designed for distributed databases. LogDB extracts and compresses log features at each database node and then aggregates these features at the master node to diagnose cluster-wide anomalies. Experiments conducted on the open-source distributed database system Apache IoTDB demonstrate that LogDB achieves robust failure diagnosis performance across different workloads and a variety of anomaly types.",CS,http://arxiv.org/abs/2505.01676v1
A Defect Taxonomy for Infrastructure as Code: A Replication Study,"Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.",CS,http://arxiv.org/abs/2505.01568v1
Overcoming Obstacles: Challenges of Gender Inequality in Undergraduate ICT Programs,"Context: Gender inequality is a widely discussed issue across various sectors, including Information Technology and Communication (ICT). In Brazil, women represent less than 18% of ICT students in higher education. Prior studies highlight gender-related barriers that discourage women from staying in ICT. However, they provide limited insights into their perceptions as undergraduate students and the factors influencing their participation and confidence. Goal: This study explores the perceptions of women undergraduate students in ICT regarding gender inequality. Method: A survey of 402 women from 18 Brazilian states enrolled in ICT courses was conducted using a mixed-method approach, combining quantitative and qualitative analyses. Results: Women students reported experiencing discriminatory practices from peers and professors, both inside and outside the classroom. Gender stereotypes were found to undermine their self-confidence and self-esteem, occasionally leading to course discontinuation. Conclusions: Factors such as lack of representation, inappropriate jokes, isolation, mistrust, and difficulty being heard contribute to harmful outcomes, including reduced participation and reluctance to take leadership roles. Addressing these issues is essential to creating a safe and respectful learning environment for all students.",CS,http://arxiv.org/abs/2505.02857v1
Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments,"Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains.",CS,http://arxiv.org/abs/2505.01307v1
BiGSCoder: State Space Model for Code Understanding,"We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.",CS,http://arxiv.org/abs/2505.01475v1
Micro-Patterns in Solidity Code,"Solidity is the predominant programming language for blockchain-based smart contracts, and its characteristics pose significant challenges for code analysis and maintenance. Traditional software analysis approaches, while effective for conventional programming languages, often fail to address Solidity-specific features such as gas optimization and security constraints.   This paper introduces micro-patterns - recurring, small-scale design structures that capture key behavioral and structural peculiarities specific to a language - for Solidity language and demonstrates their value in understanding smart contract development practices. We identified 18 distinct micro-patterns organized in five categories (Security, Functional, Optimization, Interaction, and Feedback), detailing their characteristics to enable automated detection.   To validate this proposal, we analyzed a dataset of 23258 smart contracts from five popular blockchains (Ethereum, Polygon, Arbitrum, Fantom and Optimism). Our analysis reveals widespread adoption of micro-patterns, with 99% of contracts implementing at least one pattern and an average of 2.76 patterns per contract. The Storage Saver pattern showed the highest adoption (84.62% mean coverage), while security patterns demonstrated platform-specific adoption rates. Statistical analysis revealed significant platform-specific differences in pattern adoption, particularly in Borrower, Implementer, and Storage Optimization patterns.",CS,http://arxiv.org/abs/2505.01282v1
Design for a Digital Twin in Clinical Patient Care,"Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.",CS,http://arxiv.org/abs/2505.01206v1
An instrument to measure factors that constitute the socio-technical context of testing experience,"We consider testing a cooperative and social practice that is shaped by the tools developers use, the tests they write, and their mindsets and human needs. This work is one part of a project that explores the human- and socio-technical context of testing through the lens of those interwoven elements: test suite and tools as technical infrastructure and collaborative factors and motivation as mindset. Drawing on empirical observations of previous work, this survey examines how these factors relate to each other. We want to understand which combination of factors can help developers strive and make the most of their ambitions to leverage the potential that software testing practices have. In this report, we construct a survey instrument to measure the factors that constitute the socio-technical context of testing experience. In addition, we state our hypotheses about how these factors impact testing experience and explain the considerations and process that led to the construction of the survey questions.",CS,http://arxiv.org/abs/2505.01171v1
Automatic techniques for issue report classification: A systematic mapping study,"Several studies have evaluated automatic techniques for classifying software issue reports to assist practitioners in effectively assigning relevant resources based on the type of issue. Currently, no comprehensive overview of this area has been published. A comprehensive overview will help identify future research directions and provide an extensive collection of potentially relevant existing solutions. This study aims to provide a comprehensive overview of the use of automatic techniques to classify issue reports. We conducted a systematic mapping study and identified 46 studies on the topic.   The study results indicate that the existing literature applies various techniques for classifying issue reports, including traditional machine learning and deep learning-based techniques and more advanced large language models. Furthermore, we observe that these studies (a) lack the involvement of practitioners, (b) do not consider other potentially relevant adoption factors beyond prediction accuracy, such as the explainability, scalability, and generalizability of the techniques, and (c) mainly rely on archival data from open-source repositories only. Therefore, future research should focus on real industrial evaluations, consider other potentially relevant adoption factors, and actively involve practitioners.",CS,http://arxiv.org/abs/2505.01469v1
CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++,"In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.",CS,http://arxiv.org/abs/2505.01136v1
Evaluating the Impact of Data Cleaning on the Quality of Generated Pull Request Descriptions,"Pull Requests (PRs) are central to collaborative coding, summarizing code changes for reviewers. However, many PR descriptions are incomplete, uninformative, or have out-of-context content, compromising developer workflows and hindering AI-based generation models trained on commit messages and original descriptions as ""ground truth."" This study examines the prevalence of ""noisy"" PRs and evaluates their impact on state-of-the-art description generation models. To do so, we propose four cleaning heuristics to filter noise from an initial dataset of 169K+ PRs drawn from 513 GitHub repositories. We train four models-BART, T5, PRSummarizer, and iTAPE-on both raw and cleaned datasets. Performance is measured via ROUGE-1, ROUGE-2, and ROUGE-L metrics, alongside a manual evaluation to assess description quality improvements from a human perspective. Cleaning the dataset yields significant gains: average F1 improvements of 8.6% (ROUGE-1), 8.7% (ROUGE-2), and 8.5% (ROUGE-L). Manual assessment confirms higher readability and relevance in descriptions generated by the best-performing model, BART when trained on cleaned data. Dataset refinement markedly enhances PR description generation, offering a foundation for more accurate AI-driven tools and guidelines to assist developers in crafting high-quality PR descriptions.",CS,http://arxiv.org/abs/2505.01120v1
Towards an Interpretable Analysis for Estimating the Resolution Time of Software Issues,"Lately, software development has become a predominantly online process, as more teams host and monitor their projects remotely. Sophisticated approaches employ issue tracking systems like Jira, predicting the time required to resolve issues and effectively assigning and prioritizing project tasks. Several methods have been developed to address this challenge, widely known as bug-fix time prediction, yet they exhibit significant limitations. Most consider only textual issue data and/or use techniques that overlook the semantics and metadata of issues (e.g., priority or assignee expertise). Many also fail to distinguish actual development effort from administrative delays, including assignment and review phases, leading to estimates that do not reflect the true effort needed. In this work, we build an issue monitoring system that extracts the actual effort required to fix issues on a per-project basis. Our approach employs topic modeling to capture issue semantics and leverages metadata (components, labels, priority, issue type, assignees) for interpretable resolution time analysis. Final predictions are generated by an aggregated model, enabling contributors to make informed decisions. Evaluation across multiple projects shows the system can effectively estimate resolution time and provide valuable insights.",CS,http://arxiv.org/abs/2505.01108v1
Detecting the Root Cause Code Lines in Bug-Fixing Commits by Heterogeneous Graph Learning,"With the continuous growth in the scale and complexity of software systems, defect remediation has become increasingly difficult and costly. Automated defect prediction tools can proactively identify software changes prone to defects within software projects, thereby enhancing software development efficiency. However, existing work in heterogeneous and complex software projects continues to face challenges, such as struggling with heterogeneous commit structures and ignoring cross-line dependencies in code changes, which ultimately reduce the accuracy of defect identification. To address these challenges, we propose an approach called RC_Detector. RC_Detector comprises three main components: the bug-fixing graph construction component, the code semantic aggregation component, and the cross-line semantic retention component. The bug-fixing graph construction component identifies the code syntax structures and program dependencies within bug-fixing commits and transforms them into heterogeneous graph formats by converting the source code into vector representations. The code semantic aggregation component adapts to heterogeneous data by using heterogeneous attention to learn the hidden semantic representation of target code lines. The cross-line semantic retention component regulates propagated semantic information by using attenuation and reinforcement gates derived from old and new code semantic representations, effectively preserving cross-line semantic relationships. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fixing commits. The experimental results demonstrate that our model outperforms state-of-the-art approaches, achieving significant improvements of 83.15%,96.83%,78.71%,74.15%,54.14%,91.66%,91.66%, and 34.82% in MFR, respectively, compared with the state-of-the-art approaches.",CS,http://arxiv.org/abs/2505.01022v1
Identifying Root Cause of bugs by Capturing Changed Code Lines with Relational Graph Neural Networks,"The Just-In-Time defect prediction model helps development teams improve software quality and efficiency by assessing whether code changes submitted by developers are likely to introduce defects in real-time, allowing timely identification of potential issues during the commit stage. However, two main challenges exist in current work due to the reality that all deleted and added lines in bug-fixing commits may be related to the root cause of the introduced bug: 1) lack of effective integration of heterogeneous graph information, and 2) lack of semantic relationships between changed code lines. To address these challenges, we propose a method called RC-Detection, which utilizes relational graph convolutional network to capture the semantic relationships between changed code lines. RC-Detection is used to detect root-cause deletion lines in changed code lines, thereby identifying the root cause of introduced bugs in bug-fixing commits. To evaluate the effectiveness of RC-Detection, we used three datasets that contain high-quality bug-fixing and bug-introducing commits. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fix commits. The experimental results show that, compared to the most advanced root cause detection methods, RC-Detection improved Recall@1, Recall@2, Recall@3, and MFR by at 4.107%, 5.113%, 4.289%, and 24.536%, respectively.",CS,http://arxiv.org/abs/2505.00990v1
A SCADE Model Verification Method Based on B-Model Transformation,"Due to the limitations of SCADE models in expressing and verifying abstract specifications in safety-critical systems, this study proposes a formal verification framework based on the B-Method. By establishing a semantic equivalence transformation mechanism from SCADE models to B models, a hierarchical mapping rule set is constructed, covering type systems, control flow structures, and state machines. This effectively addresses key technical challenges such as loop-equivalent transformation proof for high-order operators and modeling of temporal logic storage structures. The proposed method innovatively leverages the abstraction capabilities of B-Method in set theory and first-order logic, overcoming the constraints of native verification tools of SCADE in complex specification descriptions. It successfully verifies abstract specifications that are difficult to model directly in SCADE. Experimental results show that the transformed B models achieve a higher defect detection rate and improved verification efficiency in the ProB verification environment compared to the native verifier of SCADE, significantly enhancing the formal verification capability of safety-critical systems. This study provides a cross-model verification paradigm for embedded control systems in avionics, rail transportation, and other domains, demonstrating substantial engineering application value.",CS,http://arxiv.org/abs/2505.00967v1
Aggregating empirical evidence from data strategy studies: a case on model quantization,"Background: As empirical software engineering evolves, more studies adopt data strategies$-$approaches that investigate digital artifacts such as models, source code, or system logs rather than relying on human subjects. Synthesizing results from such studies introduces new methodological challenges.   Aims: This study assesses the effects of model quantization on correctness and resource efficiency in deep learning (DL) systems. Additionally, it explores the methodological implications of aggregating evidence from empirical studies that adopt data strategies.   Method: We conducted a research synthesis of six primary studies that empirically evaluate model quantization. We applied the Structured Synthesis Method (SSM) to aggregate the findings, which combines qualitative and quantitative evidence through diagrammatic modeling. A total of 19 evidence models were extracted and aggregated.   Results: The aggregated evidence indicates that model quantization weakly negatively affects correctness metrics while consistently improving resource efficiency metrics, including storage size, inference latency, and GPU energy consumption$-$a manageable trade-off for many DL deployment contexts. Evidence across quantization techniques remains fragmented, underscoring the need for more focused empirical studies per technique.   Conclusions: Model quantization offers substantial efficiency benefits with minor trade-offs in correctness, making it a suitable optimization strategy for resource-constrained environments. This study also demonstrates the feasibility of using SSM to synthesize findings from data strategy-based research.",CS,http://arxiv.org/abs/2505.00816v1
"The Architecture Tradeoff and Risk Analysis Framework (ATRAF): A Unified Approach for Evaluating Software Architectures, Reference Architectures, and Architectural Frameworks","Modern software systems are guided by hierarchical architectural concepts -- software architectures, reference architectures, and architectural frameworks -- each operating at a distinct level of abstraction. These artifacts promote reuse, scalability, and consistency, but also embed tradeoffs that shape critical quality attributes such as modifiability, performance, and security. Existing evaluation methods, such as the Architecture Tradeoff Analysis Method (ATAM), focus on system-specific architectures and are not designed to address the broader generality and variability of higher-level architectural forms. To close this gap, we introduce the Architecture Tradeoff and Risk Analysis Framework (ATRAF) -- a unified, scenario-driven framework for evaluating tradeoffs and risks across architectural levels. ATRAF encompasses three methods: the Architecture Tradeoff and Risk Analysis Method (ATRAM), extending ATAM with enhanced risk identification for concrete systems; the Reference Architecture Tradeoff and Risk Analysis Method (RATRAM), adapting ATRAM to the evaluation of domain-level reference architectures; and the Architectural Framework Tradeoff and Risk Analysis Method (AFTRAM), supporting the evaluation of architectural frameworks that guide entire system families. All three methods follow an iterative spiral process that enables the identification of sensitivities, tradeoffs, and risks while supporting continuous refinement of architectural artifacts. We demonstrate ATRAF through progressively abstracted examples derived from the Remote Temperature Sensor (RTS) case, originally introduced in the ATAM literature. ATRAF equips architects, reference modelers, and framework designers with a practical, systematic approach for analyzing design alternatives and managing quality attribute tradeoffs early in the lifecycle and across all levels of architectural abstraction.",CS,http://arxiv.org/abs/2505.00688v1
From Requirements to Test Cases: An NLP-Based Approach for High-Performance ECU Test Case Automation,"Automating test case specification generation is vital for improving the efficiency and accuracy of software testing, particularly in complex systems like high-performance Electronic Control Units (ECUs). This study investigates the use of Natural Language Processing (NLP) techniques, including Rule-Based Information Extraction and Named Entity Recognition (NER), to transform natural language requirements into structured test case specifications. A dataset of 400 feature element documents from the Polarion tool was used to evaluate both approaches for extracting key elements such as signal names and values. The results reveal that the Rule-Based method outperforms the NER method, achieving 95% accuracy for more straightforward requirements with single signals, while the NER method, leveraging SVM and other machine learning algorithms, achieved 77.3% accuracy but struggled with complex scenarios. Statistical analysis confirmed that the Rule-Based approach significantly enhances efficiency and accuracy compared to manual methods. This research highlights the potential of NLP-driven automation in improving quality assurance, reducing manual effort, and expediting test case generation, with future work focused on refining NER and hybrid models to handle greater complexity.",CS,http://arxiv.org/abs/2505.00547v1
LLMPrism: Black-box Performance Diagnosis for Production LLM Training Platforms,"Large Language Models (LLMs) have brought about revolutionary changes in diverse fields, rendering LLM training of utmost importance for modern enterprises. To meet this demand, multi-tenant large-scale LLM training platforms have been built to offer LLM training services. Nevertheless, due to the complexity and synchronous nature of LLM training process, performance issues occur frequently and can result in substantial resource wastage. The limited visibility from the perspective of platform providers impedes existing profiling methods and poses challenges to the monitoring and diagnosis of the performance of LLM training jobs. For the first time, this paper proposes the utilization of underlying network flow data to reconstruct the training timelines of jobs based on the distinct characteristics in the LLM training procedure. We design LLMPrism, the first black-box performance diagnosis system for LLM training platforms. By progressively recognizing LLM training jobs, identifying their parallelism strategies, and reconstructing the training timelines, LLMPrism achieves non-intrusive, lightweight, and continuous monitoring of LLM training systems. Leveraging this monitoring capability, it further effectively diagnoses potential performance issues. Since Oct. 2024, LLMPrism has been deployed on our large-scale production Platform-X, in which the evaluations and deployment experiences demonstrate that LLMPrism can achieve accurate timeline reconstruction with an error within 0.3% and effectively diagnose various performance issues.",CS,http://arxiv.org/abs/2505.00342v1
When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey,"Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model's capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.",CS,http://arxiv.org/abs/2505.00144v1
An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding,"Binary code analysis plays a pivotal role in the field of software security and is widely used in tasks such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, reverse engineers face significant challenges in understanding binary code due to the lack of intuitive semantic information. Although traditional reverse tools can convert binary code into C-like pseudo code, the lack of code comments and symbolic information such as function names still makes code understanding difficult. In recent years, two groups of techniques have shown promising prospects: (1) Deep learning-based techniques have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This has left participants wondering about the capabilities of LLMs in binary code understanding. To this end, this work proposes a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios, which covers two key binary code understanding tasks, i.e., function name recovery and binary code summarization. To more comprehensively evaluate, we include binaries with multiple target architectures as well as different optimization options. We gain valuable insights into the capabilities and limitations through extensive empirical studies of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding, and provide new directions for binary code analysis techniques.",CS,http://arxiv.org/abs/2504.21803v1
SWE-smith: Scaling Data for Software Engineering Agents,"Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.",CS,http://arxiv.org/abs/2504.21798v1
"CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation","Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks.",CS,http://arxiv.org/abs/2504.21751v1
Using quantum annealing to generate test cases for cyber-physical systems,"Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives.",CS,http://arxiv.org/abs/2504.21684v1
Canonicalization for Unreproducible Builds in Java,"The increasing complexity of software supply chains and the rise of supply chain attacks have elevated concerns around software integrity. Users and stakeholders face significant challenges in validating that a given software artifact corresponds to its declared source. Reproducible Builds address this challenge by ensuring that independently performed builds from identical source code produce identical binaries. However, achieving reproducibility at scale remains difficult, especially in Java, due to a range of non-deterministic factors and caveats in the build process. In this work, we focus on reproducibility in Java-based software, archetypal of enterprise applications. We introduce a conceptual framework for reproducible builds, we analyze a large dataset from Reproducible Central, and we develop a novel taxonomy of six root causes of unreproducibility. We study actionable mitigations: artifact and bytecode canonicalization using OSS-Rebuild and jNorm respectively. Finally, we present Chains-Rebuild, a tool that raises reproducibility success from 9.48% to 26.89% on 12,283 unreproducible artifacts. To sum up, our contributions are the first large-scale taxonomy of build unreproducibility causes in Java, a publicly available dataset of unreproducible builds, and Chains-Rebuild, a canonicalization tool for mitigating unreproducible builds in Java.",CS,http://arxiv.org/abs/2504.21679v1
A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense,"With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.",CS,http://arxiv.org/abs/2504.21480v1
Identifying Critical Dependencies in Large-Scale Continuous Software Engineering,"Continuous Software Engineering (CSE) is widely adopted in the industry, integrating practices such as Continuous Integration and Continuous Deployment (CI/CD). Beyond technical aspects, CSE also encompasses business activities like continuous planning, budgeting, and operational processes. Coordinating these activities in large-scale product development involves multiple stakeholders, increasing complexity. This study aims to address this complexity by identifying and analyzing critical dependencies in large-scale CSE. Based on 17 semi-structured interviews conducted at two Nordic fintech companies, our preliminary findings indicate that dependencies between software teams and support functions, as well as between software teams and external entities, are the primary sources of delays and bottlenecks. As a next step, we plan to further refine our understanding of critical dependencies in large-scale CSE and explore coordination mechanisms that can better support software development teams in managing these challenges.",CS,http://arxiv.org/abs/2504.21437v2
A Test Suite for Efficient Robustness Evaluation of Face Recognition Systems,"Face recognition is a widely used authentication technology in practice, where robustness is required. It is thus essential to have an efficient and easy-to-use method for evaluating the robustness of (possibly third-party) trained face recognition systems. Existing approaches to evaluating the robustness of face recognition systems are either based on empirical evaluation (e.g., measuring attacking success rate using state-of-the-art attacking methods) or formal analysis (e.g., measuring the Lipschitz constant). While the former demands significant user efforts and expertise, the latter is extremely time-consuming. In pursuit of a comprehensive, efficient, easy-to-use and scalable estimation of the robustness of face recognition systems, we take an old-school alternative approach and introduce RobFace, i.e., evaluation using an optimised test suite. It contains transferable adversarial face images that are designed to comprehensively evaluate a face recognition system's robustness along a variety of dimensions. RobFace is system-agnostic and still consistent with system-specific empirical evaluation or formal analysis. We support this claim through extensive experimental results with various perturbations on multiple face recognition systems. To our knowledge, RobFace is the first system-agnostic robustness estimation test suite.",CS,http://arxiv.org/abs/2504.21420v1
On the Encapsulation of Medical Imaging AI Algorithms,"In the context of collaborative AI research and development projects, it would be ideal to have self-contained encapsulated algorithms that can be easily shared between different parties, executed and validated on data at different sites, or trained in a federated manner. In practice, all of this is possible but greatly complicated, because human supervision and expert knowledge is needed to set up the execution of algorithms based on their documentation, possibly implicit assumptions, and knowledge about the execution environment and data involved.   We derive and formulate a range of detailed requirements from the above goal and from specific use cases, focusing on medical imaging AI algorithms. Furthermore, we refer to a number of existing APIs and implementations and review which aspects each of them addresses, which problems are still open, and which public standards and ontologies may be relevant. Our contribution is a comprehensive collection of aspects that have not yet been addressed in their entirety by any single solution.   Working towards the formulated goals should lead to more sustainable algorithm ecosystems and relates to the FAIR principles for research data, where this paper focuses on interoperability and (re)usability of medical imaging AI algorithms.",CS,http://arxiv.org/abs/2504.21412v1
Assessing LLM code generation quality through path planning tasks,"As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing.",CS,http://arxiv.org/abs/2504.21276v1
Automated Test Generation from Program Documentation Encoded in Code Comments,"Documenting the functionality of software units with code comments, e.g., Javadoc comments, is a common programmer best-practice in software engineering. This paper introduces a novel test generation technique that exploits the code-comment documentation constructively. We originally address those behaviors as test objectives, which we pursue in search-based fashion. We deliver test cases with names and oracles properly contextualized on the target behaviors. Our experiments against a benchmark of 118 Java classes indicate that the proposed approach successfully tests many software behaviors that may remain untested with coverage-driven test generation approaches, and distinctively detects unknown failures.",CS,http://arxiv.org/abs/2504.21161v1
OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification,"We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.",CS,http://arxiv.org/abs/2504.20964v1
The Development of Reflective Practice on a Work-Based Software Engineering Program: A Longitudinal Study,"This study examines the development of reflective practice among students on a four-year work-based Software Engineering program. Using two established models of reflection - Boud et al.'s Model of Reflective Process and Bain et al.'s 5R Framework for Reflection - we analyse a series of reflective assignments submitted by students over four years. Our longitudinal analysis reveals clear trends in how students' reflective abilities evolve over the course of the program. We find that more sophisticated forms of reflection, such as integration of knowledge, appropriation of skills, and reconstruction of practice, increase markedly in prevalence in later years. The complementary nature of workplace experience and university study is highlighted in students' reflections, demonstrating a key benefit of the work-based learning approach. By the final year, all students demonstrate the ability to reconstruct their experiences to inform future practice. Our findings provide insight into how reflective practice develops in Software Engineering education and suggest potential value in incorporating more structured reflection into traditional degree programs. The study also reveals instances of meta-reflection, where students reflect on the value of reflection itself, indicating a deep engagement with the reflective process. While acknowledging limitations, this work offers a unique longitudinal perspective on the development of reflective practice in work-based Software Engineering education.",CS,http://arxiv.org/abs/2504.20956v2
An Empirical Study on the Capability of LLMs in Decomposing Bug Reports,"Background: Bug reports are essential to the software development life cycle. They help developers track and resolve issues, but are often difficult to process due to their complexity, which can delay resolution and affect software quality. Aims: This study investigates whether large language models (LLMs) can assist developers in automatically decomposing complex bug reports into smaller, self-contained units, making them easier to understand and address. Method: We conducted an empirical study on 127 resolved privacy-related bug reports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using different prompting strategies. We first tested both LLMs with zero-shot prompts, then applied improved prompts with demonstrations (using few-shot prompting) to measure their abilities in bug decomposition. Results: Our findings show that LLMs are capable of decomposing bug reports, but their overall performance still requires further improvement and strongly depends on the quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT and DeepSeek) performed poorly. After prompt tuning, ChatGPT's true decomposition rate increased by 140\% and DeepSeek's by 163.64\%. Conclusions: LLMs show potential in helping developers analyze and decompose complex bug reports, but they still need improvement in terms of accuracy and bug understanding.",CS,http://arxiv.org/abs/2504.20911v1
MANILA: A Low-Code Application to Benchmark Machine Learning Models and Fairness-Enhancing Methods,"This paper presents MANILA, a web-based low-code application to benchmark machine learning models and fairness-enhancing methods and select the one achieving the best fairness and effectiveness trade-off. It is grounded on an Extended Feature Model that models a general fairness benchmarking workflow as a Software Product Line. The constraints defined among the features guide users in creating experiments that do not lead to execution errors. We describe the architecture and implementation of MANILA and evaluate it in terms of expressiveness and correctness.",CS,http://arxiv.org/abs/2504.20907v1
A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models,"The rise of Artificial Intelligence (AI)-and particularly Large Language Models (LLMs) for code-has reshaped Software Engineering (SE) by enabling the automation of tasks such as code generation, bug detection, and repair. However, these models require significant computational resources for training and fine-tuning, posing challenges for real-world adoption in resource-constrained environments. To address this, the research community has increasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of techniques that enables the adaptation of large models by updating only a small subset of parameters, rather than the entire model. In this Systematic Literature Review (SLR), we examine the growing application of PEFT techniques-across a wide range of software engineering tasks. We analyze how these methods are used to optimize various deep learning (DL) architectures, focusing on their impact on both performance and efficiency. Our study synthesizes findings from 27 peer-reviewed papers, identifying patterns in configuration strategies and adaptation trade-offs. The outcome of this review is a comprehensive taxonomy that categorizes PEFT usage by task type, distinguishing between generative (e.g., Code Summarization) and non-generative (e.g., Code Clone Detection) scenarios. Our findings aim to inform future research and guide the practical deployment of PEFT in sustainable, AI-powered software development. Our artifacts are publicly available at https://github.com/alvi75/SLR-PEFT",CS,http://arxiv.org/abs/2504.21569v1
LELANTE: LEveraging LLM for Automated ANdroid TEsting,"Given natural language test case description for an Android application, existing testing approaches require developers to manually write scripts using tools such as Appium and Espresso to execute the corresponding test case. This process is labor-intensive and demands significant effort to maintain as UI interfaces evolve throughout development. In this work, we introduce LELANTE, a novel framework that utilizes large language models (LLMs) to automate test case execution without requiring pre-written scripts. LELANTE interprets natural language test case descriptions, iteratively generate action plans, and perform the actions directly on the Android screen using its GUI. LELANTE employs a screen refinement process to enhance LLM interpretability, constructs a structured prompt for LLMs, and implements an action generation mechanism based on chain-of-thought reasoning of LLMs. To further reduce computational cost and enhance scalability, LELANTE utilizes model distillation using a foundational LLM. In experiments across 390 test cases spanning 10 popular Android applications, LELANTE achieved a 73% test execution success rate. Our results demonstrate that LLMs can effectively bridge the gap between natural language test case description and automated execution, making mobile testing more scalable and adaptable.",CS,http://arxiv.org/abs/2504.20896v1
"Secure Coding with AI, From Creation to Inspection","While prior studies have explored security in code generated by ChatGPT and other Large Language Models, they were conducted in controlled experimental settings and did not use code generated or provided from actual developer interactions. This paper not only examines the security of code generated by ChatGPT based on real developer interactions, curated in the DevGPT dataset, but also assesses ChatGPT's capability to find and fix these vulnerabilities. We analysed 1,586 C, C++, and C# code snippets using static scanners, which detected potential issues in 124 files. After manual analysis, we selected 26 files with 32 confirmed vulnerabilities for further investigation.   We submitted these files to ChatGPT via the OpenAI API, asking it to detect security issues, identify the corresponding Common Weakness Enumeration numbers, and propose fixes. The responses and modified code were manually reviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18 out of 32 security issues and resolved 17 issues but failed to recognize or fix the remainder. Interestingly, only 10 vulnerabilities were resulted from the user prompts, while 22 were introduced by ChatGPT itself.   We highlight for developers that code generated by ChatGPT is more likely to contain vulnerabilities compared to their own code. Furthermore, at times ChatGPT reports incorrect information with apparent confidence, which may mislead less experienced developers. Our findings confirm previous studies in demonstrating that ChatGPT is not sufficiently reliable for generating secure code nor identifying all vulnerabilities, highlighting the continuing importance of static scanners and manual review.",CS,http://arxiv.org/abs/2504.20814v1
Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications,"Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/",CS,http://arxiv.org/abs/2504.20801v2
"Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges","Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.",CS,http://arxiv.org/abs/2504.20799v1
Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces,"Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.   In this paper, we enhance a RL-based Adaptive User Interface adaption framework by incorporating personalized human feedback directly into the leaning process. Unlike prior approaches that rely on a single pre-trained RL model, our approach trains a unique RL agent for each user, allowing individuals to actively shape their personal RL agent's policy, potentially leading to more personalized and responsive UI adaptations. To evaluate this approach, we conducted an empirical study to assess the impact of integrating human feedback into the RL-based Adaptive User Interface adaption framework and its effect on User Experience (UX). The study involved 33 participants interacting with AUIs incorporating human feedback and non-adaptive user interfaces in two domains: an e-learning platform and a trip-planning application. The results suggest that incorporating human feedback into RL-driven adaptations significantly enhances UX, offering promising directions for advancing adaptive capabilities and user-centered design in AUIs.",CS,http://arxiv.org/abs/2504.20782v1
Using LLMs in Generating Design Rationale for Software Architecture Decisions,"Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. Based on the results, we further discussed the pros and cons of the three prompting strategies and the strengths and limitations of the DR generated by LLMs.",CS,http://arxiv.org/abs/2504.20781v1
"Understanding Large Language Model Supply Chain: Structure, Domain, and Vulnerabilities","Large Language Models (LLMs) have revolutionized artificial intelligence (AI), driving breakthroughs in natural language understanding, text generation, and autonomous systems. However, the rapid growth of LLMs presents significant challenges in the security and reliability of the Large Language Model Supply Chain (LLMSC), a complex network of open-source components, libraries, and tools essential for LLM development and deployment. Despite its critical importance, the LLMSC remains underexplored, particularly regarding its structural characteristics, domain composition, and security vulnerabilities. To address this gap, we conduct the first empirical study of the LLMSC, analyzing a curated dataset of open-source packages from PyPI and NPM across 14 functional domains. We construct a directed dependency graph comprising 15,725 nodes, 10,402 edges, and 180 unique vulnerabilities to investigate the structural characteristics of the LLMSC and analyze how security risks propagate through its dependency network. Our findings reveal that the LLMSC exhibits a ``locally dense, globally sparse'' topology, with 79.7% of dependency trees containing fewer than 5 nodes, while a few large trees dominate the ecosystem, accounting for 77.66% of all nodes. The graph is characterized by high-degree hubs, with the top 5 most connected nodes averaging 1,282 dependents each. Security analysis shows that critical vulnerabilities propagate to an average of 142.1 nodes at the second layer of dependency trees and peak at 237.8 affected nodes at the third layer. Notably, cascading risks are concentrated in critical hub nodes such as transformers, which directly or indirectly affect over 1,300 downstream packages. These findings provide quantitative insights into the structural and security dynamics of the LLMSC and emphasize the need for targeted mitigation strategies to enhance ecosystem resilience.",CS,http://arxiv.org/abs/2504.20763v1
Identifying Uncertainty in Self-Adaptive Robotics with Large Language Models,"Future self-adaptive robots are expected to operate in highly dynamic environments while effectively managing uncertainties. However, identifying the sources and impacts of uncertainties in such robotic systems and defining appropriate mitigation strategies is challenging due to the inherent complexity of self-adaptive robots and the lack of comprehensive knowledge about the various factors influencing uncertainty. Hence, practitioners often rely on intuition and past experiences from similar systems to address uncertainties. In this article, we evaluate the potential of large language models (LLMs) in enabling a systematic and automated approach to identify uncertainties in self-adaptive robotics throughout the software engineering lifecycle. For this evaluation, we analyzed 10 advanced LLMs with varying capabilities across four industrial-sized robotics case studies, gathering the practitioners' perspectives on the LLM-generated responses related to uncertainties. Results showed that practitioners agreed with 63-88% of the LLM responses and expressed strong interest in the practicality of LLMs for this purpose.",CS,http://arxiv.org/abs/2504.20684v1
CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation,"Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.",CS,http://arxiv.org/abs/2504.20673v1
ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code,"Recent advances have demonstrated the promising capabilities of large language models (LLMs) in generating register-transfer level (RTL) code, such as Verilog. However, existing LLM-based frameworks still face significant challenges in accurately handling the complexity of real-world RTL designs, particularly those that are large-scale and involve multi-level module instantiations. To address this issue, we present ComplexVCoder, an open-source LLM-driven framework that enhances both the generation quality and efficiency of complex Verilog code. Specifically, we introduce a two-stage generation mechanism, which leverages an intermediate representation to enable a more accurate and structured transition from natural language descriptions to intricate Verilog designs. In addition, we introduce a rule-based alignment method and a domain-specific retrieval-augmented generation (RAG) to further improve the correctness of the synthesized code by incorporating relevant design knowledge during generation. To evaluate our approach, we construct a comprehensive dataset comprising 55 complex Verilog designs derived from real-world implementations. We also release an open-source benchmark suite for systematically assessing the quality of auto-generated RTL code together with the ComplexVCoder framework. Experimental results show that ComplexVCoder outperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%, respectively, in terms of function correctness on complex Verilog benchmarks. Furthermore, ComplexVcoder achieves comparable generation performances in terms of functionality correctness using a lightweight 32B model (Qwen2.5), rivaling larger-scale models such as GPT-3.5 and DeepSeek-V3.",CS,http://arxiv.org/abs/2504.20653v1
Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis,"This work is concerned with the generation of formal specifications from code, using Large Language Models (LLMs) in combination with symbolic methods. Concretely, in our study, the programming language is C, the specification language is ACSL, and the LLM is Deepseek-R1. In this context, we address two research directions, namely the specification of intent vs. implementation on the one hand, and the combination of symbolic analyses with LLMs on the other hand. For the first, we investigate how the absence or presence of bugs in the code impacts the generated specifications, as well as whether and how a user can direct the LLM to specify intent or implementation, respectively. For the second, we investigate the impact of results from symbolic analyses on the specifications generated by the LLM. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations.",CS,http://arxiv.org/abs/2504.21061v1
Challenges of Requirements Communication and Digital Assets Verification in Infrastructure Projects,"Background: Poor communication of requirements between clients and suppliers contributes to project overruns,in both software and infrastructure projects. Existing literature offers limited insights into the communication challenges at this interface. Aim: Our research aim to explore the processes and associated challenges with requirements activities that include client-supplier interaction and communication. Method: we study requirements validation, communication, and digital asset verification processes through two case studies in the road and railway sectors, involving interviews with ten experts across three companies. Results: We identify 13 challenges, along with their causes and consequences, and suggest solution areas from existing literature. Conclusion: Interestingly, the challenges in infrastructure projects mirror those found in software engineering, highlighting a need for further research to validate potential solutions.",CS,http://arxiv.org/abs/2504.20511v1
Taxonomic Trace Links: Rethinking Traceability and its Benefits,"Traceability greatly supports knowledge-intensive tasks, e.g., coverage check and impact analysis. Despite its clear benefits, the \emph{practical} implementation of traceability poses significant challenges, leading to a reduced focus on the creation and maintenance of trace links. We propose a new approach -- Taxonomic Trace Links (TTL) -- which rethinks traceability and its benefits. With TTL, trace links are created indirectly through a domain-specific taxonomy, a simplified version of a domain model. TTL has the potential to address key traceability challenges, such as the granularity of trace links, the lack of a common data structure among software development artifacts, and unclear responsibility for traceability. We explain how TTL addresses these challenges and perform an initial validation with practitioners. We identified six challenges associated with TTL implementation that need to be addressed. Finally, we propose a research roadmap to further develop and evaluate the technical solution of TTL. TTL appears to be particularly feasible in practice where a domain taxonomy is already established",CS,http://arxiv.org/abs/2504.20507v1
Sleeping Giants -- Activating Dormant Java Deserialization Gadget Chains through Stealthy Code Changes,"Java deserialization gadget chains are a well-researched critical software weakness. The vast majority of known gadget chains rely on gadgets from software dependencies. Furthermore, it has been shown that small code changes in dependencies have enabled these gadget chains. This makes gadget chain detection a purely reactive endeavor. Even if one dependency's deployment pipeline employs gadget chain detection, a gadget chain can still result from gadgets in other dependencies. In this work, we assess how likely small code changes are to enable a gadget chain. These changes could either be accidental or intentional as part of a supply chain attack. Specifically, we show that class serializability is a strongly fluctuating property over a dependency's evolution. Then, we investigate three change patterns by which an attacker could stealthily introduce gadgets into a dependency. We apply these patterns to 533 dependencies and run three state-of-the-art gadget chain detectors both on the original and the modified dependencies. The tools detect that applying the modification patterns can activate/inject gadget chains in 26.08% of the dependencies we selected. Finally, we verify the newly detected chains. As such, we identify dormant gadget chains in 53 dependencies that could be added through minor code modifications. This both shows that Java deserialization gadget chains are a broad liability to software and proves dormant gadget chains as a lucrative supply chain attack vector.",CS,http://arxiv.org/abs/2504.20485v1
ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement,"In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization.",CS,http://arxiv.org/abs/2504.20434v1
CrashFixer: A crash resolution agent for the Linux kernel,"Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.   This paper introduces CrashFixer, the first LLM-based software repair agent that is applicable to Linux kernel bugs. Inspired by the typical workflow of a kernel developer, we identify the key capabilities an expert developer leverages to resolve a kernel crash. Using this as our guide, we revisit the kGym platform and identify key system improvements needed to practically run LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of code). We implement these changes by extending kGym to create an improved platform - called kGymSuite, which will be open-sourced. Finally, the paper presents an evaluation of various repair strategies for such complex kernel bugs and showcases the value of explicitly generating a hypothesis before attempting to fix bugs in complex systems such as the Linux kernel. We also evaluated CrashFixer's capabilities on still open bugs, and found at least two patch suggestions considered plausible to resolve the reported bug.",CS,http://arxiv.org/abs/2504.20412v1
Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs,"Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.",CS,http://arxiv.org/abs/2504.20406v1
An Empirical Study on Common Defects in Modern Web Browsers Using Knowledge Embedding in GPT-4o,"Technology is advancing at an unprecedented pace. With the advent of cutting-edge technologies, keeping up with rapid changes are becoming increasingly challenging. In addition to that, increasing dependencies on the cloud technologies have imposed enormous pressure on modern web browsers leading to adapting new technologies faster and making them more susceptible to defects/bugs. Although, many studies have explored browser bugs, a comparative study among the modern browsers generalizing the bug categories and their nature was still lacking. To fill this gap, we undertook an empirical investigation aimed at gaining insights into the prevalent bugs in Google Chromium and Mozilla Firefox as the representatives of modern web browsers. We used GPT-4.o to identify the defect (bugs) categories and analyze the clusters of the most commonly appeared bugs in the two prominent web browsers. Additionally, we compared our LLM based bug categorization with the traditional NLP based approach using TF-IDF and K-Means clustering. We found that although Google Chromium and Firefox have evolved together since almost around the same time (2006-2008), Firefox suffers from high number of bugs having extremely high defect-prone components compared to Chromium. This exploratory study offers valuable insights on the browser bugs and defect-prone components to the developers, enabling them to craft web browsers and web-applications with enhanced resilience and reduced errors.",CS,http://arxiv.org/abs/2504.20381v1
Automated Unit Test Case Generation: A Systematic Literature Review,"Software is omnipresent within all factors of society. It is thus important to ensure that software are well tested to mitigate bad user experiences as well as the potential for severe financial and human losses. Software testing is however expensive and absorbs valuable time and resources. As a result, the field of automated software testing has grown of interest to researchers in past decades. In our review of present and past research papers, we have identified an information gap in the areas of improvement for the Genetic Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current challenges that face automated testing has also been identified. We therefore present this systematic literature review in an effort to consolidate existing knowledge in regards to the evolutionary approaches as well as their improvements and resulting limitations. These improvements include hybrid algorithm combinations as well as interoperability with mutation testing and neural networks. We will also explore the main test criterion that are used in these algorithms alongside the challenges currently faced in the field related to readability, mocking and more.",CS,http://arxiv.org/abs/2504.20357v1
SoK: Enhancing Privacy-Preserving Software Development from a Developers' Perspective,"In software development, privacy preservation has become essential with the rise of privacy concerns and regulations such as GDPR and CCPA. While several tools, guidelines, methods, methodologies, and frameworks have been proposed to support developers embedding privacy into software applications, most of them are proofs-of-concept without empirical evaluations, making their practical applicability uncertain. These solutions should be evaluated for different types of scenarios (e.g., industry settings such as rapid software development environments, teams with different privacy knowledge, etc.) to determine what their limitations are in various industry settings and what changes are required to refine current solutions before putting them into industry and developing new developer-supporting approaches. For that, a thorough review of empirically evaluated current solutions will be very effective. However, the existing secondary studies that examine the available developer support provide broad overviews but do not specifically analyze empirically evaluated solutions and their limitations. Therefore, this Systematic Literature Review (SLR) aims to identify and analyze empirically validated solutions that are designed to help developers in privacy-preserving software development. The findings will provide valuable insights for researchers to improve current privacy-preserving solutions and for practitioners looking for effective and validated solutions to embed privacy into software development.",CS,http://arxiv.org/abs/2504.20350v2
"A survey on large language model (LLM) security and privacy:The Good, The Bad, and The Ugly","Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.",CS,https://www.sciencedirect.com/science/article/pii/S266729522400014X?via%3Dihub
Detect Anything 3D in the Wild,"Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page.",CS,https://arxiv.org/html/2504.07958v1
Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",CS,https://ieeexplore.ieee.org/abstract/document/1427769
Understanding egocentric activities,"We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.",CS,https://ieeexplore.ieee.org/abstract/document/6126269
High-performance Implementation of Elliptic Curve Cryptography Using Vector Instructions,"Elliptic curve cryptosystems are considered an efficient alternative to conventional systems such as DSA and RSA. Recently, Montgomery and Edwards elliptic curves have been used to implement cryptosystems. In particular, the elliptic curves Curve25519 and Curve448 were used for instantiating Diffie-Hellman protocols named X25519 and X448. Mapping these curves to twisted Edwards curves allowed deriving two new signature instances, called Ed25519 and Ed448, of the Edwards Digital Signature Algorithm. In this work, we focus on the secure and efficient software implementation of these algorithms using SIMD parallel processing. We present software techniques that target the Intel AVX2 vector instruction set for accelerating prime field arithmetic and elliptic curve operations. Our contributions result in a high-performance software library for AVX2-ready processors. For example, our library computes digital signatures 19% (for Ed25519) and 29% (for Ed448) faster than previous optimized implementations. Also, our library improves by 10% and 20% the execution time of X25519 and X448, respectively.",CS,https://dl.acm.org/doi/abs/10.1145/3309759
Federated Learning in Mobile Edge Networks: A Comprehensive Survey,"In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.",CS,https://ieeexplore.ieee.org/abstract/document/9060868
On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work.",CS,https://aclanthology.org/2020.emnlp-main.748/
STGSN — A Spatial–Temporal Graph Neural Network framework for time-evolving social networks,"Social Network Analysis (SNA) has been a popular field of research since the early 1990s. Law enforcement agencies have been utilizing it as a tool for intelligence gathering and criminal investigation for decades. However, the graph nature of social networks makes it highly restricted to intelligence analysis tasks, such as role prediction (node classification), social relation inference (link prediction), and criminal group discovery (community detection), etc. In the past few years, many studies have focused on Graph Neural Network (GNN), which utilizes deep learning methods to solve graph-related problems. However, we have rarely seen GNNs tackle time-evolving social network problems, especially in the criminology field. The existing studies have commonly over-looked the temporal-evolution characteristics of social networks. In this paper, we propose a graph neural network framework, namely Spatial-Temporal Graph Social Network (STGSN), which models social networks from both spatial and temporal perspectives. Using a novel approach, we leverage the temporal attention mechanism to capture social networks’ temporal features. We design a method analyzing temporal attention distribution to improve the interpretation ability of our method. In the end, we conduct extensive experiments on six public datasets to prove our methods’ effectiveness.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0950705121000095
On the capability of static code analysis to detect security vulnerabilities,"Context: Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection.Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings.Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source programs.Results: Our experiment showed that 27% of C/C++ vulnerabilities and 11% of Java vulnerabilities were missed by all three tools. Some vulnerabilities were detected by only one or combination of two tools; 41% of C/C++ and 21% of Java vulnerabilities were detected by all three tools. More importantly, static code analysis tools did not show statistically significant difference in their ability to detect security vulnerabilities for both C/C++ and Java. Interestingly, all tools had median and mean of the per CWE recall values and overall recall across all CWEs close to or below 50%, which indicates comparable or worse performance than random guessing. While for C/C++ vulnerabilities one of the tools had better performance in terms of probability of false alarm than the other two tools, there was no statistically significant difference among tools’ probability of false alarm for Java test cases.Conclusions: Despite recent advances in methods for static code analysis, the state-of-the-art tools are not very effective in detecting security vulnerabilities.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0950584915001366
Self-Supervised Representation Learning From Videos for Facial Action Unit Detection,"In this paper, we aim to learn discriminative representation for facial action unit (AU) detection from large amount of videos without manual annotations. Inspired by the fact that facial actions are the movements of facial muscles, we depict the movements as the transformation between two face images in different frames and use it as the self-supervisory signal to learn the representations. However, under the uncontrolled condition, the transformation is caused by both facial actions and head motions. To remove the influence by head motions, we propose a Twin-Cycle Autoencoder (TCAE) that can disentangle the facial action related movements and the head motion related ones. Specifically, TCAE is trained to respectively change the facial actions and head poses of the source face to those of the target face. Our experiments validate TCAE's capability of decoupling the movements. Experimental results also demonstrate that the learned representation is discriminative for AU detection, where TCAE outperforms or is comparable with the state-of-the-art self-supervised learning methods and supervised AU detection methods.",CS,https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Self-Supervised_Representation_Learning_From_Videos_for_Facial_Action_Unit_Detection_CVPR_2019_paper.html
Auto-Keras: An Efficient Neural Architecture Search System,"Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at the auto keras website. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.",CS,https://dl.acm.org/doi/abs/10.1145/3292500.3330648
Graph theoretic techniques for pruning data and their applications,"In pattern recognition tasks, we usually do not pay much attention to the arbitrarily chosen training set of a pattern classifier beforehand. This correspondence proposes several methods for pruning data sets based upon graph theory in order to alleviate redundancy in the original data set while retaining the original data structure as far as possible. The proposed methods are applied to the training sets for pattern recognition by a multilayered perceptron neural network (MLP-NN) and the locations of the centroids of a radial basis function neural network (RBF-NN). The advantage of the proposed graph theoretic methods is that they do not require any calculation for the statistical distributions of the clusters. The experimental results in comparison both with the k-means clustering and with the learning vector quantization (LVQ) methods show that the proposed methods give encouraging performance in terms of computation for data classification tasks.",CS,https://ieeexplore.ieee.org/abstract/document/709550
Secure multi-party computation problems and their applications: a review and open problems,"The growth of the Internet has triggered tremendous opportunities for cooperative computation, where people are jointly conducting computation tasks based on the private inputs they each supplies. These computations could occur between mutually untrusted parties, or even between competitors. For example, customers might send to a remote database queries that contain private information; two competing financial organizations might jointly invest in a project that must satisfy both organizations' private and valuable constraints, and so on. Today, to conduct such computations, one entity must usually know the inputs from all the participants; however if nobody can be trusted enough to know all the inputs, privacy will become a primary concern.This problem is referred to as Secure Multi-party Computation Problem (SMC) in the literature. Research in the SMC area has been focusing on only a limited set of specific SMC problems, while privacy concerned cooperative computations call for SMC studies in a variety of computation domains. Before we can study the problems, we need to identify and define the specific SMC problems for those computation domains. We have developed a framework to facilitate this problem-discovery task. Based on our framework, we have identified and defined a number of new SMC problems for a spectrum of computation domains. Those problems include privacy-preserving database query, privacy-preserving scientific computations, privacy-preserving intrusion detection, privacy-preserving statistical analysis, privacy-preserving geometric computations, and privacy-preserving data mining.The goal of this paper is not only to present our results, but also to serve as a guideline so other people can identify useful SMC problems in their own computation domains.",CS,https://dl.acm.org/doi/abs/10.1145/508171.508174
A simple differentiable programming language,"Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide.",CS,https://dl.acm.org/doi/abs/10.1145/3371106
Succinct Dynamic Data Structures,"We develop succinct data structures to represent (i) a sequence of values to support partial sum and select queries and update(changing values) and (ii) a dynamic array consisting of a sequence of elements which supports insertion, deletion and access of an element at any given index.For the partial sums problem on n non-negative integers of k bits each, we support update operations in O(b) time and sum in O(logb n) time, for any parameter b, lgn/lglgn ≤ b ≤ n∈ for any fixed positive ∈ < 1. The space used is kn+o(kn) bits and the time bounds are optimal. When b = lgn/lglgn or k = 1 (i.e., when we are dealing with a bit-vector), we can also support the select operation in the same time as the sum operation, but the update time becomes amortised. For the dynamic array problem, we give two structures both using o(n) bits of extra space where n is the number of elements in the array: one supports lookup in constant worst case time and updates in O(n ε) worst case time, and the other supports all operations in O(lgn/lglgn) amortized time. The time bound of both these structures are optimal.",CS,https://link.springer.com/chapter/10.1007/3-540-44634-6_39
Is neuro-symbolic AI meeting its promises in natural language processing? A structured review,"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.",CS,https://journals.sagepub.com/doi/10.3233/SW-223228
Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection,"Graph-based models have been widely used to fraud detection tasks. Owing to the development of Graph Neural Networks~(GNNs), recent works have proposed many GNN-based fraud detectors based on either homogeneous or heterogeneous graphs. These works leverage existing GNNs and aggregate the neighborhood information to learn the node embeddings, which relies on the assumption that the neighbors share similar context, features, and relations. However, the inconsistency problem incurred by fraudsters is hardly investigated, i.e., the context inconsistency, feature inconsistency, and relation inconsistency. In this paper, we introduce these inconsistencies and design a new GNN framework, GraphConsis, to tackle the inconsistency problem: (1) for the context inconsistency, we propose to combine the context embeddings with node features; (2) for the feature inconsistency, we design a consistency score to filter the inconsistent neighbors and generate corresponding sampling probability; (3) for the relation inconsistency, we learn the relation attention weights associated with the sampled nodes. Empirical analysis on four datasets demonstrates that the inconsistency problem is critical in fraud detection tasks. Extensive experiments show the effectiveness of GraphConsis. We also released a GNN-based fraud detection toolbox with implementations of SOTA models. The code is available at the github repo.",CS,https://dl.acm.org/doi/abs/10.1145/3397271.3401253
Analysis of Problems and Prospects of Implementation of Post-Quantum Cryptographic Algorithms ,"The paper provides an overview and analysis of the current state, problems, and prospects of post-quantum cryptography. Considered the status of the PostQuantum Cryptography Standardization Process. Organizations like the National Institute of Standards and Technology (NIST) are actively working on standardizing post-quantum cryptography. Evaluation rounds have been conducted, thoroughly analyzing numerous candidate post-quantum algorithms to select the most efficient and secure ones. Identified main categories of postquantum cryptographic algorithms. Described key size of post-quantum cryptographic algorithms. Open-source libraries like Open Quantum Safe (OQS) have been developed, offering implementations of various post-quantum algorithms. These libraries enable researchers, developers, and engineers to utilize and test post-quantum algorithms in various applications. There is growing awareness of the need to prepare for the post-quantum computing era. Many companies, organizations, and governments are exploring the implications of quantum computing for their infrastructure and data security and considering the adoption of post-quantum cryptographic solutions.",CS,https://ceur-ws.org/Vol-3504/paper4.pdf
Multiple fault localization of software programs: A systematic literature review,"Multiple fault localization (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed. The aim of this study is to systematically review existing research on MFL in the software fault localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain. Consequently, using an evidence-based systematic methodology, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process. The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most. The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0950584920300641
Data-driven contextual modeling for 3D scene understanding,"The recent development of fast depth map fusion technique enables the realtime, detailed scene reconstruction using commodity depth camera, making the indoor scene understanding more possible than ever. To address the specific challenges in object analysis at subscene level, this work proposes a data-driven approach to modeling contextual information covering both intra-object part relations and inter-object object layouts. Our method combines the detection of https://www.sciencedirect.com/topics/computer-science/individual-object and object groups within the same framework, enabling contextual analysis without knowing the objects in the scene a priori. The key idea is that while contextual information could benefit the detection of either individual objects or object groups, both can contribute to object extraction when objects are unknown.Our method starts with a robust segmentation and partitions a subscene into segments, each of which represents either an independent object or a part of some object. A set of classifiers are trained for both individual objects and object groups, using a database of 3D scene models. We employ the multiple kernel learning (MKL) to learn per-category optimized classifiers for objects and object groups. Finally, we perform a graph matching to extract objects using the classifiers, thus grouping the segments into either an object or an object group. The output is an object-level labeled segmentation of the input subscene. Experiments demonstrate that the unified contextual analysis framework achieves robust object detection and recognition over cluttered subscenes.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0097849315002009
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,"Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. Weshowthat our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",CS,https://arxiv.org/abs/1805.04276
A Survey on Bias and Fairness in Machine Learning,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",CS,https://dl.acm.org/doi/abs/10.1145/3457607
Multi-agent Path Planning and Network Flow,"This paper connects multi-agent path planning on graphs (roadmaps) to network flow problems, showing that the former can be reduced to the latter, therefore enabling the application of combinatorial network flow algorithms, as well as general linear program techniques, tomulti-agent path planning problems on graphs. Exploiting this connection, we show that when the goals are permutation invariant, the problem always has a feasible solution path set with a longest finish time of no more than n + V - 1 steps, in which n is the number of agents and V is the number of vertices of the underlying graph.We then give a complete algorithm that finds such a solution in O(nVE) time, with E being the number of edges of the graph. Taking a further step, we study time and distance optimality of the feasible solutions, show that they have a pairwise Pareto optimal structure, and again provide efficient algorithms for optimizing two of these practical objectives.",CS,https://link.springer.com/chapter/10.1007/978-3-642-36279-8_10
Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",CS,https://ieeexplore.ieee.org/abstract/document/8294186
A Three-Stage Quantum Cryptography Protocol,"We present a three-stage quantum cryptographic protocol based on public key cryptography in which each party uses its own secret key. Unlike the BB84 protocol, where the qubits are transmitted in only one direction and classical information exchanged thereafter, the communication in the proposed protocol remains quantum in each stage. A related system of key distribution is also described.",CS,https://link.springer.com/article/10.1007/s10702-006-0520-9
Practical Private Computation and Zero-Knowledge Tools for Privacy-Preserving Distributed Data Mining,"In this paper we explore private computation built on vector addition and its applications in privacy-preserving data mining. Vector addition is a surprisingly general tool for implementing many algorithms prevalent in distributed data mining. Examples include linear algorithms like voting and summation, as well as non-linear algorithms such as SVD, PCA, k-means, ID3, machine learning algorithms based on Expectation Maximization (EM), etc., and all algorithms in the statistical query model [27]. The non-linear algorithms aggregate data only in certain steps, such as conjugate gradient, which are linear in the data. We introduce a new and highly efficient VSS (Verifiable Secret-Sharing) protocol in a special but widely-applicable model that allows secret-shared arithmetic operations in such aggregation steps to be done over small fields (e.g. 32 or 64 bits). There are two major advantages: (1) in this framework private arithmetic operations have the same cost as normal arithmetic and (2) the scheme admits extremely efficient zero-knowledge (ZK) protocols for verifying properties of user data. As a concrete example, we present a very efficient zero-knowledge method based on random projection for verification that uses a linear number of inexpensive small field operations, and only a logarithmic number of large-field (1024 bits or more) cryptographic operations. Our implementation shows that the approach can achieve orders of magnitude reduction in running time over standard techniques (from hours to seconds) for large scale problems. The ZK tools provide efficient mechanisms for dealing with actively cheating users, a realistic threat in distributed data mining which has been lacking practical solutions.",CS,https://epubs.siam.org/doi/abs/10.1137/1.9781611972788.24
Machine Learning in Compiler Optimization,"In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.",CS,https://ieeexplore.ieee.org/abstract/document/8357388
"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",CS,https://arxiv.org/abs/2009.11278
A Survey on Causal Inference,"Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.",CS,https://dl.acm.org/doi/abs/10.1145/3444944
RLTCP: A reinforcement learning approach to prioritizing automated user interface tests,"User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution. This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test. We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines Reinforcement Learning (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant historical data, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the RL system can gain more insights into individual test cases. We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods. The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0950584921000574
Formal Verification of Smart Contracts: Short Paper,"Ethereum is a framework for cryptocurrencies which uses blockchain technology to provide an open global computing platform, called the Ethereum Virtual Machine (EVM). EVM executes bytecode on a simple stack machine. Programmers do not usually write EVM code; instead, they can program in a JavaScript-like language, called Solidity, that compiles to bytecode. Since the main purpose of EVM is to execute smart contracts that manage and transfer digital assets (called Ether), security is of paramount importance. However, writing secure smart contracts can be extremely difficult: due to the openness of Ethereum, both programs and pseudonymous users can call into the public methods of other programs, leading to potentially dangerous compositions of trusted and untrusted code. This risk was recently illustrated by an attack on TheDAO contract that exploited subtle details of the EVM semantics to transfer roughly $50M worth of Ether into the control of an attacker. In this paper, we outline a framework to analyze and verify both the runtime safety and the functional correctness of Ethereum contracts by translation to F*, a functional programming language aimed at program verification.",CS,https://dl.acm.org/doi/abs/10.1145/2993600.2993611
Combining Graph-Based Learning With Automated Data Collection for Code Vulnerability Detection,"This paper presents FUNDED (Flow-sensitive vUl-Nerability coDE Detection), a novel learning framework for building vulnerability detection models. Funded leverages the advances in graph neural networks (GNNs) to develop a novel graph-based learning method to capture and reason about the program’s control, data, and call dependencies. Unlike prior work that treats the program as a sequential sequence or an untyped graph, Funded learns and operates on a graph representation of the program source code, in which individual statements are connected to other statements through relational edges. By capturing the program syntax, semantics and flows, Funded finds better code representation for the downstream software vulnerability detection task. To provide sufficient training data to build an effective deep learning model, we combine probabilistic learning and statistical assessments to automatically gather high-quality training samples from open-source projects. This provides many real-life vulnerable code training samples to complement the limited vulnerable code samples available in standard vulnerability databases. We apply Funded to identify software vulnerabilities at the function level from program source code. We evaluate Funded on large real-world datasets with programs written in C, Java, Swift and Php, and compare it against six state-of-the-art code vulnerability detection models. Experimental results show that Funded significantly outperforms alternative approaches across evaluation settings.",CS,https://ieeexplore.ieee.org/abstract/document/9293321
Symbolic planning and control using game theory and grammatical inference,"A system can accomplish an objective specified in temporal logic while interacting with an unknown, dynamic but rule-governed environment, by employing grammatical inference and adapting its plan of action on-line. The purposeful interaction of the system with its unknown environment can be described by a deterministic two-player zero-sum game. Using special new product operations, the whole game can be expressed with a factored, modular representation. This representation not only offers computational benefits but also isolates the unknown behavior of the dynamic environment in a particular subsystem, which then becomes the target of learning. As the fidelity of the identified environment model increases, the strategy synthesized based on the learned hypothesis converges in finite time to the one that satisfies the task specification.",CS,https://www.sciencedirect.com/science/article/abs/pii/S0952197614002401
Energy-Efficient Algorithms,"We initiate the systematic study of the energy complexity of algorithms (in addition to time and space complexity) based on Landauer's Principle in physics, which gives a lower bound on the amount of energy a system must dissipate if it destroys information. We propose energy-aware variations of three standard models of computation: circuit RAM, word RAM, and transdichotomous RAM. On top of these models, we build familiar high-level primitives such as control logic, memory allocation, and garbage collection with zero energy complexity and only constant-factor overheads in space and time complexity, enabling simple expression of energy-efficient algorithms. We analyze several classic algorithms in our models and develop low-energy variations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays. We explore the time/space/energy trade-off and develop several general techniques for analyzing algorithms and reducing their energy complexity. These results lay a theoretical foundation for a new field of semi-reversible computing and provide a new framework for the investigation of algorithms.",CS,https://dl.acm.org/doi/abs/10.1145/2840728.2840756
A Fairness-aware Incentive Scheme for Federated Learning,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.",CS,https://dl.acm.org/doi/abs/10.1145/3375627.3375840
A Self-Regulated Learning Approach to Educational Recommender Design,"The field of education has the potential to better facilitate student learning by employing educational recommenders that adapt the learning process to the needs of individual learners. While existing research has shown promise and explores a variety of types of educational recommenders, there is currently a lack of research that ties educational theory to the design of these systems. The theory considered here, self-regulated learning, focuses on putting students in control of their learning and is appropriate in situations where learning is autonomous. This research proposes a design science approach to investigate a theoretical base of self-regulated learning (SRL) for a knowledge-based recommender design framework. Existing research on knowledge-based recommender design with an inclusion of an ontology component will guide development of this artifact. Anticipated results include the formation of design principles to inform the creation of SRL guided recommenders for both practical applications and future research.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/5/
AI for Connectivism Learning - Undergraduate Students’ Experiences of ChatGPT in Advanced Programming Courses,"Advanced programming skills are required for computing courses on merging topics, and students often struggle to develop these skills to solve complex problems. To address this challenge, faculty members provide additional lectures, practice sessions, and educational technology tools. This paper discusses the challenges faced by computer science students in developing advanced programming skills and explores the use of AI chatbots, specifically ChatGPT, as a support tool for learning. We study the engagement and effectiveness of ChatGPT in helping students learn advanced programming skills using two engagement learning frameworks (CIE and MELT) for evaluation. The study involves designing a computing lab exercise (design and code questions) for students to complete using ChatGPT and collecting data through surveys. The findings provide initial evidence that ChatGPT can be an effective tool for supporting student learning in advanced programming courses.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/16/
AI for Social Good (AI4SG) Education in an Undergraduate Introductory MIS Course,"We describe a pedagogical study in which we designed and implemented a module for undergraduate Management Information Systems (MIS) students, aimed at preparing them for an increasingly AI-impacted business environment and society. We developed a learning module that taps their inherent motivation to make a meaningful difference, challenging them to ideate applications of AI for social good (AI4SG), focused specifically on sustainability. We piloted the module in an existing introductory MIS course, first establishing a range of fundamental AI capabilities through hands-on demos and study cases. Then, with instructor guidance, the student teams, working in a social entrepreneurship ""start-up"" context, identified sustainability challenges impacting their own communities and worked together to propose and pitch AI-powered solutions. The results suggest that students find this approach deepened their understanding of sustainability issues in their communities, improved their knowledge of how AI could address social issues, and improved their confidence in their ability to innovate.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/20/
Technology-Enabled Peer Assessment and Feedback: A Design Science Research Study,"Peer assessment and feedback are important pedagogical practices that can improve student learning, engagement, and satisfaction. However, they also pose challenges such as bias, reliability, and validity. Technology-enabled peer assessment and feedback (TEPAF) systems can address some of these challenges by providing tools and features that support the process. This paper presents a design science research (DSR) study that aims to develop and evaluate a TEPAF system for higher education. The study follows the DSR methodology and uses a mixed-methods approach to collect and analyze data from students and instructors. The paper contributes to the literature by providing design principles and implications for TEPAF systems and practices.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/19/
Understanding the Impact of Digital Transformation on Teaching and Learning in Higher Education,"Digital transformation (DT) has been a significant force in reshaping higher education, especially in the wake of the COVID-19 pandemic. This study investigates the impact of DT on teaching and learning practices in higher education institutions (HEIs). Using a qualitative case study approach, the research explores the experiences of faculty members and students with DT initiatives. The findings highlight the benefits and challenges of DT, including increased flexibility, accessibility, and innovation, as well as issues related to digital divide, resistance to change, and the need for digital literacy. The study offers recommendations for HEIs to effectively implement DT strategies and enhance teaching and learning outcomes.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/9/
Bridging the Gap: An Interview Study on Challenges in Software Testing and Educational Needs,"As information systems become increasingly complex, software testing is being recognized as a key element in this process. However, organizations continue to encounter challenges in effectively implementing and executing testing procedures, highlighting the persisting difficulty arising from a lack of knowledge and awareness. Thus, it is crucial to understand the challenges related to software testing to point out educational needs in software testing. To gain a comprehensive understanding of the current challenges related to software testing, this study conducted an empirical cross-sectional interview study with 23 experts. Additionally, a literature review was conducted to investigate if there are any discrepancies or similarities between the current challenges identified in academic research and industry, emphasizing relevant educational needs in software testing. The software testing life cycle was utilized as a framework to systematically present the results. Overall, this study provides insights into the challenges of software testing and current educational needs.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/11/
Enhancing Student Engagement in Online Learning Environments: The Role of Gamification,"Online learning environments have become increasingly prevalent in higher education, but maintaining student engagement remains a challenge. Gamification, the use of game elements in non-game contexts, has been proposed as a strategy to enhance engagement and motivation. This paper examines the effectiveness of gamification in online learning environments through a review of existing literature and a case study of a gamified online course. The results indicate that gamification can improve student engagement, participation, and satisfaction, but its impact on learning outcomes is mixed. The paper discusses the implications for educators and provides recommendations for designing effective gamified online learning experiences.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/4/
Determinants of Gamification Effectiveness: Gamification Affordances and Coping Responses in the Context of Gamified ERP Training,"Gamified training is often used to facilitate the adoption of complex information systems in organizations. However, little research has focused on the factors that influence the effective use of gamified training. This study investigates the determinants of effective use of the enterprise resource planning simulation game, ERPsim. The study proposes that ERPsim affordances, such as collaboration and competition, affect coping responses, including task-oriented, emotion-oriented, and avoidance, which in turn impact the effective use of the game. The data were collected from 255 graduate students enrolled in an ERP course at a public university in the United States. The results show that collaboration affordance significantly affects all three coping responses, while competition affordance only affects task-oriented coping. Additionally, task-, and emotion-oriented coping are found to influence the effective use of ERPsim, but avoidance coping does not. The study highlights the importance of affordances and coping responses in gamified training design and implementation towards effective use.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/3/
Developing AI Literacy of Management Students using Problem and Project based Learning,"Artificial Intelligence (AI) tools are becoming everyday tool for business of all sizes. Business and management students who have good understanding of AI concepts, tools, technologies, and ethics will have an advantage in contributing to the integration of AI technologies in business. The objective of this study is to examine the effectiveness of using problem-based learning (PBL) and project-based learning (PjBL) to teach AI literacy to master's level management students in a university setting. A module on AI literacy was developed using PBL and PjBL, and a quantitative approach was employed to assess its effectiveness by measuring students' experiences and perceived AI literacy level before and after the module. The results indicate that both PBL and PjBL approaches are effective in developing AI literacy, with 69% of students reporting an increase in their AI literacy level after completing the module. The study also offers recommendations for future AI literacy education in business schools.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/13/
Do Asynchronous Courses Work? Comparisons of Student Performance in a Multimodal Undergraduate Database Course,"Advances in digital learning technologies and connectivity tools coupled with the COVID-19 pandemic have led to increased online course offerings. As online courses gain popularity, it becomes more important to assess their efficacy in assuring student learning. In the domain of Information Systems (IS) pedagogy, the teaching of database courses is under-researched. This study employs quantitative methods to evaluate differences in academic performance between students in an on-campus modality compared to an online asynchronous offering of the same introductory undergraduate database course. In particular, we compare mean scores across the performance evaluation categories of attendance, quizzes, assignments, final exam, and cumulative final scores. Results indicate the raw mean scores for the on-campus section exceeded the online section across every category, but only assignment and attendance averages were significantly higher for the on-campus section when bootstrapped across 1000 samples. These results inform remedial measures to improve student engagement in these categories.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/14/
Effects of Business Simulation Games on IS students’ Resilience: Instructors’ Perspectives,"While resilience is acknowledged as a complex construct and one that is difficult to assess, universities are recognising its importance. They are beginning to invest in research and services aimed at building student resilience. However, there is limited research into the levels of, and contributors to, student resilience. This study explored the effects of an experiential learning tool, business simulation games (BSGs), on the development of IS students' academic resilience. The researcher interviewed instructors conducting ERPsim labs to find out their views regarding the impact of BSGs on three aspects of the Resilience at University (RAU) scale: academic buoyancy, personal competence and social competency. Findings from the instructors' interviews demonstrated that Business simulations help students build resilience by offering a safe environment where they can fail, learn, and try again. Simulations may educate students on managing stress and pressure, remaining focused on their objectives, and having a good attitude.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/26/
Establishing a Gamified Programming Practice Environment with Immediate Feedback,"Feedback serves a key role of closing the gap between students’ current understanding and desired learning. Immediate feedback helps students identify and correct misconceptions, motivates them to acquire knowledge, and increases efficacy and motivation to learn. However, although traditional teaching methods provide feedback to students, students cannot get answers to problems immediately under the condition of many students taking courses, which interrupts their learning enthusiasm. Although teachers use gamification mechanisms and elements to improve students' learning motivation, in gamified learning without immediate feedback, the game elements have little help to students’ motivation. With the guidance of self-determination theory, this project changes the traditional way of handing in practice assignments and develops a highly motivating SQL programming practice exercise that is not limited by time and space, allowing exploration and trial-and-error process surroundings. The system completely records the content of each activity process and problem-solving so as to provide teachers with after-the-fact review and serve as a reference for classroom teaching. Taking the immediate feedback automated assignment correction system platform as the core, we encourage students to challenge themselves through the gamified self-element (goal setting, experiences, level, and badge) and cooperate or compete with peers through social elements (leaderboard, achievement visibility, likes and shares). We provide students with an engaging but meaningful learning experience in gamified scenarios. This project analyzes a large number of students’ learning process data recorded in the system. With the support of rich data, it shows the details of the students’ process of completing the challenge tasks like a magnifying glass, and provides a dynamic lab to verify the causal relationship between the students’ learning process and learning outcomes from the behavioral level (rather than the attitude level). From the perspective of learning gamification, this project explores how gamification elements in the learning process affect students’ learning engagement and learning outcomes. The results of this project help to gain further understanding of learning gamification.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/10/
Explain AI-Based Essay Scorings without XAI - Empirical Investigation of an User-Centered UI Design for AI-Based AES Systems,"The lack of understandability of AI-based decisions is increasingly posing trust-related and regulatory problems. This also applies to the educational sector, where AI is a central element of modern automated essay scoring (AES) systems. However, current research on explainable AI primarily focuses on complex technical approaches. These explanations usually show a lack of understandability by the actual users, who often have no knowledge of AI. Based on an experiment with 245 students at a German university, we were able to show that even the basic principles of user interface design can improve understandability and hereby trustworthiness. Thus, the use of visual elements promotes understandability even when only little information is provided. Especially when providing further AI-specific information on the scoring of AES systems, however, it must be considered that in combination with visual elements an information congruency can be observed, leading to a cognitive overload in the worst case.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/6/
Exploring the Perception of a Chatbot's Language Style in a Learning Situation,"In this Wizard-of-Oz experiment, we assess the impact of language style (dominant vs. submissive) on students’ perception of a chatbot in a learning context. 38 probands were involved in this within-subject experiment while learning about Digital Literacy in a guided text-based conversation via Slack, in which they supposedly interacted with a chatbot. We quantitatively measured constructs in a follow-up survey and discussed implications with the probands and 14 other students from a bachelor’s course on digital transformation. Results show that the dominant language style significantly negatively affected learners’ enjoyment during the interaction, but did not reveal a significant influence on perceived competence, empathy, identification, trust, or usefulness between the two contrasted language styles. Our qualitative results indicate that language style preference depends on learning context, interaction time, and learner personality. We reflect on future research needed to build upon these initial findings.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/2/
The Effect of Learning Analytics Dashboards on Student Motivation and Performance,"Learning analytics dashboards (LADs) provide students with visualizations of their learning progress and performance. This study investigates the effect of LADs on student motivation and academic performance in an undergraduate course. Using a quasi-experimental design, the research compares students who had access to LADs with those who did not. The results show that LADs can enhance students' motivation, self-regulation, and academic achievement. The paper discusses the implications for the design and implementation of LADs in educational settings.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/18/
Grade-O-Matic: A Tool to Test and Grade Programs,"To improve learning in higher education, the primary focus should be on engaging students in a process that best enhances their learning--a process that includes feedback on the effectiveness of their learning efforts. If that is true of most subjects of higher education, it is doubly so for teaching programming in the college classroom. Learning programming and related topics requires experience and personal commitment. It requires effort, often prolonged effort, over time. Unfortunately, students (and business students in particular) regularly seek ways to minimize their investment of time and maximize their grade outcome. The only recourse for MIS/IS instructors is, then, to create graded programming assignments with a variety and quantity sufficient to coerce students into investing the time necessary for learning. However, grading student programs is a labor-intensive responsibility for faculty members. It is tedious and error prone. Grading programs consistently and in a timely manner is often a challenge for faculty members. Delays in graded feedback are detrimental to student learning. This paper presents an innovative tool called Grade-O-Matic. It can be used by both students and instructors to grade programs quickly and in large quantities. By using the Grade-O-Matic tool, faculty members can assign more graded programming assignments, and students can get immediate feedback on their work before turning it in to be graded. This paper describes the Grade-O-Matic tool, and its creation, from its inception to its current state using terminology of the Design Science Research Methodology (DSRM) [Peffers, et. al., 2007]. The distinctives of the tool, and how it is different from similar efforts, are noted. The paper concludes with anecdotal evidence to support the claim that the tool, and the immediacy it affords, supports student learning. Also noted are generalized principles that may support future efforts.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/7/
How Today’s AI Content Generators Outperform Average Novice Students in Information Systems Exams,"The public availability of sophisticated AI content generators based on large-scale language models achieved enormous media attention in recent months. AI content generators are expected to have a major impact on future information systems developments as well as private and business life. Particularly in the field of education, the question arises as to how these innovations in AI technology will impact teaching, learning, and examination. With a specific focus on examinations in the information systems discipline, we show in this research-in-progress paper, based on an experimental field study, how a today’s AI content generator performs in solving a fundamental information systems exam. Based on our preliminary results, we found that the quality varies across different exam question types. With this research project, we offer start points and initial guidance on how educators can deal with the availability of AI content generators.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/12/
Impact of Technology-enabled Active Learning (TEAL) on Learning,"Using an active learning framework proposed by Shroff et al (2019), our study evaluates the impact of technology-enabled active learning (TEAL) on student learning in an Australian business school. Our study used the individual reflections of accounting students as the data collection strategy and content analysis. Our findings suggest that the three constructs of the framework - interactive engagement enabled by the technology during and after the workshops, development of problem-solving skills throughout the learning process, and individualized learning context facilitated by the accounting technology have all positively contributed to the learning effectiveness. We also found that technology-enhanced scaffolds designed in the learning process have aided in the consolidation of learning and increased learning effectiveness. However, our study observed that while technology was expected to have a positive influence on students’ curiosity and interest - the fourth factor in the framework, lack of student effort, poor timing of feedback, and the absence of a sense of challenge, have limited learning.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/15/
Exploring the Use of Virtual Reality for Collaborative Learning in Higher Education,"Virtual reality (VR) offers new opportunities for collaborative learning in higher education. This paper explores the use of VR for group projects and teamwork in a university setting. Through interviews and surveys with students and instructors, the study examines the benefits and challenges of using VR for collaborative learning. The findings suggest that VR can enhance communication, engagement, and creativity among students, but also presents technical and logistical challenges. The paper provides recommendations for integrating VR into collaborative learning activities.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/25/
Is Academic Integrity at Risk? Perceived Ethics and Technology Acceptance of ChatGPT,"The academic community is experiencing a significant disruption thanks to a new technology called ChatGPT. There is news almost daily about how Large Language Models technologies have the potential to allow students to cheat undetected. This leads many universities to discuss ways to regulate and even ban its use. But is this necessary? The previous research focuses mainly on universities' and lecturers' views but fails to consider the students’ views. Do students even want to use the system, and if they do, will they use it for nefarious purposes? Extending the Technology Acceptance Model with Perceived Ethics, this quantitative study researched how students perceive ChatGPT from an ethical perspective and how that impacted their intention to use the system for school work. 277 Students from 40 countries took part in the survey. Results indicated that Perceived Ethics has a direct positive influence on the attitude toward usage but only an indirect effect on the behavioral intention to use the system for school work. This study is one of the first to consider how students perceive ChatGPT from an ethical perspective and how that impacts their perception of and intention to use the Large Language Models technology system.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/24/
A Review of the Literature on Gender and Sexual Minorities and ICT,"Gender is essential to our identity. As information and communications technology (ICT) permeates every aspect of everyday life, a body of literature has emerged that broadens our understanding of IT and gender. A growing number of studies have shed light on the implications of ICT for gender and sexual minority populations in various areas. Gender and sexual minorities commonly referred to as LGBT consist of people whose gender identity diverges from predominant conceptions of masculinity and femininity or whose sexual orientation differs from the majority. Previous literature reviews on IT and gender have focused on the underrepresentation of women in the IT landscape. This paper presents the preliminary results of a literature review focusing on gender and sexual minorities and ICT. This paper contributes to the IS literature on gender by providing a foundation for ideas on ICT’s role in mitigating the stigma associated with gender and sexual minority status.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/7/
Augmenting the Framework for Design Science Research Goals and Evaluation Criteria with Aspects of Society 5.0,"In the field of information systems, new technologies are giving rise to a wide range of new research topics that present researchers with new challenges. Innovations, such as artificial intelligence, have an enormous impact on society, in addition to technological progress towards automation, above all or precisely because of this. In this context, Society 5.0 was initiated to establish criteria for research and its artifacts. In this paper, evaluation criteria of a Society 5.0 are identified, which should be considered for Design Science Research artifacts in the future. Through a comprehensive literature review, 14 evaluation criteria and a new DSR goal were identified and integrated into the existing framework of Hevner et al. (2018). The paper provides researchers with an augmented DSR evaluation framework with aspects of Society 5.0.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/9/
Barriers to the Use of Action Research in Information Systems - A Literature Review,"The current digitalization is a disruptive, challenging process that causes a social-technical transition in organizations. Due to that, there are more and more use cases in companies. Action research as a form of social research combines theoretical knowledge and practical execution, which is why it is suitable for current challenges. In several cycles, a literature analysis is carried out to build the knowledge base and establish hypotheses. Based on this, an action is then performed, which is subsequently analyzed, whereupon the knowledge base is expanded through literature. Since action research has not been used much in information systems so far, it is important to identify the reasons for the low use of action research in the research area of information systems. In this paper a systematic literature review was used to identify the barriers that explain the low use of action research in information systems. The findings are twelve different barriers, which are classified into three categories. The categories are ""Human Factors"", ""Method Factors"" and ""Application Factors"". In addition, the research paper concludes with specific implications for these three categories.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/11/
Design Principles in Information Systems Research: Trends in Construction and Formulation,"Design knowledge has become increasingly important in information systems research in recent years, with Design Science Research (DSR) as an approach to developing innovative and effective artifacts. Design Principles (DPs) have proven to be a popular tool for contributing to abstract design knowledge. The construction and formulation of DPs has become more professional in recent years, with publications providing guidance. However, the implementation by researchers of these rules needs to be investigated. To address this gap, we conducted a systematic literature review, analyzed the various forms of design knowledge that have emerged, and examined the state of design principle construction and formulation. Our analysis shows that in recent years there has been a significant increase in the number of publications dealing with DP design. Our results shed light on the characteristics and evolution of DPs, as well as their construction and formulation, and provide valuable guidance for future research.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/12/
Digital Transformation: From Traditional IS Theories to Emerging Assemblage,"Digital transformation (DT) is a complex and dynamic phenomenon that requires a significant shift in organizational culture and mindset, necessitating new theoretical perspectives to understand the interactions between technology and people. This article conducts a critical literature review of the extant literature on DT, contrasting traditional Information Systems (IS) theories with assemblage theory. Assemblage theory provides a dynamic framework to analyze the various components of DT and their interactions, acknowledging the significance of power dynamics and politics in shaping the direction and outcomes of DT initiatives. This paper provides different theoretical perspectives in digital transformation literature by analyzing 70 articles published in premier IS journals. This article is helpful for scholars and practitioners interested in understanding the complexities of DT through assemblage theory perspectives.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/10/
Emerging ICT for Sustainable Development. Research Concept of Literature Analysis,"The large number of publications in databases such as Web of Science or Scopus points to the growing interest of scientists in the importance of emerging Information and Communications Technologies (EICT) for ensuring sustainable development (SD). This article describes the concept of the research focusing on recognizing the state of research on artificial intelligence, big data, cloud computing, and Internet of Things adoption in various sustainability areas. The study concept was inspired by the authors' experience and the previous study results which aimed at providing a holistic view of the importance and scope of ICT research in the SD context. The article presents the four-stage research procedure and describes the results of the first three stages, which are preparatory. The scope of the study was indicated, the choice of the bibliometric database was justified, examples of prepared queries were given, the need to unify keywords was explained and examples of standardized words were given.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/2/
Gamification and Marketing Management: A Literature Review and Future Agenda,"Given the motivational effect of game elements and mechanisms on user experience in various non-game contexts, gamification has widely been used as an effective marketing technique to enhance the performance of business practices. In the past decade, a variety of studies have explored and investigated the value that gamification can provide in consumer-facing marketing activities. However, there is still a dearth of granular understanding of how gamification in marketing has been studied in the current literature. This paper follows the PRISMA literature review process and systematically reviews 93 papers consisting of 111 empirical studies on gamification and marketing management. The synthesized findings provide a holistic picture of the adopted research methods, different investigated gamification affordances and advergames, various gamified industries, and marketing performance indicators regarding products, services, and brands. Five agenda points, mainly relating to methodology and themes, are further suggested.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/1/
"Implications of Axiomatic Theory: I Come to Praise, not Bury, this Approach to Accumulating IS Knowledge Introduced by Lee et. all (2021)","Axiomatic theory is a concept introduced in Lee et al. (2021) who illustrated its application in connection with three theories borrowed from reference disciplines and extensively used in IS research. This paper aims to extend consideration of the concept to more general application beyond these particular theories. The concept suggests that some theory is so close to being a truism that it needn’t be further tested and could reliably be the basis upon which additional knowledge can be built. Thus, such theoretical propositions might be treated like axioms in mathematics rather than tentative theory subject to further testing. In this paper this premise is examined in terms of the general role of theory in accumulation of knowledge, when such axiomatic status might be earned by the assertion of particular relationships, and how such axiomatic theory might be usefully applied.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/8/
Management of IT Costs in the Digital Age – A Literature Review,"Digitalization requires organizations to strategically invest in information technology (IT). As a result, the costs associated with IT in companies are rising and technological progress changes the setting for IT management. This poses challenges for IT managers to ensure spend-efficiency and manage IT costs transparently. However, no current literature review gives an overview of how IT cost management (ITCM) research dealt with past transformations. This paper aims to investigate ITCM concepts considering their historical context. It then derives implications for the digital age and identifies future research fields. The historical literature review reveals that ITCM research evolved with technological advances and the target to manage all IT-related costs and evaluate the impact of IT spend. However, the presented concepts lack consideration of current changes that hamper spend-efficiency and strategic decisions. Hence, this paper enables future research to address the identified research gaps. Additionally, practitioners gain awareness of how they can benefit from developed ITCM concepts.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/3/
Research methods in 30 years of information systems – a bibliometric trend study,"The research methods used by information systems scholars have changed in the course of thirty years. For example, the availability of big datasets and processing power have opened new avenues of computational research. Yet few studies have quantified the changes, and we know little about their impact on and implications for the discipline. In this paper, we present a two-stage bibliometric study. We first develop search terms that are associated with research methods in IS. We then use those in 62 searches in eight leading IS journals, to chart the relative popularity of methods over time. We identify key methodological trends, including a steady rise of longitudinal research. We also find evidence of a diversification of methods, with more methods growing than declining in relative occurrence, and identify the possible implications of this changing portfolio that deserve further study.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/4/
Social Media Enabled Social Movements in Information Systems Research: A Lakatos Approach to Review of Literature,"Research on social media applications in the context of social movements is still in its nascent stages in information systems research. However, there have been a number of empirical studies in addition to conceptual studies in recent years. The objective of this paper is to propose a literature review to assess the current state of knowledge in this research program and, thus, identify opportunities and avenues to build on this body of knowledge. Informed by Lakatos (1970) approach, we analyze the core and the protective belt of the current scholarship in this area. We utilize Boell’s (2014) hermeneutic framework to conduct our literature review. Our research will assist scholars in this field to identify research papers and core theories in the research program relevant to their work, in addition to suggesting avenues for future theoretical development and research, utilizing hitherto unexplored methodologies, technological platforms, and social movement contexts.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/6/
Using Digital Performance Feedback for Behavior Change: A Review,"Feedback on task performance is a key component for improvements and learning for many behaviors. Although the IS literature has provided relevant individual contributions on the design, outcomes, and mechanisms of performance feedback, there exists no overarching conceptual framework or theory to integrate those results for a holistic explanation of behavior change through IS-enabled feedback. Further, an overview of the existing literature to facilitate theory development is still missing. This paper presents a systematic literature review on performance feedback in the IS discipline by reviewing 23 articles from high-quality journals. The review provides an overview on the research landscape by coding characteristics of feedback systems and outcomes of the research. It highlights four behavioral mechanisms that performance feedback influences to improve task performance (attention, behavioral control, personal norms, and perceived social norms) and concludes by presenting possible directions for future research.",IS,https://aisel.aisnet.org/amcis2023/meta_res/meta_res/5/
A Framework to Understand the Emergence of SQB: Observations Through a New Lens,"Although change initiatives are a frequent and critical need in contemporary organizations, an individual’s propensity to resist change is frequently reported. Resistance is the consequence of the cognitive and behavioral responses of people affected by the change. This phenomenon has the potential to change the current status quo of many individual and group-level theories, particularly those addressing the ""why"" and ""how"" of resistance to change. This study uses an interpretive research approach with the use of the grounded theory method and adopts the SQB theory to observe the resistance to change in response to SQB emergence. This study employs four focus group discussions that yield a framework to understand the emergence of SQB. The study identifies an inter-play among SQB emergence framework constructs: actor, entity and time. This study assists in identifying new frameworks and paradigms for the SQB perspective that can be used in changing conditions.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/8/
An Investigation to reduce Overreliance on Explainable AI (XAI) in light of Two System Theory,"As technology is evolving, there is a rise in the use of AI systems. The increased use of AI systems has revealed issues of gender and racial biases. To address these issues, explainable AI (XAI) is introduced, but the use of XAI has triggered various kinds of biases leading to issues such as overreliance. In this study, we seek to devise interventions to mitigate the issue of overreliance on AI by better understanding cognitive biases and acknowledging that different users have different cognitive abilities, and we need to be mindful of that when we design XAI systems. We will conduct multiple experiments using the recidivism dataset collected by ProPublica and to develop a better understanding of and solutions to mitigate the issue of overreliance. The findings from this research will allow us to design XAI systems better, improving user trust in AI and further improving AI adoption.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/1/
Decision-Making Styles in Metaverse: Effects of Immersion and Embodiment,"Decision-making is a vital skill of our daily cognitive arsenal. The rise of virtual reality (VR) worlds like the metaverse, have created a new need to investigate human behavior under a technologically novel and multisensory prism. In this study, we experimentally investigate types of decision-making in artificial realities mediated by different levels of immersion (PC monitor vs. VR HMD) and sense of embodiment (self-motion vs. self-anchored). Participants (N=183) conducted a daily-life decision-making task of financial allocation, based on evaluating either a 3D graph or 2D graph containing price information across different periods. Five decision-making styles are evaluated including Rational, Intuitive, Dependent, Avoidant, and Spontaneous. Our results indicate that decision-making styles do not differ between diverse virtual realities, and instead remain similar to the control 2D condition. However, due to the flexible nature of decision-making it is possible that content and environmental factors are still likely to influence decision-making in VR experiences.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/3/
Does being emotionally inexpressive in personal profile photos affect hiring assessments in online professional social networks?,"Online personal profile photos (PPPs) can serve as a source of impression formation and judgment about individuals. This study investigates how the emotionality expressed in a job candidate’s online PPP on a professional social network (e.g., LinkedIn) affects a recruiter’s perceptions of them. Drawing on social role theory and classifying job roles along agency (agentic) and communal dimensions, this paper proposes that job candidates whose emotional expression or inexpression in their online PPP is consistent with qualities associated with agentic and communal job roles are more likely to be perceived as competent and fit for the job role.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/2/
Early Chatbot Assistance Can Enhance Team Decision-Making by Promoting Cognitive Diversity and Information Elaboration,"As AI increasingly assists teams in decision-making, the study examines how technology shapes team processes and performance. We conducted an online experiment of team decision-making assisted by chatbots and analyzed team interaction processes with computational methods. We found that teams assisted by a chatbot offering information in the first half of their decision-making process performed better than those assisted by the chatbot in the second half. The effect was explained by the variation in teams’ information-sharing process between the two chatbot conditions. When assisted by the chatbot in the first half of the decision-making task, teams showed higher levels of cognitive diversity (i.e., the difference in the information they shared) and information elaboration (i.e., exchange and integration of information). The findings demonstrate that if introduced early, AI can support team decision-making by acting as a catalyst to promote team information sharing.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/6/
Lifting the Magic Curtain - How Transparency on Social Norm Nudging affects its Efficacy,"This study analyses how the effect of a social norm nudge changes when its function and purpose become transparent. For this purpose, a quantitative, experimental online survey was conducted among 474 participants in the context of voluntary CO2 offset. The non-transparent social norm nudge and the one with information solely on its function showed no significant effect on behaviour. However, the social norm nudge disclosing both, its function and purpose, did effect behaviour positively. Based on the results, it is recommended that social norm nudges are made transparent in practice. The recipient of a social norm nudge should therefore be explicitly notified about its function and purpose.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/5/
In or Out of Sync? A Psychophysiological Approach to Understanding Mood and Team Performance in Online vs. In-Person Dyads,"In this study, we conduct a laboratory experiment with online vs. in-person dyads who are placed in convergent or divergent moods using mood induction.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/9/
The use of cognitive neuroscience tools for evaluating the cognitive overload caused by social advertising,"Social campaigns influence attitudes in society, but too much information can cause cognitive overload. Traditional methods for measuring cognitive overload are subjective. There is potential for using cognitive neuroscience methods because these tools measure consumers' unconscious responses and provide better understanding of cognitive functions. We have elaborated an experimental framework to enable the assessment of media messages in social campaigns concerning the cognitive overload that they cause. The proposed framework will be tested by performing an experiment on specific social advertising to prove its usefulness and show the research possibilities that it offers. As a result, we expect to expand the framework into a system that could be used by practitioners for evaluating social advertising before launching the campaign.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/10/
Who is More Likely to Initiate Referrals? Effect of User’s Regulatory Focus on Referral Intention,"The online referral reward program (ORRP) is a social marketing method by which firms reward existing users and encourage them to recommend products or services to their friends. Prior research has primarily focused on the impact of ORRP design on users’ participation, however, the role of individuals’ characteristics is unexplored. Based on regulatory focus theory and self-efficacy theory, this research proposes and investigates the effect of users’ regulatory focus (promotion-focused vs. prevention-focused) on their referral intention and explores its mechanism and boundary condition. The results of three experiments show that compared to prevention-focused users, promotion-focused users have higher self- efficacy to complete the referral task; and thus, have higher referral intention. This effect will be attenuated when the tie strength between inviters and invitees is strong. The findings not only contribute to the research on ORRP and regulatory focus but also provide guidance for firms to optimize ORRPs.",IS,https://aisel.aisnet.org/amcis2023/sig_core/sig_core/7/
A Configurational Approach to Examine Technostress Creators,"The phenomenon of Technostress (TS) - the stress experienced by end-users of Information and Communication Technologies (ICTs) - is often considered as a dark side of technology use. TS creators, factors that create TS, is a second-order construct with dimensions techno-overload, techno-invasion, techno-complexity, techno-insecurity, and techno-uncertainty. This study takes a holistic approach by proposing how the complex interactions among individual characteristics, IT features, and environmental factors influence employees’ perceptions of TS creators. Based on past research, we include three personality traits, i.e., neuroticism, personal innovativeness in IT, and IT mindfulness for individual characteristics. Additionally, two technological features, presenteeism and anonymity, are considered, while literacy facilitation, technical support provision, and social support are the environmental factors. Data will be collected using an online survey from full-time employees and analyzed using fuzzy sets qualitative comparative analysis (fsQCA). We also discuss the expected results and contribution of the study.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/3/
A Double-edged Sword? Algorithmic Management and Workers' Proactive Service,"Platforms increasingly rely on AI algorithms to execute automated task allocation, monitoring, and performance evaluation of workers in the gig economy. Platforms also expect AI algorithms can prompt gig workers to provide proactive services to customers for a better reputation. However, it remains unclear how algorithmic management impacts the proactive service behavior of gig workers. Based on self-determination theory, this emergent research forum paper proposes a double-edged sword effect of algorithmic management due to conflicting work motives: algorithmic management can increase gig workers' proactive service behavior via autonomy motivation, whereas it can also inhibit proactive service behavior via controlled motivation. A three-wave longitudinal survey will be conducted to test the posited double-edged sword effect. This study has the potential to contribute to the algorithmic management literature in IS field by uncovering the 'black box' between algorithm management and proactive service behavior of gig workers from the perspective of work motivation.",IS,https://aisel.aisnet.org/amcis2023/sig_cnow/sig_cnow/3/
A Typology of Social Media Affordances during Social Movements,"This research proposes a typology of social media affordances in the context of social media-enabled social movements. Social media technologies shape and facilitate the spread of such social movements by lowering individuals' participation costs, increasing general accessibility to information, fostering connectivity, and creating a platform where users can generate content. We propose a typology of affordances to theorize the spread of these movements from the embryonic stage to its established stage. To do so, coalesced affordance, a new type of affordance, and the existing shared, collective, and connective affordances are used. We also integrate the role of social networks and network theory in developing the proposed typology. Finally, the study's implications, limitations, and future research direction are discussed.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/33/
Affordance Theory for Information Systems project implementation: a process and organizational outlook.,"This paper explores the Information Systems project implementations in organizations. It focuses on the actualization of the affordances that result from the intertwining of the Information Technology (IT) artefact and the organization and we answer to the following research question “How do organizations actualize affordances?” With a qualitative multiple case study on the different local entities of an international leading retailer, this research identifies that previous research omitted the top management sponsorship as one of the main influences for the actualization process. Moreover, constrains perception is observed in the collected data and its role is assessed. This paper contributes the development of the affordance theory by providing an updated process-based integrative theoretical framework for affordances at the organizational level, aimed to support further research on Information Systems.",IS,https://aisel.aisnet.org/amcis2023/sig_osra/sig_osra/5/
Exploring the Impact of Blockchain and Distributed Ledger Technology Adoption on Business Process Innovation,"The goal of the current paper is to examine the impact of blockchain technology adoption on an organization’s business process innovation. Blockchain technology has been widely acknowledged for its usefulness in securing business transactions and benefits from decentralization and transparency. While blockchain implementation has notable advantages in a business process, such outcomes also rely on the nature of the business. Our research employed two theoretical concepts to measure blockchain adoption and its impact on an organization’s business process innovation: technological, organizational, and environmental (TOE) framework and task-technology fit. We specified hypotheses and tested them using survey responses. A total of 419 responses were collected and we analyzed them using partial least squares (PLS). Detailed results and discussions are addressed.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/38/
Application of Fourth Industrial Revolution Technologies in Healthcare,"The aim of this systematic literature review (SLR) is to explore how Fourth Industrial Revolution (4IR) digital technologies are transforming the healthcare sector. Peer-reviewed, full-text research papers published between January 2016 and April 2021, which focused on data-driven healthcare, health data management systems, and the disruptive role of digital technologies in healthcare were retrieved from three electronic databases, namely PubMed Central, Institute of Electrical and Electronics Engineers (IEEE) and MEDLINE. Articles included in the SLR were identified and selected using the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guideline. Three main themes were identified from 84 research papers included in the SLR, namely, (i) digital technologies driving the transformation of healthcare, (ii) the impact of digital transformation on healthcare, and (iii) areas of transformation in healthcare. The results showed that technological innovations can mitigate the impact of unequal distribution of healthcare services, reduce healthcare costs, improve the quality of care, and the quality of life for patients and healthcare professionals.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/16/
Artificial Intelligence applications in diagnostic medicine: a decade of expectations,"Considered a new paradigm in healthcare, AI is expected to change diagnostic medicine in the coming decades. However, its future is still uncertain - partly because it is an emerging technology being gradually applied to a mature field of medicine. We conducted a global cross-sectional survey with more than 1,400 authors of recent scientific publications indexed by the Web of Science to foresee the future of AI in diagnostic medicine. Most respondents expect AI to change diagnostic medicine in this decade radically. In this period, the two most likely outcomes are reduced screening costs and increased diagnostic reliability. X-ray diagnosis and heart rhythm interpretation are the two diagnostic tools most likely to be integrated with AI. The two main barriers are the difficulty of incorporating it into clinical practice and the ethical-regulatory issues. Respondents’ expectations align with the literature and suggest that AI may substantially change diagnostic medicine within this decade.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/14/
Black-box Models’ Explainability: A Theoretical and Practical Perspective,"The lack of explainability remains a critical challenge to the widespread adoption of artificial intelligence (AI) in many fields. “Understanding brings in trust”, while machine-learning models offer superior prediction accuracy, understanding the underlying logic is equally important to foster trust in these models. In this paper, we present eXplainable AI (XAI) as a solution to this challenge. Our research focuses on three key aspects of XAI: mathematics, humanities and social sciences, and practical applications. We demonstrated the feasibility of XAI through the use of artificially-constructed and model-derived ground truth, and verified performances of different XAIs. We also explored three dimensions of explainable consistency and emphasized the significance of human-machine consistency. Finally, we applied our research to a real-world scenario by cooperating with a national bank in China. Our findings highlight that XAI is both mathematically and practically meaningful, but more efforts need to be dedicated to this human-machine communication field.",IS,https://aisel.aisnet.org/amcis2023/conf_theme/conf_theme/23/
Can AI Chatbots with Anthropomorphic Attributes Enhance User Engagement in Emotional Support Settings? Investigating the Role of Conversational Styles and Avatar Type,"Although AI-based chatbots have become increasingly prevalent in various business practices and show promise in providing emotional support, there is still limited understanding of the mechanism that underlies the relationship between the anthropomorphic attributes of AI-based relational chatbots and user engagement. To address this gap, we draw on the social judgment theory and propose that emotional judgment mediates the relationship between conversational styles of chatbots (agentic-style and communal-style) and user engagement. We also examine the potential boundary effect of two types of AI avatars (humanlike vs. cartoonlike) on this relationship. To test our hypotheses, we plan to conduct two studies, which will include online and lab experiments. We aim to extend existing theories involving social judgment theory and human reactivity to AI-based chatbots in emotional support settings and identify how different types of anthropomorphic attributes of AI-based relational chatbots may jointly impact user behaviors.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/8/
Can Enterprise Architecture Management Professionals Measure Technostress Levels and Help Implement Coping Strategies?,"Modern ICTs can lead to higher stress levels among employees due to increased demands and always being accessible, which is known as “technostress”. Traditionally, Human Resources (HR) departments take actions against technostress. Yet, due to its complicated nature, we argue that Enterprise Architecture Management (EAM) can also contribute in technostress management by measuring technostress levels and help co-designing coping strategies in organisations. Comparing the knowledge levels of EAM to HRM professionals in a survey, we found no significant superiority of the latter group in measuring technostress. However, EAM professionals had higher scores in five technostress and three inhibitor dimensions. The study contributes to theory and practice by i) studying the technostress knowledge levels in different contexts, ii) offering EAM as a further technostress inhibitor, and iii) inviting EAM professionals into the discourse to combat technostress in their organisations by demonstrating that they have the required knowledge to lead such conversations.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/13/
Catastrophe Bond Trading Can Boost Security Improving Cyber (Re-)Insurance Markets,"The interdependent and correlated nature of enterprise cyber-risk is a cause of failed low capital cyber (re-)insurance markets to improve cyber-security. In this paper, we propose the use of catastrophic (CAT) bonds as a radical and alternative residual cyber-risk management methodology to alleviate the big supply-demand gap in the current cyber (re-)insurance industry, by boosting capital injection in the latter industry. We lay down conditions under which it is feasible for a cyber-insurer to invest in (a) CAT bonds, and (b) cyber re-insurance services only - both (a) and (b) contributing to dense security-improving cyber insurance markets. Our main result proves that while the use of cyber-CAT bond instruments solves the capital injection problem in cyber (re-)insurance markets to improve enterprise cyber-security, the level of cost to mitigate IT/ICS client information asymmetry (IA) on enterprise cyber-posture is an important factor that can decide the density of cyber-CAT bond trading markets.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/6/
Cloud-based Accounting Information Systems in SMEs. Insights from Poland,"The literature proves that the implementation of Accounting Information Systems (AIS) and Enterprise Resource Planning (ERP) systems in Small and Medium-sized enterprises (SMEs) positively affects their performance. As to the systems offered in cloud computing, the previous research mainly focused on the benefits of choosing such a solution, eliminating many barriers in SMEs in the case of traditional on-premise solutions. In Poland, studies into Cloud-based AIS or Cloud ERP have not been previously described in scientific research. This article presents the results of an empirical study of 35 Polish SMEs running their accounts. The thematic scope went beyond the issues examined so far and included the dominant data processing models, types of AIS systems, the use of AIS to perform accounting functions, and the integration of functional areas with the accounting system. The main results indicate that in the surveyed SMEs: 1) cloud computing (CC) is the dominant model of data processing, 2) slightly more companies use stand-alone AIS than integrated ERP systems, 3) AIS is not used to perform all important accounting functions, and 4) most often the accounting system is integrated with the purchasing and sales system and the fixed assets system, and less frequently with the payroll and production service systems.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/1/
Consumer Participation in Virtual Product Design: 2D vs 3D Product Configurators,"The purpose of the proposed study is to investigate the effect of configurator properties, such as the type of visualization and interaction offered, on the value that consumers attach to co-created products and the satisfaction they derive from the design process. To do this, this study will compare 2D product configurators with 3D product configurators. 2D product configurators use a series of images for product visualization whereas 3D configurators use 3D models that provide consumers with the ability to rotate objects in three dimensions. Drawing from the theory of cognitive dissonance, the theory of effectance motivation and the behavioral decision-making literature, this paper suggests that configurator properties influence perceptions of control and effort expenditure which in turn elicit various processes that influence the perceived value of co-created products and consumers’ satisfaction with the design process.",IS,https://aisel.aisnet.org/amcis2023/sig_hci/sig_hci/3/
Corporate Social Responsibility Consistency: Moral Sensemaking from Top Management Team to Online Social Media,"Drawing on organization character theory and corporate social responsibility (CSR) sensemaking, this study explores the CSR consistency of moral sensemaking in the company from top management to social media communication channels. First, we measure CSR moral sensemaking in top management team (TMT) letters and firm-generated content (FGC) on social media platform based on the dictionary developed from qualitative content analysis, topic modeling, and CSR expert rating. Second, we compare the moral sensemaking between two communication channels. Third, we investigate the relationship between moral sensemaking consistency and CSR engagement of the company. Our results contribute to the literature in business ethics, management, and social media research in IS. The study will also offer an alternative way for CSR evaluation and bring practical implications to CSR investors and rating agencies.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/19/
Deep Neural Network at Edge: An Exploration of Hyper Parameter Tuning on Plant Seedling Dataset,"This study explored different hyperparameters and their outcomes on multiple DNN architectures by incorporating transfer learning technique on the plant seedling dataset. Optimizers can have different strengths and weaknesses, and their performance may depend on the specific characteristics of the model and the dataset being used. We have observed that the choice of the optimizer is just one of many hyperparameters that can affect the performance of deep neural network (DNN) models. Other hyperparameters, such as the droprate, number of epochs, and batch size, can also have a significant impact. MobileNetV2 demonstrated superior performance while maintaining a smaller model size, making it a highly valuable option for edge devices where size is a crucial factor.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/8/
Conversational Agents in Service Context: Towards a Classification of Human-like Design Expectations,"The increasing application of Conversational Agents (CAs) changes the way customers and businesses interact during a service encounter. For instance, chatbots are the first point of contact for many customers. In this context, prior research has shown that CAs implemented with a human-like design lead to improved service satisfaction, perceived service quality, and trustworthiness, among others. Developers have become accustomed to adopting a one-size-fits-all approach by designing CAs human-like. However, not every human-to-human service requires the same social and human-like interaction (e.g., contract cancelation vs. doctors’ appointment), which has also been shown in current research. At present, existing research lacks a synthesis of the relationship between CA design and service encounter context. Against this background, we conducted a literature review and derive classifications based on the dimensions of service context (professional/private) and human-like design (low/high), which enables the identification of relevant research gaps and related literature.",IS,https://aisel.aisnet.org/amcis2023/sig_hci/sig_hci/2/
Detection of Malicious Bots on Twitter through BERT Embeddings-based Technique,"Cutting-edge Conversational Artificial Intelligence (CAI) technologies bring ease to human life and the invention of social Artificial Intelligence (AI) bots is one of the ultimate devices in the social media sphere. However, malicious social AI bots lead to societal challenges such as data breaches, information loss, and the proliferation of misinformation. Thus, in this work, significant research has been conducted to address the problem of detecting malicious social AI bots on a microblogging platform such as Twitter. We perform classification through Bidirectional Encoder Representations from Transformer (BERT) embedding-based approach. Utilizing tagging-text features, our preliminary results show the potential of the proposed model to classify tweets as malicious AI bot-generated or human-generated.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/6/
Quantum Computing Concepts for Information Systems Researchers: A Tutorial,"Quantum computing has left the realm of science fiction and is becoming a reality. There are many challenges to overcome before quantum computers can be used to solve practical problems. However, the pace of research and development is accelerating and there are many opportunities for information systems (IS) researchers to contribute. This tutorial provides a brief overview of quantum computing concepts and identifies some research opportunities for IS researchers.",IS,https://aisel.aisnet.org/amcis2023/sig_dsa/sig_dsa/9/
Wrestling with the Truth: Post-Truth and the Crisis of (Dis)information,"The post-truth condition has significant implications for society. This paper argues that the loss of shared reality represents a crisis. As information systems (IS) scholars, we have much to contribute. After all, IS are at the center of many post-truth debates (e.g., fake news, echo chambers, filter bubbles). We must engage with the post-truth condition. Consequently, we must wrestle with truth. This paper introduces a panel that aims to stimulate discussion about the implications of post-truth for IS research, education, and practice.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/3/
Gamified Pedagogical Recommendation Agent Design for Adaptive Learning Environments: An Action Design Research (ADR) Approach,"This research investigates the design of a gamified pedagogical recommendation agent (PRA) to enhance student engagement and improve learning outcomes in adaptive learning environments (ALEs). Adopting an action design research (ADR) approach, we developed a gamified PRA prototype, incorporating game mechanics such as points, badges, and leaderboards. Preliminary findings suggest that the gamified PRA can positively influence student motivation, participation, and overall learning experience. This study contributes to the growing body of knowledge on designing effective gamified learning interventions.",IS,https://aisel.aisnet.org/amcis2023/lacais/lacais/10/
The Role of Digital Transformation in Higher Education: A Case Study of a Large Public University,"This paper explores the role of digital transformation in higher education through a case study of a large public university. The study identifies key drivers, challenges, and success factors associated with digital transformation initiatives in the higher education context. Findings indicate that a clear vision, strong leadership, stakeholder engagement, and a culture of innovation are crucial for successful digital transformation. The study also highlights the importance of addressing challenges related to infrastructure, faculty development, and student preparedness.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/1/
A Design Science Approach to Building a Conversational Agent for Mental Health Support,"This study proposes a design science approach to developing a conversational agent to support mental health. The system aims to provide empathetic responses, evidence-based advice, and crisis escalation pathways. Through iterative design and testing, the agent integrates natural language processing with cognitive behavioral techniques. Preliminary evaluations suggest improvements in perceived support and user engagement.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/3/
Ontological Approach in Smart Service Systems: A Literature Review,"Smart service systems leveraging ontologies have been investigated for decades, but the conception and design of those systems remain difficult tasks due to the lack of an overall framework served as a guideline. This paper aims at conducting a literature review of ontology-based smart service systems to address this challenge. The findings include a classification and an analysis to determine essential elements of those systems, identified research gaps, and suggestions for future research.",IS,https://aisel.aisnet.org/amcis2023/asys/sig_asys/1/
Digital Transformation Readiness: A Literature Review and A Proposed Framework,"With the recent evolution of Information and Communication Technologies and the rapid dissemination and use of digital technologies in almost all fields and levels of modern society, two highly context-dependent polysemic constructs have arisen, namely e-readiness and digital transformation. However, although the literature on e-readiness and digital transformation has increased in the last years, digital transformation readiness is still an exploratory construct, lacking a unified definition. This literature review intends to contribute to filling the conceptual gap related to a consolidated definition of the theme by performing a thematic analysis to propose a digital transformation readiness framework.",IS,https://aisel.aisnet.org/amcis2023/asys/sig_asys/2/
How to Navigate the Uncharted Waters of Cyberwar,"This paper discusses the unprecedented combination of traditional military strategy with digital warfare observed during the war in Ukraine. It explores the challenges related to technology and cyberspace before and during the conflict, the lessons learned, and the implications for software engineers, cybersecurity experts, and IT specialists.",IS,https://aisel.aisnet.org/amcis2023/conf_theme/conf_theme/1/
Metaverse Knowledge Graph Construction: An Unsupervised Relation Extraction Approach based on Semantic Mining,This study develops a novel framework integrating deep learning (BERT-BiLSTM-CRF) with unsupervised relation extraction based on semantic mining to construct knowledge graphs in the Metaverse domain. The approach aims to reduce labeling efforts and facilitate intelligent tasks such as recommendations and information retrieval.,IS,https://aisel.aisnet.org/amcis2023/conf_theme/conf_theme/10/
Towards Solving Ontological Dissonance Using Network Graphs,"The paper analyzes data models from 13 different domains to address semantic interoperability in emerging Data Spaces. Using network graphs, it identifies central data models and ontology attributes, describing the semantic heterogeneity qualitatively and proposing ways to connect different Data Spaces across domains.",IS,https://aisel.aisnet.org/amcis2023/conf_theme/conf_theme/6/
Decentralizing Data Governance: A Case Study in TELCO Data Ecosystems,"This study presents a case study in the telecommunications industry, evaluating data governance maturity models and classifying data governance mechanisms across companies. It highlights the significant differences in governing data ecosystems and offers foundational mechanisms for socio-technical networks.",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/1/
From Data Exchanges to Data Markets: An Institutional Perspective,"The paper discusses the conflict between privacy and competition policies regarding platform data. It proposes an institutional approach to make platform data saleable, aiming to balance these conflicting objectives and clarifying the differences between data markets and other concepts like data spaces.",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/2/
Kill Two Birds with One Stone: Using Multihoming Boundary Resources in Mobile App Development,"This research examines the impact of using multihoming SDKs on app quality in mobile app development. It provides insights into the trade-offs between technical design and platform governance, offering practical guidance to developers in managing resources and strategic goals.",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/3/
Effect of Digitization on Management Accountants’ Tasks and Tools,"Digitization and the increased available data volumes are triggering a fundamental change in Management Accountants' (MAs) scope of tasks and activities. In particular, the current technological developments, including analytics or automation tools, are eliminating standard tasks and processes. This article examines the shift in the job description of a MA regarding changes in technologies, methods, and tools in times of digitization. The results of an analysis of job advertisements provide an overview of the development of tasks, trends and technologies for Management Accountants (MA) in Germany. Our results highlight an increasing demand for Machine Learning, Predictive Analytics and Robotic Process Automation backgrounds, suggesting a shift of the MAs role towards a business partner role with minimization of repetitive tasks.",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/4/
What Constitutes a Dataspace? Conceptual Clarity beyond Technical Aspects,"In the data economy, data has become an essential strategic resource for gaining a competitive advantage. Data spaces represent a relatively new phenomenon aimed at encouraging businesses to fully leverage the potential of data. Despite various approaches for definitions, there remains a lack of clarity surrounding the conceptualization of data space, its perceived value, and the factors that drive its adoption. This paper addresses these issues by proposing primary properties of data space...",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/5/
Towards an Architecture for Data Monetization as a Service,"Data monetization as a service (DMaaS) is a new term in both academia and practice. DMaaS seeks to marry data monetization with servitization and has become important with the advent of technologies such as big data, cloud computing, and blockchain. As organizations seek to establish infrastructure for data monetization, an architecture for DMaaS provides them with a starting point to develop data monetization platforms. This paper leverages the service-dominant logic perspective and research...",IS,https://aisel.aisnet.org/amcis2023/eco_systems/eco_systems/6/
From Full-fledged ERP Systems Towards Process-centric Business Process Platforms,"Enterprise Resource Planning (ERP) systems are critical to the success of enterprises, facilitating business operations through standardized digital processes. However, existing ERP systems are unsuitable for startups and small and medium-sized enterprises that grow quickly and require adaptable solutions with low barriers to entry. Drawing upon 15 explorative interviews with industry experts, this study examines the challenges of current ERP systems using the task-technology fit theory...",IS,https://aisel.aisnet.org/amcis2023/enter_syst/enter_syst/1/
The Role of Combinational Usage Diversity of ES Functions in Affecting Employee Productivity: Considering the Contingent Value of Job Experience,"This study explores the value of the combinational usage diversity of Enterprise System (ES) functions in employee productivity and the potential nonlinear moderating effect of job experience. By collecting daily ES operation log records and monthly sales records, the study provides new insights into the value of ES usage. The findings identify the positive impact of the combinational usage diversity of ES functions—namely combinational variety, disparity, and separation—on employee performance...",IS,https://aisel.aisnet.org/amcis2023/enter_syst/enter_syst/2/
Exploring ERP Architecture in a Global Corporation,"In today’s rapidly changing global business environment, organizations rely on their information technology infrastructure to coordinate global operations. An important part of this infrastructure is a company’s ERP systems. While common wisdom suggests that a company’s ERP architecture should be designed on a global basis to align with strategies and organizational structures, little is known about how these systems are implemented in practice. A case study conducted to explore how ERP...",IS,https://aisel.aisnet.org/amcis2023/enter_syst/enter_syst/3/
"Understanding Barriers, Enablers, and Best Practices for Creating Effective Multi-generational Digital Workspaces","Based on two systematic literature reviews, this study contributes to the understanding of multi-generational digital workspaces by consolidating a list of known barriers and enablers in digital workplaces and identifying best practices to mitigate the former and promote the latter. Further, it analyzes how these factors are perceived by different generations—X, Y, and Z—which increasingly work together. These findings are important as companies struggle to create future digital or hybrid workplaces.",IS,https://aisel.aisnet.org/amcis2023/fow/fow/1/
The Impact of Metaverse Technology Use on Team Creativity in Virtual Teams,"As a crucial contributor to organizational performance and competitive advantage, the ability to develop original and effective ideas has been identified as a key component. As digital technologies advance, businesses increasingly utilize virtual teams due to their cost-effectiveness and efficiency. Moreover, in recent years, there has been an increase in interest in a new concept of work environments known as metaverses. However, the creative performance of virtual teams in the metaverse setting...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/2/
Supporting Effective Communication in Remote Cybersecurity Analyst Teams,"The COVID-19 pandemic has led to increased remote work in the cybersecurity industry, posing challenges to effective communication among cybersecurity analysts. A collaborative analysis support system called AOH-Map has been developed to address this problem by capturing and integrating the analysis processes of a group of analysts into a visual map. This study explores the media capability of AOH-Map in supporting communication among cybersecurity analysts in a virtual team setting...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/3/
To Assert or Defend My Role as an OSS Developer: How IT Infrastructure Access Changes the Effect of Digital Platform Behavior on Firm Mobility,"Increasingly, firms seek and employ open source software (OSS) developers using digital platforms (e.g., LinkedIn and GitHub). OSS developers display their skills in various ways on these platforms to attract firms. However, it remains unclear whether all OSS developer behaviors influence an OSS developer’s firm mobility in the same way. This work takes an impression formation lens to understand how OSS developer behaviors enacted in digital platforms lead to firm mobility...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/4/
From the Pandemic to War: The Role of Digital Technologies in Ukrainian Businesses Responding to Discontinuities and Building Resilience,"Disasters and crises are a part of human existence. While upsetting the normal operations of societies and revealing the inherent fragility of infrastructure and social order, they also push us to rethink “the normal” way of living and working. At this moment, IS research on the experiences of businesses in times of war as an extreme crisis is very limited. Taking the case of Ukrainian businesses, this study asks: How have organizations adapted to war discontinuities and built resilience?...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/5/
Digital Transformation Team’s Capabilities for Process Automation,"This study argues that digital transformation teams should build the right competence profiles to deliver changes using digital tools. Based on dynamic capability theory, it proposes that the team’s digital problem-solving capability and benefits management capability are essential for digital transformation success. Using process automation as a context, the paper suggests that digital transformation teams should build data literacy, business process management competence, and...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/6/
Developing a Data Literacy Assessment Tool for the Business Workforce,"This paper aims to develop a tool to assess the data literacy of individuals in the business workforce. Using a design science research approach, a data literacy assessment tool was developed based on 18 core competencies across three skill levels. The survey tool was tested with 243 respondents in a large North American financial company. The artifact provided an accurate assessment of intra- and inter-individual competencies and the identification of typical profiles required...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/7/
Artificial Intelligence in the Workplace: Implementation Challenges and Opportunities,"This pilot study investigates the challenges and opportunities of implementing human-machine systems in organizations. Findings indicate that organizations have realized positive benefits from AI projects through high levels of communication, stakeholder consultation, problem management, ethics, and transparency. However, significant work is required in managing staff motivation and empowerment, trust in AI technologies, and managing novel cyberthreat issues...",IS,https://aisel.aisnet.org/amcis2023/fow/fow/8/
Vantagens e Desvantagens do Desenvolvimento de Sistemas de Informação com Tecnologias Low-Code – Uma Revisão de Literatura,"In a world marked by great unpredictability, organizations need to respond quickly and with quality to the evolving needs of their stakeholders. Organizational development using Information Technologies (IT) is fundamental in this context. However, companies often face difficulties in hiring qualified human resources for efficient and effective IT development. Low-code technologies present themselves as a solution to this problem, allowing rapid development of applications and enabling...",IS,https://aisel.aisnet.org/amcis2023/lacais/lacais/1/
Impact of Court’s Case Management Systems: Perspectives on Efficiency,"Justice organizations are central to democracy. However, even democratic countries report high costs, slowness, and unreasonable backlogs in the justice system. Many countries have invested heavily in Information Technology (IT) to address efficiency issues. This research analyzes whether IT fulfills its promise of higher court efficiency. The research model contemplates three hypotheses related to the efficiency of courts of justice, focusing on clearance rate, process duration, and...",IS,https://aisel.aisnet.org/amcis2023/lacais/lacais/12/
Um Framework de Concepção de Software com Design Thinking em um Programa de Intraempreendedorismo,"Companies need to develop more assertive and innovative solutions. In this context, Design Thinking (DT) is an approach that allows immersion in the problem, creation of collaborative solutions, and validation with prototypes. However, literature shows difficulties in adopting approaches like DT, especially for inexperienced professionals and due to cultural resistance. This work used action research to develop and apply a framework for implementing software design with DT...",IS,https://aisel.aisnet.org/amcis2023/lacais/lacais/9/
"The Future of System Goals and Human Rights: Data Governance, AI Ethics, and Lessons Learned in Covid-19 for Research Data","This symposium explores the current state of data tools, guidelines, and proposals for policies and regulations promoting trust in AI and federated systems, ultimately advancing Open Science and fundamental human rights. The implementation of advanced technologies and big data initiatives in smart cities urges attention to AI ethics, data governance, cybersecurity, confidentiality, and issues of privacy and surveillance...",IS,https://aisel.aisnet.org/amcis2023/pds/pds/1/
Navigating Uncertainty: Preparing Organizations for Digital Disruption,"This professional development symposium discusses how emerging technologies such as AI, blockchain, AR, and IoT are transforming organizations and value chains. The panel explores mechanisms that technology leaders may use as guardrails to assist in navigating complex technical, ethical, and regulatory environments common to emerging technologies...",IS,https://aisel.aisnet.org/amcis2023/pds/pds/2/
"How to Make a Respectful Impact in Service to Underserved Communities Before, During and After the Paper Gets Published","Focusing on social inclusion research, this symposium provides guidance on ethically engaging with underserved communities throughout the research process. It highlights tools and techniques for building collaborative projects that contribute to meaningful solutions, emphasizing the importance of avoiding a savior mentality and ensuring that research efforts positively impact the communities involved.",IS,https://aisel.aisnet.org/amcis2023/pds/pds/3/
The Metaverses’ Edge for Learning: Body and Identity,"This study explores how immersive technologies, such as virtual reality headsets, enable users to interact with virtual environments—referred to as metaverses—using their bodies. Prior research suggests that embodiment features in metaverses facilitate learning. However, limited attention has been given to how users’ perceptions of themselves within these environments impact learning effectiveness...",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/1/
"Assessing and Controlling the Social Desirability Bias in the Context of Victims, Perpetrators, and Bystanders of Cyberbullying","This study examines social desirability (SD) bias in the context of cyberbullying perpetrators, victims, and bystanders. To assess and control SD bias, the study uses the 16-item Balanced Inventory of Desirable Responding and the covariance technique.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/10/
"Integrating Technology Readiness, Learning Goal Orientation with TAM to explain E-learning adoption","This research proposes an integrated model that combines Technology Readiness dimensions, Learning Goal Orientation, and the Technology Acceptance Model to explain e-learning adoption among Indian MBA students. The model was tested using PLS-SEM, revealing significant relationships among the constructs.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/11/
Instances of Digital Dark Nudging: Findings of a Systematic Literature Analysis,"This paper provides a literature analysis of digital dark nudging instances, identifying 14 relevant publications and categorizing them into seven instances within five application domains. The study highlights the adverse effects of digital dark nudging and the need for clear differentiators between regular and dark nudging.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/12/
Investigating Users' Continuous Adoption of Cryptocurrency,"This study explores users’ continuous adoption intention of cryptocurrency by developing a model based on the Transaction Cost Economics Theory and perceived value. Data from 173 cryptocurrency users indicate that perceived value, influenced by benefits and transaction costs, determines continuous adoption.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/14/
Exploring Parental Influence on Children's Cyber Hygiene,"This study examines how parents influence their children's cyber hygiene, proposing that parental practices, guidance, restrictions, and digital safety measures positively impact children's cyber hygiene. A mixed-methods design, including interviews and surveys, will test the hypotheses.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/15/
Smart Contracts’ Adoption in the Healthcare Sector: a Privacy Calculus Perspective in China and U.S.A.,"This research explores factors influencing the intention to adopt smart contracts for electronic health records in the U.S.A and China. Integrating the privacy calculus model and Hofstede’s cultural dimensions, the study surveys patients in both countries to understand perceived risks and benefits.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/16/
Investigating the Determinants of Compliance Intention in Behavior Change Support Systems,"This study investigates determinants of compliance intention in Behavior Change Support Systems by extending the persuasive system design model. Surveying 234 prospective users, the findings suggest that perceptions of support and credibility influence enjoyment, persuasiveness, and compliance intention.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/17/
All Together Now: Fuzzy-set Qualitative Comparative Analysis of Personality Profiles and their Coping with Technostress,"Using the Big Five personality inventory and configuration theory, this study analyzes how personality profiles affect coping behavior in response to technostress. Survey data from 233 respondents and fuzzy-set qualitative comparative analysis reveal distinct personality configurations for coping strategies.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/18/
The Big Five Under Pressure in E-Commerce: Exploring the Role of Coping Strategy and Technostress,"This study examines the effects of the Big Five personality dimensions on purchasing intention in e-commerce, considering technostress and coping strategies. An online survey with 479 participants indicates that certain personality traits influence coping strategies, which in turn affect purchase intention.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/19/
Discovering Telemedicine Usage Motivation Using a Trust-based Valence Framework,"This research investigates patient motivation for adopting telemedicine services using the Extended Valence Framework, including perceived trust, risk, benefit, and intent to use telemedicine services. Findings show that trust and benefit significantly influence intent, while perceived security and familiarity impact trust.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/2/
Cross-cultural and Institutional Perspectives on Information Technology Governance,"This study examines how different organizational cultures, as defined by the OCAI model, influence employees' perceptions of IT Governance (ITG) institutionalization. Surveying 513 workers worldwide, the research found that Market and Hierarchy cultures are associated with higher perceived ITG.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/20/
AI Healthcare Adoption: A Privacy Calculus Model Incorporating Emotions and Techno-social Factors,"This study investigates the adoption of AI-based healthcare services in the USA using a privacy calculus model incorporating AI technical characteristics, techno-social factors, and emotional states. It proposes and tests a model examining the effect of these factors on adoption intention.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/21/
A Review and Meta-analysis of Mobile Government Adoption,This meta-analysis reviews 38 empirical studies to identify critical factors influencing mobile government adoption and analyzes how economic development levels moderate these effects. The results provide theoretical and practical implications for e-government development strategies.,IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/22/
Youth Exodus! A Framework of Social Media Migration by Young Adults,"This study explores factors influencing young adults' migration from social media platforms like Facebook. A theoretical framework examines how attitudes, social norms, fake news, and privacy concerns influence abandonment. Survey results support the proposed model.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/23/
The Impact of Cloud Computing on Firm Performance through Operational Capability,"This study investigates how cloud computing adoption enhances firm operational capabilities, leading to better performance. It uses a two-stage econometric model and analyzes the role of CIO presence in moderating operational capability.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/24/
Responsible Digital Innovation in Dark: Toward Access-Control-Transparency Theory,"This paper introduces the Access-Control-Transparency (ACT) theory, a framework for designing digital artifacts that promote responsible digital innovation. It emphasizes access, control, and transparency and offers research directions.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/25/
Social Media Addiction: A Techno-centric View,"This study adopts a techno-centric perspective to explore the role of social media platform features in addiction, utilizing the needs-affordances-features framework. The analysis reveals feature-need alignments contributing to problematic use.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/26/
How Does AI-Work Interaction Affect Individuals' Behavior? The Role of Appraisals,This study applies the transactional theory of stress to understand how AI integration in workplaces affects behavior. It explores how individuals appraise AI as augmentation or automation and how expertise shapes their reactions.,IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/27/
Investigating the Role of Gender in Active and Healthy Ageing Supported by ICT,This paper examines gender differences in the use of ICT for active ageing in Poland and Sweden. Value-Focused Thinking is used to identify key motivations and differences in technology adoption patterns.,IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/28/
Unrevealing the Dynamic Impact of Extended Reality Adoption on Offline Channel Performance,"This study analyzes the impact of extended reality (XR) adoption on offline channel performance in multichannel retail. Findings suggest a non-linear relationship: initial improvement, then diminishing returns.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/29/
The Impact of Use of Big Data-Driven Systems on Decision-Makers’ Power,"This research studies how big data-driven systems affect the power of decision-makers in organizations. Using strategic contingencies theory, it models power redistribution due to increased data use.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/30/
Empirical Assessment of Big Data Technology Adoption Factors for Organizations with Data Storage Systems,"This study identifies factors influencing big data technology adoption in organizations with existing storage systems. A survey and PLS-SEM reveal industry pressure, system compatibility, and cost as major drivers.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/31/
Managing Misinformation within the Public Sector: Cases from the Global South,This empirical study investigates how public sector organizations in developing countries manage misinformation. It highlights short- and long-term responses and contextual challenges.,IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/34/
Digital Transformation in the Public Sector: A Comparative Study of E-Government Implementation in Developing Countries,"This comparative case study explores e-government implementation in developing countries. It identifies technological, institutional, and human factors that affect success and suggests policy recommendations.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/35/
Blockchain Adoption in Supply Chain Management: An Empirical Investigation,"This paper analyzes blockchain adoption in supply chains, focusing on transparency and trust. Through empirical investigation, it identifies drivers, barriers, and performance impacts.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/36/
Understanding Blockchain Adoption from an Institutional Theory Perspective,"The study explores blockchain adoption through the lens of Institutional Theory, proposing a framework that includes technological pressure alongside the traditional institutional pressures. Data was gathered via semi-structured interviews with blockchain experts using the Delphi method.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/39/
Towards the Dark Side of AI Adoption: How Generative AI Extenuates the Perception of Chatbot Errors,"Investigates how generative AI chatbots, like ChatGPT, affect user perceptions of errors. A 2x2 experiment reveals that errors are less noticeable in generative-based chatbots, leading to higher perceived competence and trust, despite the presence of errors.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/4/
Fit in a Bit: eSport and eFitness Adoption in Qatar,"This study proposes a model to understand factors influencing the adoption of VR-based eSports and eFitness in Qatar, considering environmental conditions and socio-cultural practices. It draws from IT adoption literature to conceptualize adoption factors in the GCC region.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/40/
What’s the Harm in Waiting? Examining Disfluency from System Response Time in Decision Tasks with Conversational Agents,"Examines how system response time affects user decision-making with conversational agents. Findings suggest that slower responses can discourage users from using helpful features like filters, leading to suboptimal decisions, especially in disfluent situations.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/41/
Infodemic and its Cure: A Digital Nudging Approach,"Addresses the spread of health misinformation during the COVID-19 pandemic. The study explores how digital nudging can shift users from psychological ownership to dis-ownership of false information, using concepts from psychological ownership motivation and social exchange theory.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/42/
"“Jane Sent Me This Article, So It Must Be True!” – How Tie Strength and Emotional Tone Influence Information Behavior","Investigates how emotional tone and the strength of social ties affect the spread of fake news via instant messaging. An experimental study examines their impact on willingness to fact-check and intention to share, mediated by sender credibility and news believability.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/43/
Battling Disinformation Intermediaries: An Analysis of Information Policies,"This study investigates disinformation policies, focusing on intermediaries—channels and mechanisms that facilitate disinformation dissemination. Through inductive thematic analysis of policies in China and the United States, the research identifies various intermediaries subject to regulation and examines the contexts of disinformation regulation in both countries, providing empirical insights for researchers and policymakers.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/44/
The Interplay of IT Identity and Digital Mindset in the Workplace,"This paper examines how IT identity and digital mindset influence job satisfaction in the workplace. Surveying 167 employees, the study finds that IT identity's effect on job satisfaction is fully mediated by job identity, with digital mindset moderating the relationship between IT and job identities, offering insights into identity formation and its impact on job satisfaction in digitally-enabled workplaces.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/5/
Gamification Impacts on Technostress,"This research explores how gamification affects technostress by applying basic psychological needs theory. It proposes a framework where game element characteristics influence technostress through the satisfaction or frustration of psychological needs, suggesting that incorporating gamification elements that fulfill these needs can help manage technostress in individuals.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/6/
Institutional Legitimation of IT Fashions,"Many emerging information technology (IT) innovations are proclaimed to revolutionize business – if not the world. This recurring phenomenon of hyperbolic expectations followed by their disillusionment has drawn vast attention amongst researchers in the fields of management and information systems (IS), leading to its conceptual embedding within fashion theory. This proposed study aims to extend the sparse empirical body on IT fashion research, thereby deepening the understanding of the associated organizational consequences regarding an essential organizational outcome, namely organizational legitimacy. In this course, I focus on organizational legitimacy in the eyes of sell-side securities analysts which acknowledges recent findings that identify them as a key source of institutional pressures for legitimacy. Further, I plan to analyze whether and how the respective perception of legitimacy might differ contingent upon different firm characteristics such as industry affiliation or ownership structure.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/7/
Smart Parking Systems and their Impact on the Efficiency of Parking Officers,"Analyzing data from Los Angeles between 2015 and 2019, this study examines how smart parking meters and time limit policies affect parking officers’ efficiency. Utilizing transaction cost economics and a fixed-effect estimation model, findings indicate these technologies positively influence the number of parking citations.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/8/
How Would Individuals Like to Use AI-Enabled Applications? An Empirical Study Based on HCI Framework,"This research applies the Human-Computer Interaction (HCI) framework to study factors influencing individuals’ acceptance of AI-enabled applications. Survey and PLS-SEM results show user trust, interaction, and tech fit significantly affect usage intention.",IS,https://aisel.aisnet.org/amcis2023/sig_adit/sig_adit/9/
A Process Model for the Practical Adoption of Federated Machine Learning,"This paper outlines a structured process model for managing Federated Machine Learning (FML) projects. It details activities, resources, and interdependencies to aid transparent and effective FML adoption.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/1/
The Art of Inspiring Creativity: Exploring the Unique Impact of AI-generated Images,"Investigates how DALL-E-2 impacts user creativity. An online experiment shows AI-generated images significantly enhance creative output, highlighting their role as creativity boosters.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/10/
Dynamics of Trust: Unpacking Trust in Human-AI Collaboration in Decision-Making,"This review categorizes human, AI, and context-based trust factors in AI collaboration. Proposes a dynamic trust development framework to guide trustworthy system design.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/11/
Building Trust in AI: A New Understanding of the Role of Reliability,Interviews at a global auto firm reveal that users tolerate less-than-perfect AI reliability when supported by organizational trust-building measures.,IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/12/
Unlocking the Potential of ChatGPT: A Grounded Theory Exploration of Its Impact on the Business Landscape,"A grounded theory study across 11 industries shows ChatGPT’s dual roles in automation and augmentation, reshaping business models and customer behavior.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/13/
Using AI and ChatGPT in Brand Storytelling,"This study explores how AI chatbots, like ChatGPT, can assist companies in creating brand stories and communicating with stakeholders. It examines the effects of varying language formality and the role of informed communicators on individuals’ attitudes toward the brand, considering psychological distance.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/14/
Revealing the Dark and Bright Sides of ChatGPT: An Exploratory Study on User Perceptions,"An exploratory qualitative content analysis of Reddit posts reveals both positive and negative user perceptions of ChatGPT. While users appreciate its assistance and value creation, concerns include algorithm limitations, job displacement, privacy issues, and misuse for cheating.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/15/
A Genetic Algorithm Based Consensus Reaching Method on Malware Labels,"This paper introduces a novel scoring system, the Pairwise Consensus Score (PCS), combined with a Genetic Algorithm to achieve consensus on malware labeling. The approach aims to provide more consistent and trustworthy antivirus labels by addressing inconsistencies among vendors.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/16/
The Work of Students and ChatGPT Compared: Using Machine Learning to Detect and Characterize AI-Generated Text,"An experiment compares student-written assignments with ChatGPT-generated responses. Using text classification, the study identifies stylistic differences, noting that AI-generated text tends to be more formal. The findings aid in detecting AI-generated content in academic settings.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/17/
AI-based Technologies for Conversational Agent Design – Development Tools and Architectures for Intelligent Interactions,"This research develops a taxonomy to characterize AI-based tools for conversational agents. Through cluster analysis, it derives archetypes and meta-architectures, providing insights into integrating AI services into conversational agent designs for enhanced human-machine interaction.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/18/
Alexa is the New Influencer: An Empirical Study Based on a Relational View,"Investigating voice assistants (VAs) as influencers, this study finds that factors like parasocial relationships, peer influence, and self-image congruence positively affect consumers’ decisions to follow VA recommendations, highlighting VAs’ potential in marketing strategies.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/2/
How Does User Engagement Support Content Moderation? A Deep Learning-based Comparative Study,"This study proposes a framework incorporating user engagement metrics into content moderation. By integrating credibility and stance into graph learning, the model outperforms traditional deep learning approaches, enhancing moderation decisions for user-generated posts.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/3/
From “Handmade” to “AI-Made”: Mitigating Consumers’ Aversion towards AI-Generated Textual Products,"This study explores consumer aversion to AI-generated textual products. Through 41 consumer interviews, it identifies five signals—quality control, hard output paradigm, information transparency, process transparency, and positive reviews—that can mitigate concerns and enhance acceptance of AI-generated content.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/5/
Toward Identifying the Factors Associated with Conversational Agents’ Performance,"Focusing on text-based conversational agents (chatbots), this research identifies chatbot attributes, user attributes, and task attributes that impact performance from the users’ perspective. Interviews with users of rule-based chatbots inform a theoretical framework for chatbot performance evaluation.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/6/
Response Failure of Mental Health Chatbots: Examining the Impact on Discontinuance,"This paper investigates how response failures in digital mental health interventions (chatbots) influence user discontinuance. Utilizing expectation disconfirmation theory and cognitive dissonance theory, it examines the effects of satisfaction, ease of use, and usefulness on continued usage.",IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/7/
Perceived Warmth and Intelligence of AI: Impact on Employee Performance,The study examines how employees’ perceptions of AI’s warmth and intelligence affect their identification with AI technologies and subsequent job performance. Data from 319 customer service representatives interacting with chatbots reveal significant impacts on employee performance.,IS,https://aisel.aisnet.org/amcis2023/sig_aiaa/sig_aiaa/9/
Exploring perceptions of pro-environmental educational mobile applications based on semantic field analysis,"This paper identifies user perceptions of pro-environmental educational mobile apps through semantic field analysis and emotional temperature measurement. Findings suggest that reward systems reinforce environmentally friendly behavior, while user behavior monitoring is viewed negatively.",IS,https://aisel.aisnet.org/amcis2023/sig_ccris/sig_ccris/1/
IT Security and Espoused Cultural Values: A Comparative Analysis of Pakistan and the United States,"Extending Protection Motivation Theory, this study examines how espoused cultural values influence IT security behaviors in Pakistan and the US. It introduces indulgence vs. restraint and survival vs. self-expression as cultural dimensions affecting security measure adoption.",IS,https://aisel.aisnet.org/amcis2023/sig_ccris/sig_ccris/2/
Psychological resilience and job satisfaction and the effectiveness of social media marketing employees,"This study investigates the relationship between mental resilience, job satisfaction, and effectiveness among social media marketing employees in Poland. Utilizing the SPP-25 resilience scale and job satisfaction measures, the research analyzes data from 51 respondents across three marketing companies.",IS,https://aisel.aisnet.org/amcis2023/sig_ccris/sig_ccris/3/
Navel Gazing or the Long View?: The Influence of Firm Positioning with Respect to OSS Communities on a Developer’s Firm Embeddedness,"This paper explores how a firm’s orientation towards open-source software (OSS) communities affects developers’ embeddedness within the firm. Using a conservation of resources framework, the study finds that developers perceive internal-focused firms as more supportive.",IS,https://aisel.aisnet.org/amcis2023/sig_cnow/sig_cnow/1/
The Changing Nature of Telemedicine Processes: Adaptations and Triggers,"Investigating telemedicine in two Indian public hospitals, this study examines how digitalization prompts adaptations in care processes. Applying Adaptive Structuration Theory, the research identifies triggers and mechanisms leading to changes in technology use, tasks, and roles.",IS,https://aisel.aisnet.org/amcis2023/sig_cnow/sig_cnow/2/
The Use of Gamification to Enhance Internal Software Quality - Structured Literature Analysis,This structured literature review analyzes the application of gamification to improve internal software quality aspects like code complexity and documentation.,IS,https://aisel.aisnet.org/amcis2023/sig_cnow/sig_cnow/4/
The Role of ICT Permeability on Work and Family Outcomes When Working from Home,This research develops a framework to assess how ICT permeability impacts work and family outcomes among dual-career couples working from home.,IS,https://aisel.aisnet.org/amcis2023/sig_cnow/sig_cnow/5/
Blockchain Transformation for Banking and Financial Services: Examining the Opportunities and Risks,This paper explores how blockchain technology can transform banking and financial services. It identifies opportunities and risks and offers implementation recommendations.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/10/
Digital Transformation: How Scaling Agility Affects Value Creation Paths,"Investigating the impact of scaling agility on digital transformation, this study conducts multiple case studies to understand how structural changes influence value creation.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/11/
Leveraging Digital Platforms Through Business Model Innovation in SMEs: A Capability Perspective,This research examines how SMEs can leverage digital platforms through business model innovation.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/12/
Take It or Not? Impact of Taking Investments from Tech Giants on IT Startups’ Future Funding,This study examines how taking investments from tech giants influences future funding of IT startups.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/13/
Organizational Capabilities in Emerging Blockchain-Enabled Platform Ecosystems,This paper identifies organizational capabilities needed for value creation in blockchain ecosystems.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/14/
Digital Entrepreneurs: What Do We Know about Them,This review analyzes 43 papers to understand digital entrepreneur typologies and capabilities.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/15/
Cryptocurrency Market: The Interplay Between Microblog Dynamic Opinion Leader Sentiments and Bitcoin Pricing,This study investigates how Bitcoin tweet sentiment and price affect each other using VAR modeling.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/16/
Navigating SMEs’ Innovation in the Digital Age: From Manufacturing to Servitization,A case study on how a manufacturing SME used digital servitization for innovation.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/17/
Design Thinking for Effective Collaboration in Multi-Stakeholder Requirements Engineering,An action research study proposes a design-thinking-enhanced requirements engineering process.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/18/
A Collective Opinion Mechanism for Crowdfunding Recommendation,This paper proposes a system to recommend crowdfunding projects based on backer opinion and engagement.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/19/
Corporate Trade War Uncertainty and Patent Bubble,This paper investigates how perceived trade uncertainty influences strategic patenting in China.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/2/
Understanding the Theoretical Foundations of Digital Transformation Literature: A Systematic Review,A review of 183 journal articles identifying theoretical foundations of digital transformation literature.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/20/
On Technology Ecosystems: De-Jure Standards Setting by Firms,This study analyzes collaboration factors that affect standard-setting in tech ecosystems.,IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/3/
Digital Platform Entry Barriers for Family-run SMEs,"Family-run SMEs usually have a long tradition. They have specified their organizational structures and processes to their product or service over the years. Now these companies are facing various difficulties, such as the increasing shortage of skilled workers or new competition from digital technologies. We argue that digital platforms can be a challenge as well as an opportunity. In our perception, however, German family-run SMEs show a restrained adoption of those technologies. This article therefore examines the barriers to the use of digital platforms in the value creation process of family businesses. For this purpose, we followed the approach of design-oriented information systems. We evaluated transcripts of eleven interviews with family-run SMEs and identified five categories as barriers to enter: Customer, Socio-technical, Personal, Resources, and non-specific Others. Finally, in the discussion we mapped these barriers to actors we assume to be capable of tearing them down in the future.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/4/
Causal Matching for Startups: Methods for Controlling Confounding,"Causal inference is a critical methodology that allows researchers to empirically test hypotheses and build theories. Ideally, the design method for establishing this causality would be through randomized controlled trials. However, studying the impact of different practices (hereinafter treatments) on initial outcomes when treatments are not applied exogenously is a challenge. Matching methods are an appropriate way to provide these study groups. The objective of this work is to provide an algorithm-based solution to match startups in a way that can mimic a human match. To do this, we use the human matching effort by Yu (2020) as a ground truth to evaluate various natural language processes and compare the performance of each with human raters. By comparing automated approaches with matching, we provide guidance for researchers interested in the causal analysis of startups using textual data and other covariates.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/5/
Evolution of Digital Innovation Labs – How Organizational Learning Contributes to Digital Transformation,"Digital Innovation Labs (DILs) have become a tool for companies to handle disruption and digital innovation. However, success was mixed. Some DILs failed to reach their targets whereas others provided the pathway toward digital transformation. This study examines the evolution of DILs and shows how Organizational Learning advances innovation. Using a case study approach, it looks at two developmental stages of one particular DIL. Over time, the DIL has changed fundamentally and established a funnel approach where the lab structure supports the entire innovation process. Eight recommendations are derived that may contribute to the success of DILs in companies.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/6/
How is my IT department leading to Digital Innovation Success?,"Digital innovation, driven by IT departments, is a key enabler for success in all types of organizations around the world. Although there are a lot of touted benefits, there is not much scholarly attention as to how digital innovation success can be achieved in organizations. Drawing upon social capital theory, this study plans to examine how functional IT employees’ relationship with the Chief Information Officer (CIO) can help organizations in achieving digital innovation success. The study proposes that the social capital of the relationship between CIO and functional IT employees will enhance both external and internal digital innovation success through more closely engaging functional IT employees in the decision-making processes of CIO. Furthermore, the study aims to identify the role of environmental turbulence and organizational culture as moderators in the proposed model.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/7/
Decoding Digital Risk from Corporate Disclosure: A Neural Network Approach,"Digital risk-or the likelihood of losses from organizational context, digital infrastructure, IS sourcing, data management and IS applications, is a key consideration in the valuation of firms. We apply a neural network approach to construct and measure digital risks by extracting linguistic relations from the 10-K disclosure (Section “Item 1A”). We develop novel firm-level digital risk measures based on these linguistic relations from three perspectives: (i) presence (whether the digital risk is mentioned or not), (ii) intensity (text coverage of digital risk relative to other issues), and (iii) diversity (the different types of digital risk that are mentioned). We validate our measures for digital risk by demonstrating that they correlate significantly with firm risk, as proxied by the volatility of the stock market. Overall, our findings suggest that the utility of leveraging this type of text-based measure for digital risk is practically feasible, scalable, and economically meaningful.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/8/
Investigating the Impact of Digital Transformation on Organizational Identity in an SME: Insights from an In-Depth Case Study,"The relationship between digital transformation and organizational identity is an immanent topic in IS, which is especially relevant for small- and medium-sized enterprises (SMEs) due to their reliance on implementing digital technologies to stay competitive. However, little is known about this relationship in this specific context. Based on theories and concepts about digital transformation, organizational identity, and SMEs, we conducted an in-depth case study with a German Mittelstand company in the engineering sector. Building on a conceptual archetype framework, we reveal that different manifestations of digital transformation have different impact on organizational identity. Thereby, we contribute to IS research by highlighting the mutual dependency of these concepts and by underlining the necessity for being aligned to the specifics of SMEs. Moreover, we derive practical implications for managers that help to anticipate the effects of digital transformation within their strategic decision processes.",IS,https://aisel.aisnet.org/amcis2023/sig_dite/sig_dite/9/
Exploring the Synergies in Human-AI Hybrids: A Longitudinal Analysis in Sales Forecasting,"Despite the promised potential of artificial intelligence (AI), insights into real-life human-AI hybrids and their dynamics remain obscure. Based on digital trace data of over 1.4 million forecasting decisions over a 69-month period, we study the implications of an AI sales forecasting system’s introduction in a bakery enterprise on decision-makers’ overriding of the AI system and resulting hybrid performance. Decision-makers quickly started to rely on AI forecasts, leading to lower forecast errors. Overall, human intervention deteriorated forecasting performance as overriding resulted in greater forecast error. The results confirm the notion that AI systems outperform humans in forecasting tasks. However, the results also indicate previously neglected, domain-specific implications: As the AI system aimed to reduce forecast error and thus overproduction, forecasting numbers decreased over time, and thereby also sales. We conclude that minimal forecast errors do not inevitably yield optimal business outcomes when detrimental human factors in decision-making are ignored.",IS,https://aisel.aisnet.org/amcis2023/sig_dsa/sig_dsa/1/
Outside the Bean Belt: A Study on the Suitability and Sustainability of Coffee Plants in Southern California due to Climate Change using GIS,"California has a unique climate that is suitable for growing a variety of agricultural crops. As stated by the California Department of Food and Agriculture, California’s agricultural abundance includes more than 400 commodities, and of these, over a third of the vegetables and three-quarters of the fruits and nuts consumed in the United States, are grown in California. Mark Gaskell, Ph.D., a California Cooperative Extension Farm Advisor, recently began an experiment to grow coffee plants; he believed that due to changing climatic conditions, California farmers could begin to grow coffee plants in their fields. Several farmers agreed to experiment as well and found that the plants did indeed grow. The issue, however, is that farmers would like to know if they can reliably plant coffee on their farms and sustain coffee production for years to come. As a result, this study will utilize the maximum entropy approach, a well-established algorithm for modeling habitat suitability over geographical domains, to measure the impact of future climate change on the sustainability of growing coffee plants in California in the years ahead. Layers of worldwide climate data, that represent 19 bioclimatic variables, will be utilized in this study. WorldClim is an online repository that contains historic, monthly, meteorological data averaged across 30 years (1970–2000) as well as climate data representing possible future conditions based on various climate prediction models. While this study is focusing on the impact of climate change on coffee plants, it is expected the methodological processes developed, and the resultant findings will be applicable to a wide variety of agricultural crops.",IS,https://aisel.aisnet.org/amcis2023/sig_dsa/sig_dsa/3/
Identifying Intimacy of Self-Disclosure: A Design Based on Social Penetration Theory and Deep Learning,"Research on the peer-to-peer (P2P) platforms, privacy, and digitalized business environment has overwhelmingly treated the intimacy of self-disclosure as a survey-based, subjective, and cognitive construct. A few studies have conducted topic analysis using objective data, but are still limited by the difficulty of capturing the degree of intimacy, which hinders the development of the transaction antecedents of P2P platforms. Building upon social penetration theory, we propose an innovative approach to identifying the intimacy of self-disclosure using a deep learning algorithm and an expert-compiled intimacy corpus in the context of P2P platforms. Adopting a sample dataset of 10,000 hosts’ self-descriptions in Airbnb, we introduce the computational and verification process of operationalizing the intimacy of self-disclosure. Through an empirical study, we demonstrate the theoretical feasibility of our quantification method of intimacy and show the potential of using deep learning to measure self-disclosure, expanding the theoretical development of social penetration theory and self-disclosure.",IS,https://aisel.aisnet.org/amcis2023/sig_dsa/sig_dsa/7/
Skills of IT Graduates and Cross-border Mobility: Experiences from Norway and Poland,"The dynamic nature of the ICT field makes it extremely difficult and challenging for educators to determine what skills and technologies are continuously pertinent and in-demand. Therefore, a frequent evaluation of critical skills supply and demand is necessary for education programs to stay relevant and effectively teach state-of-the-art skills. The main aim of our ongoing research project is to analyze how the current educational curricular guidelines address the requirements for competences of modern ICT workplaces in Poland and Norway, and thus, prepare the academia to adequately respond to the challenges imposed by the industry. The goal is to produce a general framework and models for future curriculum guidelines, and course planning and design, as a step towards workforce development that possess skills relevant for the future challenges of the modern ICT workplace environments.",IS,https://aisel.aisnet.org/amcis2023/sig_ed/sig_ed/23/
Enhancing Citizen Engagement: Experiences from a Virtual Reality Workshop,"In e-Governance, information and communication technologies such as virtual reality are used to aid communication between government and citizens, and engage citizens in government policies and strategies. The aim of this study is to explore virtual reality as a tool to engage citizens in social issues such as mobility, climate change and sustainable urban development. A study was conducted in a workshop setting, where virtual reality displays were utilized to engage users. The users rated their experiences on a 7-point Likert scale, and the results indicated that perceived usability, intention to use, enjoyment, and novelty positively influenced citizen engagement. Further, it was revealed that citizen engagement had a positive impact on self-efficacy and citizens’ interest in the topics. Our contributions include theoretical and practical implications, along with a preliminary relationship model. Overall, the findings suggest that virtual reality displays have the potential to be an effective tool for engaging citizens.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/1/
Digital Divide in the Public Sector in Panama,"Digital transformation has taken over the world post pandemic, and the digital divide has increased just as much.  In this paper, this subject will be looked at from the perspective of the gap that continues to grow between people that are able to adapt and have access to new technologies and those that do not.   The lack of scholarly research on the subject in Panama and the need to gain a deeper understanding of the issue so eventually new solutions start to emerge, is the research gap this paper aims at.  Panama is a small country and one of the most technologically advanced in the Latin American region whose public sector has undergone massive digital transformation during the pandemic, thus the focus of the research. The method used, is an explorative and qualitative case study, that looks at the perceptions of the participants, to get their insight and gain more understanding.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/10/
Citizen Participation Level in Smart Governance: A Literature Review,"This literature review used and expanded Cardullo & Kitchin's (2018) “scaffold of smart citizen participation” to classify the participation level found in the smart governance literature. This review highlights the importance of the use of a common metric for measuring citizens level of participation in smart governance. The departure point of this work is the literature review developed by Tomor et al. (2019). Their review was updated and compared. We confirmed some challenges positioned by the literature on the indiscriminate use of citizen participation as both an intrinsic characteristic of governance and as an outcome of the collaboration (Meijer & Bolívar, 2015; Tomor et al., 2019). This lack of a common understanding of the level of citizen autonomy creates a confusion in the literature regarding the actual level of participation, which highlights a need for further research about citizen participation and collaboration in smart governance.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/11/
The NYC311 App & Community Engagement in Coproducing Municipal Services,"In the public sector, governments and the people they serve increasingly collaborate to coproduce public services. To support the coproduction of municipal services, specifically, local governments have incorporated various digital technologies into their information systems. How do digital technologies affect community residents' engagement in coproducing municipal services? Our analysis of the service requests submitted to New York City's municipal 311 system shows that both the introduction and use of the NYC311 app were associated with increased request volume in every community, but the app's positive effect on community engagement was weaker for minority communities, especially those with high percentage of black populations at median-low to median-high income levels. These findings help bridge public administration and technology perspectives on community engagement by showing how ethnoracial and socio-economic factors moderate the effectiveness of digital technology. Accordingly, governments are advised to heed these factors when adopting digital technology to support public service coproduction.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/12/
Household Digital Twin for Disaster Response,"Using simulation of digital twins can help save lives and reduce the impact of disasters on households. By accessing a virtual process and making recommendations to families before a disaster hits, the risk of harm can be minimized. Developing a household digital twin for disaster response can lay the foundation for effective household disaster management. This study proposes a design of household digital twin for disaster response and demonstrates how the platform can be used to simulate hazardous processes before they occur for damage control.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/13/
Attributes of Continuity in Support Information Systems: A Study of Social Media Listening Experiences,"The increasing recurrence of crises threatens the continuity of services provided by public service organizations (PSOs) to their communities, especially those in need. Beyond contingency plans, PSOs must rely on robust support information systems able to withstand transitions between day-to-day activities and crisis response with as minimal interruptions as possible. Through the analysis of experiences of use of social media analytics as a support system in PSOs, this study proposes a set of attributes that take place while transitioning between non-crisis and crisis states. The sociotechnical nature of transitions calls for a zoomed-out analysis based on shared patterns that could lead to generalization in the future. The results of our study contribute towards understanding how continuity and equilibrium are exercised in volatile and dynamic environments. In addition, our results could help organizations self-assess their social media analytics teams or other support systems within the proposed parameters.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/14/
E-participation from the Perspective of ICT Adoption in Local Government Units: Drivers and Association with  Sustainability,"This research examines the ICT adoption drivers of e-participation and the association between e participation and sustainability. A theoretical model of e-participation driven by ICT adoption in government units and associated with sustainability was proposed and verified among randomly selected 304 government units in Poland. It was revealed that e-participation is driven by ICT adoption in government units, particularly by thoughtful spending on ICT in government units, employees' ICT skills, managers' mastery, implementation of legal regulations on ICT usage, and front-office system security. Furthermore, a positive association between e-participation and sustainability was confirmed.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/2/
Fostering the Adoption of Smart E-Government Services in Germany,"Governments worldwide recognize a need to provide services online to their citizens. Increasingly, these services have become sophisticated through the application of advanced technologies. Nevertheless, the adoption of such services often lags behind expectations. Thus, this study proposes and tests an extension of the unified model of electronic government adoption (UMEGA) to the context of smart e-government services in Germany. It was empirically tested using the data of 330 respondents. The adapted model greatly exceeds other studies that applied an adapted UMEGA regarding the explanatory power on attitude, while the study proves that resistance to change in the investigated context unfolds a highly significant impact on behavioral intention compared to other findings. Concluding, the proposed model supports governments in planning smart e-government services and corresponding strategies more holistically by understanding the factors that influence citizens' adoption.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/3/
Trust in E-Government: An Investigation of the Socio-Technical in Election Systems,"The fallout of the 2020 US presidential election has left a legacy of distrust in election systems in some subsets of the population. It is critical to the continued success of the US democratic system to analyze the mechanics of how the electorate forms a basis of trust that leads to their intention to participate in democracy. If enough people reject the process that supports democracy, this can result in instability and chaos in social systems. This study analyzes these constructs through the lens of the Socio-Technical Model (STM) (Bostrom and Heinen, 1977). This lens facilitated the creation of a new Election Security Model (ESM) and survey instrument. This survey instrument was validated in a pilot study. A subsequent large-scale study of the entire US population was then carried out. The findings showed the STM precursors were strongly correlated to trust. Surprisingly, trust was only weakly correlated to intention to use the system.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/4/
Personality and E-petition Success: Perspectives of Online Leadership,"Online petitions have been a popular and effective means for the public to voice their opinions and potentially influence society. E-petitioners can be regarded as influencers or leaders who leverage their social impacts beyond the digital world. In the current study, we collected 41,952 e-petitions from change.org and used an advanced text-mining model (Roberta-base model) to detect petitioners' Big Five personality traits. Then, by applying the trait theory and leadership literature, we developed a research model and provided an in-deep understanding of how petitioners' personalities can influence the petition's success. Our findings indicate that while neuroticism turns out to have a positive effect, agreeableness and conscientiousness are significant negative factors that can hinder the success of an online petition.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/5/
Examining The Moderating Effects of The Digital Divide on The Nexus of E-Government Use And Public Value: Evidence From a Developing Economy,"Continuous investments into the digitalization of public services call for assessments of the success of such initiatives. However, studies on e-government evaluation from citizens’ perspectives are scanty, particularly in developing countries. Despite the negative effects the digital divide has on emerging economies, the impact of multi-dimensional digital divide on the success of e-government systems in such countries has not been adequately examined. Contemporary Internet-based systems characterized by personalization and customization generate varying user experiences. Hence, calls for information systems (IS) success development measures to capture intangible and subjective benefits derived not only from traditional and utilitarian values but also social values to reflect the contemporary complexities of online user interactions. This study investigates e-government success as public value from citizens’ perspective amidst the effects of the digital divide. Data will be collected through a survey of e-government users in Ghana and analyzed with partial least square structural equation modeling (PLS-SEM) approach to evaluate the proposed model.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/6/
Towards a multi-stakeholder framework for evaluating digital government success,"In this paper, we develop an inclusive framework for identifying the direct and indirect stakeholders in digital government projects. Using a public value lens, we develop a systems view of digital government initiatives. We use an illustrative case study to show how stakeholders, especially indirect stakeholders, can exert influence on digital government projects, including what projects are perceived as legitimate, what factors are considered as important for success, and what stakeholders are involved in post-implementation evaluation. We also review several lenses commonly used for evaluation of digital government projects and illustrate how they fit into our framework.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/7/
Is COVID-19 a Driver for e-Participation? Insights from Participatory Budgeting in Poland,"This paper explores how COVID-19 has impacted e-participation, i.e., participation by electronic means, in public decision-making, specifically in certain forms of local community budgeting. We expound the concept of e-participation and its sub-concepts and investigate these as applicable to participatory budgeting. 34 managers in five City Halls in Poland were interviewed on their views and experiences with moving public interactions on-line during the COVID-19 pandemic, particularly as these interactions relate to participatory budgeting. The findings indicate that COVID-19 has indeed accelerated the digitalization of the participatory budgeting procedures, and to some extent may have increased community participation in general.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/8/
Pro-innovation Behavioral Profile of the organization in e-government managers’ view in Poland,"This research analyzes the Pro-innovation Behavior Profile of government organizations in Poland from the perspective of e-government managers during the COVID-19 Pandemic in Poland, a post-transformation economy. We adopted the Pro-innovation Behavioral Profile of Organization Questionnaire from Barbosa et al. (2018) for data collection via a random online survey of 61 e-government managers. They worked at certification organizations representing the Certification Authority - providing e-services, verifying entities' identities (websites, email addresses, companies, or individuals), and binding them to cryptographic keys by issuing electronic digital certificates. E-government managers perceive their organization as supporting creativity, structure, and environmental factors that lead to innovation. Although leaders motivate pro-innovation behavior in employees, the organization's culture could be more innovative. The main contribution of our study is the first adaptation and use of the PIB questionnaire to collect data from e-government managers in Poland, a post-transformation economy. The results can support e-government managers and political leaders in shaping the Pro-innovation Behavioral Profile of government organizations.",IS,https://aisel.aisnet.org/amcis2023/sig_egov/sig_egov/9/
Longitudinal analysis assessing swift guanxi on live streaming shopping,"Given the cultural uniqueness of China, swift guanxi has been demonstrated as a substantial factor in directing purchase intention in social commerce. Although many studies examined swift guanxi in live-streaming shopping, the majority of the studies were conducted using a cross-sectional design, suffering from shortages in which no understanding of the long-term causal effect of swift guanxi and its antecedents. The purpose of this study is to examine how swift guanxi has a continuous long-term effect on purchase intention. In addition, how streamers' characteristics (i.e. attractiveness, expertise) and social interaction (i.e., WOM) affect swift guanxi over a period of time. A conceptual model is developed and empirically tested based on four waves of a longitudinal survey of viewers who have repeated watching experiences for the same live-streaming shopping program. This study contributes to increasing the validity of the research model concerning swift guanxi in the context of live-streaming shopping using th",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/2/
Improving Communication at a University: an Action Research,"In the paper, the proposal for a system improving communication at universities is presented. First, problems concerning the use of multiple communication channels are identified. Second, examples of systems for improving communication are discussed, and their properties are analysed. Next, factors of effective communication, especially at universities, are examined. Then, a system for improving content management by intelligent messages targeting to interested parties only is proposed. Finally, a SWOT analysis of the system is carried out, and further actions leading to the implementation of the proposed solution and its evaluation are indicated.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/3/
e-Justice for Socioeconomic Development,"e-Justice, also called cyberjustice, refers to the incorporation of information and communication technologies (ICT) into the justice system. This includes offering court services via electronic means as well as other digital services for dispute resolution purposes. The objective of this study is to explore the potential contribution of e-justice as one of the components of e-democracy to socioeconomic development. Based on the analysis of the examples of e-justice initiatives, an initial, conceptual framework that links socioeconomic development with e-justice is constructed.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/4/
Information and Communication Technology and Human Emancipation Among Vulnerable Groups,"Skilled human capital is crucial for information and communication technology (ICT) progress, yet many countries lag far behind in this demand for digital skills. In low- and middle-income countries (LMIC), inclusive technological education faces enormous challenges, such as a lack of digital infrastructure, socioeconomic instability, and low quality of formal education. Drawing upon Freire's tenets on emancipatory pedagogy and the components of human emancipation in IS literature, we investigate how local actors can promote human emancipation through ICT programs focusing on vulnerable groups. We adopted an interpretive case study associated with a Brazilian ICT program focused on human values. Our findings show that emancipatory pedagogy facilitates the understanding of human need, and the expansion of autonomy in life. However, contradictions and tensions in social impact initiatives hinder sustainable and equitable solutions. In that sense, technology is a tool that changes the world, but it must be reoriented in the right direction: solving social needs.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/5/
Digital Transformation of Public Sector Manual Procurement and Socioeconomic Development: A Developing Country Case Study,"Digital transformation in the public sector offers opportunities for developing countries to address socioeconomic development challenges related to citizen wellbeing. While countries in the developed world digitally transform their public sector procurement for socioeconomic development, many in the developing world are yet to do so. Existing information systems research on digital procurement in developing countries has focused more on adoption, strategy, and design. Little is known about how manual procurement practices are digitally transformed in the public sector of developing countries for socioeconomic development. This study employs interpretive case study as a methodology and activity theory as analytical lens to investigate how manual procurement activities in the public sector get digitally transformed and its effect on socioeconomic development in a developing country context. The findings have implications for research, practice, and policy.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/6/
Bridging Healthcare Barriers with mHealth: A Systematic Review of Tuberculosis Applications and their Limitations,"Tuberculosis remains one of the most significant burdens for resource-constrained environments with limited healthcare supply, such as the Global South. However, factors such as the adoption of mobile phones create a favorable environment for mobile health technology, yielding a promising avenue for the development of the Global South through digital innovation. Previous studies, however, have not provided a comprehensive assessment of available tuberculosis applications and their features. To bridge this gap, we conducted a systematic review of existing tuberculosis applications to gain a deeper understanding of the current landscape. Our findings indicate that current tuberculosis apps have significant limitations, such as requiring an internet connection or being available in only one language. Our study contributes to extant research by presenting the first systematic analysis of tuberculosis applications and provides practical insights by highlighting areas of improvement that should be addressed in future app development.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/7/
Linking United Nations Sustainable Development Goals with the Impact of ICT4D Research – A Theory of Change Approach,"Information and communication technology for development (ICT4D) research are increasingly impactful in the field of IS, as they aspire to deliver developmental benefits across the different domains of application. Unfortunately, many ICT4D projects often fail to demonstrate in a quantifiable way how they have contributed to development. The United Nations mechanisms for global development, currently the Sustainable Development Goals (SDGs), offers a quantifiable basis for assessing developmental goals attainment in general, and has been adopted for some ICT4D projects.. However, there is a knowledge gap regarding how SDGs can be achieved through ICT4D initiatives at the local levels, especially when indicators for SDGs are all at a global level. In this research, we propose a Theory of Change (ToC) approach to link ICT4D project outcomes to specific SDG targets through a set of clearly articulated change pathways. We demonstrate how project-level indicators can then be developed to measure the ICT4D project’s impact on SDGs. An example of the approach is provided in the context of the Techies Without Borders project called 'Continues Medical Education Solutions (CMES). The case study shows how ToC is used to explicitly create project-level indicators that are linked to a specific SDG target. This research makes an important contribution by providing guidance to IS researchers seeking to align ICT4D projects with UN SDGs.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/8/
Blockchain technology perception regarding supporting the digital transformation of Supply Chain Management,"The globalization of the world economy increases the role of value chains, where disruptions in the realization of the logistics processes occur repeatedly. Climate changes, pandemics, and military conflicts further increase the difficulties in matching supply and demand in a vast majority of networks. As a result, the digital transformation of Supply Chain Management (SCM) often referred to as Digital Supply Chain (DSC) is considered an important solution for several SCM concerns. Extensive subject matter literature research pointed out that several technologies have been studied and implemented as DSC solutions contrary to blockchain technology. Therefore, the research aims of this article is to examine the challenges of DSC with particular emphasis on blockchain technology perception in this context. Research has been conducted via structured in-depth interviews with SCM experts. Study results highlighted that blockchain is perceived by SCM experts as a technology with significant capability to support the creation of DSC, but unfortunately used at all. They also pointed out that blockchain should not be the technology of the first choice when initiating the digitization of SCM processes, but potentially beneficial for mature DSC.",IS,https://aisel.aisnet.org/amcis2023/sig_globdev/sig_globdev/9/
Smart Mobility Meets Industry: Enhancing Energy Flexibility Potentials by Combining Industrial Production & Electric Vehicle Charging,"The increasing share of renewable energy sources poses the challenge of volatile energy generation, requiring demand side management (DSM) to manage such volatility. In view of the high industrial electricity demand and the increasing charging demand of electric vehicles (EVs), both industry and mobility represent relevant areas of DSM. However, the combination of EV charging and energy-intensive industrial processes still contains untapped synergy potentials. Thus, we present a mixed-integer linear program and quantify the economic and ecologic potential of combining energy flexibilities of industry and electric mobility within a case study. For our evaluation, we compare the results of our model to a benchmark case that separately manages industry and EV flexibilities. Our findings suggest that implementing our approach would yield both economic and ecological benefits, resulting in a reduction in anticipated costs and emissions, as well as a decrease in associated uncertainty.",IS,https://aisel.aisnet.org/amcis2023/sig_green/sig_green/13/
Scalable Sustainability Monitoring of Financial Reports: A Design Science Artifact,"The United Nations Climate Change (UNCC) report in 2022 warns that the likelihood of global warming exceeding 1.5°C by 2026 has surpassed 50%. Climate change is a global problem that requires collective action, and companies have a significant role to play in mitigating climate change. Companies have already started to incorporate climate change risk and liabilities in their annual reports. Monitoring a company's progress in transitioning to sustainability can help stakeholders make informed decisions. However, existing approaches for sustainability monitoring of annual reports are limited in scalability, mainly because of the manual steps involved. Therefore, we designed and evaluated a more scalable artifact using state-of-the-art natural language processing (NLP) techniques to monitor companies' sustainability targets based on their annual reports. This work presents a prototype that improves upon the current state of practice and contributes to the body of knowledge by outlining key design decisions.",IS,https://aisel.aisnet.org/amcis2023/sig_green/sig_green/18/
The Trash is Always Greener on the Other Side: A Life Cycle Assessment of IOT Implementation,"The 2030 Agenda has pushed practitioners as well as academia to renew their efforts on promoting sustainability, e.g. in how digital technologies can support cities to improve their environmental performance. However, as scholars focus their attention on the positive outcomes of implementation, they often neglect the environmental impact of the artefact itself. We present a study of a Green IS implementation – a municipal Internet of Things (IoT) solution which was expected to decrease the carbon emissions produced by urban waste collection in Sweden. Using a mixed methods approach, we present qualitative findings from interviews & project meetings as well as quantitative findings from a Life Cycle Assessment (LCA). We find that (1) the environmental impact of the connected litter-bins – however small – is not necessarily offset by any significant benefits, and (2) the most significant way for the stakeholders to reduce environmental impact is to utilize more ecologically friendly trash bags",IS,https://aisel.aisnet.org/amcis2023/sig_green/sig_green/4/
The Effects of Digital Nudging on User Satisfaction in Online Customization,"As an emerging technological means to influence user, digital nudging is being used by more and more custom manufacturers. This research explores the influence of digital nudging design in online customization system on user satisfaction. I propose that for novices, the adoption of digital nudging can reduce the design effort and increase their satisfaction; for experts, the adoption of digital nudging will reduce users’ feeling of accomplishment and reduce satisfaction. In addition, the influence of overt digital nudging (ODN) is stronger than that of covert digital nudging (CDN). We designed 3 experiments to investigate them. This research can extend the understanding of the study on user customization satisfaction and the effect of digital nudging and provide guidance for manufacturers to optimize the design of customization systems.",IS,https://aisel.aisnet.org/amcis2023/sig_hci/sig_hci/9/
Barriers Driving Nonuse of Online Medical Records: Latent Class Analysis of Chronic Patients,"Online Medical Records (OMR) platforms can provide benefits to chronic disease patients. Yet, OMR use among them is suboptimal. The study identifies clusters among nonusers of OMR among chronic patients. The Health Information National Trends Survey (HINTS) iteration 5, Cycle 3 data were used to analyze 1071 respondents. Latent Class Analysis was run on the six reasons for nonuse (no record, speaking directly, privacy or security of the website, no Internet, login issues, and no need to access) and resulted in 3 clusters. About 19% subjects expressed multiple reasons and 69% just one strong reason. Demographic and clinical attributes were partially associated. For electronic wearable/tracking device use or electronic communication, differences among clusters were noted; persistent resisters showed lower propensities to use. Interventions to improve patient use of Internet-based health technologies should be customized and help produce patient-generated data facilitating healthcare decision-making.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/1/
Instademic: Deriving spatiotemporal contact networks from tagged social media images,"Identification of persons who have come in contact with COVID-19 and future viruses is necessary for early identification of potential carriers to improve public health. A Spatiotemporal Colocation Network (SCN) represents entities in terms of being present at the same location within a specified temporal window. A key factor of image sharing systems is user annotation, or tagging. By generating the network of collocated entities in image files across a temporal window we can map expected viral diffusion and identify potential carriers. Actual absolute location of entities is not important here. In colocation analysis for virus carrier detection “relative colocation” is important. i.e. we want to know that X was collocated with Y at time t, and Y was collocated with Z at time t’ which will enable us to build a “contact network”. Our proposed method uses tagging meta-data and does not require sharing of actual images.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/10/
From Seeker to Provider: User Ability and Role Switching in Online Health Communities,"Online health communities offer a platform for patients to seek support from other users with similar conditions. As those support seekers’ situation improves, they may ultimately want to give back to the community and provide support to others. This study explores how seekers’ ability to provide support, operationalized using their cognitive state and social network structure, can influence the seekers’ propensity to switch roles and become support providers. We leverage the Dual Perspective Model of Agency and Communion and Social Networks literature to theorize a seeker’s ability to provide support. We apply survival analysis to study seekers’ first role switch to a provider and the duration from seekers’ initial post to role switch. We find that while seekers who have higher positive agency and closeness centrality are more likely to switch roles to a provider, those having higher positive communality, degree centrality, and eigenvector centrality are less likely to switch.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/11/
Content Quality in Online Q&A Communities: An Approach for Measuring Content Quality,"Research into online support communities is becoming more important as researchers strive to understand the dynamics behind these communities and the interactions of their participants. Since these communities are largely Q&A platforms where seekers request information or support, the quality of responses is increasingly relevant in most studies, yet this is a difficult variable to measure. We propose a method for measuring answer quality by using text analytics to determine the topics that are addressed in the questions and then determining the quality of responses based on whether the topics in the answer match the topics in the question. This research discusses how to generate this type of metric and demonstrates its validity using a unique data set from multiple mental health communities in China.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/12/
"Policy Change, Social and Cultural Capital and Medical Crowdfunding Use: A Quasi-Natural Experiment","Medical crowdfunding is an emerging means for patients to seek financial support. This study investigates the impact of health policy, specifically the Individual Mandate Repeal (IMR), on the use of medical crowdfunding. We employ a difference-in-differences research design and find that IMR has led to an increase in medical crowdfunding usage, with significant heterogeneity among users. Specifically, individuals with high cultural capital and bridging social capital, but low bonding social capital, are more likely to increase their use of medical crowdfunding. These results suggest that the socio-technology for healthcare financing purpose (i.e., online medical crowdfunding) responds to the gap of the Affordable Care Act (i.e., IMR), and its usage is shaped by individual differences in cultural and social capital. The policy implication is that medical crowdfunding platform provides opportunities to take on the shortcomings of health policy, but the opportunity exploration or its usages are disparate across populations with varying levels of capital. Public health policymakers may develop policies fostering bridging social capital to expand the healthcare affordable opportunities for underserved populations without imposing taxes and mandate regulations.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/13/
Social Media Analysis of Modifiable Lifestyle Factors in Multiple Sclerosis,"Around 1 million people in the USA and 2.8 million people around the globe are living with Multiple Sclerosis (MS), which currently has no cure. Incomplete recovery, long-term disability, worsening of symptoms leads to disruption in the Quality of Life among MS patients. However, modifiable lifestyle factors (MLFs) have demonstrated the potential to improve health and wellbeing in MS patients. Therefore, by leveraging social media analytics, this study aims to understand the public perspective on MLFs as reflected in the online generated information. The study identified topics pertaining to MLFs and evaluated the relative prevalence of these topics in social media discourse. The results and analysis showed that Diet and Temperature are the two main factors for MS patients to constantly maintain and monitor. Further, this study can help the medical community in recommending the best lifestyle measures that is necessary for an MS patient.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/15/
Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial Intelligence,"Artificial Intelligence (AI) aims to elevate healthcare to the pinnacle by aiding clinical decision support. Overcoming the challenges related to designing ethical AI will enable clinicians, physicians, healthcare professionals, and other stakeholders to use and trust AI in healthcare settings. This study attempts to identify the major ethical principles influencing the utility performance of AI at different technological levels such as data access, algorithms, and systems through a thematic analysis. We observed justice, privacy, bias, lack of regulations, risks, and interpretability are the most important principles for ethical AI. This data-driven study has analyzed secondary survey data from the Pew Research Center (2020) of 36 AI experts to categorize the top ethical principles of AI design. In order to incorporate the resolution to the ethical issues identified by the meta-analysis and domain experts, we propose a new utilitarian ethics-based theoretical framework for designing ethical AI in the healthcare domain.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/17/
Cross-cultural Analysis of Online Patient Reviews from the Caregiver Perspective,"Online reviews have had an undeniable impact in the field of healthcare. Despite the growing trend of online patient reviews, physician attitudes towards them vary greatly and their effect on physician behavior is uncertain. We propose a study exploring how physicians perceive online reviews through the lens of social-psychological theory and propose a framework for examining the phenomenon of online patient review valuation from the caregiver perspective.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/18/
How the Uniqueness of Initial Posts Affect User Participation in Online Cancer Communities,"Online cancer communities (OCCs) are crucial resources for cancer patients, providing support for the complex and challenging aspects of the disease. Patients often face unique problems, and OCCs offer a place to seek support when friends and family may not understand. This study explores how unique topics discussed in OCCs impact user participation in a thread, measured by the number of replies, repliers, length of replies, and response speed. A deep-learning-based natural language processing topic model is used to calculate the uniqueness scores of posts in OCCs, based on the prevalence of each topic. The study has implications for OCC administrators, researchers, and physicians in terms of understanding and addressing unique health topics expressed in online communities.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/19/
The Influence of Health Data on the Use of the Electronic Health Record (EHR) – a Mixed Methods Approach,"The use of electronic health records (EHRs) offers many benefits, but users are also critical of them because of concerns about privacy and the sensitivity of health data. Given the ambiguity of data sensitivity, we examine which characteristics of health data influence users' upload behavior, and which are perceived as sensitive. A semi-structured interview (N=30) was conducted to locate characteristics of health data, which influence the user's upload behavior. These characteristics were then empirically evaluated in a user study (N=50). The complexity of diseases and their stigmatization potential are characteristics which influence the uploading behavior. We verify empirically that the uploading of health data is rejected when there is a high potential for stigmatization, while it is accepted when the complexity of the disease increases. Health data are considered by users to have different sensitivities, relative to stigmatization potential. Privacy Calculus regarding EHRs should consider these characteristics of health data.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/2/
"Technological, Human, and Procedural Factors that Influence Nursing Documentation Errors","Over the years, there has been a drastic increase in awareness of medical mistakes by researchers, hospitals, insurance companies, etc. The rise in mistakes has occurred for several reasons. One reason is that hospitals and other healthcare facilities spend millions incorporating information systems and technology into healthcare, which is thought to help reduce the number of mistakes. Even with the significant improvements technology has brought, mistakes are made every day, costing the United States healthcare system billions and patients’ lives. This study explores technological, human, and procedural factors that influence nursing documentation errors, resulting in negligent medical mistakes. There is no easy solution to reducing the number of medical mistakes, but improving documentation in Electronic Medical Records (EMR) systems is a start.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/20/
Towards Technology of Global Governance – A Novel Approach to Health System Performance Assessment by MCDA,"Healthcare systems must constantly evolve to meet the growing demands arising from the health needs of populations conditioned by an aging society, multimorbidity, and infectious diseases. In recent years, the Covid-19 pandemic has forced healthcare systems worldwide to increase efforts and implement new solutions. Assessment of coping with the Covid-19 pandemic requires appropriate indicators and methods that consider both multiple criteria with different objectives and the dynamics of performance in successive years. Therefore, our paper presents the developed multi-criteria model for evaluating the healthcare system in terms of coping with the Covid-19 pandemic, which includes nine indicators belonging to five main dimensions. Next, we introduce a novel multi-criteria temporal method called DARIA-TOPSIS. The research results showed that among the countries that coped best with the pandemic concerning the selected criteria are Scandinavian countries such as Denmark, Norway, and Finland and Western European countries such as Luxembourg and Ireland.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/21/
Mechanical ventilation settings advisory system Odyn,"COVID-19 pandemic impaired function of healthcare systems globally by increased workload at intensive care units in hospitals that underwent a massive influx of patients in critical condition in need of tailored and specialized care. Prevalent respiratory failure increased the demand for ventilators and respiratory therapists. Due to personnel shortages, doctors and nurses, not trained in the field of intensive care, were expected to provide adequate care for patients, supporting themselves with help of consulting specialists and navigating equipment shortages. To deal with this problem we present the research result, conducted in cooperation between computer scientists and anesthesiologists, namely the advisory system that supports physicians who do not have expertise in operating ventilators for typical COVID-19 patients. The system has been implemented at two hospitals. We present the system architecture and some internals of its recommendation modules based on decision graphs and recurrent neural networks. Further research will embrace analysis and optimization of the modules to provide yet more accurate support, as early as possible and in a broader spectrum of patient’s conditions.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/22/
A Suggested Blockchain Architecture for Healthcare Data Sharing,"In today’s knowledge economy, healthcare generates massive amounts of highly sensitive data ranging from genomic data to medical records that are central for clinical knowledge discovery and decision making. However, storage and sharing of these health data has led to several challenges most notably around ownership, privacy, data breaches and data theft. Intelligent digital transformation of healthcare is required to overcome these challenges by storing and sharing such health data responsibly and securely. In recent years, blockchain, particularly private blockchain, has been explored as a technology to address some of these issues in various industries. Blockchain developments in the healthcare sector are still very nascent. To address this key void, we proffer a generic private blockchain platform for healthcare to address the growing data challenges. We adopt the design science research methodology in our work. In this paper, we present our proffered model and the proposed blockchain architecture.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/23/
Enhancing Information Systems Success Outcomes with Electronic Health Records Systems Post-Adoption Implementation,"Despite the monetary incentives and the meaningful use mandate to implement electronic health record (EHR) systems in the workplace, users still feel threatened due to the mixed outcomes of using EHR systems. This current study examines success outcomes that enhance users’ EHR adoption in an organizational setting through the lens of information systems success model. A theoretical framework is proposed to understand what EHR implementation success means to healthcare professionals, and what systems and user characteristics contribute to EHR implementation success. The proposed model suggests that user capability, EHR usability, and EHR usefulness will affect EHR success outcomes (perceived satisfaction with usage and perceived helpfulness over usage process). The study considers the interaction between technology and user capabilities to impact IS success outcomes and discusses some implications to theory and management.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/24/
Enabling Innovation in Digital Hospital Ecosystems - A Literature Review,"Healthcare information technology holds great potential for improving services in hospital environments. This transformation is fueled by the fact that current extraordinary situations, such as the COVID-19 pandemic, indicate that the realization of digital health innovation is essential to delivering most healthcare services for patients. To facilitate an environment that allows digital innovation and integration of internal and external stakeholders a movement toward a digital hospital ecosystem is needed. We conducted a structured literature review to identify the current state of knowledge about enabling mechanisms that facilitate the development of a digital hospital ecosystem. Therefore, we draw on the current literature on digital infrastructures and platform ecosystems. We identified the challenges hospitals face during their transformation and address them by deriving key mechanisms of the digital hospital ecosystem. Thus, this study contributes to research by presenting insights into the transformation toward a digital hospital ecosystem.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/25/
What Factors Affect the Elderly’s Adoption of Digital Technologies for Daily Life and Health Management: A UTAUT Study,"In 2021, 30% (97.6 million) of the U.S. population were 55 years or older, and this number will keep growing. This segment of the U.S. population has been hit the hardest by the COVID-19 pandemic which has led to an increase in their social and physical isolation. One way that people have coped with such isolation is to rely on digital technologies to connect with others and maintain daily life activities, and even manage their chronic health conditions. However, little research has focused on how the elderly use digital technologies in their daily life and managing their chronic health issues. The present study utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) framework to examine which factors affect the acceptance of digital technology and chronic health management application by seniors in the U.S.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/26/
Adoption of Blockchain to Address Healthcare Financing System Challenges: A Systematic Review,"The healthcare industry is currently going through a digital transformation, and the implementation of blockchain technology has emerged as a promising solution to overcome many of the difficulties faced by healthcare financing systems. This research paper presents a systematic review of the existing literature to recognize the issues that healthcare financing systems are currently facing and how they can be addressed through blockchain solutions. The review also identifies potential solutions and frameworks proposed for the implementation of blockchain solutions. The study has revealed that the combination of blockchain technology and machine learning has high potential in developing a smooth healthcare financing system that is secure, accessible, and has data integrity. The use of blockchain can provide a secure and transparent platform for storing and sharing healthcare data, leading to improved accuracy and efficiency of healthcare financing systems. By integrating blockchain 3.0 with healthcare 4.0 processes and technologies, a real-time and reliable blockchain-based healthcare system for financing can be developed that is highly secure, transparent, and can execute transactions on any IoT-enabled device. The study has revealed that the Ethereum blockchain network is the most suitable platform to implement a distributed ledger to address the challenges faced by healthcare financing systems. This is due to the various applications of the platform already being used within remote medical healthcare operations and other healthcare processes as found in the literature. The study's findings are expected to contribute to the ongoing discussion on the digital transformation of the healthcare sector through emerging technologies.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/28/
The Longitudinal Financial Impact of Health Information Technology Investments in Hospitals: A Panel Data Analysis,"Investments in health information technology (HIT) are known to improve operational and financial outcomes in hospitals. However, it is less understood whether this effect is short-term, medium-term, or long-term. This paper investigates the effect of HIT investments on hospitals’ cost-to-charge ratio, a financial metric that accounts for hospital costs and revenues, at different time lags following the initial investment. Using panel data on U.S. hospitals from 2010 to 2021, we report that the impact of HIT on hospital cost-to-charge ratio is realized with a lag of 0 to 4 years, when controlled for hospital differences such as rural vs urban location, public vs private ownership, proportion of uncompensated care, and year-over-year variations. This effect becomes non-significant in subsequent years as the effect of HIT wears out. We also quantify the returns from HIT investment. Every 1% increase in HIT investment results in a reduction of 3.3 to 6.0% in cost-to-charge ratio between 0 and 4 years after the HIT investment. Implications of these findings for research and practice are described.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/4/
A Qualitative Study on Aging Adults' Opinions and Perceptions about Using Companion Robots in Daily Activities,"In the 21st century, Agetech (age technology) and tech devices can be integral to healthcare systems in increasing patient engagement, maintaining independence, and helping seniors live happier and healthier lives. Studying how seniors use and interact with technology can help researchers, technology designers, and vendors understand the possible benefits and risks of using technology products designed to meet older adults’ needs. This study attempts to use a qualitative approach to examine the elderly opinions about using intuition robotics (i.e., companion robots). The findings indicate a positive attitude toward using this Agetech due to health and wellness, companionship and support, and technology design benefits. However, participants also raise concerns about digital dependency and social disconnection concerns, information integrity and online resilience risks, and implementation costs. Through a model, we suggest a need for more education and awareness about robotics's potential gains and concerns.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/5/
Would You Accept Doctor ChatGPT: An Empirical Study Based on the UTAUT Model,"Since its introduction, ChatGPT has generated significant interest in many areas in just a few months, including healthcare. Recently, researchers have claimed that it has passed the U.S. medical licensure examination (Kung et. al, 2022). However, few empirical studies have investigated whether ChatGPT would be valued and accepted by public in the healthcare context. To understand the public's willingness to accept Artificial Intelligence-Generated Content (AIGC) applications like ChatGPT in the healthcare sector, this study proposes a model of factors affecting the user acceptance of ChatGPT for healthcare purposes integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) with the Trust Theory, the Perceived Risk Theory, and the Perceived Illness Theory. We will analyze the data collected from questionnaires using Structural Equation Modeling (SEM). The findings of this study will provide insights into the factors affecting the user acceptance of ChatGPT for healthcare services.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/6/
The Heterogeneous Effects of Remote Patient Monitoring on Patients with Chronic Heart Conditions,"While prior research has provided evidence that, on average, Remote Patient Monitoring (RPM) use has a positive impact on patient outcomes, very few have examined the heterogeneous effects of RPM, and none have looked at how different hospital and county configurations fit different types of patients with chronic heart conditions. Using Resource Orchestration as our theoretical framework, we will leverage causal machine learning to find patterns of heterogeneity on the impact of RPM and describe how different combinations of hospital and county configurations facilitate RPM use for each type of patient with chronic heart conditions. We expect to find differences in treatment effects across patients with different ages, socioeconomic statuses, and payment sources. Hence, our research will contribute to the literature by providing insights that can help health care providers craft strategies to facilitate RPM use for different types of patients with chronic heart conditions.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/7/
"Studying the Relationship Between Timely and Effective Care, Preventive Care, and Patient Hospital Recommendation","The problem of managing timely and effective care within emergency departments (ED) and preventive care in hospital facilities are often associated with patients’ overall satisfaction. Hence, the objective of this study is to determine the relationship between independent variables such as average ED wait time, percentage of left without being seen (LWBS), ED stay time, hospital overall rating, percentage of sepsis care, and percentage of patients’ response to hospital recommendation as a dependent variable. Given the objective, the study performed a linear regression analysis. The results of the study determined that patients’ willingness to recommend a hospital was significantly related to the average time a patient spent in ED (p < 0.01), sepsis care (p < 0.026), left without being seen percentage in ED (p < 0.001), and hospital overall rating (p < 0.001). Our findings suggest that variables related to ED throughput, and preventive care have a significant relationship with patient’s willingness to recommend / not recommend the hospital.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/8/
Mapping the Emotional Journey of STD patients in Online Forums,"People inflicted with diseases such as STDs experience a tremendous number of varying emotions from the onset of symptoms to being cured/living with it. Leveraging the power of bidirectional encoder representation transformers our study builds a custom-named entity recognition model which identifies uniquely associated emotions at various stages of the disease. Our study uses over 1 million posts derived from online communities regarding STDs. We further provide analytical insights into varying emotions at different stages based on contextual reasonings. Finally, we utilize the resulting insights to produce prescriptive measures for future online recommendation systems on online communities, and for stakeholders hoping to build devices that address problems about stigmatized disorders such as STDs.",IS,https://aisel.aisnet.org/amcis2023/sig_health/sig_health/9/
IT Project Selection and Organisational Agility – An Integrated Conceptual Framework,"Sustained competitive advantage is elusive to organisations, in a volatile contemporary context. Organisational agility (OA), enabled by information technology, offers hope. However, organisations that pursue agility encounter incompatibility with IT project selection (ITPS) – as they select and fund projects, before implementation. Existing ITPS literature provides rich insights into methods, processes and people but is fragmented. Without integration, the incompatibility cannot be resolved. To address this problem, this study conducted a literature review of 159 articles to develop an integrated framework for understanding OA and ITPS. Findings show that methods, processes and people in ITPS can both enable and disable OA. Following conceptual-framework-analysis, ITPS is redefined as an enabler of agility. The resultant three-layered conceptual framework shows that firms can deploy ITPS methods, processes and people that enable sensing, deciding and responding, leading to OA. This study contributes by integrating literature to facilitate further research in this area.",IS,https://aisel.aisnet.org/amcis2023/sig_itpm/sig_itpm/1/
A Survey of Technology Selection Approaches in Data Science Projects,"In a world where the amount of generated and stored data is steadily increasing, Data Science (DS) has emerged as an integral discipline for organizations to extract knowledge and value from these resources. Due to the high failure rate in these undertakings, new approaches to support the project management are urgently needed. The use of appropriate technologies constitutes a success factor in DS and its selection process is regarded as a strategic project management activity. As technology selection is not described sufficiently in current DS process models, a structured literature review is performed in this article that concentrates on the addressed technology layers, the applied decision procedures, and the specific technology instances of the identified approaches. The study shows that holistic technology selection approaches that consider all activities of the DS lifecycle and the entire required tool stack are severely lacking and should, therefore, be further researched in the future.",IS,https://aisel.aisnet.org/amcis2023/sig_itpm/sig_itpm/2/
Exploring Hybrid Project Management Methodologies and Selection Processes,"Project management methodology is important to the success of a project. However, changes in technology and practitioner approach have led to a mix of methodologies known as hybrid. This research sought to identify the types of project management methodologies practitioners currently use and determine factors that impact how a project manager selects a methodology. A survey questionnaire was used to identify methodologies and types of project management tools, techniques, and processes used. Correlation and chi-square analysis analyzed if three specific factors (industry, project manager experience level, and technology project type) influenced the selection of a predictive, adaptive, or hybrid methodology. The results showed that industry and technology project type are factors in the methodology selection, but project manager years of experience were not. Overall, the analysis supported that there is no “one size fits all” when it comes to methodology selection, but hybrid is emerging as the predominant approach.",IS,https://aisel.aisnet.org/amcis2023/sig_itpm/sig_itpm/3/
Agile CRISP-DM for Analytics Projects: Understanding Diabetes Treatment Using fsQCA,"Despite the increasing popularity and demand for analytics projects, the uncertainty in goals and dynamic processes involved in these projects behoove IS researchers to further understand the project management techniques applicable to these projects. In this paper, I illustrate the process of carrying out an analytics project using an agile project management approach. In this project, following the project management concept of ""tailoring"", fsQCA is used to analyze the effective treatment configurations of traditional Chinese medicine and Western medicine for diabetes. This illustrated process could be potentially used in various small-sized analytics projects carried out by IS researchers, and R&D efforts. The practical implications of the analysis for medical providers are also discussed.",IS,https://aisel.aisnet.org/amcis2023/sig_itpm/sig_itpm/4/
Balancing Agility and Hierarchical Culture through Leadership Sensemaking,"Many organizations are embracing agile values and principles as a strategic priority. However, improving agility is never an easy task. In their agile journeys, organizations still face significant barriers and challenges, such as culture clashes and lack of leadership. Agile principles and values are especially hampered by hierarchical cultures, which are still common in many companies and organizations. Using a sense-making theoretical lens, this study conducted a case study in two organizations to understand agile leadership roles in achieving a balance between agility and hierarchical cultures. The results show how agile leaders play mediator and navigator roles in vertical and horizontal sense-making processes, respectively, to build psychological safety and an agile mindset at an organizational level.",IS,https://aisel.aisnet.org/amcis2023/sig_itpm/sig_itpm/5/
Towards a domain-driven distributed reference architecture for big data systems,"The proliferation of digital devices, rapid development of software and the infrastructure of today, have augmented user’s capability to produce data at an unprecedented rate. The accelerated growth of data could be called the era of big data and forced a paradigm shift in data engineering because the variety, velocity and volume of data overwhelmed existing systems. While companies attempt to extract benefit from big data, success rates are still low. Challenges such as rapid changes in technology, organizational culture, complexity in data engineering, impediments to system development, and a lack of effective big data architectures mean that only an estimated 20% of companies achieved their goals. To this end, this study explores a domain-driven distributed big data reference architecture that addresses issues in data architecture, data engineering, and system development. This reference architecture is empirically grounded and evaluated through deployment in a real-world scenario as an instantiated prototype, solving a problem in practice. The results of the evaluation demonstrate utility and applicability but with architectural trade-offs and challenges.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/11/
"Exploring Privacy Concerns, Privacy Risks and Benefits on Users’ Trust and Engagement in Continual Use of Service Robots","Service robot applications continue to increase globally which create competitive advantages for organizations and individuals. This study investigates the privacy concerns, privacy risks and perceived benefits on users’ trust and engagement in continual use of service robots, leading to the development of a conceptual framework within the theoretical background of the theory of planned behavior (TPB) and the privacy calculus theory for better exploring users’ trust and engagement in the continual use of service robots that can be tested and validated. From the theoretical side, this study will contribute to the IS body of knowledge on privacy and risk determinants, benefits and technology adoption by identifying factors affecting users’ trust and engagement in the continual use of service robots. On the practical side, this study will provide information into the influence of privacy concerns, privacy risks and perceived benefits on users’ trust and engagement in continual use of service robots, and provides insight for service robot providers to address those issues, and make a sensible decision in the adoption of service robots.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/13/
Digital Nudging and Transparency: An Experimental Study of Two Types of Recommendation Badges,"This paper investigates the impacts of digital nudging on customer purchase decisions. Digital nudge is an online choice architecture that alters individual’s behavior in a predictable way while preserving all the available options and keeping the same economic incentives. Most recently, academic research started to address the relationship between nudges and Artificial Intelligence/Machine Learning (AI/ML) and found that personalized targeting algorithms influences individuals and collective behaviors in various ways that include undesired consequences for both end-users and firms. Drawing on literature of nudge and anchoring effect, this study proposes two types of nudges based on the transparency level: ambiguous badge (ex., Amazon’s Choice) and specific badge (ex., Best Seller). We further hypothesize that specific badge will manipulate user’s preferences to a less extent than ambiguous badge. This study will contribute to the ethical use of digital nudging in different contexts.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/16/
Effect of AI Decision Speed on User Adoption in Human-AI Collaboration: The Moderating Role of Historical Decision Quality,"Artificial intelligence (AI) has been widely used in many products and services and has become an important means to assist users in decision-making. In the context of human-AI collaboration, both the quality and speed of AI’s decision-making are the two system features that users can readily identify. However, the existing research focuses on decision-making quality and pays little attention to the effect of AI’s decision-making speed. Drawing from the theory of cue utilization, this research explored the effect of AI decision speed on user’s adoption intention. The results of two experiments show that users have higher AI decision adoption intention at high AI decision speed than at low; the perceived intelligence and perceived risk in decision-making play a mediating role in the above effects. Additionally, historical decision quality moderates the impact of AI decision speed on users’ adoption by weakening the above impact in the high-quality condition. The findings enrich the research on AI adoption and have some practical implications for AI service providers and developers.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/22/
The Role of AI Assistants in Online Shopping Platforms: Evidence from Livestream Selling,"Livestream technology is transforming consumers’ online shopping experience. This research addresses how livestream selling platforms mitigates the tension between streamers’ constrained service capacity and individual service demands with AI assistants. We conduct a large-scale field experiment wherein the consumers in the treatment group have access to an AI assistant that predicts consumers’ potential needs and provides individualized services, while the consumers in the control group do not have access to such an AI assistant. We find that the introduction of the AI assistant increases sales by 2.61% and reduces product returns by 62.86%. Our models on multiple-stage purchase decision-making reveal that an AI assistant increases the duration of the awareness and consideration stages, improves the probability of placing an order in the evaluation stage and reduces the likelihood of product returns. Further, our analyses reveal that interacting with the AI assistant also reduces consumers’ expressions of affection and positive emotions.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/3/
The Invisible Hand: Uncovering the Impact of AI Incidents on Human Dignity,"In the last decade, Artificial Intelligence (AI) has undergone significant advancements and breakthroughs. At the same time, AI incidents, defined as events of unexpected, undesirable, or unintended action or outcome resulting from AI-powered systems, may take various forms, such as malfunction, security breaches, and even ethical violations. Hence, it is important to understand how such AI incidents may influence human dignity (i.e., how people perceive their values). Drawing on the dignity literature, this paper seeks to answer the research question: How do AI incidents violate three dimensions of human dignity, namely inherent dignity, meritocratic dignity, and behavioral dignity? We proposed three new constructs of dignity violations, i.e., perceived violation of inherent dignity (PVID), perceived violation of meritocratic dignity (PVMD), perceived violation of behavioral dignity (PVBD). An experiment is to be designed to test how AI incidents may lead to dignity violations. The paper highlights the need for ethical considerations in the development and deployment of AI systems to enhance, rather than inhibit, human dignity. The findings of this paper contribute to the ongoing discourse on the responsible use of AI and its implications for society.",IS,https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/5/
It's Time to Smarten Up! - A Framework for Building Smart Service Systems,"Smart service systems are a network of people, technologies, as well as processes and rely on learning, adaptivity, and decision-making to provide smart services to their users. The complexity of designing smart service systems results in a plethora of potential implementation obstacles, often leading to an overly technical focus and resulting in isolated solutions. There is, therefore, a need for a guideline for companies that helps them facilitate the development of complex smart service systems. To this end, we apply action design research used in a multi-year design study for digital twins in the manufacturing industry. The resulting four-quadrant framework (domain, system design, organization, and ecosystem) can help researchers and companies guide the development of smart service systems or evaluate past progress. The evaluation of the framework shows that there might be dominant quadrants and certain paths in the framework, which we would like to investigate in further research.",IS,https://aisel.aisnet.org/amcis2023/sig_osra/sig_osra/2/
Multi-tier Supply Chain Complexity and Buyer Performance in ICT Industry,"The characteristics of suppliers in multi-tier supply chains have drawn more attention recently in the post-pandemic age. Due to the increasing reliance on suppliers, information-communication technology (ICT) buying firms expect to map out their upstream suppliers to identify the characteristics of those suppliers, which might affect their performance. In this paper, we utilize horizontal, vertical, and spatial complexities to examine how complex characteristics of the first- and second-tier suppliers affect the ICT buying firms’ financial performance. Our empirical test shows that ICT buying firms’ sales growth is affected by multi-tier suppliers from different industries. Our results bring insights into the role of multi-tier suppliers and their effects on ICT buying firms’ financial performance. The research findings contribute to the literature on the complexity of multi-tier supply chains. We also provide practical implications for managers from ICT firms to improve their decision-making and collaboration with multi-tier suppliers.",IS,https://aisel.aisnet.org/amcis2023/sig_osra/sig_osra/8/
What is Digital about Digital Innovation? An Ontological Discussion of Digitality,"“Digital” is a popular adjective for concepts related to electronic, computerized or networked processes in current research and practice. From an ontological-theoretical perspective, the prevalence of digital things triggers the question, how Digitality itself could be conceptualized. This article studies the ontology of Digitality and consequently discusses Digital Innovation (DI) from a new perspective. First, literature on Digitality is analyzed. Based on the findings, an ontological conceptual framework of Digitality (OCFD) is proposed. It describes four layers of Digitality: physical foundations, virtuality, digital culture and world, and digital metaphysics. From a human perspective, requirements to grasp and embody Digitality are highlighted, as well as the expansion of reality. Finally, Digitality is discussed as a paradigm of DI and exemplary digital phenomena are projected onto the OCFD. This paper contributes to DI research and practice through theory-building towards a better understanding of ontological characteristics and practical implications of Digitality in DI.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/1/
Strategies for the Aesthetic Experience of Business Processes,"Business processes require knowledge and generate knowledge when data and information are combined with personal experience. Information systems can support business processes in highlighting the importance of formal, but also hidden knowledge structures. In particular, the implicit knowledge incorporated in people and organizational routines contributes to the uniqueness of business processes. Aesthetic business processes offer a holistic knowledge management concept that aims to mobilize implicit knowledge. Based on C. Alexander's pattern theory and visualizations, it is about perceiving the Gestalt of business processes. The metaphor Gestalt refers to the atmosphere of business processes, which is to be sensually experienced. This article aims to (a) show different strategies, (b) model the Gestalt of aesthetic business processes, and (c) visualize information culture to make aesthetic experiences successful in everyday organizational life. Aesthetic experiences might remain fragmentary or become fulfilling when we ""have an experience"", but ultimately open up a variety of perspectives.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/2/
What can Information System Consultants do to Assist Client Leveraging Digital Transformation? A Case Study in China,"What can information system consultants do to assist client leveraging digital transformation to serve better the well-being of employees? This paper describes the case of a high-tech company in China which initially wanted to rely on an external consulting group for conducting digital transformation in order to solve organizational problems. However, the first evaluation of these organizational problems conducted internally led the company to decide to postpone the digital transformation. The focus shifted toward an internal consulting project in charge of first investigating further these organizational problems and then conducting an internal transformation independent from technology. These organizational problems appeared to be related to the well-being of employees. From “How to assist client leveraging digital transformation to serve better well-being of employees”, the project became “How information system consultants and managers can assist with organizational and systems design for work procedures, and processes that improve performance”. The theoretical contribution of this paper is to suggest a framework for analyzing the well-being of employees. This framework of ambidexterity reveals the way how employees are torn apart between exploration and exploitation. On the practical standpoint, the contribution of this paper is to identify a collaborative communication network approach for solving ambidextrous organizational problems related to the well-being of employees that rely less on technology than on human and organizational transformation.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/4/
Operationalizing Ethics for Information Systems Design – A Tool for Ethical Software Assessment,"Propagation of information technology in the increasingly diverse domains raises questions concerning moral implications. While the number of ethical principles in IT design is growing, they do not necessarily apply to specific products, which means developers and, e.g., purchasing agents need to contextualize them for each specific IT product. This paper describes an actionable artifact that allows assessment of ethical non-functional requirements of an information system through operationalization of ethical values, Ethical Software Assessment Tool (ESAT). It is based on a survey of values that are currently considered in research and practice as well as in legal initiatives in IT context. The ethical approach and principles operationalized in ESAT are presented and discussed.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/5/
Digital Transformation and Human-Digital Competitiveness Model,"Despite the potential benefits that digital transformation can bring, the success rate of its implementation remains low at only 30% (BCG, 2020). The situation is even more critical and challenging in a highly competitive manufacturing industry. The dynamic environment with increasing uncertainties leads to decreased communication between departments, thus diminishing the advantages of joint venture cooperation. This paper analyzes the challenges of implementing information technology (IT) and organizational reform in an international joint venture through the perspective of the Human-Digital Competitiveness Model (Monod, et al., 2021). Since three dimensions of the Model can analyze those problems in a comprehensive way. It adopts a qualitative methodology by conducting a case study (Yin, 2017) through interviews with 17 staff and related party-in-interests from both sides, as well as corporate observation. Theoretically, this paper contributes to a thorough analysis of the challenges of digital transformation and organizational changes within companies from three different dimensions. Practically, it contributes to an understanding of the impact that IT and digital technologies have on the joint venture and offers insights into how business processes can be innovated to enhance competitive advantages.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/6/
Digital Transformation and Entrepreneurship,"The implementation of digital transformation often fails. This paper aims to identify what kinds of organizational issues exist in a leading technology company regarding the implementation of information technologies. The theories are Socio-Economic Approach to Management (SEAM) and Socio-Technical Approach. The research method is a case study of OMRON Corporation. The research findings include dysfunctions and the impact of IT and digital technologies: The teammate's work in the department needs to be more cohesive, and there needs to be more communication and coordination between teammates. This may cause a delay in new product development and quality problems later; in introducing new materials, problems often occur among the R&D department, materials procurement department, and quality department. The social factors of these dysfunctions are the organization is too vertically divided and sectionalism is strong. The managers should lead the transformation with entrepreneurship. The technical aspects are the tremendous success of the company's past technological development. It could have been more objective toward product development that emphasizes too much technology and function on the product. The contribution to theory is applying SEAM and STA theory to clarify the organizational problem in this research. The contribution to practice is analyzing the factor of dysfunction and the impact of implementing IT and data technologies.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/7/
Digital Transformation from Sun Tzu’s System Thinking Perspective,"About Sun Tzu’s Art of War, lots of research has been done. However, most of them were fragment studies, that is, done by sampling one or two management doctrines. About its core, the system thinking, few have been conducted using a General Systems Theory. What’s more, these studies were in most cases a theoretical one. The objective of digital transformation (DT) was re-establishing organization’s business model through innovation by using digital technologies to sharpen organization sustainable competitiveness, it is just like a war campaign. This study is to use case study method to show how Sun Tzu’s System Thinking could be used in DT to defeat potential rivals to maintain competitive advantage. The combination of theoretical study and practitioner application will be of high value both to those practitioners in digital transformation process and contribute academically to research methods in the future study of Sun Tzu’s Art of War.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/8/
Overuse of Social Media Influence Process Performance,"Overuse and even addiction of social media has gradually evolved to problematic issue, especially most of the employees in working hours. However, a little research has been done regarding overuse and addiction of social media could distract energy and occupy too much time that should be otherwise used to study work related knowledge, such as digital key technologies and domain knowledge. Excessive use also leads to coordination and cooperation issue, which decrease team/process performance. The study shall adopt case study method to explore in-depth causes of overuse of social media and its potential hazardous effects to regular operations of organizations, especially in a medium sized typical manufactory scenario. A potential solution for top management in organization would be extremely important, especially in the manufacturing sector of the No. 2 Economy of the world.",IS,https://aisel.aisnet.org/amcis2023/sig_phil/sig_phil/9/
Fostering Scalable Citizen Development in Organizations: Towards a Guiding Framework,"Low-code development platforms (LCDPs) offer organizations the potential to reassign time-intensive software development tasks from professional to citizen developers, freeing up resources for professionals while empowering domain experts. However, while software can be rapidly built through citizen development (CDD), organizations must look beyond implementation and consider whether software built by individuals with limited technical backgrounds is scalable and durable. In this study, we investigate the scalability of CDD in organizations by conducting a multivocal literature review, integrating both the current phenomenon (CDD) as well as related, established research streams like end-user development. We identify four dimensions affecting CDD scalability and four temporal stages of CDD maturity, and present these as a CDD scaling framework. Moreover, we identify three overarching themes in scaling CDD. Finally, we present an integrated research agenda based on the CDD scaling framework and overarching themes.",IS,https://aisel.aisnet.org/amcis2023/sig_sand/sig_sand/1/
Survival of the Fittest: A Business Model Perspective to explain Innovation Ecosystem Membership,"The manufacturing industry is one of the biggest beneficiaries of artificial intelligence (AI). However, to reap the value and fulfill all the necessary activities, they realize it involves a multitude of different capabilities and resources. Therefore, firms increasingly establishing innovation ecosystems that animate both, (inter)-organizational cooperation, and intra-ecosystem competition among participants. But beyond resources and capabilities, what are the reasons that independent organizations become members in the first place, and how do they secure their position in a field of dynamic competition and collaboration? This study applies an exploratory multi-case study with ten cases from the manufacturing industry. Based on a business model and role of value network perspective, our findings reveal 16 BM characteristics that add to a competitive advantage and therefore ecosystem membership. Our study contributes to the research on competitive advantage in innovation ecosystems, strategic management, and the growing streams of research on artificial intelligence.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/1/
"Information Management Capability, IT Ambidexterity and Agility: A Site Management Dashboard Study","The research topic in this paper is how can the relationship of Information Management Capability and IT Ambidexterity impact on Agility? The question was answered through an action research, proposing a research model and developing a dashboard in two phases: firstly using existing resources, which served for the initial analysis and results, and secondly, the final dashboard with existing and new resources (IT Ambidexterity), resulting in better levels of information management and organizational agility, through IMC and the ability to predict and respond to problems quickly, in Agility. The research contributed positively to the capability’s relationship, corroborating the research model, adding the information dashboard as a crucial link between capabilities. At the organizational level, information management through visual elements on the dashboard made it possible to quickly identify gaps in production, reducing the analysis of problems from more than 2 hours to 5 minutes, positively affecting the organization's decision-making.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/10/
CIO Turnover and its Consequences for Competitors,"As information technology is considered a key driver of firm’s innovation, the role of a chief information officer (CIO) has been emphasized. However, firms face frequent CIO turnover. Thus, in this study, we argue that it is important to investigate the impact of CIO turnover on a firm’s innovativeness, and the spillover effect of CIO turnover on competitors’ innovativeness. We then propose a novel approach to testing these causal inferences of CIO turnover. We found that CIO turnover did not have a significant direct effect on its focal firm. However, the spillover effect is significant and positive, implying that firms produce more patents when their competitors experience CIO turnover than if CIOs at competing firms had not been dismissed. Since losing a CIO has ripple effects throughout the whole industry, firms should consider the unintended consequences of their decisions regarding CIO turnover.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/11/
Doing More with Less: A Resource-Considerate Framework for SMEs on the Digital Transformation Journey,"Digital transformation (DT) is widely acknowledged as an effective strategy to enhance revenue and business operations efficiency. However, Small and Medium-sized Enterprises (SMEs) often face significant challenges in implementing DT initiatives due to their inherent resource constraints, such as limited financial resources or a small workforce. Unfortunately, existing literature on how SMEs can effectively embrace digital transformation is limited; available frameworks are primarily tailored to large organizations with more resources. Hence, this study aims to develop a pragmatic DT framework specifically designed to aid SMEs in overcoming resource constraints that impede their successful DT transformation, while also leveraging their agile capability. The digital transformation framework was developed by combining prior literature with insights gathered through focus group discussions with industry experts from various industries. Our research contributes to the existing digital transformation literature by formulating a novel resource-considerate framework that caters to SMEs' unique characteristics, allowing them to implement DT initiatives without overburdening their limited resources.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/12/
A Configurational Set-Theoretic Approach to an Innovation Capability Maturity Model,"Constantly evolving economic demands caused by phenomena such as digital transformation necessitate businesses and their underlying models to adapt. One possible solution for enhancing innovation potential and increasing revenue is implementing an innovation capability maturity model. This research replicates an existing maturity model development approach, which accounts for the unique characteristics of a class of companies based on their size and sector. Thus, by using a data-driven method, we provide an innovation capability maturity model for small industrial companies. We identified 21 combinations of factors that can affect innovation capability maturity, grouped into a knowledge and organizational dimension. Using a set-theoretic approach, we identified configurations of these factors that lead to low, medium, and high levels of innovation capability maturity. Based on these findings, we support an empirically driven, set-theoretic approach for developing a maturity model that can be used for higher applicability and representativity in individual classes of companies.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/13/
Creating Data Policies for Digital Business Ecosystems,"Data policies are high-level guidelines that define how the organization handles data. In the age of cross-organizational digital business ecosystems, organizations are facing the need to define data policies that allow them to operate in decentralized scenarios. We conducted a Design Science Research (DSR) project to develop an approach for data policy management for digital business ecosystems. Our artifact was developed and demonstrated in a leading European IT provider. Our results include (1) an approach for data policy creation and (2) a data policy cycle. For theory, our work extends the literature with an approach for data policy management in digital business ecosystems in a highly regulated sector. For practice, our approach can support practitioners in developing the necessary data policies based on their context, considering their location, data-related regulations, available data assets, and organizational environment.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/14/
Inter-Organizational IT-Governance for a System of Systems,"Critical Infrastructures (CI) are systems that are essential for maintaining vital societal functions, and their disruptions can have a significant negative impact on a nation's well-being. CI has become the focus of many digital advancements, resulting in them becoming increasingly intertwined. Therefore, a more holistic approach to infrastructure management must be adopted, in which CI typically organize themselves as a System of Systems (SoS). The main issue that CI organizations face is the unclarity of how to make their IT function as one system. IT-Governance (ITG) is critical to guide the implementation of SoS within individual organizations. However, current literature acknowledges the lack of research on Inter-organizational IT-Governance (IoITG), which is necessary for creating a digital SoS. This research-in-progress paper first proposes a definition for digital SoS for CI and then highlights the need for further research on IoITG capability in the context of SoS for CI.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/15/
Knowledge Management Systems and Artificial Intelligence Adoption for Increasing Business Sustainability,"Sustainable business success is essential for achieving performance in a market in the contemporary technology age. Worldwide, businesses should overcome a number of challenges by using innovative practices and technologies to achieve long-term business sustainability. This study intended to inspect the relationship between knowledge management systems (KMS) and sustainable business, and to examine the moderating role of artificial intelligence (AI) adoption in the relationship. To achieve this objective, we collect data from Iraqi firms. Using structural equation modelling, the finding of this study show that knowledge management systems significantly increased sustainable business. In addition, AI adoption moderates the relationship between KMS and sustainable business. Hence, this research makes significant implications to literature and policymakers regarding applying and understanding knowledge management systems and AI adoption to enhance sustainable business.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/2/
Cryptocurrency Payment Method for Retailers: A Systematic Review,"The retail industry is highly competitive and requires businesses to continuously explore and implement technological innovations that will set them apart. Adopting cryptocurrency as a payment method is one innovation that gives businesses a competitive edge. We conducted a systematic literature review based on an in-depth analysis of 28 published articles. The literature analysis shows that cryptocurrency payments adoption is a growing research area, although only a few studies have been conducted in the last decade. Our analysis of the results showed that retailers consider a few factors regarding cryptocurrency payment adoption. The considerations include the company's size, innovativeness, regulatory requirements, perceived ease of use, perceived usefulness, attracting new customers, competitive advantage, meeting demand from existing customers, and using the technology as a marketing tool. The study's findings will be helpful to those retailers who wish to leverage cryptocurrency payments as a business tool to gain a competitive advantage.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/3/
"Right Place, Right Time: Configurations of Technology Use for Business Model Innovation","In the rapidly evolving digital era, firms need to innovate their business models (BMs) to thrive and benefit from new digital technologies. While digital BM innovation (DBMI) can create competitive advantage through differentiation, firms often struggle to create and extract value from digital technologies. In this paper, we analyze 41 cases of BM change and identify three configurations that lead to DBMI and two configurations that do not. These configurations illustrate the BM elements in which digital technologies are used for DBMI and explain how firms use digital technologies for strategic innovation. Combined with existing research on DBMI, this research provides a missing piece of the puzzle to connect specific technologies in the BM to implement a digital business strategy and undergo a digital transformation. For practical purposes, the findings suggest three alternative strategies for DBMI that identify which elements need to be changed together to achieve DBMI.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/4/
Unravelling the Business Value of Artificial Intelligence,"This study aims to investigate how firms can prepare for artificial intelligence (AI) adoption and how AI creates business values for adopting firms. We propose that, first, leveraging AI for value creation depends on the presence of an enabling resource foundation, consisting of a stack of technological and managerial resources. According to the real options theory, the stronger such a resource foundation is, the more powerful the option it creates for a firm to adopt AI, and consequently the firm is more likely to exercise the option to adopt. Second, we propose that, after adoption, the depth of AI use can have negative impacts on employee productivity, while the breadth of AI use can have positive impacts. We empirically validate these hypotheses based on a sample of 6,845 manufacturing firms in a coastal province in China.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/5/
Embracing the Chaos: A Framework for Chaos Engineering,"Complexity has always been a topic for IS scholars. While research has addressed this problem in the past, the exponential rise of complexity due to the pervasive use of IS in our society has made complexity an even bigger justification of concern for both scholars and organizations. One of the outcomes of this risen complexity is the number and impact of IS incidents that occur due to organizations failing to prepare for a plethora of possible scenarios that can have negative consequences. We expand on Mehrizi and colleagues’ (2022) theoretical framework for learning from IS incidents. Our research presents a Chaos Engineering (CE) Framework that helps organizations in generating knowledge through experimentation, in a capacity to tame complexity. CE is the discipline of experimenting on a system to build confidence and resilience, by simulating different complex behaviors. This allows organizations to learn more effectively without the real-life costs of other learning processes.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/6/
"Reconciling Strategy, Maturity, and Performance Measurement in Industry 4.0","Maturity models are essential tools to evaluate Industry 4.0 readiness. However, they have limitations in supporting organizational strategies over time and are insufficient to create company-specific Industry 4.0 roadmaps. This paper presents a research project to steer Industry 4.0 transformation in a leading paper pulp company listed on the Euronext Lisbon stock exchange. The contribution to the body of knowledge is a novel approach for Industry 4.0 adoption that extends the Balanced Scorecard with a fifth perspective and exposes the value of maturity model fragments: fractions of maturity models pertinent to a custom strategy. The proposed solution can support continuous improvement in long-term digital transformation strategies. Moreover, the lifecycle of maturity model fragments, namely, (1) dimensions and criteria elicitation, (2) personalization, and (3) strategic alignment, opens innovative prospects for maturity model design, tailorability, and practical relevance.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/7/
"Supply Chain Analytics Capability: Conceptualization, Instrument Development","Grounded in the capability-building mechanism, business analytics literature, and the Supply Chain Operations Reference model, this study proposes a framework for developing a context-specific and comprehensive instrument for supply chain analytics capability (SCAC). The proposed framework lays the foundation for and is the first stage of a large-scale in-progress research project that aims to develop and validate a context-specific and comprehensive measurement for SCAC.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/8/
Achieving Strategic IT Alignment in Dynamic Environments: On the Value of Loose and Tight Coupling,"Executives continue to grapple with a variety of issues involving strategic IT alignment (SITA) and adaptation to unexpected market events. For many firms, the rise of global competition and increasing volatility have frustrated efforts to pursue SITA and with it, efforts to achieve organizational agility. In the face of a volatile environment, aligning IT to a changing business strategy can be difficult if IT is slow to respond-any ensuing rigidity traps could hinder agility. This paper introduces a model of SITA based on the concept of loose coupling. Focusing on the forms of SITA based on degrees of coupling between strategy and business strategy, we look beyond performance impacts of SITA to investigate how SITA is shaped by environmental change and the resulting implications to organizational agility and performance. Propositions are introduced to help guide future research on SITA in dynamic environments.",IS,https://aisel.aisnet.org/amcis2023/sig_scuidt/sig_scuidt/9/
A deep learning-based Cyber-risk Management framework for smart cities,"A malicious hacker can compromise the electronic speed limit signs in a smart city, causing an autonomous vehicle to misread the speed limit signs and result in collision or congestion on the pathways. Based on protection-motivation theory (PMT), we propose a Deep Learning-based Cyber-risk Assessment and Mitigation (DL-CRAM) model comprising three modules for the smart city administrator. In line with the threat appraisal component of PMT, our cyber-risk assessment (CRA) module uses a Convolutional Neural Network (CNN) algorithm, takes electronic two-digit speed limit sign images as an input, learns its features, and outputs the probability of misreading it. Then, using the Bayesian inference model, we compute the conditional probability of an autonomous vehicle misreading both the digits or one of the two digits. Subsequently, based on the concepts of risk theory, the cyber-risk quantification (CRQ) module calculates the expected loss for a smart city due to collision or congestion on the pathways. In line with the coping appraisal component of PMT, our cyber-risk mitigation (CRM) module proposes strategies for the smart-city administrator to reduce cyber-risk using technological means and pass the residual risk to third-party cyber-insurer.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/1/
DEFIYIELD: Exploitation of Open Blockchain Platforms,"We analyze the most common security vulnerabilities in distributed ledger systems. We performed predictive analytics on the REKT database to predict the attack pattern using predictive algorithms, including logistic regression and random forests. The results show that the month of the attack and the cryptocurrency chain affected were significant predictors of the type of scam that occurred. The most important predictors were the Log of funds lost, the chain or platform of the cryptocurrency attacked, and the Log of funds returned after the attack. The study highlights the need for greater scrutiny and improved security measures in DeFi projects to mitigate the risks associated with the DeFi ecosystem.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/10/
Benchmarking the Robustness of Phishing Email Detection Systems,"Social engineering attacks are currently the most cited cybersecurity threat to organizations. Phishing emails are the most salient form of social engineering attacks. Organizations are increasingly implementing AI-enabled systems to detect phishing emails. However, AI-enabled systems are often susceptible to textual perturbations, where an adversary makes a small change to cause a misclassification. In this study, we sought to identify the performance of prevailing phishing email detection systems (PEDS) against character, word, sentence, and multi-level adversarial text perturbations. Through a principled benchmarking framework, we quantitatively demonstrated the lack of robustness prevailing PEDS have to specific types of text-based adversarial perturbations (e.g., character, word, sentence, multi-level). The results of this study provide new insights into the robustness of AI-based PEDS and highlight the need for organizations to adopt a multi-layered approach to phishing protection. Additionally, organizations can implement our benchmark framework to test their PEDS against adversarial perturbations.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/11/
Navigating the Wave of Cybersecurity Regulations: A Systematic Analysis of Emerging Regulatory Developments,"As the volume and publicity about cyber-attacks continue to increase, new cybersecurity regulations are being enacted or proposed by governments around the world. But will they be beneficial or harmful in improving cybersecurity? This paper provides an overview of the recent regulatory developments and trends in cybersecurity, with a focus on those in the US and Europe. Specifically, this paper presents a categorization of the main regulatory areas that are currently being discussed. We then identified patterns, gaps, and areas of improvement in the current regulatory environment. Our findings provide valuable direction to organizations navigating the flood of new cybersecurity regulations and the governments enacting these regulations.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/12/
Are Information Security Practices Driven by the Likelihood and potential Impact of Security Events?,"Organizations have opened up various digital interfaces through which customers gain access to online services. Digital interaction between the organization and its customers increases security risks. This study examines how the risk profile of customers defined by their perception of the likelihood and potential impact of a security incident is related to their information security behaviors. We collected data in the context of online banking services to empirically evaluate this research question. The results showed that individuals with low likelihood and high impact perceptions of security incidents had authentication and device security practices significantly higher than that of other groups. Surprisingly, there was no difference in security practices across groups with high likelihood/high impact and low likelihood/low impact perceptions of security incidents. Implications of these results are discussed along with ways we plan to extend this research study.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/13/
Root Causes of Healthcare Data Breaches – A Text Analytic Approach,"What might be the root cause of the breaches in healthcare industry? Is it human negligence or technical? If it is human or insider, then what types of insider threats are there? How can these threats be detected? These are the questions this emergent research is going to address. A literature search returned very few empirical research in this area. This research analyzes the textual data as reported by the entities and examines the root causes of the data breaches. Specifically, we found nearly 40% of breaches were due to insiders. We are interested in characteristics of these insider threats since the breaches are based inside the technical perimeter of the hospital. While most research is based on numerical data, this research analyzes the textual data of breach reports. Preliminary results indicate there are different types of insider threats and awareness training has poor effectiveness.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/14/
Value-Driven Policy Agenda to Manage Online Privacy of Young Adults,"This paper discusses the challenges associated with managing online privacy of young adults and how a policy can be developed. The paper argues that online privacy for GenZ is important to protect. It also argues that protection can be ensured if we understand and know what privacy-related values behold GenZ and hence define their objectives accordingly. Information privacy is defined as a process by which one can have freedom from unauthorized intrusion hence resulting in seclusion. In a final synthesis, 22 main objectives are identified – 6 fundamental objectives and 16 means objectives. Collectively the objectives help in developing online privacy policies for young adults.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/15/
A Qualitative Study on Acceptance Factors of Economic Approaches on IT Security Investment Decisions,"Cyber-attacks can cause severe economic damages to businesses today. Therefore, every company needs to protect its IT systems from such attacks by implementing effective IT security measures. When it comes to deciding on an IT security measure, there is a variety of decision approaches that can offer support on choosing an economically reasonable one. In practice, however, such approaches are rarely applied and a company’s security investment decisions are often based on intuitive factors. We therefore want to explore which factors drive the acceptance and application of economic approaches on IT security investment decisions. For this, we used semi-structured interviews and a moderated focus group to generate insights from both science and practice. Not only did our research yield new scientific knowledge about IT security investment decisions, but the results can also be useful in increasing the application of economic decision approaches.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/16/
Vulnerability-based Cyber-Risk Management: A Text-mining Approach,"Vulnerable information systems inside an organization make it prone to cyber-attacks, leading to loss of reputation, financial loss, customer churn, and loss of future prospects. In our study, we assess, quantify, and mitigate the cyber-attack risk generated due to the vulnerable information technology assets using our proposed Vulnerability-based Cyber-Risk Management Model (VCRMM). We leverage Protection Motivation Theory and cyber-kill chain to assess the cyber risk based on specific characteristics of vulnerabilities. We perform text mining using the topic modelling technique, Latent Dirichlet Allocation, find a correlation between the topics, and then classify the severity rating of vulnerabilities. The higher the severity rating of any vulnerability, the greater the probability of cyber-attack (p) any organization faces. Next, we quantify the cyber-attack risk in terms of expected losses. Finally, based on Rational Choice Theory and NIST-guided Vulnerability Management Process, we propose mitigation strategies to reduce, accept, or transfer the cyber-attack risk.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/17/
"A Social Engineering Research Partnership in Higher Education to Improve Information Security Education, Training, and Awareness (SETA) Programs","The education sector continues to be challenged by phishing attacks. A simulation study of phishing response behavior across four industries found that the education sector had the highest number of employees opening and clicking a phishing email. Understanding the factors contributing to the high phishing susceptibility within the education sector would help develop more comprehensive information security education, training, and awareness (SETA) programs. This would allow universities to become better protected from attempts at using social engineering to gain access to sensitive personal and university data. However, despite the increased levels of phishing attacks within educational institutions, limited research has systematically examined how conditions unique to the sector can influence user engagement and phishing susceptibility. Against this backdrop, our research collaborates with the IT department of a large public university to examine how certain attributes within educational institutions influence phishing susceptibility.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/18/
The Effect of Neutralization: Before and After Employee Violating ISP,"Researchers have adopted neutralization techniques to explain why individuals conduct norm-violating behaviors, such as violating information security policy (ISP). Even though those techniques may be used before and after taking action, more past studies focused on the pre-action stage and attempted to explore the impact of the technique utilization on behavioral intention. In this study, we followed Kaptein and van Helvoort (2019) work and classified those techniques into two types: denying personal responsibilities and denying deviant behavior. In addition, we attempted to extend the application of neutralization techniques from pre-action to post-action. We argue that while denying deviant behavior is more important in the pre-action stage, denying personal responsibility is more critical in the post-action stage. By doing so, we contribute to neutralization studies by including the post-action stage and highlighting the relative importance of different techniques in different stages.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/19/
Identifying Practical Challenges in the Implementation of Technical Measures for Data Privacy Compliance,"Modern privacy regulations provide a strict mandate for data processing entities to implement appropriate technical measures to demonstrate compliance. In practice, determining what measures are indeed “appropriate” is not trivial, particularly in light of vague guidelines provided by privacy regulations. To exacerbate the issue, challenges arise not only in the implementation of the technical measures themselves, but also in a variety of factors involving the roles, processes, decisions, and culture surrounding the pursuit of privacy compliance. In this paper, we present 33 challenges faced in the implementation of technical measures for privacy compliance, derived from a qualitative analysis of 16 interviews with privacy professionals. In addition, we evaluate the interview findings in a survey study, which gives way to a discussion of the identified challenges and their implications.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/2/
Exploring Post-Quantum Cryptographic Schemes for TLS in 5G Nb-IoT: Feasibility and Recommendations,"Narrowband Internet of Things (NB-IoT) is a wireless communication technology that enables a wide range of applications, from smart cities to industrial automation. As a part of the 5G extension, NB-IoT promises to connect billions of devices with low-power and low-cost requirements. However, with the advent of quantum computers, the incoming NB-IoT era is already under threat by these devices, which might break the conventional cryptographic algorithms that can be adapted to secure NB-IoT devices on large scale. In this context, we investigate the feasibility of using post-quantum key exchange and signature algorithms for securing NB-IoT applications. We develop a realistic ns-3 environment to represent the characteristics of NB-IoT networks and analyze the usage of post-quantum algorithms to secure communication. Our findings suggest that using NIST-selected post-quantum key-exchange protocol Kyber does not introduce significant overhead, but post-quantum signature schemes can result in impractical latency times and lower throughputs.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/20/
Security Networks as an Effective Means to Reduce Spear Phishing Susceptibility,"Over the past few years, cybersecurity has gained importance for organizations as social engineering attacks in general, and in particular phishing attacks evolved. Previous phishing research focused on improving the susceptibility of individuals towards phishing attacks, while collective security behavior has been neglected. In this emergent research forum paper, we aim to contribute to the existing phishing literature by extending the scope from the individual towards the collective and the effect of security networks on phishing susceptibility. To observe the collective behavior, we performed a field experiment with three teams and targeted them with spear phishing messages on an online social network. The work plans to evaluate the effect of trust, interdependence, connectivity, relational strength, position within a network and the impact within a network on spear phishing susceptibility. Thus, our work intends to show whether security networks can improve the collective behavior when being targeted on online social networks.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/21/
Phishing Susceptibility – a Cognitive Dissonance Persuasion View,"Phishing remains the most commonly employed technique for executing cybercrime activity. At its core, phishing relies on persuasive techniques that exploit human vulnerabilities. Yet, the current knowledge and understanding of how people respond to persuasiveness in phishing are scarce. Looking through the lens of cognitive dissonance theory, this research proposes a five-step theoretical framework and derives an initial psychometric model to examine and compare the six persuasion techniques on phishing susceptibility. We argue that the cognitive dissonance generated by persuasive techniques influences phishing susceptibility. We also argue for the mediating mechanism of preference for cognitive consistency and mindful attention awareness. This research contributes to understanding human vulnerabilities to phishing by introducing a general sequential model. The model permits the manipulation and testing of different contextual and individual attributes’ constructs, provides flexibility to the whole and part assessment, and allows building and expanding knowledge about the persuasive effect of phishing.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/22/
Cybersecurity Awareness and Adaptive Behavior: Does Prior Exposure Lead to Adaptive Behavior?,"We adopt and use technology almost as fast as it comes into existence, however, our mental models and intuitions advance at a much slower pace to the vulnerabilities and threat that come with new technologies. This creates an opportunity for perpetrators to take advantage of the cyber citizens. It is common knowledge that human beings will remain as the ultimate defense (or the final firewall) of cyber assets, and yet they are most vulnerable to a variety of cyber-attacks. Considering these assertions, this research aims to explicate whether knowledge and experience build the necessary awareness that leads to action. The proposed model expands on the Knowledge-Attitude-Behavior model to explore the role of prior cybersecurity training, incidents, awareness, and attitude towards cybersecurity education in individuals' adaptive responses/behaviors given the cybersecurity risks. The findings could inform the academic community and policymakers on creating effective cybersecurity awareness programs to promote cyber hygiene in today's digital world.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/23/
The Interplay of InfoSec Mindfulness and Sanctions on Extra Role Security Behaviors: A Trait Activation Perspective,"Organizational information security performance is inextricably tied to the security behavior of employees. While employee behavior has long been considered a challenge to information security, recent shifts to a more remote work force have amplified organizational focus on employees to protect information assets. Under these conditions, InfoSec Extra Role Behaviors (ERBs) have become increasingly important to mitigating security threats. The literature suggests that mindfulness is an important human factor that may lead to InfoSec ERBs. We examine mindfulness through the lens of trait activation theory. Trait activation theory (TAT) explicitly ties individual traits to situational features of the environment to predict trait related behavior. In TAT, a relevant feature of the organizational environment is situation strength which constrains behavioral variance through trait expression. Thus, we operationalize sanctions as a measure for situation strength in the InfoSec context and propose a model that examines the interplay between a relevant human factor, mindfulness, and a common organizational intervention, sanctions, and the impact that interplay has on InfoSec ERB enactment.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/24/
Using a Nudge to Close the Intention–Behavior Gap for Information Privacy,"The acceptance and use of information technology (IT) and information systems (IS) are two of the most critical topics in IT and IS research. In these fields, intention has usually functioned as a direct and immediate predictor of actual behavior according to most theories and models of behavioral change. However, there exists a considerable gap between the two, particularly in the area of privacy protection. Despite the intent to protect their privacy, many users continue to disclose highly personal information and accept intrusive privacy policies without using privacy-protection tools. This research aims to bridge the gap by investigating the efficacy of nudging interventions in lowering cognitive load and helping users in carrying out their intentions to protect their online privacy. By utilizing persuasive technology and the PSD framework, we designed and implemented nudges to overcome cognitive limitations for privacy protection. Our study is expected to make a significant contribution to the field of privacy and information technology/information systems by providing a potential solution to the privacy paradox issue, extending existing intention-oriented behavioral models, and emphasizing the importance of actual behavior. Furthermore, our findings should aid in users’ safeguarding of their privacy, designers’ development of privacy-protection tools, and policymakers’ improvement of general privacy-protection systems.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/25/
Using stakes consciousness to better understand information security awareness,"Several studies have identified employees within organizations as the weakest link in information security. Although many measures are in place to stimulate employee interest in cyber security, the challenge seems not always at the organizational level but sometimes at the employee behavioral level. In this study, we develop a construct named stakes consciousness to provide a better understanding of employee information security awareness within organizations. We focus on employees' origins and environments to explain their risk averseness and understand their information security awareness. We use the characteristics of organizations, such as the firm sector, sector density, and network centrality, to explain the importance of employees' information security awareness. Finally, we provide an overview of the application of this construct in technological, organizational, and individual contexts.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/26/
How Does Technological Entitlement Affect Cyberdeviance? The Role of Organizational Justice.,"The widespread use of technology in the workplace has led to an increase in cyberdeviance, where employees deliberately violate organizational norms when using IT resources. Drawing on literature on technological entitlement and organizational justice, this study explores the relationship between cyberdeviance, technological entitlement, and organizational justice. We propose that technological entitlement is a personality trait of employees who are more likely to engage in cyberdeviance and use organizational justice as a justification for their behavior. This paper contributes to recent studies on technological entitlement and has implications for organizations seeking to mitigate cyberdeviance.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/27/
Workarounds from an Information Security Perspective: Literature Review,"The security of every organization is critical and should be proactively planned. The research field of security, workplace behavior, security safeguards, workarounds, and cybersecurity explores how organizations can effectively strategize and use adaptive security plans to minimize the security vulnerability of organizations. These solutions rely heavily on the technology adopted by organizations that seek to align with policies that safeguard their information assets consciously. However, despite several works in the literature on workarounds, the concept of workarounds that poses a security risk in an organization is still riddled with ambiguities. This review covers current research involving 28 articles across the Association for Information Systems (AIS) basket of eight conferences and other IS security-related journals. Our results identify four major categories that reveal the intersection of workarounds and security in the workplace from recent literature. We further discuss their implications and propose avenues for future research.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/28/
Digital Forensics Use-Case of Blockchain Technology: A Review,"Digital forensics is an ever-growing field of science that is heavily involved in various fields. However, because of the expanding complexity of computer science, many new problems have arisen for digital forensics such as data scalability and cloud computing forensics data privacy, data traceability, integrity, and chain of custody. Blockchain can be useful for digital forensics to ensure security, like Proof-of-Work and Proof-of-Stake. Proof-of-Work verifies that one node has done adequate computational work to other nodes on the network. Proof-of-Stake is a similar consensus mechanism but relates more to cryptocurrency. In literature, there are several attempts to synthesize the blockchain’s digital forensics capabilities from a technical standpoint. After analyzing these review papers, we identify there is still a lack of classifying the solutions following an artifact classification taxonomy, understanding the broader impact of technical solutions following theories such as socio-technical theory, and summarizing the research contributions towards developing a design theory. The objective of this paper, thus, is to develop research propositions regarding the digital forensics use-case of blockchain following artifact taxonomy, socio-technical theory, and design theory.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/29/
Risk Factors for Malicious Insider Threats – An Analysis of Attack Scenarios,"Regarding malicious (intentional) insider threats, the damage potential is particularly high. There are many suggestions for prevention and defense, but there is a lack of reliable knowledge about which measures actually are effective in regard to this kind of attacks. Using the serious game ""Operation Digital Butterfly"", 40 roles and attack scenarios as well as possible countermeasures were collected in 10 game sessions with participants from research institutions, companies and public authorities. These are the starting point for an analysis of effective risk factors at the levels of technology, organization, people and infrastructure, which is presented in this paper.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/3/
"A Process-Based Approach to Information Security Investment Evaluation: Design, Implementation, and Evaluation","In recent years, the importance of information security has grown significantly due to the rise of cyber threats and attacks. However, evaluating investments in information security can be challenging, as traditional methods often rely solely on monetary factors and fail to capture the dynamic nature of business processes. This paper introduces a novel process-based evaluation method for assessing the effect of investments in information security on business processes. The paper outlines practical design requirements for the method and its instantiation as a prototype, which is then evaluated using a three-step approach with two companies from the healthcare and energy sectors. The evaluation results demonstrate the proposed method's usefulness in information security investment decisions. This paper contributes to the field of information security investment evaluation by providing a proof-of-concept that potentially paves the way for future research to increase the quality and economics of investments in information security.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/30/
Predicting Analysts' Needs for Explainable Artificial Intelligence (XAI) in Cybersecurity Analysis,"As cyberattacks become more sophisticated and prevalent, analysts face several new challenges, such as a data influx and a high percentage of false alerts. Many AI-driven tools have been utilized in detection systems to help analysts find anomalies. However, the black-box nature of many AI models makes it difficult for analysts to utilize them effectively. This leads to a growing need for Explainable AI (XAI) to make AI models more transparent. However, there is a lack of personalized XAI that would enable analysts to receive explanations tailored to their needs. To address this problem, we develop a system that can predict an analyst’s need for explainability based on a Bayesian Network (BN). We first identify the factors that impact analysts' needs for explainability and then use a data-driven method to build the network structure. The performance of the system will be evaluated in an experiment involving twenty participants.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/4/
Re-Assessing Privacy in the Blockchain-based Metaverse,"The metaverse refers to merging the physical and virtual environments, providing new opportunities for transferring real-world identities into the digital space. Mapping identities into the metaverse is done using blockchain tokens. Therefore, we define the set of metaverse services associated with a blockchain as “blockchain-based metaverse” (BM). While blockchain applications have long been considered more privacy-preserving than centralized applications, we argue that the underlying premises of this assumption have changed for the BM context. Since blockchain transactions are pseudonym-based, it is generally possible to link them to real-world identities. This probability has increased in the BM, making inferences about identities more likely, which completely changes the privacy situation. In our paper, we conceptually re-assess the privacy assumption of blockchains and suggest four propositions to demonstrate how the privacy level changes in the BM. We also provide technical and organizational measures to address the privacy shortcomings and present a research agenda.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/5/
Detecting Privacy Threats with Machine Learning: A Design Framework for Identifying Side-Channel Risks of Illegitimate User Profiling,"Privacy leakage has become prevalent and severe with the increasing adoption of the internet of things (IoT), artificial intelligence (AI), and blockchain technologies. Such data-intensive systems are vulnerable to side-channel attacks in which hackers can extract sensitive information from a digital device without actively manipulating the target system. Nevertheless, there is a scarcity of IS research on how businesses can effectively detect and safeguard against side-channel attacks. This study adopts the design science paradigm and lays the groundwork for systematic inquiry into the assessment of privacy risks related to side-channels. In this paper, we a) highlight the privacy threats posed by side-channel attacks, b) propose a machine learning-driven design framework to identify side-channel privacy risks, and c) contribute to the literature on privacy analytics using machine learning techniques. We demonstrate a use case of the proposed framework with a text classification model that uses keystroke timings as side-channel.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/7/
Roles of Peer Support and Perceived Organizational Support in Reducing Nonmalicious Information Security Violations: The Interacting Effect of Personal Goal Setting,"Nonmalicious information security violations (NISV) that employees engage in can pose a problem to organizations. The Social Cognitive Theory was used to investigate the roles of three constructs, (a) peer support, (b) perceived organizational support, and (c) personal goal setting in reducing NISV. The study also examined the interactions among the three variables. Relevant hypotheses were formulated, and survey data were obtained from 204 German workers. The partial least squares technique was used for data analysis. The results showed that the three factors could reduce employee engagement in NISV. Another key finding of the study indicates that the interaction between personal goal setting in reducing engagement in NISV and peer support significantly reduced employee participation in NISV. In contrast, the interaction between perceived organizational support and personal goal setting related to NISV avoidance did not. The implications of the study’s findings for practice and contributions to research were discussed.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/8/
Revisiting the Privacy Paradox: A Novel Framework for Privacy Calculus,"This paper aims to address the gaps in the existing literature on the privacy paradox and privacy calculus by exploring the role of contextual and situational factors, such as the specific circumstances in which privacy decisions are made, and the influence of individual moral value differences across diverse populations. To achieve this, we present three research questions that aim to shed light on how users' privacy values influence paradoxical behavior and decision-making in online contexts. Our research is expected to contribute to a more comprehensive understanding of the privacy calculus framework and provide insight into how individuals' privacy values and situational factors shape their privacy-related decision-making. Ultimately, our research aims to improve individual's ability to protect their personal information in online contexts, by developing effective privacy policies and interventions that are sensitive to cultural differences and values.",IS,https://aisel.aisnet.org/amcis2023/sig_sec/sig_sec/9/
Conception and Requirements Identification of Gaia-X-Based Service Offerings,"Gaia-X is an initiative to develop the next generation of a secure and federated European data infrastructure to promote digital sovereignty for data exchange to fuel innovations. This paper introduces the basics of Gaia-X, in particular the mobility domain, followed by the federated system and its standards. In addition, a research methodology is presented to help conceptualize and derive requirements for service offerings on a Gaia-X-based data space. This is elaborated with a use case from the project “GAIA-X4 AMS” (Gaia-X4-Advanced Mobility Services), which reflects implementation in Gaia-X ecosystems and its added value.",IS,https://aisel.aisnet.org/amcis2023/sig_services/sig_services/1/
Design Knowledge for GAIA-X-compliant Ecosystems: A Literature Review,"Integrating digital technologies offer companies a range of potentials for optimizing business processes, exploring new business models and collaborating along value chains to co-create value. However, this integration increases complexity, especially within SMEs that lack extensive resources to drive data transformation. GAIA-X offers potential solutions in form of federated services as a low-threshold way to participate in federated data ecosystems. Thus, in this paper we survey the current state of research on GAIA-X by means of a literature review extracting and collecting design knowledge as well as to prepare further implementation within the scope of our research project. Therefore, we build a concept matrix in which we differentiate the identified body of knowledge by three concepts with 18 characteristic expressions in total. Our analysis of the identified papers highlights architectural approaches for designing a GAIA-X compatible data ecosystem, augmented by additional factors to consider when designing these ecosystems.",IS,https://aisel.aisnet.org/amcis2023/sig_services/sig_services/2/
Designing Emerging Social Determinants of Health Apps for Novice Digital Health Users,"This paper investigates the design considerations for health information technologies (HITs) that are aimed at supporting lower socioeconomic status (SES) individuals in addressing social determinants of health (SDOH) needs. While emerging technologies offer new opportunities for improving quality of life outcomes, many lower SES individuals may be novice users of digital health technologies, including SDOH technologies. To address this gap, we conducted qualitative research using user-centered design tools during the development of an app designed to help individuals address SDOH needs. Our findings provide insight into the unique characteristics, needs, and preferences of lower SES digital novices, and highlight multiple aspects of their eHealth literacy. We discuss the implications of these findings for designing and implementing HITs for novice users.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/1/
Gratitude with Expectations: Exploring the Role of Restaurant Delivery and Food Availability on Online Customer Reviews During the COVID-19 Pandemic,"Online reviews have become a popular tool for understanding consumers' perceptions of service and product quality. The COVID-19 pandemic has significantly impacted the restaurant industry, including negative impacts on profitability. This study utilizes gratitude theory and expectation confirmation theory to investigate how food delivery and food desert status moderated the impact of the pandemic on consumers' perceptions of restaurant quality. We find that consumers still held expectations for restaurants despite the COVID-19 pandemic crisis and their lack of food availability. Our findings also highlight the importance of food accessibility and availability, particularly in underprivileged communities during a crisis. This study has broader implications for businesses operating during crisis events providing insights into how they can improve their service and product quality to meet the needs of their consumers.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/10/
Controversies' Roots in Digital Business Models-The Case of Academic Publishing,"Digital business models (DBMs) are built on digital technologies with complexity-inducing characteristics. Thus, they form ever-more complex (eco)systems as they connect heterogeneous actors for value co-creation and blur the boundaries of organizations, markets, and industries. The co-existence of actors’ multiple logics in pluralistic settings has been associated with tensions and controversies. However, information systems (IS) research has ignored controversies’ role in DBMs in settings of logic multiplicity. Drawing on the ‘orders of worth’ framework, we follow the controversies over DBMs in the academic publishing ecosystem. Building on our in-depth case study data, we develop a model highlighting three roots of controversies in sociotechnical DBMs. We add to the literature by furthering our understanding of controversies’ role in DBMs and enriching the community’s methodological toolbox in applying a theory from French pragmatic sociology to study pluralistic contexts.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/2/
Women Mentoring Programs to Reduce the Gender Gap in IT Professions: A Literature Review and Critical Reflection,"Technical professions and industries remain heavily male-dominated. To counteract this imbalance, measures are being taken, including mentoring programs for women. In this study, a literature review is conducted to answer the research question of how women's mentoring programs need to be designed in order to contribute to the reduction of the gender gap. The results of 13 empirical studies from 2013 to 2022 are analyzed and 21 factors influencing the design of women's mentoring programs are identified and grouped under three headings (relationship aspects, content-related aspects, and organizational aspects). On this basis, an exemplary women's mentoring program is discussed, the core of which consists of a hybrid e-mentoring program. While mentoring programs are considered helpful at the individual level, closing the gender gap at the structural level definitely needs more focus from both research and practice.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/3/
Designing Culturally Appropriate Hackathons to Increase Data Literacy in Indigenous Communities,"Data literacy skills are extremely important, yet it is still lacking in many Indigenous communities across Canada due to systemic barriers faced by Indigenous peoples. Hackathons have been used as a common approach to promote data literacy. However, hackathons are socio-technical innovation processes in which local cultures and values play a role in their conduct and outcomes. Hackathons that are designed with Indigenous cultures and values in mind can be more effective at promoting data literacy for Indigenous peoples, yet there is limited guidance on how to do so. In this paper, we adopt the Action Design Research approach and Indigenous research methods to collaboratively develop and deliver culturally appropriate hackathons to increase data literacy within Indigenous communities. During this process we will also co-develop a suite of design principles to guide the design of these hackathons.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/4/
Impact of social influence on continuous use of mobile game applications,"Mobile games dominate the mobile application market in terms of the share of applications and total revenue. Organizations and developers of mobile game applications are challenged to identify factors that influence users’ intentions and continuous use of the games. From this perspective, we draw on social identity theory (SIT) and plan to identify the moderating effect of social influence on the relationship between user satisfaction and continuous use of mobile game applications. This study also aims to investigate the direct impact of social influence on continuous use of mobile game applications. We propose that social influence (compliance, internalization, and identification) will positively moderate the transition from initial adoption to continuous use. We will conduct an online survey in two phases to test the research model and hypotheses.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/5/
A Longitudinal Examination of AI Fairness on Online Labor Markets,"While online labor markets (OLMs) provide many benefits including flexibility and data-driven AI matching systems, gender and other social biases have been shown in OLMs, and research demonstrates AI can also perpetuate bias. However, previous OLM research assumes bias is static over time and independent of the AI algorithm. To help design OLMs that minimize the detrimental impact of biases on marginalized social groups, we investigate the interaction among individual characteristics and AI sources of biases over the long-term and evaluate auditing strategies using an agent-based simulation model. We then plan to develop and empirically test a framework to evaluate AI fairness and the interaction of different biases on OLMs and test an audit strategy to mitigate biases. We plan to extend the literature on OLMs by integrating fairness and intersectionality research to evaluate the impact of biases.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/6/
Examining Underrepresented Communities’ Intention Towards Digital Entrepreneurship: A Dual-Theory Framework,"Digital entrepreneurship has been touted as accessible with promises of opportunities for all and has the potential to create a more inclusive entrepreneurial environment because, compared to traditional entrepreneurship, it can lower the barriers to start a new venture. Yet, there are concerns that the underrepresented communities face the challenge in engaging in digital entrepreneurship due to lack of technology access, usage, and skills. This paper proposes a conceptual framework based on a dual-theory perspective from Theory of Planned Behavior (TPB) and Technology Acceptance Model (TAM), to examine factors affecting the underrepresented communities’ intentions toward digital entrepreneurship. We plan to conduct a survey study at Historical Black Colleges and Universities (HBCUs), where the students’ population are representative of the underrepresented communities, to identify what are the key factors that influence the intentions of this sector towards selecting digital entrepreneurship as a career option.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/7/
Native American Rural Community Digital Divide: Student Insights,"The digital divide continues to be an issue for many Native American individuals in rural tribal areas. This research used a qualitative grounded theory method from the data collection of semi-structured interviews with Native American university students. The open coding of the transcribed responses was used to analyze the text data from individual Native American experiences. The data analysis codes included cost, location, access, digital literacy, and technology knowledge as continuing issues. The coding also shows limited technical support or training availability in the communities. The absence of technology use increases the need to understand factors that remain digital divide barriers for Native American communities. The digital divide - individual experiences model (DD-IEM) is based on three main categories: community, education, and home environments. Six propositions produced the DD-IEM that encompasses digital environments within the three settings that are unique to each individual.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/8/
ICT for Good: Digital Accessibility in Local Government,"This paper describes a case study of a local government in the United States that is in the process of developing an inclusive and accessible ICT infrastructure for the delivery of e-government resources and services to its citizens with specific attention paid to people with disabilities. Developing such an inclusive infrastructure has proven to be difficult for many municipalities. Attempts to comply with the legal requirements to improve the possibilities for civic engagement for people with disabilities have met with mixed results. The research reported here focuses on a municipality engaged in a complex, user-driven initiative intended to transition its existing ICT infrastructure to one that is more inclusive and accessible to all.",IS,https://aisel.aisnet.org/amcis2023/soc_inclusion/social_inclusion/9/
Exploring the Interactions between Social Movement Activists and Target Organizations on Social Media,"Social movements are recognized as significant agents of change in modern societies. Their influence is increasingly channeled through social media platforms to mobilize support and direct collective action toward targeted organizations. However, targeted organizations also resort to social media channels to counter the actions of social movements that are directed at them. There is little research on organizations’ responses to movement's actions on social media. This study endeavors to explore the dynamics of interactions between social movement activists and their target organizations by utilizing narrative network analysis. Drawing on framing theory, this research endeavors to elucidate how social movement actors employ various frames. Specifically, we examine the Twitter accounts of various Canadian banks and activists targeting them for their investments in fossil fuels. We analyze the narrative networks and counter-networks to develop a theoretical understanding of how the social media interactions between activists and target organizations evolve over time.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/1/
Psychological factors affecting social media usage: A U&G theory perspective,"The COVID-19 pandemic has led to a significant increase in social media usage, raising concerns about its potential impact on mental health. The pandemic has created unique stressors and challenges that have worsened the mental health conditions of individuals due to prolonged social media usage. Hence, this study explores the psychological factors affecting social media usage post-pandemic. A mixed-method approach was utilized grounded on U&G theory to identify fear of missing out, peer pressure, self-esteem, loneliness, social comparison, and habit as factors affecting social media usage. The study found that those with higher levels of peer pressure, social comparison, habit and fear of missing out (FOMO) tend to use social media more frequently, suggesting its use as a coping mechanism. The study emphasizes the need for continued investigation to understand the complex relationship between social media usage and mental health post-pandemic.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/10/
Exploring Overall Service Quality and Customer Contribution in Online-to-Offline Commerce,"Customer contribution is the most important in the company’s profit. However, due to difficulties in data collection, little research has investigated the antecedents and measurement of customer contribution. This study mainly explores the effect of overall service quality and cloud trust on consumer contribution to enhancing online customers’ actual purchasing behaviors. Through literature review and expert interview, we identify five factors of overall service quality: website design, fulfillment, customer service, security/privacy, and third-party payment. The research model treats cloud trust as a second-order reflective construct driven by perceived warranty, perceived competence, consumer confidence, source credibility, and diagnosticity. The hypotheses proposed by the study help develop the best customer value management strategy in the electronic commerce industry.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/11/
Disasters Information-Seeking Behavior via Social Networking Sites,"Disasters bring uncertainties and substantial health, economic, social, and psychological challenges. Effective management of disasters requires a thorough understanding of their causes, nature, and consequences. Affected individuals may, therefore, seek instant, trustworthy information from different sources, including social networking sites (SNSs). This study investigates what leads individuals to seek disaster information on SNSs. The investigated potential drivers are source credibility (SC), argument quality (AQ), self-efficacy (SEF), perceived usefulness of information (OUI), and behavioral intention (INT). The results confirm the newly emerged source & argument quality (SAQ) construct, SEF, and PUI are significant antecedents of INT, which in turn influences disaster information-seeking behavior (ISB) via SNSs.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/12/
The Influence of Professional Embeddedness and Public Reputation on Critic Review Behavior,"As one popular form of online word-of-mouth, critic reviews are provided by experts as a quality signal. In this research-in-progress, we consider two forms of social features of critics’ reviewing environment and their impact on critics’ reviews: critics’ professional reputations based on how embedded they are in their professional networks, and critics’ public reputations enabled by reviewing platforms. We collected a dataset of 68,450 critic reviews posted by 716 critics for 3,654 movies from Rotten Tomatoes and constructed these critics’ professional networks based on the publications they work for. We used social network analysis techniques to obtain each critic’s embeddedness in their networks and used their “top critic” status to capture their public reputation. We discussed future analysis techniques and anticipated contributions.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/13/
Insights on Attention Capture Dark Patterns in Social Networking Sites,"Over the last decade, organizations have almost doubled the users' average daily time spent on social networking sites. A potential driver is the incorporation of attention capture dark patterns in social networking sites, which are designed to maximize users' time spent, daily visits, and interactions with social networking sites. However, up to this date, the actual impact of attention capture dark patterns on users is largely unexplored. Our research aims to bridge this gap by examining the impact of attention capture dark patterns on users through a full factorial between-subjects vignette study. We expect our final research to contribute to dark patterns research by generating insights into how users respond to attention capture dark patterns. This enables us to derive practical implications for social networking sites' interface and interaction design.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/14/
Online Sex Trafficking Policy Research: A Systematic Review,"Online sex trafficking cases have been increasing every day. The field of Information Systems (IS) has not significantly explored research on online sex trafficking specifically to evaluate existing literature and provide future research directions. In this paper, we use Latent Semantic Indexing (LSI) to review the literature on online sex trafficking. Our focus is on policies related to online sex trafficking. We then summarize our findings and suggest a future research agenda.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/15/
Immediate and Long-term Effects of Live Streaming Selling on Product Sales,"Live streaming commerce has gained considerable traction in recent years, which transforms the traditional online shopping experience by enabling real-time interactions between streamers and customers with the leverage of live streaming technology. While existing studies have suggested that live streaming selling could enhance sales by increasing customers’ purchase intentions, scant research has confirmed this effect with large-scale empirical sales data. Additionally, extant literature failed to distinguish between the immediate and long-term impact of live steaming on product sales. Based on the data collected from Amazon Live and Amazon.com, this study examines the immediate and long-term influence of live streaming on product sales. By using discontinuous growth modeling, the preliminary results show that product sales will increase immediately after live streaming selling, but will have a declining trend in a long run, strongly supporting our hypotheses. These findings can provide significant theoretical and practical implications for live streaming commerce operation.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/16/
How negative emotions spread on social media: the case of celebrity suicides,"Social Media can spread messages, emotions, and behaviors among large audiences. Particularly, the consequences of emotion propagation become alarming when negative and shocking events like celebrity suicides happen and influence many vulnerable users. Analyzing social media discussions enables us to understand the mechanisms by which negative emotions spread, and also design effective health interventions. Here we investigate the suicide events of four celebrities and the subsequent Twitter discussions that appeared in the form of cascades – chains of retweets. By using a state-of-the-art BERT-based language model to identify emotion scores, we find that sadness and fear are the leading emotions expressed in each event and that the speed, size, and lifetime of dialogues vary depending on their emotional composition. Further analysis aims to provide new theoretical explanations.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/17/
To Skimp or Step in: Does Timely Service Recovery via Social Media Win Customers’ Hearts?,"Due to the public nature of the interactions and the ability to foster quicker responses than traditional media, social media has become a powerful channel for both travelers to post their trip-related questions and issues, and airlines to build their brand images. Based on the Signaling theory and Expectations-Confirmation theory, we propose that airline service recovery via Twitter will boost profitability, and such influence is more vital for network airlines than focused airlines and airlines with higher corporate social responsibility (CSR).",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/18/
Fake Review Detection - The Value of Domain-Specificity,"The issue of fake reviews, a perennial challenge faced by e-commerce companies, has been worsened in recent years. In this paper, we investigate the trade-off between the application of general-purpose and domain-specific model by selecting training data using transformer-based models to identify fake reviews. Therefore, we compare two scenarios using different set ups of data selection. First, a general-purpose model was identified and applied on specific domains. Afterwards, domain-specific models were trained and tested. Then, the results from these scenarios were compared, yielding the conclusion that models trained on data from a similar product category outperform general-purpose models in classification performance up to 21% (on average by 4%). Our findings send an important message to e-commerce companies to rethink their strategy on training general-purpose models to identify fake reviews to create several, more domain-specific models on that task according to present data domains.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/2/
Values of Social Commerce: Influence of Affordance,"Social commerce is growing. This paper aims to understand how users interact on social commerce platforms. The design of the social commerce platform is unique, giving both buyers and sellers autonomy. The paper seeks to understand the consumer values of both buyers and sellers, how those have led to the social commerce platform design, and what feature enabled affordances help to satisfy the consumers' values. The study will help system designers better design social commerce platforms and understand some of the consumer pain points. Additionally, in literature, the seller behavior is understudied, so the nature of the platform and the autonomy it provides to the seller allows us to study seller behavior. We aim to understand consumer behavior better, how sellers and buyers interact on a social commerce platform, and how these platforms enable those interactions to facilitate transactions.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/3/
A Chat-based personal recommendation mechanism with opinion intelligence,"With the rapid development of the Internet and mobile devices, users around the world have accumulated a large amount of comment data on the Internet. Nowadays, users tend to search for information on the Internet before making purchase decisions. How to provide personalized services while avoiding information overload has gradually become an important issue. This study proposes a dialogue-based personalized recommendation mechanism with opinion intelligence. In addition to understanding user preferences through dialogue and interaction with users, we further analyze the content of reviews and the social influence of review authors to better help users. The results show that the dialogue mechanism proposed in this study has a good recommendation effect and can effectively improve user experience.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/4/
SNS Guilt Feelings: Causes and Influence on Discontinuance,"Although experiencing guilt feelings after social networking service (SNS) usage has become a prevalent phenomenon among SNS users, these SNS guilt feelings-and especially their causes-have received little scholarly attention. This paper therefore focuses on why SNS users may develop SNS guilt feelings and their impact on discontinuance intention. We utilize the lens of normative dissociation to develop a model to identify causes of SNS guilt feelings and apply a quantitative research approach to test the model. We contribute by demonstrating that two identified causes-perceived time spent and perceived procrastination-significantly influence SNS guilt feelings, while perceived meaningfulness does not. Furthermore, SNS guilt feelings influence discontinuance intention and mediate the relationship between several constructs and discontinuance intention. By employing the perspective of normative dissociation, we reinforce the relevance of SNS guilt feelings.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/5/
Influencing the Influencers? The Case of @CelebJets and the Role of Social Media in Empowering Citizens to Conduct Climate Justice Activism,"Online climate change activism is growing. However, its effectiveness is not yet firmly established. One of the challenges is for citizens to make sense of the vast amount of data to understand the impact of specific behaviors. The aggregation of publicly available information in a comprehensible and playful manner could potentially trigger effective evidence-based activism. However, it is not yet clear if and how such an intervention works. In this paper, we make a preliminary analysis of a use case of such an approach by investigating the @CelebJets phenomena, which automatically tracks and shares the private jet flights of celebrities on Twitter. Our analysis demonstrates that this approach can elicit user engagement and provide pertinent material for the press, potentially leading to a shift in behavior, particularly among celebrities who are promoting sustainability.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/7/
Relationships between Involuntary Visual Cueing and Sensemaking of YouTube Videos,"Sensemaking plays a critical role in the everyday use of social media, which are increasingly visual as video, images, and other visual content permeate. Yet, research on the sensemaking of visual information on social media is minimal. This study addresses this research gap by investigating the relationships between involuntary visual cues (color contrast and visual complexity) and making sense of videos on YouTube and Reddit. It finds that color contrast has a significant association with sensemaking on both platforms and that visual complexity is a significant predictor of sensemaking only on Reddit. Our study adds to the research on sensemaking on social media. It expands social media analysis from textual to visual information and contributes to the knowledge of visual sensemaking on social media. It also provides practitioners with guidelines for designing videos that facilitate visual sensemaking.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/8/
Social Norms and Misinformation Sharing of Politicized Products,"Misinformation has been on the rise, and in many cases, misinformation is intentionally shared for political reasons. This phenomenon first started with political news but now has spread in consumer products. To understand how misinformation sharing occurs differently between politicized versus non-politicized products, we conducted an experiment and surveyed 800 respondents in an online crowdsourcing platform. The results show that for a politicized product, individual bias disposition and social norms indeed sway an individual’s evaluation of information credibility, her trust, and intention to share. However, this effect is not observed for non-politicized products. This calls for more research to understand the role of social norms in stopping the spread of misinformation.",IS,https://aisel.aisnet.org/amcis2023/social_comput/social_comput/9/
Online Video Sharing Platform: Exploring Factors Affecting User Tipping Behavior,"In this study, we examine user tipping behavior on China's leading online video platform, BiliBili.com. We analyze a dataset of 57,767 videos and categorize them into two types: knowledge content and entertainment content. We investigate the differences in user tipping behavior between these content types and also explore the impact of contributor experience and content length on tipping behavior. Additionally, we investigate the moderating effect of content type on these influencing factors. Our findings reveal that compared to entertainment content, knowledge content receives fewer tipping rewards from users. We also find that contributor experience has a negative impact on user tipping behavior, while content length has a positive impact.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/1/
Can Influencers Overcome the Impact of Weak Ties to Achieve Viral Propagation? Applying Social Identification & Social Capital to Predict Virality,"Marketers have struggled with the question of how to achieve virality. In this paper we seek to understand and provide a model that predicts virality of their content. We draw from the social identity theory and social capital theory to argue that in-group/out-group classification plays a greater role in predicting virality than social capital cues like tie strength and number of connections. Furthermore, we use a decision-making approach to understand users' processes as they decide on whether or not to propagate content. Using the Lens Model and Fast and Frugal Decision-Making approach, we propose a 3-step model that integrates both social identification and social capital cues to predict user content propagation behavior. The model expands our understanding of content propagation behavior and virality. The model can be tested through experimental work to further understand the decision-processes of users that lead to virality.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/10/
Privacy is Important! Or not? – Commenting and Liking Under Confirmation Bias on Social Media,"Social media provides great opportunities to engage with content based on commenting or liking. Especially confirmation bias was found to be highly influential in engaging with content. However, social media also imposes privacy risks on its users. Past research found that trust in the platform facilitates, while privacy risk beliefs reduce engagement on social media. However, research did not address how confirmation bias interacts with privacy concepts on social media. To address this research gap, we conducted a scenario-based study and manipulated confirmation bias in the context of COVID-19 vaccination. The results show that privacy risk beliefs are heavily diminished under confirmation bias to the point where trusting beliefs dominate. We provide a novel view of the influence on commenting and liking by entangling another piece of the privacy paradox.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/11/
Referral Intention vs. Continuous Referral Intention: Incentive Mechanism in Multi-tasking Social Referral Programs,"Social referral reward programs (SRRPs) aim to incentivize existing customers to recommend a product or service to others. Based on the reward threshold, we classify social referrals into two categories: single-tasking social referral (SSR) and multi-tasking social referral (MSR). Considering that MSR involves multiple responders, we explore how to design an effective reward mechanism in this new context. Our primary interests in outcomes include users’ willingness to recommend and their continuous referral intentions. Drawing from fairness theory and loss aversion theory, we propose three reward models based on the keeping percentage of rewards obtained: keep-it-all (KIA), discounted keep-it-all (DKIA), and all-or-nothing (AON). We designed an experiment to test the hypotheses regarding the effects of reward types on referral intention and continuous intention. This study will provide important implications for research and practice in designing an effective reward mechanism in MSR.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/12/
5504 Sharing Patterns in the Digital Sharing Economy,"Large-scale sharing networks rely on digitally enabled platforms that connect users beyond the limits of social and geographical proximity. The phenomenon of the Digital Sharing Economy (DSE) embraces a broad diversity of sharing patterns. This paper aims to present an exhaustive classification of these patterns. It illustrates 5504 different sharing models that vary in the attributes of their fundamental business model components, i.e. the type of shareable resources, resource providers, resource receivers and sharing practices. This classification can be useful to characterize the DSE and visualize sharing and exchange patterns within its networks. In particular, the classification of digital sharing models can help platform providers position themselves in the broad spectrum of market and non-market-based sharing and tap into practicable business model innovations in the DSE domain.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/13/
Why Do I Share My Predictions of Stock Returns in Online Communities? An Empirical Study on StockTwits,"Online investment communities have been widely adopted by investors to disclose investment-related information, such as predictions of stock returns. Although it benefits both platforms by attracting more users and other investors by providing additional finely-processed information, it may hurt publishers due to the potential loss of their unique valuable private information. Therefore, understanding why users share their own predictions in online communities becomes an important issue. Drawing on the ability-motivation-opportunity framework, we seek to identify three important factors influencing users’ willingness to share predictions. Utilizing data obtained from StockTwits, our preliminary results show that the number of followers, prediction accuracy, and historical stock performance negatively affect users’ sharing of their predictions of stock returns. Our findings can contribute to the literature on information sharing and provide managerial implications for online investment communities.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/2/
Designing Intelligent Characters Representing Organisations in Digital Worlds,"Advances in technology have led to the creation of virtual worlds where people can interact with intelligent characters representing organizations. As these interactions increase, it's crucial to understand digital character design and its societal and business implications. This research aims to investigate the design elements of digital characters and provide prescriptive knowledge for organizations. Using the design science research paradigm, we'll answer the question of how to design digital characters that best represent a company. We draw on theories from human-computer interaction, marketing and brand design, corporate communications, and character design from industries such as game design and screenwriting.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/3/
Exploring Socio-Technical Factors of Group Engagement in Metaverse Gaming: A Multi-Method Approach,"As an emerging virtual platform, the Metaverse is increasingly applied to various collaboration contexts such as group gaming, team collaboration in work, and online group learning. To investigate users’ group engagement in the Metaverse, this study applies a socio-technical perspective to the context of Metaverse gaming. We propose a theoretical model and test it with a multi-method approach, consisting of structural equation modeling (SEM) and fuzzy-set qualitative comparative analysis (fsQCA). The SEM results show that all the proposed social (social presence and group harmony), individual (demand toward social activities and acceptance toward virtual world), and technological factors (self-representation) positively determine users’ group engagement in Metaverse gaming. Our fsQCA results further reveal the interdependent relationships among those factors (i.e., multiple configurations), which are nuanced under different contexts of their gender and level of prior experience. While the current findings are insightful and useful, they should be further studied through future research.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/4/
"Hello, Avatar: The impact of avatars on individuals’ emotions and behaviors from three congruity perspectives","The rapid growth of social media platforms in the digital age has led to an explosion of the metaverse, injecting new vitality into avatars. Previous research has mainly focused on the application of avatars in virtual environments, with limited research on the impact of avatar interactions on individuals, except for avatar similarity, customization, and the uncanny valley effect. Considering the positive and negative perceptions brought about by avatars to individuals, we adopted a mixed-methods approach to propose a model that reveals the influencing mechanism of individuals’ perceptions of an avatar on their emotions and behaviors from a holistic perspective. This study contributes to existing knowledge and expands dual-congruity theory. Additionally, the findings have practical implications for managers of metaverse platforms and individuals alike.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/5/
Text-image Relationships in Social Media Communication: A Literature Review,"This study synthesizes the literature on content factors in social media communication within the Information Systems (IS) field and identifies five dimensions of text-image congruence: quality congruence, quantity congruence, emotion congruence, connotation congruence, and information appeal. Utilizing the S-O-R framework, the paper provides a comprehensive understanding of text-image congruence and suggests future research directions in IS. The findings contribute to the knowledge of effective social media communication strategies and lay the groundwork for future research on text-image congruence and relationships.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/7/
How Does (In)consistent Performance Feedback Affect Business Model Digitalization？,"Despite the merits of business model (BM) digitalization to sustain organizational competitive advantages, many firms are cautious to digitalize their BMs due to the efforts needed and the high risks accompanying them. To date, the existing understanding of the antecedents of BM digitalization is still underdeveloped. We draw on the behavioral theory of the firm to develop a comprehensive understanding of what motivates firms to promote BM digitalization. In so doing, this study explores how (in)consistency between historical and social performance feedback affects BM digitalization. Moreover, we theorize that board experience with BM digitalization from interlock firms is an important contingency that influences these relationships. Based on panel data from 2,373 Chinese-listed firms from 2008 to 2019, the results provide general support for our arguments. This study contributes to the BM digitalization literature by disentangling the critical roles of (in)consistent performance feedback and board experience.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/8/
The Roles of Opinion Leaders in Elevating Online Health Discussion: Evidence from Reddit,"Social networking sites and online communities are used by billions of people globally. It is important to understand the participation in these collaborative platforms and the roles different participants play. In this study, we examine the roles both local and global opinion leaders play and their influence on the community using content related to Covid mental health from Reddit. We employed social network analysis to identify opinion leaders based on their network position and connections, and we used sentiment analysis and econometric models to examine their impact on online health community participation. Our study reveals that global opinion leaders have more influence on thread participation than local opinion leaders. In addition, negative emotions expressed encourage more participants’ involvement in online health community discussion.",IS,https://aisel.aisnet.org/amcis2023/vcc/vcc/9/
The e-government adoption ecosystem from the perspective of stakeholder theory: A case study on the village information systems in Indonesia,"The village information system (VIS) is a form of e-government that villages in Indonesia have adopted. However, many village administrations still have difficulty implementing it. Gunungkidul Regency, Indonesia, is a regency where all villages have successfully adopted VIS in a sustainable manner. This study aims to portray the ecosystem that the adoption or implementation of VIS in the regency has formed. The researchers carried out data collection using observation techniques, document studies, and interviews with entities involved in the management and utilization of VIS in the regency. We used a historical approach and a stakeholder-theory lens to capture the roles and interests of each entity that comprises the ecosystem. Our reveal the role and interest in VIS of regional heads, several local government (or supra-village) organizations, vendors or non-governmental organizations (NGOs), village-owned enterprises (BUMDes), civil society organizations (CSO), and villagers in an ecosystem portrait. They are entities that contribute to the management and utilization of VIS. These results can provide an overview of how VIS must be managed collaboratively by involving various stakeholders. Collaborative governance is still rarely found in e-government applications in general, such as e-service, e-procurement, or e-participation, which only tend to provide general e-government roles such as automation and increasing information flow. The results of this study also offer lessons learned for other village administrations in sustainably implementing the VIS.",IS,https://www.semanticscholar.org/paper/ed5826b546b5ed1d5e81c13393e7181e50a80660
ACCEPTANCE ANALYSIS OF TECHNOLOGY-BASED PERSONNEL MANAGEMENT INFORMATION SYSTEMS (SIMPATIK) IN SUPPORTING THE IMPLEMENTATION OF E-GOVERNMENT IN THE SEMARANG CITY MANPOWER SERVICE,"In this digital era, information technology has an important role for institutions, companies, communities, and governments. In carrying out the main tasks of the Semarang City Manpower Service, one of them is evaluating employee performance. In assessing the performance of the Semarang City Manpower Service employees using SIMPATIK which was developed by the Semarang City BKPP which was launched in 2018. Where the implementation of the new system needs to be evaluated further. This is due to the failure to implement e-government services, not because of the quality and capacity of the system, but because of the low user acceptance of these services. Thus, it is necessary to analyse the level of acceptance in using SIMPATIK services at the Semarang City Manpower Service whether it can be evaluated further. So that SIMPATIK can still exist to be used for various kinds of services contained in the application by ASN at the Semarang City Manpower Service. The method applied to data collection is the interview and questionnaire method. While the method applied to evaluate SIMPATIK is the Technology Acceptance Model (TAM) by applying four research variables, namely perceived usefulness (PU), perceived ease of use (PEOU), attitude toward using (ATU), and behavioural intentions (BI). To examine the data adopted the method of Structural Equation Model-partial Least Square (SEMPLS). The conclusion reached in this study is that all hypotheses developed in this study were accepted, except for PU on BI rejected because the of T-Statistics (0.846) < T-Table (1.96) and P-s were more than 0.05 with a of 0.398 .",IS,https://www.semanticscholar.org/paper/dc2834711529cb94917d5fc88001a8b40c292fa1
Understanding citizen's continuance intention to use e-government services: the case of the Indian railway e-ticket booking site,"The emergence and expansion of e-government initiatives has brought interesting facets of e-services on the internet. To sustain such initiatives, it is important to understand the factors that improve the users' intention to reuse such e-services. This research tries to identify the factors that influence the citizens to avail the e-service provided by the Ministry of Railways, Government of India. The result of this study shows that the two major determinants of technology acceptance model (TAM) namely perceived usefulness and perceived ease of use along with trust and computer self-efficacy explain over 70% of the variance in the user's intention to reuse the e-ticket booking site of the Indian railways. The show that the e-ticketing website should contain user-friendly, high-quality information content and excellent website quality in order to enhance citizen's intent to reuse their services.",IS,https://www.semanticscholar.org/paper/ac766e5a150e238bf1cb1f5930dde0069fe5f7a0
E-Government Information Systems (IS) Project Failure in Developing Countries: Lessons from the Literature,"E-government information systems (IS) projects experience numerous challenges that can lead to total or partial failure. The project failure factors have been identified and studied by numerous researchers, but the root causes of such failures are not well-articulated. In this study, literature on e-government IS project failures in developing-world contexts is reviewed through the application of qualitative meta-synthesis, design–reality gap analysis, and root cause analysis. In the process, 18 causal factors and 181 root causes are identified as responsible for e-government IS project failures. The most prevalent of the 18 causal factors are found to be inadequate system requirements engineering (with 22 root causes), inadequate project management (19 root causes), and missing or incomplete features (16 root causes). These can be of use to future researchers, policymakers, and practitioners seeking to identify methods of avoiding e-government IS failures, particularly in developing-world contexts.",IS,https://www.semanticscholar.org/paper/ed42aab773eec05727c0b33b2cf4d912543058ba
An examination of citizen satisfaction with mandatory e-government services: comparison of two information systems success models,"The of this study is to empirically examine the utility of information systems (IS) success models in mandatory e-government services, as opposed to the volitional ones that have been the focus of previous studies. The models include the technology acceptance model (TAM) (1989) and Seddon’s model (1997), which involve three (ease of use, usefulness and citizens satisfaction) and four variables (system quality, information quality, usefulness and citizen satisfaction). /approach The models were compared based on a survey conducted on 780 foundation year students of government universities in Saudi Arabia. The Saudi Government has launched a mandatory e-government service geared to assist high school graduates in the university academic admission process. The goodness-of-fit and parsimony of fit indices and the explanatory power were used to compare the two models. The structural equation modeling techniques revealed that overall, the two models both exhibited reasonable fit with the collected data, whereas TAM showed the best fit to the sample data and yielded superior goodness-of-fit indices over Seddon’s model. In terms of explanatory power, Seddon’s model predicted 28% (R2 = 0.28) of the variance explained for citizen satisfaction, whereas TAM predicted 21% (R2 = 0.21). All the parsimony of fit indices favored TAM over Seddon’s model. Research limitations/implications This study examined the validity of TAM and Seddon’s model, using citizen satisfaction as the dependent variable to compare them. TAM and Seddon’s model were modified to better fit the current research context of mandatory e-government services; thus, the may not hold for their original or other voluntary settings. In addition, the focus on a single survey for a certain time in a certain territory of mandatory e-government service may have limited the generalizability of the results to other mandatory contexts. Future research should make use of large, cross-sectional samples in different mandatory contexts to enhance result generalization. This study’s can provide e-government practitioners with deeper perceptions of how to address citizen satisfaction with mandatory e-government services. The results exposed usefulness as the common and major construct, having the strongest influence on citizen satisfaction in both TAM and Seddon’s model; thus, maximizing the benefits of e-government services for citizens is crucial to their success. The causal relationship between information quality and citizen satisfaction was not supported. This supports the perspective that e-government services are currently evolving quickly, becoming more integrated and easier-to-use, generally requiring only a few clicks and less information. /This study has extended the assessment of the validity of IS success models to a mandatory IS usage setting. The comparison study of different IS success models is crucial as it acts as a guide for researchers to determine the trade-off between the models used to conduct research on a particular context. The study concludes that TAM is the most parsimonious and universal model for the study of user satisfaction in mandatory contexts. The will provide e-government practitioners with insights into IS success measures suited to enhance the effectiveness of newly and future mandated e-government services.",IS,https://www.semanticscholar.org/paper/f976984ae6da07e2667ea8d392d0609d8b4fb887
"Mobile infrastructure quality, regulatory quality, government effectiveness: Does e-government development matter?","There is a great deal of interest in how mobile infrastructure quality, regulatory quality, and a nation's capacity to develop efficient e‐government systems affect government effectiveness. The literature ignores the part electronic government plays in either the regulatory quality‐government effectiveness or mobile infrastructure quality‐government effectiveness nexuses, despite the fact that existing research indicates that the effects of these factors on government effectiveness vary across nations. Secondary data for 52 African nations was extracted from World Governance Indicator (WGI), Mobile Connectivity Index (MCI), and United Nations E‐government Database to empirically investigate the mediating relationship of e‐governments using hypothesized model. The positivist paradigm is considered appropriate for this study because it focuses on validating and testing a hypothesized model. The results show a significant relationship between mobile infrastructure quality and e‐government development, regulatory quality and e‐government development, e‐government and government effectiveness. However, there was no significant relationship between mobile infrastructure quality and government effectiveness. Additionally, e‐government was found to mediate the relationship between regulatory quality and government effectiveness and the relationship between mobile infrastructure quality and government effectiveness. This suggests that government effectiveness is enhanced when a country is able to develop and implement a good electronic system. The successful implementation of e‐governance practices offers better delivery of services to citizens, improved interactions with business, citizen empowerment through access to information, greater convenience, and cost reductions. In addition, establishing protections and legal reforms will be needed to ensure, among other things, the privacy, security and legal recognition of electronic interactions and electronic signatures.",IS,https://www.semanticscholar.org/paper/26db5a1a517f28954def1e09e29b75effff58894
Watch who you trust! A structured literature review to build a typology of e-government risks,"The information systems, e-business, and e-government literature has unanimously shown that trust and risk are antecedents of the use of information technology and technology-based services. However, a deeper understanding of the relationship between trust and risk, especially when taking into account the extensive knowledge created in fields such as organisational science and psychology, is often missing. With this article, we aim at conceptualizing risk in e-government use. Based on a structured review of the trust-related e-government literature, we derive a typology of relevant e-government risks. We analyse this typology in light of extant trust and risk literature. The typology can be used both to understand the behaviour of system or service users and to design systems and services that can be and are trusted. As such, this research can serve as a basis for future research on the role of trust and risk in designing and using e-government services. The generalizability to e-business services and information systems in general is discussed.",IS,https://www.semanticscholar.org/paper/48750538603ecac9e35de7ec9c619bf16f6a6779
INVESTIGATING THE ROLE OF SELF-EFFICACY ON ACCEPTANCE OF E-GOVERNMENT IN TANZANIA,"E-government is the process of delivering government services through electronic media or platforms. It provides alternative options for offering services with minimal need for physical contact. Through E-government, there has been improvement in all sectors on the way citizens access services and share important information. The acceptance of E-government practices and tools is influenced by factors as explained by prominent models such as TAM, UTAUT and TRA. This research involves empirical evidence from the Tanzanian context to e-government self-efficacy has a significant impact on the acceptance of E-government systems. The research employs a survey of 159 respondents followed by analysis using Smart PLS 4. The conceptual framework was developed by extending the Technology Acceptance Model with E-government Self-Efficacy before testing it in quantitative research. Results of the model show that all the relationships were found to be significant. Among others, this research provides theoretical underpinnings to the area of acceptance of technologies as well as providing areas for future research and policy implications.",IS,https://www.semanticscholar.org/paper/85254c4e6ef96b18e96e9bff70279d0de251352b
The Effectiveness And Challenges Of E-Government Implementation Through The Media Center In The City Of Palangka Raya,"Public administration is crucial for government operations and public welfare as mandated by the 1945 Constitution of Indonesia. Technology, especially E-Government, has made public services more efficient. Since Presidential Instruction No. 3 of 2003, various government agencies, including local governments, have adopted E-Government to improve services. However, challenges such as isolated information systems, security issues, data inconsistency, and inadequate infrastructure require standardization. The Palangka Raya City Government utilizes information technology through the Media Center, supported by the Ministry of Communication and Informatics, to enhance information dissemination and networking between local, provincial, and central governments. This study analyzes the effectiveness of the Media Center in Palangka Raya, identifying challenges such as a lack of competent human resources, outdated facilities, limited internet access in remote areas, and insufficient administrative budgets. Data were collected through documentation, observation, and interviews, and analyzed qualitatively. The highlight major challenges and suggest improvements for electronic public services, ultimately contributing to increased community engagement and welfare.",IS,https://www.semanticscholar.org/paper/1202afbd3dd45bb52b6cdebed155ca0f507023b0
Analysis and Design of Village Management Information Systems (VMIS) based on MVC and E-Government in Indonesia,"E-Government is the government's program and commitment in the effort to develop electronic-based governance and transforms to facilitate the activities of society and business to the knowledge-based society. E-Governmet can be said to be a system that contains collections of modules that can be integrated with others. Considering the many module components in the e-government authors in this study limits only to population modules such as service and management of population data, KK data, data on population mutations such as population (moving, coming, born, dead) built using the Model View method Web-based and online controller. As well as how to simplify the management of the letter, in addition to facilitate the search population data and information about the development of villagers in each village in real time with terintegerasinya data to each village in addition to facilitate and accelerate the service request and manufacture reporting. So with the existence of this system the kecamatan easier to see the development of data of the population of each village and with this system the kecamatan and village easier to manage the data letter and can facilitate in sending mail to each village. With this research, it is expected Sub-district offices can provide improvement of information service and also data processing of its population.",IS,https://www.semanticscholar.org/paper/da64e8662ec81ae65779dd23313b6bc056f84b0d
Information Systems User Satisfaction: Application of a model for e-Government,"The growing increase in the digitization of government services points to the need to measure the success of the information systems used. One solution to evaluate these systems is to measure user satisfaction. The aim of this article is to present a model for measuring user satisfaction in information systems for government use. The research was of the descriptive type, carried out through a questionnaire to a sample of 1260 users of the system. The research model was calculated using structural equations via variance. The results revealed that the DeLone & McLean model is an adequate model for measuring government information systems. In this study, the model managed to explain 76.9% of user satisfaction in information systems, with perceived utility, quality of information, quality of the system and quality of services being the most important variables, respectively.",IS,https://www.semanticscholar.org/paper/887f48ffa8f2c379fc54299966469fcf762f11cf
Assessment of E-Government Portals,"This research revealed the importance of public service web portals for an e-government information system. An e-government portal is interacting with its administrators, citizens, businesses and other governments helping them increase their operations performance. The authors have developed, modeled, formulated and compared an efficient assessment framework for e-government portals. In order to accomplish such task many quantitative factors and indicators were taken under consideration; also, other frameworks have been studied and compared. The authors focused on the web portals services quantity that the interested parties should use, in order to create an well designed public services’ web portal. This research provides a framework model to evaluate the basic common digital public services that a government offers to its interactive stakeholders, so that all other countries across the world can predefine weaknesses and strengths, improve existing or formulating new e-services. The importance of the assessment framework model is thoroughly explained through the results.",IS,https://www.semanticscholar.org/paper/e5be73f6b844e27ba5e8604685cfa09a25605c14
"Violated factors in building citizen-centric e-government websites: insights from the performance of the federal, state and local governments websites in Malaysia","Common evaluation tools on e-government websites are available globally and locally to standardise and improve the quality of information and services. However, a commonly ignored aspect is the way to obtain detailed measurements of factors influencing citizen centricity; in other words, how official websites cater to the needs and contributions of citizens at different levels of government. Thus, this paper aims to apply a citizen-centric framework in evaluating the e-government websites of three different levels of authority in Malaysia: federal, state and local. /approach The adapted citizen-centric checklist for e-government websites (aCCEW) with 40 characteristics across four components – openness (21), transparency (5), participation (10) and responsiveness (4) – was adopted to evaluate case studies of 36 government agency websites in Malaysia. Any conformity between the characteristics was marked using a binary measure, and the citizen-centric was calculated for each component/characteristic. Through website observations, ratings and descriptive comparisons, this study found that the aCCEW is a useful tool, especially for identifying certain critically violated factors. These were deficiencies in e-decision-making, revealing successful initiatives created through open data, revealing fund transfers and expenditure records and the level of social media responsiveness. Research limitations/implications The research contributes theoretically by improvising characteristics in the CCEW to become aCCEW and testing it in multiple levels of government in Malaysia to see its applicability to be adopted in other similar research of e-governments. This could become a new benchmark through the additional research insights it offers into similar perspectives of public s realisation in e-government website design that focuses on more than merely functionality. Attempt to relate the violated factors and strengths of aCCEW website design components to the level of centralisation (power) of federal, state and local governments was also genuine in the e-government research. Regardless of the many different government systems, federal, state and local governments can benchmark the examples assessed in this study, rethink their power relationships, and further improve their e-platforms to suit the contexts of their users/residents’ needs and contributions. /To the best of the authors’ knowledge, this study contributed to the first Malaysia-based research that identifies and compares factors that contribute to citizen-centric e-government website building at the federal, state and local government levels. The discussion adds by comparing different systems and levels of e-government websites to their power possession.",IS,https://www.semanticscholar.org/paper/aeb12f0b3d95171b04ec546d272f3775e9568176
E-government System Based on Blockchain,"With the advancement of digitalization and intelligence, the government affairs system is experiencing significant transformations. The E-government system, which is founded on blockchain technology, is a novel government service mode that offers numerous advantages, such as enhanced data security, facilitated information sharing, reinforced system stability, and improved system performance. Consequently, this system presents a fresh solution for fostering innovation and development within government services. By examining and evaluating the fundamental framework and algorithm optimization of the blockchain-based E-government affairs system, this paper concludes that the system can successfully achieve the objectives of data sharing and protection, business collaboration and automation, intelligent and compliant governance. Consequently, it can effectively enhance the efficiency and credibility of government services.",IS,https://www.semanticscholar.org/paper/ebe00fd232352653bb743a02ed99cc83319ac4a6
Success evaluation model of E-government systems for tax administrations: Recognizing the public value from the taxpayer's perspective,"Through the E-government systems, the tax administrations of the different levels of government interrelate with their target user, in this case the taxpayer, ensuring that the latter achieves compliance with its tax obligations in an agile, timely and efficient manner, promoting in turn, the construction of public s in this interested party. However, it is important to assess whether these information systems (IS) having the capabilities to meet the objectives pre-established by this type of organization, which have missions, interest groups, perspectives, contexts, factors and particular benefits compared to other government entities, and in which its success or failure, influences government finances and the development of a territory. Therefore, this document proposes the construction of a model for evaluating the success of E-government systems for tax administrations, which proposes a particular and not a general character, where the traditional factors of success theories must be complemented with relevant constructs to this context, such as: demographic considerations, the capacity for timely parameterization due to regulatory changes (flexibility), the digital gap, allowing to classify its benefits from social, economic, individual, organizational, environmental and public aspects, developing in this way a holistic model for this type of government organization, seeking that the taxpayer and their intermediaries are the protagonists of this assessment.",IS,https://www.semanticscholar.org/paper/897eeebf6ebf019770d5113ec723317a73e2f3ab
Clarifying the Role of E-Government Trust in E-Government Success Models: A Meta-analytic Structural Equation Modeling Approach,"E-government implementation success is of critical importance for nations. Prior information systems (IS) success models emphasize the effects of information quality, service quality, system quality, and user satisfaction but do not consider e-government trust. This study incorporates e-government trust into the IS success model and empirically tests the model on empirical reported in 67 prior studies using meta-analysis methods and structural equation modeling. Our analysis shows that: a) information quality, service quality, system quality, and user satisfaction influence e-government trust, and b) system use mediates the effect of e-government trust on intention to use e-government systems in the future.",IS,https://www.semanticscholar.org/paper/3027c54a11a239c49496233e0f46f6a9bcdd3891
"The impact of e‐government services on customer satisfaction in the private sector: A case study of the Kingdom of Bahrain (SIJILAT), an online commercial registration","Countless e‐government program initiatives have been introduced to profit each section of society in the Kingdom of Bahrain: citizens, residents, government entities, and workers' business ventures. It is getting more important to have e‐Government systems to supply services to businesses that use them (e‐G2B). To improve the Kingdom's national economy, the Information and Government Authority (IGA) has launched a virtual one‐stop solution available through multiple channels offering a unified service where citizens and Commercial Registration holders can use a single streamlined form to apply, renew or terminate multiple licenses concurrently; this is the Business License Information System (SIJILAT). This study aims to assess Bahraini customers' satisfaction with the SIJILAT service. A quantitative method is followed with a questionnaire to gather data; six factors: accessibility, information, security, reliability, trust, and perceived ease of use found to effectively affect service quality, and, in which it affects positively and enhances customer satisfaction. The theoretical contribution of this research will enhance the theoretical literature and knowledge related to its topic. The current research will contribute to the few studies conducted in Bahrain and the Middle East on e‐government and customer satisfaction with these services. A key outcome of the research will be the ability of e‐government officials who make decisions to determine the significant factors that play a role in G2B success, particularly ones to which they should pay attention to obtain the maximum return on their technology investment.",IS,https://www.semanticscholar.org/paper/e36ee470b50b5d8488a4c7d089baeec4d80625ff
A Framework for An Integrated E- Government System for Public Service Sectors in Developing Countries Using Design Science Research Methodology,"This study aimed to develop an integrated framework to enhance government services and improve service delivery for the Namibian government. The Delone and McLean Information Systems success model and the Organizational Information Processing Theory served as the theoretical foundations for this study. The target population consisted of members of the public and a Government Ministry in Namibia. A sample size of 25 participants was selected for the study, utilizing the purposive sampling technique for government employees and the snowball sampling technique for members of the public. Thematic analysis was employed to analyze the data gathered. The study's revealed several significant insights. Despite the existence of multiple e-government systems, poor integration among them hindered the ability of the Namibian public service to provide effective and efficient services to citizens. Additionally, the study identified a strong demand from the public for services to be more accessible, convenient, and responsive, including a preference for online applications and faster service delivery. Moreover, the study developed an integrated e-government framework specifically tailored for the Namibian public service, aiming to enhance accessibility, convenience, responsiveness, and cost-effectiveness of services provided to citizens. The implementation of this framework is expected to promote an open and accountable government. It is crucial to emphasize that the successful achievement of e-Government goals relies on factors such as political leadership, support across all government levels, key infrastructure, skilled human resources, a suitable legal and regulatory framework, information resources, and citizen-centric services.",IS,https://www.semanticscholar.org/paper/53717f9d78042c67c6f7c0c8d41e4daa3b0f2e11
Online security in e-government as an antecedent of cost-effectiveness and quality in business operations,"The Colombian Government launched an e-government initiative in 2008 to facilitate communication among the government, citizens and organizations. Considering the high level of mistrust of citizens and businesses toward governmental institutions, the government must ensure the security of the information handled and provided by online users. Results to date have not been adequate in the usage of e-government systems. The of this study is to evaluate whether the level of online security affects usage and impacts the cost-effectiveness and quality of the operations and, consequently, the operational effectiveness of organizations using e-government systems. /approach Structural equation modeling was used to analyze the antecedents and outcomes of operational effectiveness. To this end, 440 usable questionnaires were collected from managers and personnel from Colombian organizations using e-government systems. According to the , there is a positive predictive relationship between online security and the dimensions of electronic government effectiveness. Furthermore, neither online security nor any of the dimensions of electronic government effectiveness affect the operational costs of organizations. Nonetheless, the quality of information has a positive effect on the quality of operations. As a result, through the quality of the information, online security has an indirect impact on the quality of operations. Research limitations/implications The authors used a convenience sample, carefully selecting respondents based on their operations and practice knowledge and implementation of online security processes. Besides, compared to previous research conducted in developed nations, the sample size is relatively small. Because the survey is based on responses from official companies, it must also be taken into account that over 50% of Colombian labor is informal. Furthermore, Colombia is a nation with a high level of mistrust. When considering these factors, generalizability to all industrial sectors is questionable. Nevertheless, the of this study offer relevant information that indicates the need for more extended and comprehensive quantitative research. Improvements in organizations that use e-government systems, based on the benefits that high-quality information brings to operational performance – cost and quality – will help them survive and become more sustainable and competitive. Furthermore, this study supports the assertion that aspects like online security are critical in promoting information and communication technology uptake and user acceptance in transition and rising economies like Colombia. /There is still a scarcity of information on assessing the effectiveness of electronic government systems and their impact on the quality and cost of operations in organizations that use them. Additionally, Colombia, as a country with low levels of trust between citizens, organizations and government, still lacks information about the impact of online security on the effectiveness of its operations.",IS,https://www.semanticscholar.org/paper/90344eee7d2fe7d58bb79eb4ddeefed2b7ccce23
Design Science in Information Systems Research,"Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.",IS,https://www.semanticscholar.org/paper/0ee5a26a6dc64d3089c8f872bd550bf1eab7051d
An updated and expanded assessment of PLS-SEM in information systems research,"Following the call for awareness of accepted reporting practices by Ringle, Sarstedt, and Straub in 2012, the purpose of this paper is to review and analyze the use of partial least squares structural equation modeling (PLS-SEM) in Industrial Management & Data Systems (IMDS) and extend MIS Quarterly (MISQ) applications to include the period 2012-2014.,Review of PLS-SEM applications in information systems (IS) studies published in IMDS and MISQ for the period 2010-2014 identifying a total of 57 articles reporting the use of or commenting on PLS-SEM.,The results indicate an increased maturity of the IS field in using PLS-SEM for model complexity and formative measures and not just small sample sizes and non-normal data.,Findings demonstrate the continued use and acceptance of PLS-SEM as an accepted research method within IS. PLS-SEM is discussed as the preferred SEM method when the research objective is prediction.,This update on PLS-SEM use and recent developments will help authors to better understand and apply the method. Researchers are encouraged to engage in complete reporting procedures.,Applications of PLS-SEM for exploratory research and theory development are increasing. IS scholars should continue to exercise sound practice by reporting reasons for using PLS-SEM and recognizing its wider applicability for research. Recommended reporting guidelines following Ringle et al. (2012) and Gefen et al. (2011) are included. Several important methodological updates are included as well.",IS,https://www.semanticscholar.org/paper/f0627534178060ccf6d2cd006b1346efd3e7f5f5
Understanding Information Systems Continuance: An Expectation-Confirmation Model,"This paper examines cognitive beliefs and affect influencing one's intention to continue using (continuance) information systems (IS). Expectation-confirmation theory is adapted from the consumer behavior literature and integrated with theoretical and empirical findings from prior IS usage research to theorize a model of IS continuance. Five research hypotheses derived from this model are empirically validated using a field survey of online banking users. The results suggest that users' continuance intention is determined by their satisfaction with IS use and perceived usefulness of continued IS use. User satisfaction, in turn, is influenced by their confirmation of expectation from prior IS use and perceived usefulness. Post-acceptance perceived usefulness is influenced by users' confirmation level. This study draws attention to the substantive differences between acceptance and continuance behaviors, theorizes and validates one of the earliest theoretical models of IS continuance, integrates confirmation and user satisfaction constructs within our current understanding of IS use, conceptualizes and creates an initial scale for measuring IS continuance, and offers an initial explanation for the acceptance-discontinuance anomaly.",IS,https://www.semanticscholar.org/paper/a3dff66b60240d1515cb92fe72e3a415bd8aafa6
Information Systems Success: The Quest for the Dependent Variable,"A large number of studies have been conducted during the last decade and a half attempting to identify those factors that contribute to information systems success. However, the dependent variable in these studies-I/S success-has been an elusive one to define. Different researchers have addressed different aspects of success, making comparisons difficult and the prospect of building a cumulative tradition for I/S research similarly elusive. To organize this diverse research, as well as to present a more integrated view of the concept of I/S success, a comprehensive taxonomy is introduced. This taxonomy posits six major dimensions or categories of I/S success-SYSTEM QUALITY, INFORMATION QUALITY, USE, USER SATISFACTION, INDIVIDUAL IMPACT, and ORGANIZATIONAL IMPACT. Using these dimensions, both conceptual and empirical studies are then reviewed a total of 180 articles are cited and organized according to the dimensions of the taxonomy. Finally, the many aspects of I/S success are drawn together into a descriptive model and its implications for future I/S research are discussed.",IS,https://www.semanticscholar.org/paper/a04145f1ca06c61f5985ab22a2346b788f343392
A Set of Principles for Conducting and Evaluating Interpretive Field Studies in Information Systems,"This article discusses the conduct and evaluatoin of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflect and debate on the important subject of grounding interpretive research methodology.",IS,https://www.semanticscholar.org/paper/fcd71a2ca1a15d3b6171670ce82177c81cd9745e
Information systems success: the quest for the dependent variable,"Article history: Received July 18, 2014 Accepted December 1 2014 Available online December 11 2014 This paper presents an empirical investigation to identify and rank the factors of information market development systems influencing on the market share development. The population of this survey includes all managers who work for SMEs in city of Tehran, Iran. The study selects a sample of 230 people randomly and a questionnaire is distributed among them in Likert scale. Cronbach alpha has been calculated as 0.814, which is well above the minimum desirable level. Using structural equation modeling the study has determined seven factors including valid data, information, strategic information, organizational information, supportive information, customer information, development information and data analysis, which influence market share development. Growing Science Ltd. All rights reserved. 5 © 201",IS,https://www.semanticscholar.org/paper/46c2603d83fc1a7c3cb5e7b79eaeed137455d204
The Nature of Theory in Information Systems,"The aim of this research essay is to examine the structural nature of theory in Information Systems. Despite the importance of theory, questions relating to its form and structure are neglected in comparison with questions relating to epistemology. The essay addresses issues of causality, explanation, prediction, and generalization that underlie an understanding of theory. A taxonomy is proposed that classifies information systems theories with respect to the manner in which four central goals are addressed: analysis, explanation, prediction, and prescription. Five interrelated types of theory are distinguished: (1) theory for analyzing, (2) theory for explaining, (3) theory for predicting, (4) theory for explaining and predicting, and (5) theory for design and action. Examples illustrate the nature of each theory type. The applicability of the taxonomy is demonstrated by classifying a sample of journal articles. The paper contributes by showing that multiple views of theory exist and by exposing the assumptions underlying different viewpoints. In addition, it is suggested that the type of theory under development can influence the choice of an epistemological approach. Support is given for the legitimacy and value of each theory type. The building of integrated bodies of theory that encompass all theory types is advocated.",IS,https://www.semanticscholar.org/paper/d9064a6fbf2627c0ab53bb600f0e14bf84d1fc09
The Case Research Strategy in Studies of Information Systems,This article defines and discusses one of these qualitative methods - the case research strategy. Suggestions are provided for researchers who wish to undertake research employing this approach. Criteria for the evaluation of case research are established and several characteristics useful for categorizing the studies are identified. A sample of papers drawn from information systems journals is reviewed. The paper concludes with examples of research areas that are particularly well-suited to investigation using the case research approach.,IS,https://www.semanticscholar.org/paper/838ec45eeb7f63875742a76aa1080563f44af619
User Acceptance of Hedonic Information Systems,"This paper studies the differences in user acceptance models for productivity-oriented (or utilitarian) and pleasure-oriented (or hedonic) information systems. Hedonic information systems aim to provide self-fulfilling rather than instrumental value to the user, are strongly connected to home and leisure activities, focus on the fun-aspect of using information systems, and encourage prolonged rather than productive use. The paper reports a cross-sectional survey on the usage intentions for one hedonic information system. Analysis of this sample supports the hypotheses that perceived enjoyment and perceived ease of use are stronger determinants of intentions to use than perceived usefulness. The paper concludes that the hedonic nature of an information system is an important boundary condition to the validity of the technology acceptance model. Specifically, perceived usefulness loses its dominant predictive value in favor of ease of use and enjoyment.",IS,https://www.semanticscholar.org/paper/fe7789110de0535f0bcfb3755a737e4b779975b1
Specifying Formative Constructs in Information Systems Research,"While researchers go to great lengths to justify and prove theoretical links between constructs, the relationship between measurement items and constructs is often ignored. By default, the relationship between construct and item is assumed to be reflective, meaning that the measurement items are a reflection of the construct. Many times, though, the nature of the construct is not reflective, but rather formative. Formative constructs occur when the items describe and define the construct rather than vice versa. In this research, we examine whether formative constructs are indeed being mistaken for reflective constructs by information systems researchers. By examining complete volumes of MIS Quarterly and Information Systems Research over the last 3 years, we discovered that a significant number of articles have indeed misspecified formative constructs. For scientific results to be valid, we argue that researchers must properly specify formative constructs. This paper discusses the implications of different patterns of common misspecifications of formative constructs on both Type I and Type II errors. To avoid these errors, the paper provides a roadmap to researchers to properly specify formative constructs. We also discuss how to address formative constructs within a research model after they are specified.",IS,https://www.semanticscholar.org/paper/c3b1350c9a91708abc632823bc1fef90ecc3d309
Construction with digital twin information systems,"The concept of a “digital twin” as a model for data-driven management and control of physical systems has emerged over the past decade in the domains of manufacturing, production, and operations. In the context of buildings and civil infrastructure, the notion of a digital twin remains ill-defined, with little or no consensus among researchers and practitioners of the ways in which digital twin processes and data-centric technologies can support design and construction. This paper builds on existing concepts of Building Information Modeling (BIM), lean project production systems, automated data acquisition from construction sites and supply chains, and artificial intelligence to formulate a mode of construction that applies digital twin information systems to achieve closed loop control systems. It contributes a set of four core information and control concepts for digital twin construction (DTC), which define the dimensions of the conceptual space for the information used in DTC workflows. Working from the core concepts, we propose a DTC information system workflow—including information stores, information processing functions, and monitoring technologies—according to three concentric control workflow cycles. DTC should be viewed as a comprehensive mode of construction that prioritizes closing the control loops rather than an extension of BIM tools integrated with sensing and monitoring technologies.",IS,https://www.semanticscholar.org/paper/d403d3c0f4112bf5c06b9ad82e85d5968ccbb7d9
"Review: the resource-based view and information systems research: review, extension, and suggestions for future research","Information systems researchers have a long tradition of drawing on theories from disciplines such as economics, computer science, psychology, and general management and using them in their own research. Because of this, the information systems field has become a rich tapestry of theoretical and conceptual foundations. As new theories are brought into the field, particularly theories that have become dominant in other areas, there may be a benefit in pausing to assess their use and contribution in an IS context. The purpose of this paper is to explore and critically evaluate use of the resource-based view of the firm (RBV) by IS researchers. The paper provides a brief review of resource-based theory and then suggests extensions to make the RBV more useful for empirical IS research. First, a typology of key IS resources is presented, and these are then described using six traditional resource attributes. Second, we emphasize the particular importance of looking at both resource complementarity and moderating factors when studying IS resource effects on firm performance. Finally, we discuss three considerations that IS researchers need to address when using the RBV empirically. Eight sets of propositions are advanced to help guide future research.",IS,https://www.semanticscholar.org/paper/997f0644155b4c3d0be4164250a8bd6ad4b9a161
Information systems in the age of pandemics: COVID-19 and beyond,"This issue of the European Journal of Information Systems (EJIS) is a special issue on Business Process Management and Digital Innovation. Guest editors Jan Mendling, Brian Pentland, and Jan Recker...",IS,https://www.semanticscholar.org/paper/4e6261ef73d19888d5917d67b2d22b0d1fa8a919
Accounting information systems,"ACNT 2332 Accounting Information Systems. This course will The course reflects how IT is altering the nature of accounting. Specifically . Accounting Information Systems 2nd. Edition by Robert L. Hurt. Publisher: Tutors and solution manuals are availa Aug 21, 2012 Accounting Information Systems: Basic Concepts and Current Issues, Robert L. Hurt,. McGraw-Hill, 3rd Edition, ISBN-13: 978-0-07-802533-4. Systems Understanding Aid, Arens & Ward, Armond . your solutions be the result of your own effor Mason: OH 2005. 14. Hurt, Robert L. Accounting Information Systems: Basic Concepts and Current Issues. 2nd Edition. McGraw-Hill, New York: NY 2010. 15. book. I know that can present a challenge to folks who have used the earlier Dr. Robert L. Hurt California State Polytechnic University, Pomona . mously helpful in preparing the third edition of Accounting Information Systems: Basic Instructor's Res.",IS,https://www.semanticscholar.org/paper/f3641139ae8d33c320ba6444f42837a78a2fdecd
Research Commentary - The New Organizing Logic of Digital Innovation: An Agenda for Information Systems Research,"In this essay, we argue that pervasive digitization gives birth to a new type of product architecture: the layered modular architecture. The layered modular architecture extends the modular architecture of physical products by incorporating four loosely coupled layers of devices, networks, services, and contents created by digital technology. We posit that this new architecture instigates profound changes in the ways that firms organize for innovation in the future. We develop (1) a conceptual framework to describe the emerging organizing logic of digital innovation and (2) an information systems research agenda for digital strategy and the creation and management of corporate information technology infrastructures.",IS,https://www.semanticscholar.org/paper/7e488326735cac3b5f830aa5f542f78eda74de85
A Framework for Information Systems Architecture (Abstract of Tutorial),"With increasing size and complexity of the implementations of information systems, it is necessary to use some logical construct (or architecture) for defining and controlling the interfaces and the integration of all of the components of the system. This paper defines information systems architecture by creating a descriptive framework from disciplines quite independent of information systems, then by analogy specifies information systems architecture based upon the neutral, objective framework. Also, some preliminary conclusions about the implications of the resultant descriptive framework are drawn. The discussion is limited to architecture and does not include a strategic planning methodology.",IS,https://www.semanticscholar.org/paper/bda6aa67d0aaf6ec07d0946244b1563bedc5f861
Linking information systems and entrepreneurship: A review and agenda for IT‐associated and digital entrepreneurship research,"More than 50 years ago, information technology (IT) began to change society, the economy, and industries worldwide. This change has included waves of technological disruption that have been exploited by entrepreneurial actors who seize the associated new opportunities. Research on related phenomena is spread across different disciplines. Recently, there have been calls for further research on the marriage of information systems (IS) and entrepreneurship. We review 292 articles in the IS, entrepreneurship, and general and strategic management literature to create an overview of the IT‐associated entrepreneurship research landscape. On the basis of that review, we elaborate on the different roles that IT can assume to support entrepreneurial operations and value creation in these settings. Our findings suggest that IT plays four major roles in entrepreneurial operations: as a facilitator, making the operations of start‐ups easier; as a mediator for new ventures' operations; as an outcome of entrepreneurial operations; and as a ubiquity, becoming the business model itself. Leveraging these roles of IT, we develop a set of definitions to clear up definition uncertainties surrounding IT‐associated new ventures such as digital start‐ups and digital business models. We also outline a research agenda for IT‐associated entrepreneurship research based on identified roles, types, and gaps.",IS,https://www.semanticscholar.org/paper/8516a346147324101d93bd053d2acf3b33cf1dd4
How Habit Limits the Predictive Power of Intention: The Case of Information Systems Continuance,"Past research in the area of information systems acceptance has primarily focused on initial adoption under the implicit assumption that IS usage is mainly determined by intention. While plausible in the case of initial IS adoption, this assumption may not be as readily applicable to continued IS usage behavior since it ignores that frequently performed behaviors tend to become habitual and thus automatic over time. This paper is a step forward in defining and incorporating the ""habit"" construct into IS research. Specifically, the purpose of this study is to explore the role of habit and its antecedents in the context of continued IS usage. Building on previous work in other disciplines, we define habit in the context of IS usage as the extent to which people tend to perform behaviors (use IS) automatically because of learning. Using recent work on the continued usage of IS (IS continuance), we have developed a model suggesting that continued IS usage is not only a consequence of intention, but also of habit. In particular, in our research model, we propose IS habit to moderate the influence of intention such that its importance in determining behavior decreases as the behavior in question takes on a more habitual nature. Integrating past research on habit and IS continuance further, we suggest how antecedents of behavior/behavioral intention as identified by IS continuance research relate to drivers of habitualization. We empirically tested the model in the context of voluntary continued WWW usage. Our results support the argument that habit acts as a moderating variable of the relationship between intentions and IS continuance behavior, which may put a boundary condition on the explanatory power of intentions in the context of continued IS usage. The data also support that satisfaction, frequency of past behavior, and comprehensiveness of usage are key to habit formation and thus relevant in the context of IS continuance behavior. Implications of these findings are discussed and managerial guidelines presented.",IS,https://www.semanticscholar.org/paper/8aa6ff0597c33b8bbf5f87db63380e773ef799a4
Information Systems Success Measurement,"Researchers and practitioners alike face a daunting challenge when evaluating the ""success"" of information systems. The purpose of this monograph is to deepen, researchers and practitioners, understanding of the complex nature of IS success measurement driven by the constantly changing role and use of information technology. This monograph covers the history of IS success measurement as well as recent trends and future expectations for IS success measurement. The monograph also identifies the critical success factors that drive information system success and provides measurement and evaluation guidance for practitioners. This comprehensive study of IS success measurement is designed to improve measurement practice among researchers and managers.",IS,https://www.semanticscholar.org/paper/2945986d75ecd4c7c7de1737a2b98412210224ef
Knowledge and information systems,"Knowledge and Information Systems (KAIS) provides an international forum for researchers and professionals to share their knowledge and report new advances on all topics related to knowledge systems and advanced information systems. This monthly peer-reviewed archival journal publishes state-of-the-art research reports on emerging topics in KAIS, reviews of important techniques in related areas, and application papers of interest to a general readership. The journal focuses on knowledge systems and advanced information systems, including their theoretical foundations, infrastructure and enabling technologies. We solicit submissions of original research, and experience and vision papers that address this theme. We publish critical review papers to discuss the state of the art in particular areas, as well as state-of-the-art research reports. Accepted papers are grouped for publication so that individual issues focus on a small number of theme areas. In addition to archival papers, the journal also publishes significant on-going research in the form of Short Papers (limited to 3000 words), and very short papers on ""visions and directions"" (no more than 1000 words, excluding bibliography). We conduct reviews in a timely fashion and inform authors of decisions with a target turnaround time of 3 months. Selected papers from relevant conferences are welcome. Good papers with high quality reviews can be accepted after the expansion and revision is verified by an Associate Editor of the Editorial Board. Conference organizers are invited to contact the Editor-in-Chief kais@cs.uvm.edu for further information.",IS,https://www.semanticscholar.org/paper/f6cf67499c0562abee3eec43391882a5449d9416
Toward Meaningful Engagement: A Framework for Design and Research of Gamified Information Systems,"Gamification, an emerging idea for using game design elements and principles to make everyday tasks more engaging, is permeating many different types of information systems. Excitement surrounding gamification results from its many potential organizational benefits. However, few research and design guidelines exist regarding gamified information systems. We therefore write this commentary to call upon information systems scholars to investigate the design and use of gamified information systems from a variety of disciplinary perspectives and theories, including behavioral economics, psychology, social psychology, information systems, etc. We first explicate the idea of gamified information systems, provide real-world examples of successful and unsuccessful systems, and, based on a synthesis of the available literature, present a taxonomy of gamification design elements. We then develop a framework for research and design: its main theme is to create meaningful engagement for users; that is, gamified information systems should be designed to address the dual goals of instrumental and experiential outcomes. Using this framework, we develop a set of design principles and research questions, using a running case to illustrate some of our ideas. We conclude with a summary of opportunities for IS researchers to extend our knowledge of gamified information systems, and, at the same time, advance existing theories.",IS,https://www.semanticscholar.org/paper/bcc234de950c2376c4206d423caa39eec5f31110
Paradox Lost? Firm-Level Evidence on the Returns to Information Systems Spending,"The ""productivity paradox"" of information systems IS is that, despite enormous improvements in the underlying technology, the benefits of IS spending have not been found in aggregate output statistics. One explanation is that IS spending may lead to increases in product quality or variety which tend to be overlooked in the aggregate statistics, even if they increase output at the firm-level. Furthermore, the restructuring and cost-cutting that are often necessary to realize the potential benefits of IS have only recently been undertaken in many firms. Our study uses new firm-level data on several components of IS spending for 1987-1991. The dataset includes 367 large firms which generated approximately 1.8 trillion dollars in output in 1991. We supplemented the IS data with data on other inputs, output, and price deflators from other sources. As a result, we could assess several econometric models of the contribution of IS to firm-level productivity. Our results indicate that IS spending has made a substantial and statistically significant contribution to firm output. We find that the gross marginal product MP for computer capital averaged 81% for the firms in our sample. We find that the MP for computer capital is at least as large as the marginal product of other types of capital investment and that, dollar for dollar, IS labor spending generates at least as much output as spending on non-IS labor and expenses. Because the models we applied were similar to those that have been previously used to assess the contribution of IS and other factors of production, we attribute the different results to the fact that our data set is more current and larger than others explored. We conclude that the productivity paradox disappeared by 1991, at least in our sample of firms.",IS,https://www.semanticscholar.org/paper/a89f4018cc37c1473646dc8cafa106adb9bbe463
Information Systems,"An information system is a primitive structure that defines which agents can initially get information and how such information is then distributed to others. From political and organizational economics to privacy, information systems arise in various contexts and, unlike information itself, can be easily observed empirically. We introduce a methodology to characterize how information systems affect strategic behavior. This involves proving a revelation principle result for a novel class of constrained information design problems. We identify when such systems better distribute information and, as a result, impose more constraints on behavior. This leads to a novel notion of an agent’s influence in the system. Finally, we apply our theory to examine how current patterns of news consumption from mass media may affect elections.",IS,https://www.semanticscholar.org/paper/1d597832f79fdf3e2eb19a15beb8f418a2f1acf4
"Message Equivocality, Media Selection, and Manager Performance: Implications for Information Systems",A field study of middle- and upper-level managers was undertaken to explain managers' selection of communication media. The findings indicate that media vary in their capacity to convey information cues. Managers prefer rich media for ambiguous communications and less rich media for unequivocal communications. The data suggest that high performing managers are more sensitive to the relationship between message ambiguity and media richness than low performing managers. Implications for managers' use of information systems and electronic media are discussed.,IS,https://www.semanticscholar.org/paper/35909ac53d8014b35b66ecc62bba2b1837f1635a
[Geographic information systems].,"The aim of this study was to geospatially explore the occurrence rates of car accidents involving pedestrians in Cercado de Lima (Lima District), Peru. Car accidents involving pedestrians recorded in the 2015 National Police Station Census of the National Statistics and Information Institute were described and georeferenced. Subsequently, a Kernel Density analysis was carried out to locate areas with high, medium, and low density events. Records of 171 car accidents involving pedestrians were studied: the types of vehicles involved were automobiles (56.7%) and smaller vehicles (22.8%). The highest percentage of car accidents involving pedestrians (38.6%) took place between 12:00 p.m. and 5:00 p.m. There were two densely populated areas and two areas with intermediate density for car accidents involving pedestrians, locations that were previously reported as critical due to their deficiencies and high probability of traffic accidents. The use of geographic information systems offers a quick overview of the occurrence rates of car accidents involving pedestrians to make comparisons and enable the local implementation of strategies.",IS,https://www.semanticscholar.org/paper/fd3d9feda084be01707ee389aebbcaf6d92a42b7
Standing on the Shoulders of Giants: Challenges and Recommendations of Literature Search in Information Systems Research,"The “standing on the shoulders of giants” metaphor is often used to acknowledge the work of others when undertaking research and, in particular, stresses the importance of literature reviews in scientific inquiry. Though the significance of literature reviews has never been in doubt, researchers, especially novice researchers, still struggle with developing effective strategies for reviewing literature. An important reason for this difficulty is the rapidly increasing number of potentially relevant publications—not all of which necessarily add value to a literature review. As such, avoiding standing on the shoulders of dwarfs literature search emerges as a major issue in crafting an effective literature review. In this paper, we discuss challenges of literature searches in the increasingly dynamic context of information systems (IS) research and make recommendations for how to deal with them. We present practical guidelines and a checklist to help researchers with planning and organizing their literature searches.",IS,https://www.semanticscholar.org/paper/5a1940325609fb6d19cf35569e53cf3b521cce42
The Sustainability Imperative in Information Systems Research,"This paper reports on a panel discussion at the pre-ICIS 2015 Workshop on Green Information Systems on the current state and future perspectives of SIGGreen—the Association of Information Systems’ special interest group on green information systems—and of green information systems (green IS) research in general. Over the past years, IS scholars have made important contributions advancing our knowledge about how information systems can contribute to solving problems associated with the degradation of the natural environment. However, it would appear that many view green IS as just another research topic in the IS field and not a very important one at that. This is questionable because sustainability is too important to be relegated as a footnote in the greater scheme of things. We suggest that the IS community should embrace sustainability as a core research imperative and integrate sustainability-related dimensions to research in theory and method, in rigor and relevance, and in the areas one chooses to research. We provide some actionable recommendations on how we as IS researchers and, indeed, how the IS field could help society and business interests make the transition to a sustainable world.",IS,https://www.semanticscholar.org/paper/252a9bc13fcf61f05733a2834c4ae4a14b4a9332
An Integrated Model of Information Systems Adoption in Small Businesses,"Based on theories from the technological innovation literature, this study develops an integrated model of information systems (IS) adoption in small businesses. The model specifies contextual variables such as decision-maker characteristics, IS characteristics, organizational characteristics, and environmental characteristics as primary determinants of IS adoption in small businesses. A questionnaire survey was conducted in 166 small businesses. Data analysis shows that small businesses with certain CEO characteristics (innovativeness and level of IS knowledge), innovation characteristics (relative advantage, compatibility, and complexity of IS), and organizational characteristics (business size and level of employees' IS knowledge) are more likely to adopt IS. While CEO and innovation characteristics are important determinants of the decision to adopt, they do not affect the extent of IS adoption. The extent of IS adoption is mainly determined by organizational characteristics. Finally, the environmental characteristic of competition has no direct effect on small business adoption of IS.",IS,https://www.semanticscholar.org/paper/796531125787f9a99f547186452181f100c1fe27
Towards a Framework of Literature Review Process in Support of Information Systems Research,"This paper introduces an initial effort towards developing a framework for writing an effective literature review. The target audience for the framework are novice IS researchers or other researchers who are constantly struggling with the development of an effective literature-based foundation for the proposed research. The proposed framework follows the systematic data processing approach comprised of three major stages: 1) inputs (literature gathering and screening), 2) processing (Blooms Taxonomy), and 3) outputs (writing the review). This paper provides the rationale for developing a solid literature review and addresses the central stage, processing the literature. The paper concludes by providing arguments for the value of an effective literature review as well as implications for future work in this proposed framework.",IS,https://www.semanticscholar.org/paper/5e37fd999ff34f0b488a712bd205acbfb830cbdd
A method for taxonomy development and its application in information systems,"A fundamental problem in many disciplines is the classification of objects in a domain of interest into a taxonomy. Developing a taxonomy, however, is a complex process that has not been adequately addressed in the information systems (IS) literature. The purpose of this paper is to present a method for taxonomy development that can be used in IS. First, this paper demonstrates through a comprehensive literature survey that taxonomy development in IS has largely been ad hoc. Then the paper defines the problem of taxonomy development. Next, the paper presents a method for taxonomy development that is based on taxonomy development literature in other disciplines and shows that the method has certain desirable qualities. Finally, the paper demonstrates the efficacy of the method by developing a taxonomy in a domain in IS.",IS,https://www.semanticscholar.org/paper/6b56454a84dcc4de62aea4334c414dc979c42a31
A Guide to Conducting a Systematic Literature Review of Information Systems Research,"This working paper has been thoroughly revised and superseded by two distinct articles. The first is a revised and peer-reviewed version of the original article: Okoli, Chitu (2015), A Guide to Conducting a Standalone Systematic Literature Review. Communications of the Association for Information Systems (37:43), November 2015, pp. 879-910. This article presents a methodology for conducting a systematic literature review with many examples from IS research and references to guides with further helpful details. The article is available from Google Scholar or from the author's website. The second extension article focuses on developing theory with literature reviews: Okoli, Chitu (2015), The View from Giants’ Shoulders: Developing Theory with Theory-Mining Systematic Literature Reviews. SSRN Working Paper Series, December 8, 2015. This article identifies theory-mining reviews, which are literature reviews that extract and synthesize theoretical concepts from the source primary studies. The article demonstrates by citation analysis that, in information systems research, this kind of literature review is more highly cited than other kinds of literature review. The article provides detailed guidelines to writing a high-quality theory-mining review.",IS,https://www.semanticscholar.org/paper/bb65c7f3d778a108e39ab38fba8ae84191fc650c
Review: A Review of Culture in Information Systems Research: Toward a Theory of Information Technology Culture Conflict,"An understanding of culture is important to the study of information technologies in that culture at various levels, including national, organizational, and group, can influence the successful implementation and use of information technology. Culture also plays a role in managerial processes that may directly, or indirectly, influence IT. Culture is a challenging variable to research, in part because of the multiple divergent definitions and measures of culture. Notwithstanding, a wide body of literature has emerged that sheds light on the relationship of IT and culture. This paper sets out to provide a review of this literature in order to lend insights into our understanding of the linkages between IT and culture. We begin by conceptualizing culture and laying the groundwork for a values-based approach to the examination of IT and culture. Using this approach, we then provide a comprehensive review of the organizational and cross-cultural IT literature that conceptually links these two traditionally separate streams of research. From our analysis, we develop six themes of IT-culture research emphasizing culture's impact on IT, IT's impact on culture, and IT culture. Building upon these themes, we then develop a theory of IT, values, and conflict. Based upon the theory, we develop propositions concerning three types of cultural conflict and the results of these conflicts. Ultimately, the theory suggests that the reconciliation of these conflicts results in a reorientation of values. We conclude with the particular research challenges posed in this line of inquiry.",IS,https://www.semanticscholar.org/paper/6b8fe767239a34e25e71e99bd8b8a64f8279d7f4
PEGASIS : Power-Efficient Gathering in Sensor Information Systems,"Sensor network consisting of nodes with limited battery power and wireless communications are deployed to collect useful information from the field. The main idea in PEGASIS is for each node to receive from and transmit to close neighbors and take turns being the leader for transmission to the BS. This approach distributes the energy load evenly among the sensor nodes in the network. Sensor nodes are randomly deployed in the sensor field, and therefore, the i th node is at a random location. The nodes will be organized to form a chain, which can either be accomplished by the sensor nodes themselves using a greedy algorithm. The algorithm to resolve the unbalanced energy consumption problem caused by long distance data transmission of some nodes in a chain formed by the greedy algorithm.",IS,https://www.semanticscholar.org/paper/44444b8e6173d2b6c5b34fb09c0fd7228613cbef
Digital Innovation as a Fundamental and Powerful Concept in the Information Systems Curriculum,"The 50-year march of Moore's Law has led to the creation of a relatively cheap and increasingly easy-touse world-wide digital infrastructure of computers, mobile devices, broadband network connections, and advanced application platforms. This digital infrastructure has, in turn, accelerated the emergence of new technologies that enable transformations in how we live and work, how companies organize, and the structure of entire industries. As a result, it has become important for all business students to have a strong grounding in IT and digital innovation in order to manage, lead, and transform organizations that are increasingly dependent on digital innovation. Yet, at many schools, students do not get such grounding because the required information systems core class is stuck in the past. We present a vision for a redesigned IS core class that adopts digital innovation as a fundamental and powerful concept (FPC). A good FPC serves as both a foundational concept and an organizing principle for a course. We espouse a particularly broad conceptualization of digital innovation that allows for a variety of teaching styles and topical emphases for the IS core class. This conceptualization includes three types of innovation (i.e., process, product, and business model innovation), and four stages for the overall innovation process (i.e., discovery, development, diffusion, and impact). Based on this conceptualization, we examine the implications of adopting digital innovation as an FPC. We also briefly discuss broader implications relating to (1) the IS curriculum beyond the core class, (2) the research agenda for the IS field, and (3) the identity and legitimacy of IS in business schools.",IS,https://www.semanticscholar.org/paper/887a9749f45743717ac7d2efd3147e2984f95cfb
Principles of Geographical Information Systems for Land Resources Assessment,"Geographical information systems Data structures for thematic maps Digital elevation models Data input, verification, storage, and output Methods of data analysis and spatial modelling Data quality, errors, and natural variation: sources of error Errors arising through processing The nature of boundaries Classification methods Methods of spatial interpolation Choosing a geographical information system Appendices Index.",IS,https://www.semanticscholar.org/paper/a33c2bfd5fd7520efb0ee16b66b0eba352bf9771
A Systems Approach to Conduct an Effective Literature Review in Support of Information Systems Research,"This paper introduces a framework for conducting and writing an effective literature review. The target audience for the framework includes information systems (IS) doctoral students, novice IS researchers, and other IS researchers who are constantly struggling with the development of an effective literature-based foundation for a proposed research. The proposed framework follows the systematic data processing approach comprised of three major stages: 1) inputs (literature gathering and screening), 2) processing (following Bloom’s Taxonomy), and 3) outputs (writing the literature review). This paper provides the rationale for developing a solid literature review including detailed instructions on how to conduct each stage of the process proposed. The paper concludes by providing arguments for the value of an effective literature review to IS research.",IS,https://www.semanticscholar.org/paper/bacef572180a1b81b5b2222149fecbcc3b2c6fc6
Privacy in the Digital Age: A Review of Information Privacy Research in Information Systems,"Information privacy refers to the desire of individuals to control or have some influence over data about themselves. Advances in information technology have raised concerns about information privacy and its impacts, and have motivated Information Systems researchers to explore information privacy issues, including technical solutions to address these concerns. In this paper, we inform researchers about the current state of information privacy research in IS through a critical analysis of the IS literature that considers information privacy as a key construct. The review of the literature reveals that information privacy is a multilevel concept, but rarely studied as such. We also find that information privacy research has been heavily reliant on studentbased and USA-centric samples, which results in findings of limited generalizability. Information privacy research focuses on explaining and predicting theoretical contributions, with few studies in journal articles focusing on design and action contributions. We recommend that future research should consider different levels of analysis as well as multilevel effects of information privacy. We illustrate this with a multilevel framework for information privacy concerns. We call for research on information privacy to use a broader diversity of sampling populations, and for more design and action information privacy research to be published in journal articles that can result in IT artifacts for protection or control of information privacy.",IS,https://www.semanticscholar.org/paper/b98e0e42fb045bb920e0564e1a03d6e9a9448ec9
Geographic Information Systems for Geoscientists: Modelling with GIS,Chapter headings. Introduction to GIS. Spatial data models. Spatial data structures. Spatial data input. Visualization and query of spatial data. Spatial data transformations. Tools for map analysis: single maps. Tools for map analysis: map pairs. Tools for map analysis: multiple maps.,IS,https://www.semanticscholar.org/paper/ad7f1fac9ad0e7150a0c7cc6aaa1f79f3dc03dc0
Generalizing Generalizability in Information Systems Research,"Generalizability is a major concern to those who do, and use, research. Statistical, sampling-based generalizability is well known, but methodologists have long been aware of conceptions of generalizability beyond the statistical. The purpose of this essay is to clarify the concept of generalizability by critically examining its nature, illustrating its use and misuse, and presenting a framework for classifying its different forms. The framework organizes the different forms into four types, which are defined by the distinction between empirical and theoretical kinds of statements. On the one hand, the framework affirms the bounds within which statistical, sampling-based generalizability is legitimate. On the other hand, the framework indicates ways in which researchers in information systems and other fields may properly lay claim to generalizability, and thereby broader relevance, even when their inquiry falls outside the bounds of sampling-based research.",IS,https://www.semanticscholar.org/paper/bc5e20c9e950a5dcedbe1caacc39afe097e3a6b0
Kriging: a method of interpolation for geographical information systems,"Geographical information systems could be improved by adding procedures for geostatistical spatial analysis to existing facilities. Most traditional methods of interpolation are based on mathematical as distinct from stochastic models of spatial variation. Spatially distributed data behave more like random variables, however, and regionalized variable theory provides a set of stochastic methods for analysing them. Kriging is the method of interpolation deriving from regionalized variable theory. It depends on expressing spatial variation of the property in terms of the variogram, and it minimizes the prediction errors which are themselves estimated. We describe the procedures and the way we link them using standard operating systems. We illustrate them using examples from case studies, one involving the mapping and control of soil salinity in the Jordan Valley of Israel, the other in semi-arid Botswana where the herbaceous cover was estimated and mapped from aerial photographic survey.",IS,https://www.semanticscholar.org/paper/381b57d3dac0a4618c7cc728fe4d4183ccf806c0
Information Systems Innovation for Environmental Sustainability,"Human life is dependent upon the natural environment, which, most would agree, is rapidly degrading. Business enterprises are a dominant form of social organization and contribute to the worsening, and enhancement, of the natural environment. Scholars in the administrative sciences examine questions spanning organizations and the natural environment but have largely omitted the information systems perspective. We develop a research agenda on information systems innovation for environmental sustainability that demonstrates the critical role that IS can play in shaping beliefs about the environment, in enabling and transforming sustainable processes and practices in organizations, and in improving environmental and economic performance. The belief-action-outcome (BAO) framework and associated research agenda provide the basis for a new discourse on IS for environmental sustainability.",IS,https://www.semanticscholar.org/paper/fa4ac6c1304a408b430f9951a35efb642c28b4a4
A Framework and Guidelines for Context-Specific Theorizing in Information Systems Research,"This paper discusses the value of context in theory development in information systems IS research. We examine how prior research has incorporated context in theorizing and develop a framework to classify existing approaches to contextualization. In addition, we expound on a decomposition approach to contextualization and put forth a set of guidelines for developing context-specific models. We illustrate the application of the guidelines by constructing and comparing various context-specific variations of the technology acceptance model TAM---i.e., the decomposed TAM that incorporates interaction effects between context-specific factors, the extended TAM with context-specific antecedents, and the integrated TAM that incorporates mediated moderation and moderated mediation effects of context-specific factors. We tested the models on 972 individuals in two technology usage contexts: a digital library and an agile Web portal. The results show that the decomposed TAM provides a better understanding of the contexts by revealing the direct and interaction effects of context-specific factors on behavioral intention that are not mediated by the TAM constructs of perceived usefulness and perceived ease of use. This work contributes to the ongoing discussion about the importance of context in theory development and provides guidance for context-specific theorizing in IS research.",IS,https://www.semanticscholar.org/paper/6e182239faa15cbfa1620810d4a6df85e185a4ce
Rigor in Information Systems Positivist Case Research: Current Practices,"Case research has commanded respect in the information systems (IS) discipline for at least a decade. Notwithstanding the relevance and potential value of case studies, this methodological approach was once considered to be one of the least systematic. Toward the end of the 1980s, the issue of whether IS case research was rigorously conducted was first raised. Researchers from our field (e.g., Benbasat et al. 1987; Lee 1989) and from other disciplines (e.g., Eisenhardt 1989; Yin 1994) called for more rigor in case research and, through their recommendations, contributed to the advancement of the case study methodology. Considering these contributions, the present study seeks to determine the extent to which the field of IS has advanced in its operational use of case study method. Precisely, it investigates the level of methodological rigor in positivist IS case research conducted over the past decade. To fulfill this objective, we identified and coded 183 case articles from seven major IS journals. Evaluation attributes or criteria considered in the present review focus on three main areas, namely, design issues, data collection, and data analysis. While the level of methodological rigor has experienced modest progress with respect to some specific attributes, the overall assessed rigor is somewhat equivocal and there are still significant areas for improvement. One of the keys is to include better documentation particularly regarding issues related to the data collection and analysis processes.",IS,https://www.semanticscholar.org/paper/2ec98ae0aef0ff983d232969e23fb28e09000410
Information Systems: Towards a System of Information Systems,"Information Systems are viewed as a set of services creating a workflow of information directed to specific groups and members. This allows individuals to share ideas and their talents with other members. In such manner, tasks can be carried out both efficiently and effectively. Due to the nature of Information Systems that revolves around creating information useful to users, and in some higher forms of Information Systems creating knowledge, management of information and/or knowledge is part of their functionalities. In this paper we aim to study the placement of Information Systems as part of a System of Systems (SoS), as these large systems poses significant technical improvement in terms of information interoperability that overcomes conceptual and technical barriers. Therefore, we move towards defining and modeling System of Information Systems (SoIS). This paper discovers what is currently known about Information Systems and Systems of Systems, and proceeds towards suggesting an architecture of a System of Information Systems that integrates several Information Systems and allows information to be transferred at ease between those different components.",IS,https://www.semanticscholar.org/paper/d5a1138a2163ea9fe3f1009220e7f4e0e7a878a7
User Awareness of Security Countermeasures and Its Impact on Information Systems Misuse: A Deterrence Approach,"Intentional insider misuse of information systems resources (i.e., IS misuse) represents a significant threat to organizations. For example, industry statistics suggest that between 50%--75% of security incidents originate from within an organization. Because of the large number of misuse incidents, it has become important to understand how to reduce such behavior. General deterrence theory suggests that certain controls can serve as deterrent mechanisms by increasing the perceived threat of punishment for IS misuse. This paper presents an extended deterrence theory model that combines work from criminology, social psychology, and information systems. The model posits that user awareness of security countermeasures directly influences the perceived certainty and severity of organizational sanctions associated with IS misuse, which leads to reduced IS misuse intention. The model is then tested on 269 computer users from eight different companies. The results suggest that three practices deter IS misuse: user awareness of security policies; security education, training, and awareness (SETA) programs; and computer monitoring. The results also suggest that perceived severity of sanctions is more effective in reducing IS misuse than certainty of sanctions. Further, there is evidence that the impact of sanction perceptions vary based on one's level of morality. Implications for the research and practice of IS security are discussed.",IS,https://www.semanticscholar.org/paper/db242cc2b524671419de4e5c3fba8cdfb872e436
Research on information systems failures and successes: Status update and future directions,"Information systems success and failure are among the most prominent streams in IS research. Explanations of why some IS fulfill their expectations, whereas others fail, are complex and multi-factorial. Despite the efforts to understand the underlying factors, the IS failure rate remains stubbornly high. A Panel session was held at the IFIP Working Group 8.6 conference in Bangalore in 2013 which forms the subject of this Special Issue. Its aim was to reflect on the need for new perspectives and research directions, to provide insights and further guidance for managers on factors enabling IS success and avoiding IS failure. Several key issues emerged, such as the need to study problems from multiple perspectives, to move beyond narrow considerations of the IT artifact, and to venture into underexplored organizational contexts, such as the public sector.",IS,https://www.semanticscholar.org/paper/459b8aee6c801d2869d22d19e2edc5cea134675c
Geographic Information Systems: An Introduction,Foreword. Preface. 1 Geographical Information Systems and Graphical Information. 2 Historical Development: Geographical Data and GIS. 3 From the Real World to GIS. 4 Basic Data Models. 5 Advanced Data Models. 6 Georeferencing Systems. 7 Hardware and Communication Technology for GIS Applications. 8 Basic Software and Databases for GIS. 9 Data Collection I. 10 Data Collection II. 11 Data Quality. 12 Database Implementation and Spatial Indexing. 13 Housekeeping Tools. 14 Basic Spatial Analysis. 15 Advanced Analysis. 16 Visualization. 17 Choosing a GIS: Organizational Issues. 18 Choosing a GIS: Technical Issues. 19 Standards and Geospatial Infrastructure. 20 Formal Problems in Establishing GIS. 21 A Vision for the Future. References. Index.,IS,https://www.semanticscholar.org/paper/f70ff542a8e52a333c05f54980255b51d6e5ac76
Predictive Analytics in Information Systems Research,"textabstractThis research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.",IS,https://www.semanticscholar.org/paper/bb02f492ee4cec6e7d52fb2832b2374c826536a4
Interpretation of Formative Measurement in Information Systems Research,"Within the Information Systems literature, there has been an emerging interest in the use of formative measurement in structural equation modeling (SEM). This interest is exemplified by descriptions of the nature of formative measurement (e.g., Chin 1998a), and more recently the proper specification of formatively measured constructs (Petter et al. 2007) as well as application of such constructs (e.g., Barki et al. 2007). Formative measurement is a useful alternative to reflective measurement. However, there has been little guidance on interpreting the results when formative measures are employed. Our goal is to provide guidance relevant to the interpretation of formative measurement results through the examination of the following six issues: multicollinearity; the number of indicators specified for a formatively measured construct; the possible co-occurrence of negative and positive indicator weights; the absolute versus relative contributions made by a formative indicator; nomological network effects; and the possible effects of using partial least squares (PLS) versus covariance-based SEM techniques. We provide prescriptions for researchers to consider when interpreting the results of formative measures as well as an example to illustrate these prescriptions.",IS,https://www.semanticscholar.org/paper/d7c0ac3d633de5c7548ae70ca3246052a357903b
"The Sociomaterialty of Information Systems: Current Status, Future Directions","Our motivation for putting together this special issue on “Sociomateriality of Information Systems and Organizing” was the mounting interest in the relationship between the social and the material, in the context of our increasingly digital society. The attention to this relationship is manifested in the emergence of studies of technology intended to augment and complement, but also and importantly, to question the received views on technology in social life (see Carlile et al. 2013a; Leonardi et al. 2012; Suchman, 2007).",IS,https://www.semanticscholar.org/paper/4ed8c1b3c51c644c04a32b278155c5ca65f149ab
Information Systems and Environmentally Sustainable Development: Energy Informatics and New Directions for the IS Community,"While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifically, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.",IS,https://www.semanticscholar.org/paper/1d606145a62a87c174dd19b78d79938cf02046c7
Interpreting Information Systems in Organizations,"From the Publisher: A coherent integrated source for an interpretive approach to understanding information systems in organizations to aid readers in their own processes of defining computer systems. Examines four major IS issues--strategy, evaluation, design and development, implementation. Features in-depth case studies to illustrate key points.",IS,https://www.semanticscholar.org/paper/fa625b7806f5a863b4b8d27cbac0b3d784c3c3da
Benefits and challenges of cloud ERP systems – A systematic literature review,"Enterprise Resource Planning (ERP) systems provide extensive benefits and facilities to the whole enterprise. ERP systems help the enterprise to share and transfer data and information across all functions units inside and outside the enterprise. Sharing data and information between enterprise departments helps in many aspects and aims to achieve different objectives. Cloud computing is a computing model which takes place over the internet and provides scalability, reliability, availability and low cost of computer reassures. Implementing and running ERP systems over the cloud offers great advantages and benefits, in spite of its many difficulties and challenges. In this paper, we follow the Systematic Literature Review (SLR) research method to explore the benefits and challenges of implementing ERP systems over a cloud environment.",IS,https://www.sciencedirect.com/science/article/pii/S2314728816300599
A Study on the Acceptance of Mobile-Banking Applications in India—Unified Theory of Acceptance and Sustainable Use of Technology Model (UTAUT),"This research makes an attempt to understand various factors that influence the adoption of mobile applications. Within the context of the “Unified theory of acceptance and use of technology” (UTAUT) modified model, considering the upcoming demand and increase in demand for mobile-banking applications, the researcher tried to explore the theoretical concept between random people of various states in India. The primary data was collected by preparing a questionnaire and circulating it using Google Forms. The collected data was further coded into Smart PLS 4 to understand the model and structural equation with reference to mobile-banking technological adoption and factors that had a significant impact. The conclusions derived from the study is that social influence, “effort expectancy”, and “trust” factors had a very strong influence on the “purchase intention”, whereas “effort” and “risk” factors had a negligible impact on purchase intent. It was also found that the UTAUT model is appropriate for evaluating the technological adoption of mobile-banking applications. With the advent of many players in the market and their unique banking management applications on mobile platforms, consumers are moving towards different third-party app than their origin bank in which they hold account. This has forced banking institutions to up the pace in the competition, introducing a lot of new features. It is also important to understand that, as a customer, there are a lot of attributes that he would be looking into for adoption. This paper is an attempt to understand the advancements in various variables that consumers would look at in the area of mobile-banking applications.",IS,https://www.mdpi.com/2071-1050/14/21/14506
Measuring e-learning systems success: Implementing D & M is success model,"Electronic learning (e-learning) has been widely used as a complement subject to traditional learning methods. Implementing e-learning is becoming one of many solutions in effecting the study process offered by higher educational institutions, such as Universitas Pembangunan Nasional “Veteran” Jakarta = UPNVJ and Sekolah Tinggi Teknologi Telematika Telkom (ST3 Telkom) Purwokerto, Central Java. The objective of this research is to measure the e-learning system success implemented by UPNVJ and ST3 Telkom Purwokerto and then compare the similarities and differences between of two. The modified DeLone & McLean Information Systems Success Model and part of Technology Acceptance Model are adopted in this study. The respondents are students who have been experience in using e-learning and are selected randomly from the Faculty of Computer Science of UPNVJ and from ST3 Telkom Purwokerto. The total respondents are 387, which consist of 215 students from UPNVJ and 172 from ST3 Telkom Purwokerto. For the results at UPNVJ, there are two hypotheses that were proven insignificant and at ST3 Telkom there are three hypotheses that were proven insignificant. Findings at UPNVJ show that the e-learning benefit can be explained by its independent variables by 53.8%%, and at ST3 Telkom by 60.6%. These percentages show that the predictors explained 53.8% for UPNVJ's e-learning benefit and 60.6% e-learning benefit for ST3 Telkom Purwokerto. These findings could be considered as novelties, because they are different from the original model.",IS,https://ieeexplore.ieee.org/abstract/document/7516343
Project management: a case study of a successful ERP implementation,"The success rate of enterprise resource planning (ERP) implementations is not high in view of the sums invested by organisations in these applications. It has often been indicated that a combination of inadequate preparedness and inappropriate project management have been responsible for the low‐success rate of ERP implementations. The purpose of this paper is to present a case study of a successful ERP implementation. In this paper, the authors use a case study of a very successful roll out of an ERP application in the Irish subsidiary of a UK multinational to investigate the validity of one of the most commonly cited project management frameworks, the project management body of knowledge (PMBOK), to ERP projects. Discussing each category of the framework in turn, the case data to illustrate where the PMBOK framework is a good fit or needs refining for ERP projects is used. It is found that, by and large, PMBOK, because it is a very broad framework, can shed light on most of the key aspects of an ERP project. However, the specificities of this type of project require a different emphasis on some of the factors, as discussed in the authors conclusions. The case analysis also raised some interesting insights into how companies evaluate the success of such highly complex change management initiatives. This research work will need to be extended to cover other case studies of ERP implementation across other industries and organisational contexts; for example in less tightly regulated industries and smaller organisations. This discussion will be of great value to ERP project managers who are in the early stages of a project and need to understand and anticipate the areas which will require specific attention on their part, based on their knowledge of the specific circumstances within their organisational context. This paper presents an investigation into the project management strategy adopted in the Pharma Inc. case and illustrates the mechanics of a successful ERP project implementation, categorised using the PMBOK framework.",IS,https://www.emerald.com/insight/content/doi/10.1108/17538370810846441/full/html
Mobile-based advisory services for sustainable agriculture: Assessing farmers’ information behavior,"Mobile-based advisory services have significant benefits, including access to agricultural information, knowledge sharing, meteorological information, marketing-related information, and financial services for smallholder farmers. This study aimed to assess farmers’ information behavior regarding mobile-based advisory services and how farmers with different characteristics and attitudes access and adopt information. Data were collected from 382 farmers in Dakhalia governorate, Egypt. The most frequently received information was related to best agricultural practices, weather forecasts, seed varieties and treatment, and water management. Cluster analysis revealed that 47% of the farmers had low information behavior. Seventy-one of the respondents had a favorable attitude toward information retrieval from mobile agricultural services. The information behavior groups of the farmers significantly differed in education, farm size, diversity of agricultural production, and attitude regarding trust and quality of the information provided. Information behavior among farmers has useful implications for policymakers in supporting the long-term benefits of mobile-based advisory services.",IS,https://journals.sagepub.com/doi/abs/10.1177/0266666920967979
Factors influencing decision support system acceptance,"While clinical DSS have many proven benefits, their uptake by GPs (general practitioners) is limited. The purpose of this research was to develop and explore a UTAUT (Unified Theory of Acceptance and Use of Technology) based model of how and why GPs accept DSS. Insight into the reasons why GPs do not use clinical DSS combined with knowledge of why GPs use DSS will allow the development of strategies to facilitate more widespread adoption with consequent improvements across many areas. Depth interviews were conducted with 37 GPs comprising a mix of education backgrounds, experience and gender. The developed model indicated that four main factors influence DSS acceptance and use including usefulness (incorporating consultation issue, professional development and patient presence), facilitating conditions (incorporating workflow, training and integration), ease of use and trust in the knowledge base.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0167923612002539
Critical success factors for ERP implementation in SMEs,"ERP implementation is regarded as complex, cumbersome and costly, and, very often, it exceeds the initial estimated resources. The process involves a thorough examination of the business processes in the organisation; selection of the best available software solution that matches the requirements of the enterprise; configuration of the selected systems;, training of staff; and customisation of the selected software solutions including development of required interfaces. Finally, the existing MIS of the organisation is replaced totally or partially by the new system. All the implementation processes should be carried out without affecting the daily operations across the whole enterprise. This can only be achieved by having an understanding of the key elements forming the infrastructure of the organisation, an effective plan for the implementation and an effective procedure to measure and evaluate the project throughout the implementation process. This paper presents the results of a study to identify and analyse the interrelationships of the critical issues involved in the implementation of ERP in small and medium sized enterprises (SMEs). Three basic research questions were addressed. First, what are the main critical success factors? Second, how do these factors interact throughout the implementation process? Third, which factors have their highest impact and in what stages? In order to answer these questions, over 50 relevant papers were critically reviewed to identify the main critical success factors (CSFs) for ERP implementation in large organisations. Then, the applicability of the identified CSFs to SMEs was investigated. Next, an industrial survey was also undertaken to identify which CSF has highest impact in what stages. The findings on relationships of the critical success factors have been utilised to develop a tool to monitor, and eventually improve, ERP implementations for SMEs. In the development of the tool, eight people from industry and academia with experience of ERP implementations were interviewed with the aim of validating the model being developed. The overall results provide useful pointers to the interplay of organisational and operational factors for the successful implementation of ERP.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0736584512000658
The effects of remote work on collaboration among information workers,"The coronavirus disease 2019 (COVID-19) pandemic caused a rapid shift to full-time remote work for many information workers. Viewing this shift as a natural experiment in which some workers were already working remotely before the pandemic enables us to separate the effects of firm-wide remote work from other pandemic-related confounding factors. Here, we use rich data on the emails, calendars, instant messages, video/audio calls and workweek hours of 61,182 US Microsoft employees over the first six months of 2020 to estimate the causal effects of firm-wide remote work on collaboration and communication. Our results show that firm-wide remote work caused the collaboration network of workers to become more static and siloed, with fewer bridges between disparate parts. Furthermore, there was a decrease in synchronous communication and an increase in asynchronous communication. Together, these effects may make it harder for employees to acquire and share new information across the network.",IS,https://www.nature.com/articles/s41562-021-01196-4
Blockchain technology in supply chain management: an empirical study of the factors affecting user adoption/acceptance,"Blockchain overcomes numerous complicated problems related to confidentiality, integrity, availability of fast and secure distributed systems. Using data from a cross-sectoral survey of 449 industries, we investigate factors that hinder or facilitate blockchain adoption in supply chains. To capture the most vital aspects of blockchain adoption in supply chains, our conceptual model integrates the unified theory of acceptance and use of technology (UTAUT) model with the task-technology fit (TTF) and information system success (ISS) models, with trust-based information technology innovation adoption constructs. Using structural equation modelling, we find that the ISS, TTF, and UTAUT models positively influence the key factors affecting supply chain employees’ willingness to adopt blockchain. Our results show that the UTAUT’s social influence factor has no significant effect on the intention to adopt blockchain, while inter-organisational trust has a significant effect on the relationship between the UTAUT dimension and intention to adopt blockchain.",IS,https://link.springer.com/article/10.1007/s10586-020-03200-4
Towards business intelligence systems success: Effects of maturity and culture on analytical decision making,"The information systems (IS) literature has long emphasized the positive impact of information provided by business intelligence systems (BIS) on decision-making, particularly when organizations operate in highly competitive environments. Evaluating the effectiveness of BIS is vital to our understanding of the value and efficacy of management actions and investments. Yet, while IS success has been well-researched, our understanding of how BIS dimensions are interrelated and how they affect BIS use is limited. In response, we conduct a quantitative survey-based study to examine the relationships between maturity, information quality, analytical decision-making culture, and the use of information for decision-making as significant elements of the success of BIS. Statistical analysis of data collected from 181 medium and large organizations is combined with the use of descriptive statistics and structural equation modeling. Empirical results link BIS maturity to two segments of information quality, namely content and access quality. We therefore propose a model that contributes to understanding of the interrelationships between BIS success dimensions. Specifically, we find that BIS maturity has a stronger impact on information access quality. In addition, only information content quality is relevant for the use of information while the impact of the information access quality is non-significant. We find that an analytical decision-making culture necessarily improves the use of information but it may suppress the direct impact of the quality of the information content.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0167923612002321
Challenges of Digital Transformation: The Case of the Non-profit Sector,"Nonprofit organizations (NPOs) are critical to the quality of life in many communities not only due to the valuable services and social impact they create, but also because of the positive economic impact within local communities. However, NPOs, just as for-profits, need to innovate in response to changing customer demands and lifestyles and to capitalize on opportunities offered by technology and changing marketplaces, structures and dynamics. Digitalization is essential to fuel NPO's innovation in order to be a differentiator in the highly competitive environment. In this paper, we first develop a review to identify the challenges of digital transformation and then we examine some of the challenges that the nonprofit sector faces in undertaking digital transformation initiative",IS,https://ieeexplore.ieee.org/abstract/document/8607762
Motivation or Manipulation? Dark UX and Persuasive Elements in Mobile Game Advertisements,"As we live in the modern time and use social media, the amount of advertisements we get exposed to is increasing more and more daily (Andrews, Van Leeuwen, & Van Baaren, 2013, p. 8), the amount of mobile game ads are parallelly increasing in social media and digital landscape. These persuasive lines can cause UX (User Experience) to become a manipulative practice and turn into Dark UX, known as ”Dark Patterns” (Brignull, 2013). This thesis addresses the Dark UX patterns employed in mobile game advertising, including persuasive techniques and deception elements, and it finds out different types of Dark UX strategies in mobile game advertising. These include explicit psychological tricks and Dark UI methods incorporated within the in-game or in-app ad frameworks. These techniques are discussed concerning empiricism, with detailed descriptions of the complex maneuvers employed. The work analyzes the underlying patterns within which the dark UX elements are hidden for the content/narrative and the mobile game UI designs. The inquiry does not just identify problems but also provides critique and design ethics. By analyzing user experiences, interviews, surveys n = 125, and empirical data, the study sheds light on the immediate effectiveness of these tactics compared to their long-term effects: reduced consumer confidence, and more remarkable instances of advertising aversion. It shows a need to switch to honest, user-focused advertising approaches and friendlier UX and UI. This thesis also contributes significantly to the current discourse on digital ethics and consumer protection in an era dominated by online advertising. It underscores the necessity of perpetually updated surveillance mechanisms since digital environments and user behavior constantly change. Besides, this study calls out to the advertisers and developers of mobile games to reassess their ethics and reorganize their advertising strategies to enhance users’ rights and improve the user experience in the short and long term.",IS,https://www.wineme.uni-siegen.de/wp-content/uploads/2024/03/EnginTeymur_Thesis_1546923-Kopie_geschwaerzt.pdf
Causes influencing the effectiveness of the post‐implementation ERP system,"This article aims to find a chain of causal relations affecting the operating effectiveness of the implemented enterprise resource planning (ERP) system instead of focusing on either the evaluation of software/vendors/consultants or critical successful factors (CSF) identification for ERP implementation, a course followed by the dominant ERP literature. This article is a process‐oriented approach and aims to give a moving picture of how one step affects another step from pre‐implementation stage, to during‐implementation stage, and to post‐implementation stage. A significant insight learned from this study is that end‐users across the organization must be educated from the onset of ERP implementation. Although education is a corner‐stone of ERP implementation, the user training is usually only emphasized and the courses are centered on computer/system operation rather than on understanding the ERP concept and spirit. This article may be interesting to some academic researchers and practical managers, and hopefully can provide a link/step for advanced researches in exploring post‐implementation ERP.",IS,https://www.emerald.com/insight/content/doi/10.1108/02635570510575225/full/html
Investigating the use of data-driven artificial intelligence in computerised decision support systems for health and social care: A systematic review,"There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action.",IS,https://journals.sagepub.com/doi/full/10.1177/1460458219900452
Application of social media analytics in tourism crisis communication,"Given the newly established communication environment of social media and highly unpredictable crisis situations, this study questioned how tourists facing an unexpected crisis situation use social media to communicate and search for information. To this end, this study developed a multi-phased social media analytic framework (data crawling, data processing and text mining, social network analysis, semantic network analysis, and network visualization) to assess the structure of information exchanges between the members of a tourism organization’s social network community and identified influential actors and information content within the social network. This study’s findings suggest genuine ways of relating with and utilizing opinion leaders and influencers in social media marketing communication as well as crisis communication. The authors expect this proposed methodological framework of social media analytics to help other scholars scientifically identify and implement the proper methodologies for utilizing social media data.",IS,https://www.tandfonline.com/doi/abs/10.1080/13683500.2018.1504900
Understanding the acceptance of teleconferencing systems among employees: An extension of the technology acceptance model,"Employing the framework of the technology acceptance model (TAM), the present study investigates the factors that affect employees’ acceptance and use of teleconferencing systems for work-related meetings in business settings. Based on survey data of 155 working professionals, a path analysis confirmed the key propositions of TAM. Importantly, the results also showed that both individual factors such as anxiety and self-efficacy, and institutional factors such as institutional support and voluntariness were significantly related to perceived ease of use (PEOU), perceived usefulness (PU), and actual use of the systems. By examining teleconferencing that typically involves group communication within organizations, this study contributes to theoretical refinement of group-based technology use and adoption. Theoretical and practical implications are discussed.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0747563214003355
Implementing enterprise resource planning systems with total quality control and business process reengineering: Survey results,"The primary objective of an enterprise resource planning (ERP) system is to help integrate an organization's business operations and processes effectively and efficiently. Not all firms have been successful in their ERP implementations and to that end research has helped to identify many factors that might be critical to a successful implementation. Such factors as the use of business process reengineering (BPR), and establishing a total quality management (TQM) culture have all shown to play important roles in ERP implementation. The focus of this survey research on US electronic manufacturing firms is to identify successful integration sequences of TQM and BPR with ERP. The findings reveal that both the sequence of implementation and the strategies selected to initiate ERP systems can significantly impact business performance successfulness.",IS,https://www.emerald.com/insight/content/doi/10.1108/01443570310467339/full/html
Building better global data governance,"In this article, we explore the challenges of global governance and the particular challenge presented by global data governance. We discuss a range of challenges to developing meaningful global governance institutions for regulating how companies and governments around the world manage and utilize consumer data. These challenges are compounded by their global nature and the complexities of Internet-based technologies. We argue that the following gaps exist for effective global data governance: (a) there is no overarching global framework for protecting consumer data, and it is partial and incomplete; (b) there is a lack of data protection for international data transfers, as much of the regulation that is being developed is not global in scale; and (c) new areas of data collection and use compound concerns to effective data governance in a globalized digital world. Moreover, we highlight important needs in terms of both global governance and impending challenges related to current and new uses of data. Any global governance framework should recognize the need for an iterative process where communication is ongoing between the necessary stakeholders. Agreements should incorporate common goals to maximize the potential development of global data governance norms. However, goals must remain flexible to the different data environments across nation-states while maintaining a global scope to ensure data protection. In addition, any agreement should consider the emerging challenges in this area. These challenges include new methods of data collection and use, as well as protecting individuals from manipulation and undue influence based on how their data are being used, processed, and collected.",IS,https://www.cambridge.org/core/journals/data-and-policy/article/building-better-global-data-governance/511E3D797EEBBAD32866F83792F6B89C
A decision support system to improve the efficiency of resource allocation in healthcare management,"Limitations in healthcare funding require hospitals to find more effective ways to utilize resources. An effective patient management system is critically dependent on the accurate analysis of individual patient outcomes and resource utilization. In the current paper, a management-oriented decision support model is thus proposed to assist health system managers in improving the efficiency of their systems. In the first stage of the model, the key variables affecting system efficiency, as well as their causal relationships, are identified through causal maps. Efficiency is measured by the total time spent in the system. In the second stage, a Bayesian Belief Network (BBN) is employed to represent both the conditional dependencies and uncertainties of the key variables. In the third stage, a sensitivity analysis is performed using a BBN to determine the most critical variable(s) in terms of impact on the system. Finally, strategies to improve system efficiency are proposed. The suggested decision support system is applied to the tomography section in the radiology department of a private hospital in Turkey.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0038012105000480
Factors Affecting Organizations’ Resistance to the Adoption of Blockchain Technology in Supply Networks,"From a supply chain perspective, new technologies such as blockchain can improve the efficiency and competitiveness of logistics and increase customer satisfaction. Although blockchain technology has been lauded as a way for firms to build sustainable supply chain networks, the rate of acceptance of this technology remains low. Therefore, this study seeks to identify the factors that discourage firms from merging blockchain with the supply chain. Instead of providing further reasons for adopting blockchain technology, we try to understand what deters firms from adding blockchain to their operations. Following the deductive approach, a confirmatory factor analysis is conducted on pre-test questionnaires to test, improve, and verify the constructs (questions) to measure the hypothesized factors. A theoretical model is proposed based on the hypotheses, and structural equation modeling is applied. The results are estimated using the partial least squares approach and a sample of 83 respondents. Our findings based on our empirical data support most of our hypotheses. We find that various factors impede the adoption of blockchain technologies, including technological barriers, constraints rooted in organizations and the environment, and system-related governmental barriers. In addition, various factors are critical determinants of resistance to blockchain in the technological, organizational, and environmental dimensions.",IS,https://www.mdpi.com/2071-1050/12/21/8882
Digital financial inclusion: A gateway to sustainable development,"The covid-19 pandemic revolutionises digital financial services, and hence digital financial inclusion is essential to ensure everyone can access digital financial services and thus promote sustainable economic growth. The development and activities promoting digital financial inclusion must align and help attain 2030 Sustainable Development Goals (SDGs). While the pandemic is anticipated to increase the usage of digital financial services, it has also created challenges for certain countries. Hence, a systematic literature review explores digital financial inclusion across countries. This research finds that developing countries, mainly Asian countries, embrace and improve digital financial inclusion to help reduce poverty. However, the results indicate that in developing countries, a persistent divide exists between gender, the wealthy and the poor, and urban and rural areas regarding access to and usage of digital financial services. At the end of the study, we propose a few recommendations, focusing on improving digital infrastructure, simplifying the complicated banking procedures, and stressing the importance of financial education, enabling the smooth implication of digital financial inclusion across countries.",IS,https://www.cell.com/heliyon/fulltext/S2405-8440(22)01054-4
Disaster and crisis management in Turkey: a need for a unified crisis management system,"Crisis management has gained importance in the policy agendas of many countries around the world due to the increases in the number of natural disasters and terrorist attacks. This paper illustrates how the Turkish Government’s Disaster and Crisis Management System has been developed and makes a qualitative evaluation of the current disaster and crisis management systems. A literature review shows that the disaster and crisis management system in Turkey has been developed after tragic events. The paper examines what kinds of initiatives were introduced and what trends in shift have occurred. After analyzing recent cases and exploring some government initiatives, alternative approaches and suggestions are included. Turkey has developed its disaster and crisis management system since 1930, which mostly depended on experiences. The current system is governed by a centralized structure that is the responsibility of different ministries. Nonetheless, the system is very weak at the local level. Furthermore, participation of non-profit organizations is very limited at both national and local levels. Thus, coordination and management of first-response operations during crises are problematic and ineffective. Particularly, the system is not designed for different types of crises such as terrorist attacks. Crisis management in Turkey needs a more unified and flexible structure to deal with current problems effectively. Further suggestions for better implementation are also provided. The effectiveness of the disaster and crisis management system is analyzed in natural and man-made disasters. Findings show that centralized and decentralized systems have different functions in different situations.",IS,https://www.emerald.com/insight/content/doi/10.1108/09653561011037977/full/html
Knowledge management system for fourth generation R&D: KNOWVATION,"Since the advent of the embryonic model almost a century ago, R&D systems have gone through an evolutionary process of development that can be classified into three generations. Today, the fourth generation of R&D is emerging that emphasizes both strategic and operational importance of knowledge management (KM). Despite the importance of KM, the network between conceptual scheme of the fourth generation R&D and practical system of KM remains a missing link. In response, the main objective of this paper is to present a framework for designing and implementing knowledge management system (KMS) for the fourth generation R&D. The proposed system is named KNOWVATION, which combines the notions of knowledge and innovation. First, the evolutionary classification of the R&D generations and the corresponding characteristics of the respective generations are defined. Second, the organizational structure and knowledge functions of the fourth generation R&D are derived. Finally, the overall design framework and detailed sub-modules are presented.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0166497204001877
From conventional governance to e-democracy: Tracing the evolution of e-governance research trends using network analysis tools,"The adoption of e-governing practices has revolutionised the administrative machinery of governments worldwide by improving efficiency, transparency, and accountability. Researchers and administrators often aim to identify emerging research fronts and the timeline of the evolution to forecast and implement technology. In this work, we systematically investigate the trajectory of the global evolution and emerging research fronts as well as the prospects for e-governance using citation network analysis. The growth curve fitted to the number of articles published per year shows that the research activities are still in the ascendant phase. We visualise the global main path of the citation network and investigate the patterns to trace the knowledge diffusion path, major milestones, and emerging research fronts. The cluster analysis identifies the major topics of research as administration and information system management, e-governance framework design, efficiency or quality evaluation, and the application of social networks and open data leading to e-democracy. The adoption of open data and social networking for user interactions with government that leads to participatory governance are the emerging research trends. We also identify research that can have a future impact based on network parameters. The results contribute to the literature by setting the focus of future research, and assisting administrators in selecting suitable models and methodologies, and manufacturers with the development of required technical devices suitable for the upcoming phase of symbiosis.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0740624X17303192
Artificial Intelligence-Augmented Decision-Making: Examining the Interplay Between Machine Learning Algorithms and Human Judgment in Organizational Leadership,"The paper discusses the bright and dark sides of the relationship between human judgment and AI-driven machine learning (ML) algorithms. While discussing important issues, such as algorithm aversion, automation bias, and trust, it probes into how AI improves decision-making efficiency through predictive accuracy, resource optimisation, and data-driven insights. Even as AI can revolutionise decision-making, its effective integration must balance algorithmic output and human judgment. The most critical challenges include automation bias resulting from over-reliance on advice given by AI and algorithm aversion driven by concerns related to AI failures. Open systems, explainable AI (XAI) frameworks, and user-centered design can help to engender confidence in AI systems and alleviate these issues. Accountability, equity, and prejudice concerns raise further ethical considerations with AI. The study proposed several tactics that might mitigate such challenges: audits of ethics, adherence to legal policy, and integration of the AI systems with the company’s values. It underlines the human-AI collaboration that will be increasingly necessary, as well as hybrid models for decision-making that bring algorithmic accuracy to human intuition. It follows the case study review and empirical findings with practical lessons for organisational leaders on ethics, best deployment practices for AI, and tactical ways to engender better collaboration and trust. The conclusion outlines the need to enhance the explainability features of AI, study cognitive dynamics in decision processes, and work out ethical schemata guiding leading positions for AI. Beyond providing a roadmap for organisations to leverage the interaction of human judgment and machine intelligence to drive and achieve more ethical and effective leadership outcomes, this paper tries to contribute to the ongoing debate on AI-augmented decision-making.",IS,https://ecohumanism.co.uk/joe/ecohumanism/article/view/6364
A Blockchain Ecosystem for Digital Identity: Improving Service Delivery in Canada’s Public and Private Sectors,"Blockchain-based solutions have the potential to make government operations more efficient and improve the delivery of services in the public and private sectors. Identity verification and authentication technologies, as one of the applications of blockchainbased solutions – and the focus of our own efforts at SecureKey Technologies – have been critical components in service delivery in both sectors due to their power to increase trust between citizens and the services they access. To convert trust into solid value added, identities must be validated through highly-reliable technologies, such as blockchain, that have the capacity to reduce cost and fraud and to simplify the experience for customers while also keeping out the bad actors. With identities migrating to digital platforms, organizations and citizens need to be able to transact with reduced friction even as more counter-bound services move to online delivery. In this article, drawing on our own experiences with an ecosystem approach to digital identity, we describe the potential value of using blockchain technology to address the present and future challenges of identity verification and authentication within a Canadian context.",IS,https://timreview.ca/sites/default/files/article_PDF/Wolfond_TIMReview_October2017.pdf
Information systems and sustainable supply chain management towards a more sustainable society: Where we are and where we are going,"The objectives of this study are to identify and systematize scholarly articles on the use of information system to support sustainable supply chain management and to suggest future research opportunities. Therefore, a structured literature review was conducted. The most relevant studies identified were classified and categorized into seven dimensions: research context, research focus, research method, sector analyzed, information system (IS) beneficiaries, relationship between IS and green supply chain practices, and performance benefits. The main authors and articles on this particular topic were identified. In addition, it was concluded that IS is an important support tool for sustainable supply chain management practices since it brings benefits to the organization, suppliers, and customers. Furthermore, IS positively influences the operational, financial, and environmental performance of the organization. However, further advances in the literature are still needed. The major contribution of this research is related to the recommendations that provide opportunities for future research.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0268401216301578
Ensuring information security in the field of remote work,"Remote work is a forced measure introduced by employers in order to prevent a viral infection. For employees, there are pluses in remote work - saving time and money on the road and high labour productivity because nothing distracts. There is a separate issue of information security for the employer when organising such a work regime for their employees. Any use of materials is allowed only with a hyperlink. Nowadays, in the realities of distance work, information security is coming first. Employees send all the information online; they use their data and send confidential information. Protection of personal data becomes a crucial point. The article deals with problems of ensuring information security in the field of remote work. The problems of information security during restrictive actions in connection with the coronavirus pandemic and the transfer of personnel to remote work are discussed. The threat of information leaks through remote workers is relatively high since the specialists responsible for the organisation's information security do not have the opportunity to apply the entire arsenal of technical means and policies, with the help of which security is ensured at workstations in the office. Information leakage will lead to severe problems, so it is essential to consider what means you can use to ensure the company's information security.",IS,https://iopscience.iop.org/article/10.1088/1742-6596/2210/1/012008/meta
Information systems integration in mergers and acquisitions: A normative model,"The role of Information Systems in Mergers and Acquisitions (M&A) becomes increasingly important as the need for speed of reaction and information is growing. A model for IS integration in M&A is presented; this includes the categories and strategic objectives of external growth as well as consideration of the possible choices for the hardware and software configuration after completion of the M&A. The basic variables of the model are defined and the possible IS integration strategies are classified in a matrix having, as dimensions, the software configuration and the computer architecture. The IS integration model resulting from these basic elements is divided into a descriptive model and a decision support model: the first one shows the relations among the involved variables, while the second provides a practical tool for decision-making. Both these models have been tested with a survey guided by a questionnaire; Factor Analysis was used to analyze the results.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0378720697000311
DEVELOPING ADAPTIVE LEARNING MANAGEMENT APPLICATION FOR PROJECT TEAM IN IT-INDUSTRY,"Keeping employees aligned with modern trends and developments in their professional areas is the main focus of a lifelong lea rning approach. That becomes even more important for such dynamic industries like Information Technology. Therefore, it’s crucial to extend existing e-learning management system with an adaptive training component that enables the effective study of on-demand skills, leading to a broader range of candidates available for project management to select from and consequently improving the overall performance of an IT company. To improve the existing learning process according to company's and employee's needs the overview of a typical learning management system functionality is given in this paper. The main benefits from the adoption of a learning management system in small and medium-sized IT-companies were discussed, analysis of their features and problems was given. The adjustment plan for the typical learning management system to be suitable for the information technology domain including module of the adaptive learning content selection using the basic principles of graph theory was proposed to reduce the time of the learning process. LMS OpenOLAT was reworked according to the adjustment plan which is reflected by the number of diagrams such as sequence diagram, IDEF0 business process description, activity diagram that shows search algorithm steps and application component diagram. Also, GUI was adjusted to provide user with a good look and feel. The benefits of the proposed approach in business processes of IT-company are shown using “Academy – Smart”. To prove the efficiency of the proposed algorithm, real courses were used. Based on the learning material, provided by “Coursera”, a number of test cases was formed and analyzed. After applying adaptive content selection algorithm according to the models of “Academy – Smart” employees, learning time was reduced and optimized. This investigation has shown significant improvement in the resource management process and acceleration of the learning process for employees.",IS,http://samit.khpi.edu.ua/article/view/2079-0023.2020.01.17
Rethinking Artificial Intelligence: Algorithmic Bias and Ethical Issues| How Process Experts Enable and Constrain Fairness in AI-Driven Hiring,"Organizations risk losing their competitive edge as they struggle to find and hire qualified talent. Hiring personnel turn to artificial intelligence (AI) tools to help acquire talent, increase efficiency, and reduce costs. Yet despite the best intentions for integrating fair and evidence-based systems, exacerbated levels of bias may occur from using these tools. Drawing from scholarship on process expertise and emerging practices of AI use at work, I provide a case study of 42 high-volume recruiters and uncover how hiring personnel enact and justify unsystematic sourcing practices within the confines of their held expertise, organizational demands, and technology choices. I explain how AI-based hiring decisions in organizations are context dependent and blend the capabilities of algorithmic-powered tools with choices and judgments made by process experts. I conclude by offering theoretical and practical considerations for expertise, hiring, and the integration of algorithms at work.",IS,https://ijoc.org/index.php/ijoc/article/view/20812
MES-integrated digital twin frameworks,"Industry 4.0-based manufacturing systems are equipped with Cyber-Physical Systems that are characterized by a strong interlinkage between the real world and the digital one: actions in one world have an impact on the other. In this paradigm, Digital Twins (DT) are defined as simulation models that are both getting data from the field and triggering actions on the physical equipment. However, most of the claimed DT in literature are only replicating the real system in a synchronized fashion, without feeding back actions on the control system of the equipment. In literature, these are referred to as Digital Shadows (DS). The paper proposes a way to integrate a DS simulation model with the Manufacturing Execution System (MES) in this way creating a DT. The MES-integrated DT is used for decision making thanks to the presence of an intelligence layer that hosts the rules and the knowledge to choose among alternatives. The paper proposes two frameworks based on the MES-integrated DT: one for managing error states and one for triggering disassembly processes as a consequence of low assembly quality. The DT simulation is developed and integrated with the MES of the Industry 4.0 Laboratory at the School of Management of Politecnico di Milano, where the proposed frameworks have been tested and validated.",IS,https://www.sciencedirect.com/science/article/abs/pii/S0278612520300716
An Organizational Culture-Based Theory of Clinical Information Systems Implementation in Hospitals,"We propose an organizational culture-based explanation of the level of difficulty of clinical information system (CIS) implementation and of the practices that can contribute to reduce the level of difficulty of this process. Adopting an analytic induction approach, we developed initial theoretical propositions based on a three-perspective conceptualization of organizational culture: integration, differentiation, and fragmentation. Using data from three cases of CIS implementation, we first performed a deductive analysis to test our propositions on the relationships between culture, CIS characteristics, implementation practices, and the level of implementation difficulty. Then, applying an inductive analysis strategy, we re-analyzed the data and developed new propositions. Our analysis shows that four values play a central role in CIS implementation. Two values, quality of care and efficiency of clinical practices, are key from an integration perspective; two others, professional status/autonomy and medical dominance, are paramount from a differentiation perspective. A fragmentation perspective analysis reveals that hospital users sometimes have ambiguous interpretations of some CIS characteristics and/or implementation practices in terms of their consistency with these four values. Overall, the proposed theory provides a rich explanation of the relationships between CIS characteristics, implementation practices, user values, and the level of difficulty of the implementation process.",IS,https://aisel.aisnet.org/jais/vol12/iss2/3/
Social media analytics: a tool for the success of online retail industry,"Social media has become a part of life for a larger segment of the people globally. Businesses are also taking advantage of it, to instantly and easily create and share information with the customers and get benefited from this seamless communication. As a result of the increase in both social media platforms and users, the need to monitor, extract, analyse and report the data being created on these platforms has also increased for the Businesses. Present work identifies the social media analytics (SMA) process and online retail application; it identifies the different phases of the SMA process with their integration to the online retail strategy, challenges, and opportunities. SMA metrics for customer life-cycle (acquisition to enhancement) are discussed. A comparative analysis has been presented for different SMA tools to identify their utility to the online retail.",IS,https://www.inderscienceonline.com/doi/abs/10.1504/IJSOI.2019.100630
Blockchain-based academic credential verification system,"In Ghana, academic record-keeping and verification methods have become increasingly burdensome, error-prone, and susceptible to data security, privacy, and fraud prevention issues. Traditional systems often rely on paper-based records or fragmented digital databases, which can lead to inefficiencies, human errors, and difficulties in authenticating and validating academic credentials. These challenges can hinder students' and graduates' efforts to pursue further education or employment opportunities and create administrative burdens for educational institutions and employers. Blockchain technology, an emerging innovation, offers a promising solution to address these challenges. By digitizing and securely storing academic records on a blockchain, the system will facilitate efficient, accurate, and tamper-proof verification of credentials. This will reduce errors and fraud and improve academic qualifications' overall reliability and credibility. Furthermore, the system will enhance data privacy by allowing individuals to control who can access their records and under what conditions. This applied project creates a user-friendly platform that simplifies the verification process for students, graduates, educational institutions, and employers. By harnessing the power of blockchain technology, we can revolutionize academic record-keeping and verification in Ghana, ensuring a more secure, efficient, and trustworthy system for all stakeholders involved.",IS,https://air.ashesi.edu.gh/items/0ce91721-a7ca-48d1-b0ef-bc8d22c70f8c
Adoption of cloud computing as innovation in the organization,"Over the years, there has been a heavy reliance on cloud computing as IT has innovated through time. In recent times cloud computing has grown monumentally. Many organizations rely on this technology to perform their business as usual and use it as a backbone of their companies' IT infrastructure. This paper investigates the organizational adaptation for cloud computing technology - reviewing case studies from various institutions and companies worldwide to provide a detailed analysis of innovative techniques with cloud computing. We investigate the features and delivery approaches cloud computing offers and the potential challenges and constraints we face when adopting cloud computing into the business setting. We also explore the cybersecurity elements associated with cloud computing, focusing on intrusion detection and prevention and understanding how that can be applied in the cloud. Finally, we investigate the future research directions for cloud computing and expand this paper into further articles with experiments and results.",IT,https://research.aston.ac.uk/en/publications/adoption-of-cloud-computing-as-innovation-in-the-organization
Deployment of Infrastructure and Services in the Open Grid Service Architecture,"The ability to deploy Grid infrastructure and services across organizational boundaries (rapidly, reliably, and scalably) is critical for the success of large-scale service based grids such as OGSA. We report the results of the UK-OGSA Evaluation Project infrastructure and services deployment experiments, and analytically compare application versus service deployment. The use of a 3rd party component deployment technology to remotely automate installation and service deployment is discussed, and outstanding problems and potential solutions and benefits are presented. We conclude that grid deployment must be treated as a first-order activity by integrating secure deployment capabilities into the middleware, to enable deployment of secured infrastructure and services across organizations.",IT,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e901707a82954217d53e431d24771df8249965ff
Exploring the deployment of software libraries in a digital innovation context,"This article reports on the deployment of such software libraries in the web and mobile (Android) contexts by 107 start-up companies in London. Our findings show that libraries owned by big-tech companies, product vendors, and communities coexist; that the deployment of big-tech libraries is unaffected by the scale of the deploying start-up; and that context evolution paths are consequential for library deployment. These findings portray a balanced picture of digital infrastructure as neither the community-based utopia of early open-source research nor the dystopia of the recent digital dominance literature. The study aims at addressing the following research question: How does the ownership of software libraries affect their deployment? A software library is a standard way of packaging code that becomes a boundary resource when it is offered outside of an organization. Software libraries are consistent with the bidirectional causality of boundary resources, as they are designed to advance the goals of both owners and users. However, the trade-offs underlying these resources are different for communities (innovation vs coherence), software vendors (complexity vs flexibility), and large platform owners (generativity vs control). These distinct trade-offs suggest that the deployment of software libraries is affected by who owns them: communities, proprietary software vendors, or big-tech providers. To answer this research question, we observe the deployment of web and mobile (Android) software libraries by 107 start-ups operating in London, one of the largest European digital innovation clusters. Our empirical analysis confirms that ownership is consequential for software library deployment, which is more significantly affected by the software development context (web or mobile) than by the scale of the deploying company.",IT,https://journals.sagepub.com/doi/pdf/10.1177/0268396220936705
Infrastructure as Software in Micro Clouds at the Edge,"Edge computing offers cloud services closer to data sources and end-users, making the foundation for novel applications. The infrastructure deployment is taking off, bringing new challenges: how to use geo-distribution properly, or harness the advantages of having resources at a specific location? New real-time applications require multi-tier infrastructure, preferably doing data preprocessing locally, but using the cloud for heavy workloads. We present a model, able to organize geo-distributed nodes into micro clouds dynamically, allowing resource reorganization to best serve population needs. Such elasticity is achieved by relying on cloud organization principles, adapted for a different environment. The desired state is specified descriptively, and the system handles the rest. As such, infrastructure is abstracted to the software level, thus enabling ""infrastructure as software"" at the edge. We argue about blending the proposed model into existing tools, allowing cloud providers to offer future micro clouds as a service.",IT,https://pubmed.ncbi.nlm.nih.gov/34770308/
Implementing Cloud-Based Enterprise Resource Planning Solutions in Small and Medium Enterprises,"Lacking strategies to implement a cloud-based enterprise resource planning (ERP) solution in small and medium-sized enterprises (SMEs) can lead to a failed implementation. SME owners can improve company performance by integrating company processes by successfully implementing a cloud-based ERP solution. Grounded in the diffusion of innovation theory augmented with business process management design for Six Sigma, the purpose of this qualitative multiple case study was to explore strategies SME owners use to implement cloud-based ERP solutions. The participants consisted of 4 SME owners in Lebanon who successfully implemented a cloud-based ERP solution and improved company performance and growth. Data were collected using semistructured interviews and a review of ERP implementation project documents and analyzed using thematic analysis. Seven themes emerged: top management support for IT implementation, requirements identification, software selection, user involvement, project management, change management, and post-implementation performance monitoring. A key recommendation for SME owners is to support IT implementation and remain involved throughout the implementation process. The implications for positive social change include the opportunity to increase employment opportunities, economic growth, and reducing the adverse environmental consequences of computing by using cloud-based technologies.",IT,https://scholarworks.waldenu.edu/cgi/viewcontent.cgi?article=10408&context=dissertations
Effects of IT Infrastructure Services on Business Process Implementation-Focus on Small and Medium Enterprises in Emerging Markets,"An organization’s information technology (IT) infrastructure capability is increasingly realized as a critical part to business effectiveness and efficiency. IT infrastructure services are particularly important for organizations looking to deploy business processes in developing markets. There has also been an interest from many small and medium sized organizations whose core business is not in IT to outsource and manage these services through third party service providers. However there is a need to create an understanding for these organizations to deploy the right infrastructure services in order to enable easier implementation or reengineering of the business process. There has been little research focusing on the patterns of the IT infrastructure capabilities in the small and medium sized organizations in the developing markets. The research aims for a comprehensive coverage by analyzing the requirements in the developing markets and proposing a selection model for the organizations to choose IT service provider in case they decide to outsource the infrastructure services. The effect of the IT infrastructure services on the business process implementation is presented with an emphasis on the boundary crossing services. Using empirical case study, the research analyses a firm in developing markets and compares it against four strategically similar organizations from different industries. Data collection was primarily qualitative and ably supported by secondary data. The requirements in developing markets reflect the same as in mature markets. The pricing is seen to play a major role in the selection of the service providers with service security not very much organization’s priority. The number of boundary crossing services effectively enables information sharing and control. These services are the drivers in simplifying the business process implementation. The findings have implications for both business and technical managers in regard to planning the IT strategy in the long term and developing appropriate infrastructure according to the process needs.",IT,https://www.diva-portal.org/smash/get/diva2:1312384/FULLTEXT01.pdf
B4: Experience with a Globally-Deployed Software Defined WAN (2013),"We present the design, implementation, and evaluation of B4, a private WAN connecting Google’s data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to fully utilize any available capacity, and iii) strict bandwidth and latency requirements that ensure reliable application performance. B4 leverages software-defined networking principles and centralized traffic engineering to achieve high utilization, while carefully managing network updates to avoid transient congestion. Our experiments show that B4 consistently operates at high utilization, carrying 60% more traffic than traditional approaches, and quickly adapts to traffic changes and failures with minimal impact on application performance.",IT,https://dl.acm.org/doi/10.1145/2534169.2486019
Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network (2015),"We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks; we built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites worldwide, scaling capacity by 100× over ten years to more than 1 Pbps of bisection bandwidth.",IT,https://dl.acm.org/doi/10.1145/2785956.2787508
SWAN: Achieving High Utilization with Software-Driven WAN (2013),"We present SWAN, a system that boosts the utilization of inter-datacenter networks by centrally controlling when and how much traffic each service sends and frequently reconfiguring the network’s data plane to match current demand. Done simplistically, such reconfigurations can cause transient congestion because different switches update at different times. We develop a novel technique leveraging a small amount of extra capacity on links to apply updates in a provably congestion-free manner, without assumptions on update ordering. Further, to scale to large networks with limited forwarding table space, SWAN greedily selects a small set of routes that best satisfy current demand. It updates this set without disrupting traffic by using small amounts of spare forwarding table capacity. Experiments on a testbed prototype and simulations on production network traces show that SWAN carries 60% more traffic than current practices.",IT,https://conferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p15.pdf
"ONOS: Towards an Open, Distributed SDN OS (2014)","We present our experiences building ONOS (Open Network Operating System), an experimental distributed SDN control platform motivated by the performance, scalability, and availability requirements of large operator networks. We describe and evaluate two ONOS prototypes. The first version implemented core features: a distributed, but logically centralized, global network view; scale-out; and fault tolerance. The second version focused on improving performance. Based on our experience with these prototypes, we identify additional steps required for ONOS to support use cases such as core network traffic engineering and scheduling, and to become a usable open-source, distributed network OS platform that the SDN community can build upon.",IT,https://dl.acm.org/doi/10.1145/2620728.2620744
ClickOS and the Art of Network Function Virtualization (2014),"Over the years middleboxes have become a fundamental part of today’s networks. Despite their usefulness, they come with a number of problems due to being hardware-based: they are costly, difficult to manage, and hard to change. To address these issues, there is a recent trend towards Network Function Virtualization (NFV), turning these middleboxes into software-based, virtualized entities. Towards this goal we introduce ClickOS, a high-performance, virtualized software middlebox platform. ClickOS virtual machines are small (~5 MB), boot quickly (~30 ms), add little delay (~45 µs), and over a hundred can run concurrently while saturating a 10 Gb pipe on commodity hardware. We implement a range of middleboxes (firewall, NAT, load balancer) and show that ClickOS can handle millions of packets per second, demonstrating its viability as an NFV platform.",IT,https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/martins
E2: A Framework for NFV Applications (2015),"We present E2, a framework for deploying and managing network function virtualization (NFV) applications at scale. E2 provides a unified system for operators to manage a wide range of network functions (NFs) using a logically centralized controller. It introduces a simple programming model for composing and scaling NFs, and an efficient execution substrate that places and chains NFs across servers. E2’s design balances centralized control with distributed data plane execution to achieve scalability and performance. In evaluations on a cluster testbed, E2 shows low overhead and can manage and scale complex NFV service chains across tens of servers while maintaining high packet processing throughput.",IT,https://doi.org/10.1145/2815400.2815423
"Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network Virtualization (2018)","This paper presents our design and experience with Andromeda, Google Cloud Platform’s network virtualization stack. Our production deployment poses challenges in performance, isolation, and feature velocity at scale. We describe Andromeda’s architecture, which uses a novel datapath leveraging hardware offloads and custom NIC features to achieve near-native performance for VMs while ensuring strong tenant isolation. We also share how Andromeda’s design and continuous evolution allowed rapid deployment of new network features and protocols without sacrificing performance or reliability. In our evaluation, Andromeda delivers high throughput and low latency close to hardware limits, with minimal interference between tenants, demonstrating an effective balance of performance and multi-tenancy in a large-scale cloud.",IT,https://dl.acm.org/doi/10.1145/3185467.3185471
P4: Programming Protocol-Independent Packet Processors (2014),"We present P4, a high-level programming language for packet processing that allows programming protocol-independent packet processors. P4’s key goals are protocol independence (the ability to reconfigure devices to support new protocols without changing hardware), field reconfigurability (the ability to modify header formats and processing), and target independence (code portability across different device types). We describe the P4 language, including its programming model and example programs, and how a P4 program is compiled down to different targets. P4 enables network operators to specify how packets are processed on switches and NICs in software-defined networks with unprecedented flexibility. Initial evaluations and use cases demonstrate that P4 can express a wide range of protocols and processing behaviors, and that P4 programs can be compiled to run efficiently on existing hardware targets.",IT,https://dl.acm.org/doi/10.1145/2656877.2656890
Hedera: Dynamic Flow Scheduling for Data Center Networks (2010),"Hedera is a scalable, dynamic flow scheduling system for data center networks. It detects long-lived large flows (elephants) at the edge switches and dynamically schedules them across multiple paths to avoid congestion and improve throughput. Hedera employs a central scheduler that periodically collects flow information and computes non-conflicting paths using a global view of network topology and utilization. We implemented Hedera on a testbed with a multi-rooted tree topology. Experiments show that Hedera achieves up to 113% higher bisection bandwidth than static flow scheduling (ECMP) under realistic workloads, effectively balancing load by moving elephant flows without impacting short flows. Hedera demonstrates that dynamic flow scheduling can significantly improve network utilization and application performance in data centers.",IT,https://www.usenix.org/conference/nsdi10/dynamic-flow-scheduling
CONGA: Distributed Congestion-Aware Load Balancing in Datacenter Networks (2014),"We introduce CONGA, a distributed congestion-aware load balancing mechanism for datacenter fabrics. CONGA operates in leaf-spine Clos networks and splits TCP flows (flowlets) over multiple paths based on real-time congestion feedback. Each leaf switch performs distributed load balancing by adjusting traffic splitting in response to congestion metrics piggybacked from remote switches. This avoids a centralized controller and reacts quickly to traffic changes. We implemented CONGA in hardware on a 40 GbE data center switching ASIC. In testbed experiments, CONGA closely tracks the optimal load distribution under varying workloads, achieving near-optimal throughput and low latency. Compared to ECMP, CONGA improves throughput by up to 2.5× in incast scenarios and significantly reduces flow completion times, all with minimal overhead and fast convergence.",IT,https://dl.acm.org/doi/10.1145/2619239.2626316
"Omega: Flexible, Scalable Schedulers for Large Compute Clusters (2013)","Increasing scale and rapidly changing requirements are hard to meet with current monolithic cluster scheduler architectures, which limit feature deployment and efficiency. We present a novel scheduling approach using parallelism, shared state, and optimistic concurrency control. We compare this approach to existing designs, evaluate scheduler interference and its practical impact, propose techniques to mitigate conflicts, and discuss a use-case highlighting advantages of our approach using real Google workload traces. Our system, Omega, uses a shared-state scheduling architecture that allows multiple parallel schedulers to independently make allocation decisions on a common resource pool, resolving conflicts optimistically. Results from simulations and Google’s production workload traces show that Omega’s parallel scheduling can achieve competitive performance relative to traditional architectures while greatly improving scheduler extensibility and agility.",IT,https://doi.org/10.1145/2465351.2465386
Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center (2011),"We present Mesos, a platform for sharing commodity clusters between multiple diverse cluster computing frameworks (such as Hadoop and MPI). Sharing improves cluster utilization and avoids per-framework data replication. Mesos shares resources in a fine-grained manner, allowing frameworks to achieve data locality by taking turns accessing each node’s data. To support today’s sophisticated frameworks, Mesos introduces a distributed two-level scheduling mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and what tasks to run on them. Our results show Mesos achieves near-optimal data locality when sharing a cluster among diverse frameworks, can scale to 50,000 nodes, and is resilient to failures. By enabling fine-grained sharing across frameworks, Mesos lets organizations run multiple specialized processing frameworks on a shared cluster efficiently.",IT,https://www.usenix.org/conference/nsdi11/mesos
Edge Computing Architecture for Applying AI to IoT (2017),"The proliferation of connected IoT devices creates a big data challenge for AI-based approaches. The response time requirements of such devices necessitate processing data at the edge, but the edge often lacks resources to train AI models. We present an edge computing architecture that preserves the advantages of edge processing (low latency, reduced bandwidth use) and cloud-centric computing (compute power for AI model training)...",IT,https://doi.org/10.1109/BigData.2017.8258272
"A Survey on Security Challenges in Cloud Computing: Issues, Threats, and Solutions (2020)","Cloud computing has gained huge attention over the past decades because of continuously increasing demands. There are several advantages to organizations moving toward cloud-based data storage solutions. These include simplified IT infrastructure and management, remote access from effectively anywhere in the world with a stable Internet connection and the cost efficiencies that cloud computing can bring. The associated security and privacy challenges in cloud require further exploration. Researchers from academia, industry, and standards organizations have provided potential solutions to these challenges in the previously published studies. The narrative review presented in this survey provides cloud security issues and requirements, identified threats, and known vulnerabilities. In fact, this work aims to analyze the different components of cloud computing as well as present security and privacy problems that these systems face. Moreover, this work presents new classification of recent security solutions that exist in this area. Additionally, this survey introduced various types of security threats which are threatening cloud computing services and also discussed open issues and propose future directions. This paper will focus and explore a detailed knowledge about the security challenges that are faced by cloud entities such as cloud service provider, the data owner, and cloud user.",IT,https://link.springer.com/article/10.1007/s11227-020-03213-1
Security Smells in Ansible and Chef Scripts: A Replication Study (2020),"Context: Security smells are recurring coding patterns that are indicative of security weakness, and require further inspection. As infrastructure as code (IaC) scripts, such as Ansible and Chef scripts, are used to provision cloud-based servers and systems at scale, security smells in IaC scripts could be used to enable malicious users to exploit vulnerabilities in the provisioned systems. Goal: The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code scripts through an empirical study of security smells in Ansible and Chef scripts. Methodology: We conduct a replication study where we apply qualitative analysis with 1,956 IaC scripts to identify security smells for IaC scripts written in two languages: Ansible and Chef. We construct a static analysis tool called Security Linter for Ansible and Chef scripts (SLAC) to automatically identify security smells in 50,323 scripts collected from 813 open source software repositories. We also submit bug reports for 1,000 randomly-selected smell occurrences. Results: We identify two security smells not reported in prior work: missing default in case statement and no integrity check. By applying SLAC we identify 46,600 occurrences of security smells that include 7,849 hard-coded passwords. We observe agreement for 65 of the responded 94 bug reports, which suggests the relevance of security smells for Ansible and Chef scripts amongst practitioners. Conclusion: We observe security smells to be prevalent in Ansible and Chef scripts, similar to that of the Puppet scripts. We recommend practitioners to rigorously inspect the presence of the identified security smells in Ansible and Chef scripts using (i) code review, and (ii) static analysis tools.",IT,https://arxiv.org/abs/1907.07159
CherryPick: Adaptively Unearthing the Best Cloud Configurations (2017),"Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to 75% search cost compared to existing solutions.",IT,https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/alipourfard
SpotOn: A Batch Computing Service for the Spot Market (2018),"Cloud spot markets enable users to bid for compute resources, such that the cloud platform may revoke them if the market price rises too high. Due to their increased risk, revocable resources in the spot market are often significantly cheaper (by as much as 10⇥) than the equivalent nonrevocable on-demand resources. One way to mitigate spot market risk is to use various fault-tolerance mechanisms, such as checkpointing or replication, to limit the work lost on revocation. However, the additional performance overhead and cost for a particular fault-tolerance mechanism is a complex function of both an application’s resource usage and the magnitude and volatility of spot market prices. We present the design of a batch computing service for the spot market, called SpotOn, that automatically selects a spot market and fault-tolerance mechanism to mitigate the impact of spot revocations without requiring application modification. SpotOn’s goal is to execute jobs with the performance of on-demand resources, but at a cost near that of the spot market. We implement and evaluate SpotOn in simulation and using a prototype on Amazon’s EC2 that packages jobs in Linux Containers. Our simulation results using a job trace from a Google cluster indicate that SpotOn lowers costs by 91.9% compared to using on-demand resources with little impact on performance.",IT,chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://lass.cs.umass.edu/papers/pdf/SoCC15-spoton.pdf
Legacy Systems to Cloud Migration: A Review from the Architectural Perspective (2023),"Legacy systems are business-critical systems that hold the organization’s core business functions developed in a traditional way using monolith architecture and usually deployed on-premises. Through time, this system is exposed to improvement changes, increasing its size and number of functionalities, thus increasing its complexity, and maintaining it becomes a disadvantage to the organization. Migration to the cloud environment becomes the primary option to improve legacy application agility, maintainability, and flexibility. However, to take advantage of the cloud environment, monolith legacy application needs to be rearchitected as microservice architecture to fully benefit from cloud advantages. This paper aims to understand the motivation for cloud migration, investigate existing cloud migration frameworks, identify the target architecture for the cloud, and establish any empirical quality issues in cloud migration from the implementation point of view. To achieve those objectives, we conducted a systematic literature review (SLR) of 47 selected studies from the most relevant scientific digital libraries covering pre-migration, migration, and post-migration stages. The SLR outcome provided us with the primary motivation for the cloud migration, existing cloud migration frameworks, targeted migration architecture patterns, and migration challenges. The results also highlight areas where more research is needed and suggest future research in this field. Furthermore, our analysis shows that current migration approaches lack quality consideration, thus contributing to post-migration quality concerns.",IT,https://www.sciencedirect.com/science/article/abs/pii/S0164121223000973
Challenges in the Adoption of Hybrid Cloud: An Exploratory Study (2018),"Cloud computing is a growing computing paradigm that provides Internet-based computer services on-demand basis. Adoption of cloud infrastructure promises enterprises numerous benefits. In particular, hybrid cloud, a combination of both public and private clouds, offers benefits of both the public and private clouds. The objective of this study is to identify the critical challenges, faced by client organisations in the adoption of hybrid cloud computing. The authors have reviewed the literature through systematic literature review (SLR) process. We have followed all the SLR steps by developing SLR protocol first which was then validated and implemented. We have identified a list of ten challenges, by extracting data from a sample of 120 papers, in the adoption of hybrid cloud. The identified challenges include three critical challenges such as: ‘public cloud security concern’, ‘efficient management issue’, and ‘integration complexity’. We have further analysed the identified challenges with respect to time and study strategy. Clients should address all the identified challenges in general and the critical challenges in particular. Our next phase of the study is validation of the identified challenges through industry practitioners and to find solutions/practices for addressing these challenges, which will be published in future.",IT,https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/joe.2016.0089
Mobile Edge Computing: A Survey (2017),"Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wireline scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.",IT,https://ieeexplore.ieee.org/document/8030322
Fog Computing: A Comprehensive Architectural Survey,"Fog computing is an emerging technology to address computing and networking bottlenecks in large scale deployment of IoT applications. It is a promising complementary computing paradigm to cloud computing where computational, networking, storage and acceleration elements are deployed at the edge and network layers in a multi-tier, distributed and possibly cooperative manner. These elements may be virtualized computing functions placed at edge devices or network elements on demand, realizing the “computing everywhere” concept. To put the current research in perspective, this paper provides an inclusive taxonomy for architectural, algorithmic and technologic aspects of fog computing. The computing paradigms and their architectural distinctions, including cloud, edge, mobile edge and fog computing are subsequently reviewed. Practical deployment of fog computing includes a number of different aspects such as system design, application design, software implementation, security, computing resource management and networking. A comprehensive survey of all these aspects from the architectural point of view is covered. Current reference architectures and major application-specific architectures describing their salient features and distinctions in the context of fog computing are explored. Base architectures for application, software, security, computing resource management and networking are presented and are evaluated using a proposed maturity model.",IT,https://www.researchgate.net/publication/340180826_Fog_Computing_A_Comprehensive_Architectural_Survey
"Data Center Network Architecture in Cloud Computing: Review, Taxonomy, and Open Research Issues (2014)","The data center network (DCN), which is an important component of data centers, consists of a large number of hosted servers and switches connected with high speed communication links. A DCN enables the deployment of resources centralization and on-demand access of the information and services of data centers to users. In recent years, the scale of the DCN has constantly increased with the widespread use of cloud-based services and the unprecedented amount of data delivery in/between data centers, whereas the traditional DCN architecture lacks aggregate bandwidth, scalability, and cost effectiveness for coping with the increasing demands of tenants in accessing the services of cloud data centers. Therefore, the design of a novel DCN architecture with the features of scalability, low cost, robustness, and energy conservation is required. This paper reviews the recent research findings and technologies of DCN architectures to identify the issues in the existing DCN architectures for cloud computing. We develop a taxonomy for the classification of the current DCN architectures, and also qualitatively analyze the traditional and contemporary DCN architectures. Moreover, the DCN architectures are compared on the basis of the significant characteristics, such as bandwidth, fault tolerance, scalability, overhead, and deployment cost. Finally, we put forward open research issues in the deployment of scalable, low-cost, robust, and energy-efficient DCN architecture, for data centers in computational clouds.",IT,https://link.springer.com/article/10.1631/jzus.C1400013
A Survey on Software-Defined Networking (2015),"Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.",IT,https://ieeexplore.ieee.org/document/6834762
Energy-Efficient Cloud Computing,"Energy efficiency is increasingly important for future information and communication technologies (ICT), because the increased usage of ICT, together with increasing energy costs and the need to reduce green house gas emissions call for energy-efficient technologies that decrease the overall energy consumption of computation, storage and communications. Cloud computing has recently received considerable attention, as a promising approach for delivering ICT services by improving the utilization of data centre resources. In principle, cloud computing can be an inherently energy-efficient technology for ICT provided that its potential for significant energy savings that have so far focused on hardware aspects, can be fully explored with respect to system operation and networking aspects. Thus this paper, in the context of cloud computing, reviews the usage of methods and technologies currently used for energy-efficient operation of computer hardware and network infrastructure. After surveying some of the current best practice and relevant literature in this area, this paper identifies some of the remaining key research challenges that arise when such energy-saving techniques are extended for use in cloud computing environments.",IT,https://www.researchgate.net/publication/46116227_Energy-Efficient_Cloud_Computing
Dapper: A Large-Scale Distributed Systems Tracing Infrastructure (2010),"Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment. Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The main goal of this paper is to report on our experience building, deploying and using the system for over two years, since Dapper’s foremost measure of success has been its usefulness to developer and operations teams. Dapper began as a self-contained tracing tool but evolved into a monitoring platform which has enabled the creation of many different tools, some of which were not anticipated by its designers. We describe a few of the analysis tools that have been built using Dapper, share statistics about its usage within Google, present some example use cases, and discuss lessons learned so far.",IT,https://research.google/pubs/pub36356
Inter-Datacenter Job Routing and Scheduling with Variable Costs and Deadlines,"To reduce their operational costs, datacenter operators can schedule large jobs at datacenters in different geographical locations with time- and location-varying electricity and bandwidth prices. We introduce a framework and algorithms to do so that minimize electricity and bandwidth cost subject to job indivisibility, deadlines, priorities, and datacenter resource constraints. In doing so, we provide a way for datacenter operators to predict their operational costs for different datacenter placements and capacities, and thus make informed decisions about how to expand their datacenter network. Our distributed algorithm uses estimated job arrivals and day-ahead electricity prices to optimize over sliding time windows. We demonstrate its effectiveness on a Google datacenter trace and investigate the effects of different cost and performance criteria. The algorithm leverages heterogeneous job resource requirements and routing and scheduling flexibility: even deadline and indivisibility constraints yield little cost increase, though they significantly improve job completion times and localization at only one datacenter respectively. We show that our algorithm reduces the cost much more than optimizing only electricity, only bandwidth, or a combination of resource costs and job completion times.",IT,https://www.andrew.cmu.edu/user/cjoewong/Routing_TSG_TR.pdf
Container Orchestration for Dispersed Computing,"In the era of Internet of Things, there is an increasing demand for networked computing to support the requirements of time-constrained, compute-intensive distributed applications. We present a container orchestration architecture for dispersed computing, and its implementation in an open source software called Jupiter. The system automates the distribution of computational tasks for complex computational applications described as an Directed Acyclic Graph (DAG) to efficiently distribute the tasks among a set of networked compute nodes and orchestrates the execution of the DAG thereafter. This Kubernetes based container-orchestration system supports both centralized and decentralized scheduling algorithms for optimally mapping the tasks based on information from a range of profilers: network profilers, resource profilers, and execution time profilers.",IT,https://anrg.usc.edu/www/wp-content/uploads/2019/10/Jupiter__Camera_Ready.pdf
B4: Experience with a Globally-Deployed Software Defined WAN,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google’s data centres across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximise average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software-Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4’s centralised traffic engineering service drives links to near-optimised utilisation, while splitting application flows among multiple paths to balance capacity against application priorities and demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",IT,https://cseweb.ucsd.edu/~vahdat/papers/b4-sigcomm13.pdf
Hedera: Dynamic Flow Scheduling for Data Center Networks,"Today's data centers offer tremendous aggregate bandwidth to clusters of tens of thousands of machines. However, because of limited port densities in even the highest-end switches, data center topologies typically consist of multi-rooted trees with many equal-cost paths between any given pair of hosts. Existing IP multipathing protocols usually rely on per-flow static hashing and can cause substantial bandwidth losses due to long-term collisions. In this paper, we present Hedera, a scalable, dynamic flow scheduling system that adaptively schedules a multi-stage switching fabric to efficiently utilize aggregate network resources. We describe our implementation using commodity switches and unmodified hosts, and show that for a simulated 8,192 host data center, Hedera delivers bisection bandwidth that is 96% of optimal and up to 113% better than static load-balancing methods.",IT,https://raghavan.usc.edu/papers/hedera-nsdi10.pdf
Distributed Congestion-Aware Load Balancing for Datacenters,"We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5 better flow completion times than ECMP even with a single link failure and achieves 2–8 better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.",IT,https://people.csail.mit.edu/alizadeh/papers/conga-sigcomm14.pdf
VL2: A Scalable and Flexible Data Center Network,"To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds – sustaining a rate that is 94% of the maximum possible.",IT,https://www.microsoft.com/en-us/research/publication/vl2-a-scalable-and-flexible-data-center-network/
MicroTE: Fine Grained Traffic Engineering for Data Centers,"The effects of data center traffic characteristics on data center traffic engineering is not well understood. In particular, it is unclear how existing traffic engineering techniques perform under various traffic patterns, namely how do the computed routes differ from the optimal routes. Our study reveals that existing traffic engineering techniques perform 15% to 20% worse than the optimal solution. We find that these techniques suffer mainly due to their inability to utilize global knowledge about flow characteristics and make coordinated decision for scheduling flows. To this end, we have developed MicroTE, a system that adapts to traffic variations by leveraging the short term and partial predictability of the traffic matrix. We implement MicroTE within the OpenFlow framework and with minor modification to the end hosts. In our evaluations, we show that our system performs close to the optimal solution and imposes minimal overhead on the network making it appropriate for current and future data centers.",IT,http://conferences.sigcomm.org/co-next/2011/papers/1569470427.pdf
Fault Tolerant Traffic Engineering in Software-defined WAN (2018),"Software-defined networking in a wide area network (SD-WAN) allows intelligent control and management of networking, and efficient utilization of network resources through traffic engineering in real time for higher performance WANs. This paper proposes a fault-tolerant reactive routing system, called a smart routing system, for SD-WAN by investigating a variety of network features needed for monitoring in WAN in real time. The system keeps track of various network status data in real time to provide less packet loss and low network latency along with high availability and reliability in SD-WAN. We evaluate our system in a real network provided by OpenLab at Juniper. Experimental results show that our approach successfully demonstrates resilience and efficiency by applying the programmability of SDN for WAN.",IT,https://doi.org/10.1109/ISCC.2018.8538606
Resilient Datacenter Load Balancing in the Wild (2017),"Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallout. Despite significant efforts, prior solutions have important drawbacks. On one hand, solutions like Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions like CONGA and CLOVE can sense congestion, but only reroute when flowlets emerge; thus they cannot always react in time to uncertainties. Moreover, these solutions fail to detect or handle failures such as blackholes and random packet drops, which greatly degrades performance. In this paper, we introduce Hermes, a datacenter load balancer resilient to such uncertainties. Hermes leverages comprehensive sensing to detect path conditions (including failures previously unattended) and reacts with timely yet cautious rerouting. It is an edge-based solution requiring no switch modification. We implemented Hermes with commodity switches and evaluated it via testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and handles uncertainties well: under asymmetric topologies, Hermes achieves up to 10%–20% better flow completion times than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.",IT,https://doi.org/10.1145/3098822.3098841
Large-Scale Cluster Management at Google with Borg (2015),"Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs—many thousands of different applications—across a number of clusters, each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg architecture and features, important design decisions, a quantitative analysis of some policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.",IT,https://doi.org/10.1145/2741948.2741964
Evaluation of Container Orchestration Systems for NoSQL Databases (2018),"Container orchestration systems such as Docker Swarm, Kubernetes, and Mesos provide automated support for deployment and management of distributed applications as sets of containers. While initially designed for stateless services, they are also being used to run database clusters due to resilience features like fast auto-recovery of failed nodes and location-transparent connections among database instances. In this paper we evaluate the performance overhead of Docker Swarm and Kubernetes for deploying and managing NoSQL database clusters (MongoDB case study). As a baseline, we use an OpenStack IaaS cloud that provides similar resilience attributes in a less automated manner. Our experiments quantify the overhead introduced by container orchestration in terms of throughput and latency, compared to a traditional IaaS-based deployment.",IT,https://doi.org/10.1109/CLOUD.2018.00066
Unikernels: Library Operating Systems for the Cloud (2013),"We present unikernels, a new approach to deploying cloud services via applications written in high-level source code. Unikernels are single-purpose appliances that are compiled into standalone kernels and sealed against modification when deployed to a cloud platform. In return they offer significantly reduced image sizes, improved efficiency and security, and should reduce operational costs. Our Mirage prototype compiles OCaml code into unikernels that run on commodity clouds and offers an order-of-magnitude reduction in code size without significant performance penalty. The architecture combines static type-safety with a single-address-space layout that can be made immutable via a hypervisor extension. Mirage contributes a suite of type-safe protocol libraries, and our results demonstrate that the hypervisor can serve as a platform to overcome the hardware compatibility issues that made past library OSes impractical to deploy in the real world.",IT,https://doi.org/10.1145/2451116.2451167
Edge Computing: Vision and Challenges (2016),"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm: edge computing, which calls for processing data at the edge of the network. Edge computing has the potential to address requirements for low response time, battery life conservation, bandwidth cost savings, and data safety/privacy. In this paper, we introduce the definition of edge computing, followed by several case studies (from cloud offloading to smart homes/cities and collaborative edge) to materialize the concept. Finally, we present several challenges and opportunities in edge computing and hope this paper will attract community attention and inspire further research in this direction.",IT,https://doi.org/10.1109/JIOT.2016.2579198
DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning (2017),"Anomaly detection is a critical step toward building secure and trustworthy systems. System logs record states and significant events at various points to help debug failures and perform root cause analysis, and thus are an excellent resource for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing LSTM (Long Short-Term Memory) to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution and detect anomalies when log sequences deviate from the model trained on normal data. We also show how to incrementally update the DeepLog model online so it can adapt to new log patterns over time. Furthermore, DeepLog constructs workflow models from the log streams so that once an anomaly is detected, users can diagnose and perform root cause analysis effectively. Extensive experiments on large log datasets show that DeepLog outperforms existing log-based anomaly detection methods based on traditional data mining.",IT,https://doi.org/10.1145/3133956.3134015
COLO: COarse-Grained LOck-Stepping Virtual Machines for Non-Stop Service (2013),"Virtual machine (VM) replication provides a software solution for business continuity and disaster recovery by replicating the state of a primary VM (PVM) to a secondary VM (SVM) on a different node. Unfortunately, current VM replication approaches suffer excessive overhead, severely limiting their applicability. In this paper, we leverage the observation that in client–server systems, the PVM and SVM can be considered in the “same” state as long as they generate the same outputs to clients. Using this insight, we propose COLO, a generic and highly efficient non-stop service solution using on-demand VM replication. COLO monitors the output responses of the PVM and SVM, and deems the SVM a valid replica of PVM if their outputs match. If responses diverge, COLO delays the external reply until the PVM’s state is synchronized to the SVM. In this way, we ensure the system is always capable of failover to the SVM. Although nondeterminism may cause internal state differences between PVM and SVM, they remain externally consistent. Unlike prior instruction-level lock-stepping approaches, COLO supports multi-processor workloads with satisfying performance. Results show that COLO significantly outperforms existing approaches, especially on typical server–client workloads like online databases and web servers.",IT,https://doi.org/10.1145/2523616.2523630
A General Approach to Network Configuration Verification (Minesweeper) (2017),"We develop an approach to verify network configurations that is (1) general (not limited by specific protocols or features), (2) accurate (no reliance on simplifying approximations), (3) complete (analyzes all possible sets of routing announcements from external sources, not just one or a few), (4) powerful (can verify a wide range of properties such as reachability, path length, fault tolerance, load balancing), and yet (5) scalable. Our approach translates unmodified network configurations into a logical formula capturing both control-plane and data-plane behaviors. An SMT solver then checks if all converged control-plane behaviors satisfy desired properties. We implement our approach in a tool called Minesweeper and apply it to configurations of 152 real networks from a large cloud provider. Despite these networks being in production for years, Minesweeper found 96 bugs, some posing serious security vulnerabilities. We also tested Minesweeper on synthetic benchmarks and found it can verify rich properties in networks of hundreds of routers in under 5 minutes, thanks to model-slicing and constraint-hoisting optimizations that speed up verification by over 460×.",IT,https://doi.org/10.1145/3098822.3098834
Energy Saving Evaluation of an Energy-Efficient Data Center using a Model-Free Reinforcement Learning Approach (2022),"To reduce cooling energy consumption, data centers are often advised to raise the server inlet air temperature. However, in tropical climates operators still tend to run at lower temperatures. In this paper, we demonstrate that using a floating setpoint (dynamically lowering the temperature when needed) yields greater overall energy savings for tropical data centers than simply raising the temperature statically. We achieve this via a deep reinforcement learning algorithm applied to a hybrid data center model built from traces of a highly efficient facility. The learned control policy minimizes energy costs while respecting operational constraints. We evaluate the policy to identify where the energy savings come from. The agent is trained by interacting with the data center model without any prior knowledge. Tests show that additional energy savings of up to 3% (full load) and 5.5% (part load) are achievable with targeted cooling provision in an already-efficient data center. We find that, contrary to common assumptions, most savings come from reducing server fan usage rather than chiller power reduction in our RL-driven data center scenario.",IT,https://doi.org/10.1016/j.apenergy.2022.119392
Automated Infrastructure-as-Code Program Testing (ACT/ProTI) (2024),"Infrastructure as Code (IaC) automates software operations and is key to high-velocity software delivery. Modern IaC frameworks like Pulumi and AWS CDK let developers write IaC programs in general-purpose languages (TypeScript, Python, etc.), but bugs in IaC can disrupt entire systems. Surprisingly, testing practices are rarely applied to IaC code: as of August 2022, under 1% of Pulumi-based IaC projects on GitHub had any tests. Existing IaC testing methods either slow development or demand heavy manual effort. We propose Automated Configuration Testing (ACT), a methodology to rapidly test IaC programs under many configurations with minimal effort. ACT automatically mocks all cloud resources in the IaC program, and uses pluggable generator and oracle modules for test input generation and output validation. We implement ACT in ProTI, a testing tool for Pulumi TypeScript that provides type-driven input generation, oracles, and supports application-level specifications. Our evaluation on 6,081 Pulumi scripts from GitHub plus benchmarks shows that ProTI can be directly applied to real IaC programs without changes, finds bugs within seconds in cases where prior techniques are infeasible, and leverages existing generators/oracles via its plugin architecture.",IT,https://doi.org/10.1109/TSE.2024.3393070
ABACUS: A FinOps Service for Cloud Cost Optimization (2023),"As enterprises migrate infrastructure to the cloud, they face challenges in achieving holistic visibility into cloud spending and optimizing costs. FinOps practices offer a way to bring financial accountability and cost optimization to cloud usage. This paper presents ABACUS – Automated Budget Analysis and Cloud Usage Surveillance – a FinOps solution for cloud cost optimization. ABACUS allows organizations to set cloud budgets, enforce those budgets by blocking new deployments when necessary, and alert teams if spending exceeds thresholds. It leverages Infrastructure-as-Code integration to warn engineering teams about the expected cost of a deployment before cloud resources are provisioned. We discuss ABACUS’s architecture and demonstrate how it helps reduce waste, improve cost forecasting, and scale cloud usage sustainably. Finally, we outline future research directions to advance FinOps practices for cloud cost management.",IT,https://www.amazon.science/blog/five-ways-the-abacus-label-advances-nature-based-carbon-removal
Firecracker: Lightweight Virtualization for Serverless Applications (2020),"Serverless containers and functions are widely used for cloud deployment due to their lower operational costs, improved hardware utilization, and faster scaling compared to traditional methods. The economics and scale of serverless computing require that multi-tenant workloads run on shared hardware with minimal overhead while providing strong security and performance isolation. Traditionally, one had to choose between VM-based virtualization (strong isolation, high overhead) and container-based systems (low overhead, weaker isolation) – a tradeoff unacceptable to cloud providers who need both. We developed Firecracker, an open-source Virtual Machine Monitor (VMM) specialized for serverless workloads but broadly useful for containers, functions, and other computations under certain constraints. Firecracker provides the security of hardware-grade VMs with the low overhead of containers. It has been deployed in AWS Lambda and AWS Fargate, supporting millions of workloads and trillions of requests per month. We describe how focusing on serverless use-cases shaped Firecracker’s design and share lessons from migrating AWS’s serverless customers transparently to Firecracker.",IT,https://www.amazon.science/publications/firecracker-lightweight-virtualization-for-serverless-applications
A Survey of Data Center Network Architectures,"Large-scale data centers form the core infrastructure support for the ever expanding cloud based services. Thus the performance and dependability characteristics of data centers will have significant impact on the scalability of these services. In particular, the data center network needs to be agile and reconfigurable in order to respond quickly to ever changing application demands and service requirements. Significant research work has been done on designing the data center network topologies in order to improve the performance of data centers. In this paper, we present a survey of data center network designs and topologies that have published recently. We start with a discussion on various representative data center network topologies, and compare them with respect to several properties in order to highlight their advantages and disadvantages. Thereafter, we discuss several routing protocols designed for these topologies, and compare them based on various criteria: the basic algorithms to establish connections, the techniques used to gain better performance and the mechanisms for fault-tolerance. A good understanding of the state-of-the-art in data center networks is essential for designing future architectures.",IT,https://ics.uci.edu/~cs237/reading/reading2020/DataCenterSurvey.pdf
"Secure Cloud Infrastructure: A Survey on Issues, Current Solutions","Cloud computing is currently becoming a well-known buzzword in which business titans, such as Microsoft, Amazon, and Google, among others, are at the forefront in developing and providing sophisticated cloud computing systems to their users in a cost-effective manner. Security is the biggest concern for cloud computing and is a major obstacle to users adopting cloud computing systems. Maintaining the security of cloud computing is important, especially for the infrastructure. Several research works have been conducted in the cloud infrastructure security area; however, some gaps have not been completely addressed, while new challenges continue to arise. This paper presents a comprehensive survey of the security issues at different cloud infrastructure levels (e.g., application, network, host, and data). It investigates the most prominent issues that may affect the cloud computing business model with regard to infrastructure. It further discusses the current solutions proposed in the literature to mitigate the different security issues at each level. To assist in solving the issues, the challenges that are still unsolved are summarized. Based on the exploration of the current challenges, some cloud features such as flexibility, elasticity and the multi-tenancy are found to pose new challenges at each infrastructure level.",IT,https://researchportal.port.ac.uk/en/publications/secure-cloud-infrastructure-a-survey-on-issues-current-solutions-
Design and Implementation of Scalable Network Architecture for High-Density Data Centers,"This paper presents the design and implementation of a scalable network architecture tailored for high-density data centers, which are essential for modern cloud computing, big data, and distributed applications. As data centers grow in size and complexity, traditional network designs face challenges in delivering the required bandwidth, fault tolerance, and low-latency communication between a large number of servers. Our design emphasizes the role of 5G and Edge Computing and also Machine Learning in network operations. This scalable solution meets the growing demands of high-density data centers, offering a practical and efficient network infrastructure for enterprises and cloud service providers.",IT,http://ijircce.com/admin/main/storage/app/pdf/dmN13LujxPS3T3Y48Rhn6NKfSnanRFT5qimYxLfV.pdf
Cloud Computing: Survey on Energy Efficiency,"In this article, we perform a comprehensive analysis of an infrastructure supporting the cloud computing paradigm with regards to energy efficiency. We start with an overview of energy efficiency in cloud computing and then discuss metrics and frameworks for measuring and optimizing energy consumption. We analyze existing solutions for improving energy efficiency at different levels of the cloud infrastructure, including hardware, virtualization, and workload management. The survey identifies key challenges and future research directions for sustainable cloud computing systems.",IT,https://dl.acm.org/doi/10.1145/2656204
Rethinking Fat-Tree Topology Design for Cloud Data Centers,"Data center network (DCN) topologies have recently been the focus of many researchers due to their vital role in achieving high DCN performances in terms of scalability, power consumption, throughput, and traffic load balancing. This paper presents a comprehensive comparison between two most commonly used DCN topologies, Fat-Tree and BCube, with a focus on structure, addressing and routing, and proposes a new DCN topology that is better suited for nowadays data center networks. We show that our proposed topology, termed Circulant Fat-Tree, alleviates traffic congestion at the core switches, improves network latency, and increases robustness against switch and server failures when compared to traditional Fat-Tree DCN topologies.",IT,https://web.engr.oregonstate.edu/~hamdaoui/papers/2018/DC-topology-GC-2018.pdf
Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics,"The increasing demand for storage, networking and computation has driven intensification of large complex data centers that run many of today’s Internet, financial, commercial and business applications. A data center comprises of many thousands of servers and can use as much energy as small city. Massive amount of computation power is required to drive and run these server farms resulting in many challenging like huge energy consumptions, emission of green house gases, backups and recovery; This paper proposes energy efficiency and low carbon enabler green IT framework for these large and complex server farms to save consumption of electricity and reduce the emission of green house gases to lower the effects of global warming. The framework uses latest energy saving techniques like virtualization, cloud computing and green metrics to achieve greener data centers. It comprises of five phase to properly implement green IT techniques to achieve green data centers. The proposed framework seamlessly divides data center components into different resource pools and then applies green metrics like Power Usage Effectiveness, Data Center Effectiveness and Carbon Emission Calculator to measure performance of individual components so that benchmarking values can be achieved and set as standard to be followed by data centers.",IT,https://www.sciencedirect.com/science/article/pii/S1364032112001979
An optimal infrastructure design method of cloud computing services from the BDIM perspective,"For IT service providers, infrastructure construction plays a significant role in the maximization of business profits. Infrastructure design is becoming more and more crucial as IT service environments evolve from deployment on site to software as a service (SaaS), and, most recently, to cloud computing. In this paper, a cloud computing architecture is proposed from the viewpoint of business-driven IT management (BDIM); then an optimal cloud infrastructure design methodology is devised, whereby numbers of servers, routers and communication bandwidth can be calculated through considering both infrastructure costs and business losses incurred by service level agreement (SLA) violations. Finally, a complete numerical example is discussed to testify the proposed method.",IT,https://ieeexplore.ieee.org/abstract/document/5406408
Fast autoscaling algorithm for cost optimization of container clusters,"Container clusters are widely used to execute containerized applications in cloud environments. An essential characteristic implemented by these clusters is autoscaling, which is the ability to automatically adapt the computing resources of a cluster to support variable workloads. Precise adjustment of cluster resources to its workload in each autoscaling operation is essential to control cluster deployment costs. Several resource allocation models have been developed with the objective of cost minimization. However, as the number of containers and virtual machines of the cluster increases, resource allocation problems become too complex, and cannot be solved in reasonable time by existing resource allocation models. In this paper we present FCMA (Fast Container to Machine Allocator), a resource allocation algorithm designed to calculate a suitable allocation of the resources of a cluster in autoscaling operations, to minimize cluster deployment costs. The main motivation for the development of FCMA has been to significantly reduce the solving time of the resource allocation problem compared to a previous state-of-the-art optimal Integer Linear Programming (ILP) model. In addition, FCMA addresses secondary objectives to improve fault tolerance and reduce container and virtual machine recycling costs, load-balancing overloads and container interference. We have conducted an experimental evaluation to assess the effectiveness of FCMA, using the ILP model and two heuristics as a baseline. The experiments show that FCMA is much faster than the ILP model, with an average solving time reduction of two orders of magnitude. This gain in speed does not compromise the quality of the solutions, which have a cost on par with those of the ILP model. In comparison to the heuristics, FCMA achieves similar solving times while consistently delivering more cost-effective solutions.",IT,https://doi.org/10.1186/s13677-025-00748-7
Server deployment strategies considering robustness and resource constraints in edge computing,"Edge computing, as an emerging computing paradigm, efficiently offloads computationally intensive tasks to edge servers, extending services from the cloud center to the edge and significantly improving the efficiency of network data processing. However, existing research mostly focuses on the ideal deployment and optimization of edge servers, such as minimizing latency and load balancing, with little attention given to strategies for handling abnormal situations like soft attacks or sudden failures. This study delves into how to formulate server deployment strategies aiming to maximize system robustness and minimize costs when edge servers face soft attacks or sudden failures. Drawing insights from graph theory and network connectivity, we propose a server deployment strategy that enhances system robustness under resource constraints. Specifically, we introduce the concept of edge-delay-tolerant networks, ensuring the rapid establishment of a complete “backup link” in case of service interruption by constructing backup relay nodes and routing information tables to safeguard system continuity and stability. Additionally, we innovatively design adaptive edge server deployment methods in “silent” and “active” modes to cater to varying demands in different fault scenarios. To validate the effectiveness of the proposed strategies, we construct a multi-objective optimization problem through mathematical modeling and devise an hybrid optimization algorithm for solving it. Experimental simulation results demonstrate that the algorithm can provide near-optimal performance, effectively enhancing system robustness under resource constraints. This research not only offers new insights and methods for fault response mechanisms in edge computing but also provides valuable references for practical server deployment strategies, thereby promoting further development of edge computing technology.",IT,https://doi.org/10.1186/s13677-025-00741-0
Enhancing configuration security with heterogeneous read points,"Configuration files are widely used for customizing the status and behavior of cloud systems without modifying source code. The configurable system performs flexibly to meet different requirements. Several security risks come with the flexibility, since the configuration files are directly accessible to users. In this work, we propose config-flow analysis to locate suspicious usage and design three types of code-level heterogeneous operations to build security protection for related read points. The config-flow analysis can address the propagation of configuration options and further help to boost configuration security from read points to the end of usage sequence. For the three types of commonly used configuration files, i.e., key-value pairs, serialization data, and scripts, we evaluated the effectiveness of read point identification and heterogeneous operations on 14 open-source projects. The experimental results show that the overall precision of file and option read point identification is 97% and 96%, and our approach can ensure projects keep security against configuration-related vulnerabilities with acceptable performance loss.",IT,https://doi.org/10.1186/s13677-025-00740-1
Utility-driven virtual machine allocation in edge cloud environments using a partheno-genetic algorithm,"Mobile Edge Computing alleviates network congestion and reduces latency by offloading tasks to the network edge. However, fluctuating Quality of Service (QoS) and service compositions significantly challenge service reliability and utility optimization. To address these challenges, this paper proposes a novel virtual machine allocation framework designed to maximize the utility of edge cloud service provisioning under QoS constraints. First, the task processing mechanism is modeled as an M/M/m queuing system, with service loss and revenue functions defined to quantify the quality and profitability of edge services. Next, the framework dynamically reallocates virtual machines across sub-service centers, based on task arrival rates and varying QoS requirements, to optimize overall service utility. Finally, we develop a partheno-genetic algorithm based on integer coding to solve the service utility maximization (SOPGA) to determine the optimal virtual machine allocation strategy. Simulation results demonstrate that the proposed virtual machine allocation algorithm improves service utility by more than 20% compared to other virtual machine allocation algorithms, significantly enhancing service utility in edge cloud environments while maintaining robust QoS guarantees.",IT,https://doi.org/10.1186/s13677-025-00739-8
Secure and efficient ownership verification for deduplicated cloud computing systems,"Cloud storage services offer a scalable platform to store a large amount of data at a low cost. It attracts a large number of customers to outsource their data to the cloud. To manage the massive growth in the size of outsourced data, cloud service providers employ deduplication, i.e., a technique to reduce space and bandwidth requirements by eliminating the upload and storage of redundant data. However, it poses the following severe security threat: “A malicious user who learns deduplication tag of the file, i.e., a small piece of information, can convince the server to allow access to the entire file”. A proof of ownership ( POW ) concept was introduced that allows the server to challenge the user to prove that s/he owns the entire file. The existing state-of-the-art POW solutions are either not considering the complete file for determining the proof or not efficient in terms of I/O, communication, and computational overheads on the user. In this paper, we propose a secure and efficient POW scheme. The proposed scheme ensures that the user must possess the complete file to generate ownership proof. In addition, our scheme causes minimal I/O, communication, and computational overhead on the user side. We implement the proposed scheme in a real cloud scenario using Google Firebase cloud services. The performance analysis indicates that our scheme is efficient in terms of I/O, computational, and communication overheads than the existing state-of-the-art solutions.",IT,https://doi.org/10.1186/s13677-025-00743-y
Optimizing energy task offloading technique using IoMT cloud in healthcare applications,"The Internet of Medical Things (IoMT) has revolutionized patient data and healthcare surveillance, enabling continuous monitoring without costly human resources and low error rates. IoMT uses medical devices as nodes to monitor and collect patient data efficiently and cost-effectively. IoMT issues emergency alarms and monitors people in hospitals and at home to help physicians track their health. It analyzes EEGs, ECGs, blood sugar, blood pressure, and other health markers. Real-time analysis is essential in crucial situations, these latency-sensitive scenarios are suitable for cloud-based IoT platforms. This research proposes an Efficient Augmented Moth-Flame Optimization (EA-MFO) technique for task offloading. The method focuses on prioritizing critical tasks to ensure deadlines are met while optimizing energy consumption for other tasks. EA-MFO enhances the moth-flame optimization process by incorporating chaos-based initialization, adaptive position updates with weighted adjustments, and strategies to improve population diversity. The chaos-based logistic map is used to increase diversity during initialization. Simulation results reveal that EA-MFO outperforms E-PSO, GWO, MQGA, and MATO in terms of energy consumption, makespan, and total execution time (TEC). Specifically, EA-MFO achieves a total execution time of 0.63 s, a makespan of 52.13 s, and energy consumption of 592.78 kWh.",IT,https://doi.org/10.1186/s13677-025-00733-0
Virtual machine scheduling and migration management across multi-cloud data centers: blockchain-based versus centralized frameworks,"Efficiently managing virtual resources in the cloud is crucial for successful recourse utilization. Scheduling is a vital technique used to manage Virtual Machines (VMs), enabling placement and migration between hosts located in the same or different data centers. Effective scheduling not only ensures better server consolidation but also enhances hardware utilization and reduces power consumption in data centers. However, scheduling VMs across a Wide Area Network (WAN) poses considerable challenges due to connectivity issues, slower communication speeds, and concerns around data integrity and confidentiality. To enable informed scheduling decisions, it is critical to facilitate the exchange of real-time and accurate status information between cloud data centers, ensuring optimal resource allocation and minimizing latency. To address this, we propose a novel distributed cloud management solution that utilizes blockchain technology to facilitate efficient sharing of VM characteristics across multiple data centers. BigchainDB platform has been used as a blockchain-based ledger database to effectively share information required for VM scheduling and migration across different data centers. The proposed framework has been validated and compared with a Virtual Private Network (VPN)-based centralized management solution. The proposed model utilizing blockchain-based solution achieves 41.79% to 49.85% reduction in number of communication messages and 2% to 12% decrease in total communication delay comparing to the centralized model.",IT,https://doi.org/10.1186/s13677-024-00724-7
Cloud-edge hybrid deep learning framework for scalable IoT resource optimization,"In the dynamic environment of the Internet of Things (IoT), edge and cloud computing play critical roles in analysing and storing data from numerous connected devices to produce valuable insights. Efficient resource allocation and workload distribution are vital to ensuring continuous and reliable service in growing IoT ecosystems with increasing data volumes and changing application demands. This study proposes a novel optimisation approach utilising deep learning to tackle these challenges. The integration of Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) offers a practical approach to addressing the dynamic characteristics of IoT applications. The hybrid algorithm's primary characteristic is its capacity to simultaneously fulfil multiple objectives, including reducing response times, enhancing resource efficiency, and decreasing operational costs. DQN facilitates the formulation of optimal resource allocation strategies in intricate and unpredictable environments. PPO enhances policies in continuous action spaces to guarantee reliable performance in real-time, dynamic IoT settings. This method achieves an optimal equilibrium between policy learning and optimisation, rendering it suitable for contemporary IoT systems. This method improves numerous IoT applications, including smart cities, industrial automation, and healthcare. The hybrid DQN-PPO-GNN-RL model addresses bottlenecks by dynamically managing computing and network resources, allowing for efficient operations in low-latency, high-demand environments such as autonomous systems, sensor networks, and real-time monitoring. The use of Graph Neural Networks (GNNs) improves the accuracy of resource representation, while reinforcement learning-based scheduling allows for seamless adaptation to changing workloads. Simulations using real-world IoT data on the iFogSim platform showed significant improvements: task scheduling time was reduced by 21%, operational costs by 17%, and energy consumption by 22%. The method reliably provided equitable resource distribution, with values between 0.93 and 0.99, guaranteeing efficient allocation throughout the network. This hybrid methodology establishes a novel benchmark for scalable, real-time resource management in extensive, data-centric IoT ecosystems, consequently enhancing system performance and operational efficiency.",IT,https://doi.org/10.1186/s13677-025-00729-w
Analyzing the trend of government support for cloud computing usage in e-government architecture,"The revolution in improving services to the community carried out by the current government is genuine. It is not easy for government organizations, especially local governments, to directly implement e-government services in full. One solution that is considered appropriate and can solve these problems is the application of cloud computing to support e-government services in local governments. The advantage of cloud computing for e-government is that it can increase security. In contrast, cloud-based storage can be valuable because data stored in cloud computing is guaranteed secure and various regulations and standards of information safety practices. We propose a systematic literature review approach to analyze trends in studies on cloud computing of e-government. This research is a type of descriptive qualitative research using a bibliometric approach. To assess the trend of cloud computing in e-government, we use CiteSpace’s latest bibliometric software to achieve a comprehensive knowledge overview of cloud computing in e-government. The Findings of this paper reveal a dynamic scenario influenced by advancing technological progress and administrative priorities. Across the globe, governments are progressively acknowledging the capacity of cloud computing to improve the effectiveness, accessibility, and scalability of e-government services. Overall, challenges persist, spanning from concerns regarding data safekeeping and privacy, but it also signifies a strategic transition towards harnessing digital technologies to provide more agile, citizen-focused public services.",IT,https://doi.org/10.1186/s13677-025-00735-y
Autonomous decision-making of UAV cluster with communication constraints based on reinforcement learning,"Artificial intelligence techniques are increasingly applied in the study of autonomous decision-making in unmanned clustered distributed systems. However, communication constraints has become a big bottleneck that restricts its performance. To address the need for unmanned aerial vehicles(UAVs) to execute collaborative attack missions in complex communication-constrained environments, this paper propose an autonomous decision-making method for UAVs based on Multi-Agent Reinforcement Learning (MARL). Firstly, the autonomous decision-making processes of UAV clusters are modeled as Decentralized Partially Observable Markov Decision Processes(Dec-POMDPs). Next, the algorithm is enhanced within the framework of Multi-Agent Deep Deterministic Policy Gradient(MADDPG) by designing an explicit inter-intelligent communication mechanism to achieve information exchange among UAVs. Subsequently, the algorithm utilizes Long Short-Term Memory(LSTM) networks to process the local observations of the UAVs, enhancing the effectiveness of the information sent by combining historical data with current observations. Finally, multiple rounds of experiments are conducted across various communication-constrained scenarios. Simulation results indicate that the proposed method improves the task completion capability by 46.0% and enhances stability by 24.9% compared to baseline algorithm MADDPG. Additionally, the algorithm demonstrates better generalization and exhibits good scalability, effectively adapting to varying numbers of UAVs. This research provides new theoretical insights and a technical framework for the collaboration of UAVs in environments with communication constraints, which holds great practical importance in improving the ability and application scope of UAV systems.",IT,https://doi.org/10.1186/s13677-025-00738-9
Blockchain-enabled Zero Trust-based Secure and Energy Efficient scheme for 6G-enabled UASNs,"Marine networks such as the Underwater Acoustic Sensor Network (UASN) are the backbone for the exploration and monitoring of marine environments. These sensor nodes are deployed in extremely harsh underwater conditions, which are prone to various problems, including safety threats and energy drain. However, with their dramatic significance in multiple aspects, solutions for ensuring secured and efficient communications of these networks become essential. Herein, we put forth a new scheme termed Blockchain-enabled Zero Trust-based Secure and Energy Efficient (BZTSEE), where blockchain and a zero trust notion have been worked together into UASNs whose communications are enabled through 6G. BZTSEE works on identifying security and privacy issues while providing an energy-efficient solution. Using blockchain ensures secure and trustworthy communication, transparency in data sharing, maintenance of trust among nodes, and protection of user privacy. The scheme uses a PBFT protocol to defend itself against malicious actions and keep data secured. Several simulations show how BZTSEE works well under diverse conditions, including the number of nodes and attacks. The results render it quite effective for both small and large UASNs. In short, BZTSEE considerably augments the aspects of security, privacy, trust, and energy savings-a perfect fit for future UASNs.",IT,https://doi.org/10.1186/s13677-025-00744-x
An approach for multipath optimal selection of network service combinations based on golden eagle optimizer with double learning strategies,"The traditional optimal-path algorithm can address a single constraint in small and straightforward networks. However, in complex multipath distributed cloud services, the network nodes no longer exhibit singular or deterministic path characteristics. It requires the optimal paths that not only determines the shortest routes, but also combine the safety, speed, and enhanced service quality across multiple service nodes in the network topology. The Golden Eagle Optimization Algorithm (GEO) is specialized for optimizing these network service combinations. On this basis, the Golden Eagle Optimizer with Double Learning Strategies (GEO-DLS) resolved the multipath optimal service selection issues within intricate network environments. The algorithm modeled the hunting tactics of wild golden eagles, efficiently targeting the best prey in minimal time by dynamically adjusting two critical components, such as the attack and cruising strategies. In GEO-DLS, the enhanced GEO significantly broadened the search scope for food sources by using personalized learning and mirror reflection techniques. These advancements notably enhanced the GEO search capabilities and improved the solution accuracy. Key contribution include GEO-DLS can converge to the optimal solution faster by optimizing the search strategy and parameter settings. This means that in the problem of network service composition, algorithms can quickly find the optimal path that meets the quality of service (QoS) requirements. To validate the effectiveness of GEO, a set of ten standard benchmark functions was utilized to evaluate its performance. The results from these evaluations consistently presented its superior performance in tackling optimization challenges compared to other five metaheuristic algorithms and five enhanced algorithms.",IT,https://doi.org/10.1186/s13677-025-00728-x
Compliance and feedback based model to measure cloud trustworthiness for hosting digital twins,"Cloud-based digital twins use real-time data from various data sources to simulate the behavior and performance of their physical counterparts, enabling monitoring and analysis. However, one restraining factor in the use of cloud computing for digital twins is its users’ concerns about the security of their data. This data may be located anywhere in the cloud, with very limited control of the user to ensure its security. Cloud-based digital twins provide opportunities for researchers to collaborate yet security of such digital twins requires measures specific to cloud computing. To overcome this shortcoming, we need to devise a mechanism that not only ensures essential security safeguards but also computes a Trustworthiness value for Cloud Service Providers (CSP). This would give confidence to cloud users and enable them to choose the right CSP for their data-related interaction. This research proposes a solution, whereby the Trustworthiness of CSPs is calculated based on their Compliance with data security controls, User Feedback, and Auditor Rating. Two additional factors, Accuracy of Compliance Measurement and Control Significance Factor have been built in, to cater for other nonstandard conditions. Our implementation of Data Security Compliance Monitor and Data Trust as a Service, along with three CSPs, each with ten different settings, has supported our proposition through the devised formula. Experimental outcomes show changes in the trustworthiness value with changes in compliance level, user feedback and auditor rating. CSPs with better compliance have better trustworthiness values. However, if the Accuracy of Compliance Measurement and Control Significance Factor are low the trustworthiness is also proportionately less. This creates a balance and realism in our calculations. This model is unique and will help in creating users’ trust in cloud-based digital twins.",IT,https://doi.org/10.1186/s13677-024-00690-0
Enhancing lung cancer diagnosis with data fusion and mobile edge computing using DenseNet and CNN,"The recent advancements in automated lung cancer diagnosis through the application of Convolutional Neural Networks (CNN) on Computed Tomography (CT) scans have marked a significant leap in medical imaging and diagnostics. The precision of these CNN-based classifiers in detecting and analyzing lung cancer symptoms has opened new avenues in early detection and treatment planning. However, despite these technological strides, there are critical areas that require further exploration and development. In this landscape, computer-aided diagnostic systems and artificial intelligence, particularly deep learning methods like the region proposal network, the dual path network, and local binary patterns, have become pivotal. However, these methods face challenges such as limited interpretability, data variability handling issues, and insufficient generalization. Addressing these challenges is key to enhancing early detection and accurate diagnosis, fundamental for effective treatment planning and improving patient outcomes. This study introduces an advanced approach that combines a Convolutional Neural Network (CNN) with DenseNet, leveraging data fusion and mobile edge computing for lung cancer identification and classification. The integration of data fusion techniques enables the system to amalgamate information from multiple sources, enhancing the robustness and accuracy of the model. Mobile edge computing facilitates faster processing and analysis of CT scan images by bringing computational resources closer to the data source, crucial for real-time applications. The images undergo preprocessing, including resizing and rescaling, to optimize feature extraction. The DenseNet-CNN model, strengthened by data fusion and edge computing capabilities, excels in extracting and learning features from these CT scans, effectively distinguishing between healthy and cancerous lung tissues. The classification categories include Normal, Benign, and Malignant, with the latter further sub-categorized into adenocarcinoma, squamous cell carcinoma, and large cell carcinoma. In controlled experiments, this approach outperformed existing state-of-the-art methods, achieving an impressive accuracy of 99%. This indicates its potential as a powerful tool in the early detection and classification of lung cancer, a significant advancement in medical imaging and diagnostic technology.",IT,https://doi.org/10.1186/s13677-024-00597-w
Harmfulness metrics in digital twins of social network rumors detection in cloud computing environment,"Social network rumor harm metric is a task to score the harm caused by a rumor by analyzing the spreading range of the rumor, the users affected, the repercussions caused, etc., and then the harm caused by the rumor. Rumor hazard metric models can help rumor detection digital twins to understand and analyze user behaviors and assist social network network managers to make more informed decisions. However, there is a lack of models that can quantify the harm of rumors and automated harm metric models in rumor detection digital twins. To address this issue, this paper proposes an innovative social network rumor harm metric based on rumor propagation knowledge and a large language model (LLM), RSK-T5. The method first completes the joint task of rumor comment stance detection and sentiment analysis to capture critical features of rumor propagation. Then, this knowledge is used in the pre-training process of LLM to improve the model's understanding of rumor propagation patterns. Finally, the fine-tuning phase focuses on the hazard metrics task to improve the generalization energy. We compare with some existing variants of rumor detection methods, and experimental results demonstrate that RSK-T5 achieves the lowest MSE scores on three well-known rumor detection datasets. The ablative learning work demonstrates the effectiveness of RSK-T5's knowledge of two rumor spreads.",IT,https://doi.org/10.1186/s13677-024-00596-x
Unified ensemble federated learning with cloud computing for online anomaly detection in energy-efficient wireless sensor networks,"Anomaly detection in Wireless Sensor Networks (WSNs) is critical for their reliable and secure operation. Optimizing resource efficiency is crucial for reducing energy consumption. Two new algorithms developed for anomaly detection in WSNs—Ensemble Federated Learning (EFL) with Cloud Integration and Online Anomaly Detection with Energy-Efficient Techniques (OAD-EE) with Cloud-based Model Aggregation. EFL with Cloud Integration uses ensemble methods and federated learning to enhance detection accuracy and data privacy. OAD-EE with Cloud-based Model Aggregation uses online learning and energy-efficient techniques to conserve energy on resource-constrained sensor nodes. By combining EFL and OAD-EE, a comprehensive and efficient framework for anomaly detection in WSNs can be created. Experimental results show that EFL with Cloud Integration achieves the highest detection accuracy, while OAD-EE with Cloud-based Model Aggregation has the lowest energy consumption and fastest detection time among all algorithms, making it suitable for real-time applications. The unified algorithm contributes to the system's overall efficiency, scalability, and real-time response. By integrating cloud computing, this algorithm opens new avenues for advanced WSN applications. These promising  approaches for anomaly detection in resource constrained and large-scale WSNs are beneficial for industrial applications.",IT,https://doi.org/10.1186/s13677-024-00595-y
Multiobjective trajectory optimization algorithms for solving multi-UAV-assisted mobile edge computing problem,"The Internet of Things (IoT) devices are not able to execute resource-intensive tasks due to their limited storage and computing power. Therefore, Mobile edge computing (MEC) technology has recently been utilized to provide computing and storage capabilities to those devices, enabling them to execute these tasks with less energy consumption and low latency. However, the edge servers in the MEC network are located at fixed positions, which makes them unable to be adjusted according to the requirements of end users. As a result, unmanned aerial vehicles (UAVs) have recently been used to carry the load of these edge servers, making them mobile and capable of meeting the desired requirements for IoT devices. However, the trajectories of the UAVs need to be accurately planned in order to minimize energy consumption for both the IoT devices during data transmission and the UAVs during hovering time and mobility between halting points (HPs). The trajectory planning problem is a complicated optimization problem because it involves several factors that need to be taken into consideration. This problem is considered a multiobjective optimization problem since it requires simultaneous optimization of both the energy consumption of UAVs and that of IoT devices. However, existing algorithms in the literature for this problem have been based on converting it into a single objective, which may give preference to some objectives over others. Therefore, in this study, several multiobjective trajectory planning algorithms (MTPAs) based on various metaheuristic algorithms with variable population size and the Pareto optimality theory are presented. These algorithms aim to optimize both objectives simultaneously. Additionally, a novel mechanism called the cyclic selection mechanism (CSM) is proposed to manage the population size accurately, optimizing the number of HPs and the maximum function evaluations. Furthermore, the HPs estimated by each MTPA are associated with multiple UAVs using the k-means clustering algorithm. Then, a low-complexity greedy mechanism is used to generate the order of HPs assigned to each UAV, determining its trajectory. Several experiments are conducted to assess the effectiveness of the MTPAs with variable population size and cyclic selection mechanisms. The experimental findings demonstrate that the MTPAs with the cyclic selection mechanism outperform all competing algorithms, achieving better outcomes.",IT,https://doi.org/10.1186/s13677-024-00594-z
An enhanced state-aware model learning approach for security analysis in lightweight protocol implementations,"Owing to the emergence and rapid advances of new-generation information and digitalization technologies, the concept of model-driven digital twin has received widespread attentions and is developing vigorously. Driven by data and simulators, the digital twin can create the virtual twins of physical objects to perform monitoring, simulation, prediction, optimization, and so on. Hence, the application of digital twin can increase efficiency and security of systems by providing reliable model and decision supports. In this paper, we propose a state-aware model learning method to simulate and analyze the lightweight protocol implementations in edge/cloud environments. We introduce the data flow of program execution and network interaction inputs/outputs (I/O) into the extended finite state machine (EFSM) to expand the modeling scope and insight. We aim to calibrate the states and construct an accurate state-machine model using a digital twin based layered approach to reasonably reflect the correlation of a device’s external behavior and internal data. This, in turn, improves our ability to verify the logic and evaluate the security for protocol implementations. This method firstly involves instrumenting the target device to monitor variable activity during its execution. We then employ learning algorithms to produce multiple rounds of message queries. Both the I/O data corresponding to these query sequences and the state calibration information derived from filtered memory variables are obtained through the mapper and execution monitor, respectively. These two aspects of information are combined to dynamically and incrementally construct the protocol’s state machine. We apply this method to develop SALearn and evaluate the effectiveness of SALearn on two lightweight protocol implementations. Our experimental results indicate that SALearn outperforms existing protocol model learning tools, achieving higher learning efficiency and uncovering more interesting states and security issues. In total, we identified two violation scenarios of rekey logic. These situations also reflect the differences in details between different implementations.",IT,https://doi.org/10.1186/s13677-024-00593-0
An integrated SDN framework for early detection of DDoS attacks in cloud computing,"Cloud computing is a rapidly advancing technology with numerous benefits, such as increased availability, scalability, and flexibility. Relocating computing infrastructure to a network simplifies hardware and software resource monitoring in the cloud. Software-Defined Networking (SDN)-based cloud networking improves cloud infrastructure efficiency by dynamically allocating and utilizing network resources. While SDN cloud networks offer numerous advantages, they are vulnerable to Distributed Denial-of-Service (DDoS) attacks. DDoS attacks try to stop genuine users from using services and drain network resources to reduce performance or shut down services. However, early-stage detection of DDoS attack patterns in cloud environments remains challenging. Current methods detect DDoS at the SDN controller level, which is often time-consuming. We recommend focusing on SDN switches for early detection. Due to the large volume of data from diverse sources, we recommend traffic clustering and traffic anomalies prediction which is of DDoS attacks at each switch. Furthermore, to consolidate the data from multiple clusters, event correlation is performed to understand network behavior and detect coordinated attack activities. Many existing techniques stay behind for early detection and integration of multiple techniques to detect DDoS attack patterns. In this paper, we introduce a more efficient and effectively integrated SDN framework that addresses a gap in previous DDoS solutions. Our framework enables early and accurate detection of DDoS traffic patterns within SDN-based cloud environments. In this framework, we use Recursive Feature Elimination (RFE), Density Based Spatial Clustering (DBSCAN), time series techniques like Auto Regressive Integrated Moving Average (ARIMA), Lyapunov exponent, exponential smoothing filter, dynamic threshold, and lastly, Rule-based classifier. We have evaluated the proposed RDAER model on the CICDDoS 2019 dataset, that achieved an accuracy level of 99.92% and a fast detection time of 20 s, outperforming existing methods.",IT,https://doi.org/10.1186/s13677-024-00625-9
Non-orthogonal multiple access-based MEC for energy-efficient task offloading in e-commerce systems,"Mobile edge computing (MEC) reduces the latency for end users to access applications deployed at the edge by offloading tasks to the edge. With the popularity of e-commerce and the expansion of business scale, server load continues to increase, and energy efficiency issues gradually become more prominent. Computation offloading has received widespread attention as a technology that effectively reduces server load. However, how to improve energy efficiency while ensuring computing requirements is an important challenge facing computation offloading. To solve this problem, using non-orthogonal multiple access (NOMA) to increase the efficiency of multi-access wireless transmission, MEC supporting NOMA is investigated in the research. Computing resources will be divided into separate sub-computing that will be handled via e-commerce terminals or transferred to edge sides by reutilizing radio resources, we put forward a Group Switching Matching Algorithm Based on Resource Unit Allocation (GSM-RUA) algorithm that is multi-dimensional. To this end, we first formulate this task allocation problem as a long-term stochastic optimization problem, which we then convert to three short-term deterministic sub-programming problems using Lyapunov optimization, namely, radio resource allocation in a large timescale, computation resource allocating and splitting in a small-time frame. Of the 3 short-term deterministic sub-programming problems, the first sub-programming problem can be remodeled into a 1 to n matching problem, which can be solved using the block-shift-matching-based radio resource allocation method. The latter two sub-programming problems are then transformed into two continuous convex problems by relaxation and then solved easily. We then use simulations to prove that our GSM-RUA algorithm is superior to the state-of-the-art resource management algorithms in terms of energy consumption, efficiency and complexity for e-commerce scenarios.",IT,https://doi.org/10.1186/s13677-024-00680-2
Efficiently localizing system anomalies for cloud infrastructures: a novel Dynamic Graph Transformer based Parallel Framework,"Cloud environment is a virtual, online, and distributed computing environment that provides users with large-scale services. And cloud monitoring plays an integral role in protecting infrastructures in the cloud environment. Cloud monitoring systems need to closely monitor various KPIs of cloud resources, to accurately detect anomalies. However, due to the complexity and highly dynamic nature of the cloud environment, anomaly detection for these KPIs with various patterns and data quality is a huge challenge, especially those massive unlabeled data. Besides, it’s also difficult to improve the accuracy of the existing anomaly detection methods. To solve these problems, we propose a novel Dynamic Graph Transformer based Parallel Framework (DGT-PF) for efficiently detect system anomalies in cloud infrastructures, which utilizes Transformer with anomaly attention mechanism and Graph Neural Network (GNN) to learn the spatio-temporal features of KPIs to improve the accuracy and timeliness of model anomaly detection. Specifically, we propose an effective dynamic relationship embedding strategy to dynamically learn spatio-temporal features and adaptively generate adjacency matrices, and soft cluster each GNN layer through Diffpooling module. In addition, we also use nonlinear neural network model and AR-MLP model in parallel to obtain better detection accuracy and improve detection performance. The experiment shows that the DGT-PF framework have achieved the highest F1-Score on 5 public datasets, with an average improvement of 21.6% compared to 11 anomaly detection models.",IT,https://doi.org/10.1186/s13677-024-00677-x
Distance optimization and directional overcurrent relay coordination using edge-powered biogeography-genetic algorithms,"The effective functioning and regulation of power systems crucially rely on the coordination of distance and directional overcurrent relays. Accurate fault detection and successful clearing sequences require support for each relay and the maintenance of the coordination time interval (CTI) between major distance relays, directional overcurrent relay support, and other relay zones. Efficiently initiating relays while adhering to complex coordination limitations poses a challenging task that demands innovative solutions. This study addresses the intricate problem of relay coordination by employing heuristic methods, specifically genetic algorithms (GA) and biogeography-based optimization (BBO), in both a 9-bus and 39-bus system. The primary objective is to determine the most efficient time setting factor (TSM) that minimizes the duration of relay operation. Additionally, the intelligent features of the overcurrent relay are carefully chosen to enhance the research's results. The integration of edge computing capabilities plays a significant role in advancing this coordination method. By incorporating advanced algorithms and communication technologies at the edge, the prompt activation of relays becomes possible, thereby meeting coordination demands. This study explores the combination of edge-based servers with genetic algorithms (GA) and biogeography-based optimization (BBO) techniques to enhance relay coordination. The findings indicate a notable enhancement compared to conventional approaches. However, comparative research suggests that BBO's performance is similar to GA, without a distinct advantage in achieving higher outcomes.",IT,https://doi.org/10.1186/s13677-024-00672-2
Energy-efficient virtual machine placement in distributed cloud using NSGA-III algorithm,"Cloud computing is the most widely adapted computing model to process scientific workloads in remote servers accessed through the internet. In the IaaS cloud, the virtual machine (VM) is the execution unit that processes the user workloads. Virtualization enables the execution of multiple virtual machines (VMs) on a single physical machine (PM). Virtual machine placement (VMP) strategically assigns VMs to suitable physical devices within a data center. From the cloud provider's perspective, the virtual machine must be placed optimally to reduce resource wastage to aid economic revenue and develop green data centres. Cloud providers need an efficient methodology to minimize resource wastage, power consumption, and network transmission delay. This paper uses NSGA-III, a multi-objective evolutionary algorithm, to simultaneously reduce the mentioned objectives to obtain a non-dominated solution. The performance metrics (Overall Nondominated Vector Generation and Spacing) of the proposed NSGA-III algorithm is compared with other multi-objective algorithms, namely VEGA, MOGA, SPEA, and NSGA-II. It is observed that the proposed algorithm performs 7% better that the existing algorithm in terms of ONVG and 12% better results in terms of spacing. ANOVA and DMRT statistical tests are used to cross-validate the results.",IT,https://doi.org/10.1186/s13677-023-00501-y
COPSA: a computation offloading strategy based on PPO algorithm and self-attention mechanism in MEC-empowered smart factories,"With the dawn of Industry 5.0 upon us, the smart factory emerges as a pivotal element, playing a crucial role in the realm of intelligent manufacturing. Meanwhile, mobile edge computing is proposed to alleviate the computational burden presented by substantial workloads in smart factories. Nonetheless, it is very challenging to effectively incorporate edge computing resources to improve the efficiency of resource deployment in smart factories. Accordingly, we devise a novel approach based on Proximal Policy Optimization algorithm with the Self-Attention Mechanism to implement computing resource allocation in MEC-Empowered Smart Factories. More specifically, the self-attention mechanism is incorporated to enable dynamic focus on state information, accelerates convergence and facilitates global control. A great number of experiments conducted on both simulated and real datasets have verified the superiority of our proposed approach compared to the state-of-the-art baselines.",IT,https://doi.org/10.1186/s13677-024-00714-9
Joint Autoscaling of Containers and Virtual Machines for Cost Optimization in Container Clusters,"Autoscaling enables container cluster orchestrators to automatically adjust computational resources, such as containers and Virtual Machines (VMs), to handle fluctuating workloads effectively. This adaptation can involve modifying the amount of resources (horizontal scaling) or adjusting their computational capacity (vertical scaling). The motivation for our work stems from the limitations of previous autoscaling approaches, which are either partial (scaling containers or VMs, but not both) or excessively complex to be used in real systems. This complexity arises from their use of models with a large number of variables and the addressing of two simultaneous challenges: achieving the optimal deployment for a single scheduling window and managing the transition between successive scheduling windows. We propose an Integer Linear Programming (ILP) model to address the challenge of autoscaling containers and VMs jointly, both horizontally and vertically, to minimize deployment costs. This model is designed to be used with predictive autoscalers and be solved in a reasonable time, even for large clusters. To this end, improvements and reasonable simplifications with respect to previous models have been carried out to drastically reduce the size of the resource allocation problem. Furthermore, the proposed model provides an enhanced representation of system performance in comparison to previous approaches. A tool called Conlloovia has been developed to implement this model. To evaluate its performance, we have conducted a comprehensive assessment, comparing it with two heuristic allocators with different problem sizes. Our findings indicate that Conlloovia consistently demonstrates lower deployment costs in a significant number of cases. Conlloovia has also been evaluated with a real application, using synthetic and real workload traces, as well as different scheduling windows, with deployment costs approximately 20% lower than heuristic allocators.",IT,https://doi.org/10.1007/s10723-023-09732-4
Analyzing Energy-Efficient and Kubernetes-Based Autoscaling of Microservices Using Probabilistic Model Checking,"Microservices are widely used to enable agility and scalability in modern software systems, while cloud computing offers cost-effective ways to provision computing resources on demand. However, ensuring the correctness of scaling decisions and their impact on energy consumption is a challenging problem that has not been sufficiently addressed in previous research. Thus, in this paper, we present an innovative approach for analyzing host energy consumption and energy violations influenced by microservice autoscaling policies using probabilistic model checking (PMC). We propose four variations of the Markov Decision Process (MDP) models that incorporate various scaling constraints inspired by Kubernetes-based Horizontal Pod Autoscaler, and we encode these models using two different approaches, namely, bounded-by-action (BBA) and bounded-by-state (BBS). We use PMC to verify the scaling policies in terms of host energy consumption and energy violations, and we conduct sensitivity analysis to demonstrate the effectiveness of our models in generating energy-efficient scaling policies. Our results show that the latency and energy-based MDP model offers the most suitable policies for ensuring energy efficiency in microservice systems. Additionally, the number of pods and the scale-out action significantly affect energy consumption and violations. Sensitivity analysis also reveals that incorporating latency into scaling decisions is key to energy efficiency, while variations in the maximum pod threshold significantly influence energy consumption and violation. Our approach provides a formal method for ensuring the correctness of microservice autoscaling decisions in cloud environments at design time and can help reduce energy consumption and violations while ensuring service-level objectives are met.",IT,https://doi.org/10.1007/s10723-024-09789-9
KOSMOS: Vertical and Horizontal Resource Autoscaling for Kubernetes,"Cloud applications are increasingly executed onto lightweight containers that can be efficiently managed to cope with highly varying and unpredictable workloads. Kubernetes, the most popular container orchestrator, provides means to automatically scale containerized applications to keep their response time under control. Kubernetes provisions resources using two main components: i) Horizontal Pod Autoscaler (HPA), which controls the amount of containers running for an application, and ii) Vertical Pod Autoscaler (VPA), which oversees the resource allocation of existing containers. These two components have several limitations: they must control different metrics, they use simple threshold-based rules, and the reconfiguration of existing containers requires stopping and restarting them. To overcome these limitations this paper presents KOSMOS , a novel autoscaling solution for Kubernetes. Containers are individually controlled by control-theoretical planners that manage container resources on-the-fly (vertical scaling). A dedicated component is in charge of handling resource contention scenarios among containers deployed in the same node (a physical or virtual machine). Finally, at the cluster-level a heuristic-based controller is in charge of the horizontal scaling of each application.",IT,https://doi.org/10.1007/978-3-030-91431-8_59
Enhancing Edge Environment Scalability: Leveraging Kubernetes for Container Orchestration and Optimization,"Kubernetes is an open‐source container orchestration platform, offers a comprehensive suite of features for managing containerized applications effectively. These features encompass horizontal scaling, per‐node‐pool cluster scaling and automated resource request adjustments. This research endeavors to harness these capabilities to address the limitations experienced by fog servers in edge environments, particularly those arising from restricted network connectivity and scalability challenges. In this research paper, the primary focus is on Kubernetes role of enhancing scalability, providing a robust framework for managing containerized applications. The proposed approach involves creating a predefined number of pods and containers within a Kubernetes cluster, specifically designed to efficiently handle incoming requests while optimizing CPU and memory usage. This method implements a microservice architecture for the web tier, with separate pods for the front end, back end and database, ensuring modular and scalable design. All pods communicate and integrate through REST APIs, facilitating seamless interaction and data exchange between the services. When handling web requests, the approach enables and controls both internal and external networks, ensuring secure and efficient communication. The analysis then examines the CPU and memory utilization of the pods, as well as node bandwidth, to provide a comprehensive evaluation of container scalability and performance within the Kubernetes cluster. These findings effectively demonstrate Kubernetes' capability in managing container scalability and optimizing resource utilization, highlighting its efficiency and robustness in a microservice environment.",IT,https://doi.org/10.1002/cpe.8303
Container Scaling Strategy Based on Reinforcement Learning,"Elasticity capability is one of the most important capabilities of cloud computing, which combines large-scale resource allocation capability to quickly achieve minute-level resource demand provisioning to meet the elasticity requirements of different scale scenarios. The elasticity capability is mainly determined by the container start-up speed and container scaling strategy together, where the container scaling strategy contains both vertical container scaling strategy and horizontal container scaling strategy. In order to make the container scaling policy more effective and improve the application service quality and resource utilization, we briefly introduce Kubernetes’ horizontal pod autoscaling (HPA) strategy, analyze the existing problem of HPA, and develop a container scaling strategy based on reinforcement learning. First, we analyze the problems of Kubernetes’ existing HPA container autoscaling strategy in the scale-up and scale-down phases, respectively. Second, the Markov decision model is used to model the container scaling problem. Then, we propose a model-based reinforcement learning algorithm to solve the container scaling problem. Finally, we compare the experimental results of the HPA scaling strategy and the model-based reinforcement learning strategy with the results from the resource utilization of the application, the change of the number of pods, and the application response time; through the experimental analysis, we verify that the reinforcement learning-based container scaling strategy can guarantee the application service quality and improve the utilization of the application resources more effectively than the HPA strategy.",IT,https://doi.org/10.1155/2023/7400235
STAM-LSGRU: a spatiotemporal radar echo extrapolation algorithm with edge computing for short-term forecasting,"With the advent of Mobile Edge Computing (MEC), shifting data processing from cloud centers to the network edge presents an advanced computational paradigm for addressing latency-sensitive applications. Specifically, in radar systems, the real-time processing and prediction of radar echo data pose significant challenges in dynamic and resource-constrained environments. MEC, by processing data near its source, not only significantly reduces communication latency and enhances bandwidth utilization but also diminishes the necessity of transmitting large volumes of data to the cloud, which is crucial for improving the timeliness and efficiency of radar data processing. To meet this demand, this paper proposes a model that integrates a spatiotemporal Attention Module (STAM) with a Long Short-Term Memory Gated Recurrent Unit (ST-ConvLSGRU) to enhance the accuracy of radar echo prediction while leveraging the advantages of MEC. STAM, by extending the spatiotemporal receptive field of the prediction units, effectively captures key inter-frame motion information, while optimizations to the convolutional structure and loss function further boost the model’s predictive performance. Experimental results demonstrate that our approach significantly improves the accuracy of short-term weather forecasting in a mobile edge computing environment, showcasing an efficient and practical solution for processing radar echo data under dynamic, resource-limited conditions.",IT,https://doi.org/10.1186/s13677-024-00660-6
ABWOA: adaptive boundary whale optimization algorithm for large-scale digital twin network construction,"Digital twin network (DTN) as an emerging network paradigm, have garnered growing attention. For large-scale networks, a crucial problem is how to effectively map physical networks onto the infrastructure platform of DTN. To address this issue, we propose a heuristic method of the adaptive boundary whale optimization algorithm (ABWOA) to solve the digital twin network construction problem, improving the efficiency and reducing operational costs of DTN. Extensive comparison experiments are conducted between ABWOA and various algorithms such as genetic algorithm, particle swarm optimization, artificial bee colony, differential evolution algorithm, moth search algorithm and original whale optimization algorithm. The experimental results show that ABWOA is superior to other algorithms in terms of solution quality, convergence speed, and time cost. It can solve the digital twin network construction problem more effectively.",IT,https://doi.org/10.1186/s13677-024-00667-z
Load balancing scheduling mechanism for OpenStack and Docker integration,"With the development of cloud-edge collaborative computing, cloud computing has become crucial in data analysis and data processing. OpenStack and Docker are important components of cloud computing, and the integration of the two has always attracted widespread attention in industry. The scheduling mechanism adopted by the existing fusion solution uses a uniform resource weight for all containers, and the computing nodes resources on the cloud platform is unbalanced under differentiated resource requirements of the containers. Therefore, considering different network communication qualities, a load-balancing Docker scheduling mechanism based on OpenStack is proposed to meet the needs of various resources (CPU, memory, disk, and bandwidth) of containers. This mechanism uses the precise limitation strategy for container resources and a centralized strategy for container resources as the scheduling basis, and it generates exclusive weights for containers through a filtering stage, a weighing stage based on weight adaptation, and a non-uniform memory access (NUMA) lean stage. The experimental results show that, compared with Nova-docker and Yun, the proposed mechanism reduces the resource load imbalance within a node by 57.35% and 59.00% on average, and the average imbalance between nodes is reduced by 53.53% and 50.90%. This mechanism can also achieve better load balancing without regard to bandwidth.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00445-3
Comparative analysis of metaheuristic load balancing algorithms for efficient load balancing in cloud computing,"Load balancing is a serious problem in cloud computing that makes it challenging to ensure the proper functioning of services contiguous to the Quality of Service, performance assessment, and compliance to the service contract as demanded from cloud service providers (CSP) to organizations. The primary objective of load balancing is to map workloads to use computing resources that significantly improve performance. Load balancing in cloud computing falls under the class of concerns defined as ""NP-hard"" issues due to vast solution space. Therefore it requires more time to predict the best possible solution. Few techniques can perhaps generate an ideal solution under a polynomial period to fix these issues. In previous research, Metaheuristic based strategies have been confirmed to accomplish accurate solutions under a decent period for those kinds of issues. This paper provides a comparative analysis of various metaheuristic load balancing algorithms for cloud computing based on performance factors i.e., Makespan time, degree of imbalance, response time, data center processing time, flow time, and resource utilization. The simulation results show the performance of various Meta-heuristic Load balancing methods, based on performance factors. The Particle swarm optimization method performs better in improving makespan, flow time, throughput time, response time, and degree of imbalance.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00453-3
ReactiveFnJ: A choreographed model for Fork-Join Workflow in Serverless Computing,"Function-as-a-Service (FaaS) is an event-based reactive programming model where functions run in ephemeral stateless containers for short duration. For building complex serverless applications, function composition is crucial to coordinate and synchronize the workflow of an application. Some serverless orchestration systems exist, but they are in their primitive state and do not provide inherent support for non-trivial workflows like, Fork-Join. To address this gap, we propose a fully serverless and scalable design model ReactiveFnJ for Fork-Join workflow. The intent of this work is to illustrate a design which is completely choreographed, reactive, asynchronous, and represents a dynamic composition model for serverless applications based on Fork-Join workflow. Our design uses two innovative patterns, namely, Relay Composition and Master-Worker Composition to solve execution time-out challenges. As a Proof-of-Concept (PoC), the prototypical implementation of Split-Sort-Merge use case, based on Fork-Join workflow is discussed and evaluated. The ReactiveFnJ handles embarrassingly parallel computations, and its design does not depend on any external orchestration services, messaging services, and queue services. ReactiveFnJ facilitates in designing fully automated pipelines for distributed data processing systems, satisfying the Serverless Trilemma in true essence. A file of any size can be processed using our effective and extensible design without facing execution time-out challenges. The proposed model is generic and can be applied to a wide range of serverless applications that are based on the Fork-Join workflow pattern. It fosters the choreographed serverless composition for complex workflows. The proposed design model is useful for software engineers and developers in industry and commercial organizations, total solution vendors and academic researchers.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00429-3
Edge computing-based digital management system of game events in the era of Internet of Things,"With the great development of Internet of Things (IoT) and edge computing, the development of sports activities depends on the development of information technology and it is inevitable to pay attention to the combination and optimization of resources. The combination of IoT and edge computing will be critical in sports activities. This paper elaborates on the application of network skill in sports event information management, that is, through the effective gathering of sports event data, to realize the use of sports event information, to achieve the purpose of information and digitization. Furthermore, the goal is to investigate the effect of sports event in the era of IoT. The impact of sports events on the economy and culture of the hosting city is investigated using IoT concept of edge computing. By analyzing the advantages and disadvantages of traditional centralized optimization method, we present a series of performance indicators and utility functions and show that the method is effective and achieves the optimal purpose. Through vital research, it is found that with the development of the edge computing and IoT industry, the scale of sports events is constantly expanding. By 2019, there has been a scale of 1,271 billion yuan. An increase of 981 billion yuan, compared with 290 billion yuan in 2013. Therefore, the use of the IoT technology in combination with edge computing to manage sports events will greatly encourage the expansion of sports activities. Furthermore, the holding of sporting events reflects a city’s overall strength and enhances the city’s exposure and fame. The investigation offers a certain reference point for cities looking to increase their influence through events.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00419-5
Memory sharing for handling memory overload on physical machines in cloud data centers,"Over-committing computing resources is a widely adopted strategy for increased cluster utilization in Infrastructure as a Service (IaaS) cloud data centers. A potential consequence of over-committing computing resources is memory overload of physical machines (PMs). Memory overload occurs if memory usage exceeds a defined alarm threshold, exposing running computation tasks at a risk of being terminated by the operating system. A prevailing measure to handle memory overload of a PM is live migration of virtual machines (VMs). However, this not only consumes network bandwidth, CPU, and other resources, but also compels a temporary unavailability of the VMs being migrated. To handle memory overload, we present a memory sharing system in this paper for PMs in cloud data centers. With memory sharing, a PM automatically borrows memory from a remote PM when necessary, and releases the borrowed memory when memory overload disappears. This is implemented through swapping inactive memory pages to remote memory resource. Experimental studies conducted on InfiniBand-networked PMs show that the memory sharing system is fully functional. The measured throughput and latency are around 929 Mbps and 1.3 $$\mu$$                   μ                 s, respectively, on average for remote memory access. They are similar to those from accessing a local-volatile memory express solid-state drive, and thus are promising in real applications.",IT,https://doi.org/10.1186/s13677-023-00405-x
A privacy protection approach in edge-computing based on maximized dnn partition strategy with energy saving,"With the development of deep neural network (DNN) techniques, applications of DNNs show state-of-art performance. In the cloud edge collaborative mode, edge devices upload the raw data, such as texts, images, and videos, to the cloud for processing. Then, the cloud returns prediction or classification results. Although edge devices take advantage of the powerful performance of DNN, there are also colossal privacy protection risks. DNN partition strategy can effectively solve the privacy problems by offload part of the DNN model to the edge, in which the encoded features are transmitted rather than original data. We explore the relationship between privacy and the intermedia result of the DNN. The more parts offloaded to the edge, the more abstract features we can have, indicating more conducive to privacy protection. We propose a privacy protection approach based on a maximum DNN partition strategy. Besides, a mix-precision quantization approach is adopted to reduce the energy use of edge devices. The experiments show that our method manages to increase at most 20% model privacy in various DNN architecture. Through the energy-aware mixed-precision quantization approach, the model’s energy consumption is reduced by at most 5x comparing to the typical edge-cloud solution.",IT,https://doi.org/10.1186/s13677-023-00404-y
Enriching computing simulators by generating realistic serverless traces,"Serverless computing is stepping forward to provide a cloud environment that mainly focuses on managing infrastructure, resources and configurations on the behalf of a user. Research in this field can’t rely on commercial providers such as AWS and Azure, as their inflexibility and cost often limits the required levels of reproducibility and scalability. Therefore, simulators have been opted as an alternative solution by the research community. They offer a reduced-cost and easy-setup environment. To get respectable precision, simulators use real traces collected and offered by commercial providers. These traces represent comprehensive information of executed tasks that reflect user behaviour. Due to serverless computing’s recency, typical workload traces employed by IaaS simulators are not well adoptable to the new computing model. In this paper, we propose an approach for generating realistic serverless traces. We enhance our previous generator approach that was based on the Azure Functions dataset. Our new, genetic algorithm based approach improves the statistical properties of the generated traces. We also enabled arbitrary scaling of the workload, while maintaining real users’ behaviour. These advances further support reproducibility in the serverless research community. We validated the results of our generator approach using the coefficient of determination (R^2), which shows that our generated workload closely matches the original dataset’s characteristics in terms of execution time, memory utilisation as well as user participation percentage. To demonstrate the benefits of the reusability of the generated traces, we applied them with a diverse set of simulators and shown that they offer reproducible results independently of the simulator used.",IT,https://doi.org/10.1186/s13677-023-00397-8
"Data pipeline approaches in serverless computing: a taxonomy, review, and research trends","Serverless computing has gained significant popularity due to its scalability, cost-effectiveness, and ease of deployment. With the exponential growth of data, organizations face the challenge of efficiently processing and analyzing vast amounts of data in a serverless environment. Data pipelines play a crucial role in managing and transforming data within serverless architectures. This paper provides a taxonomy of data pipeline approaches in serverless computing. Classification is based on architectural features, data processing techniques, and workflow orchestration mechanisms, these approaches are categorized into three primary methods: heuristic-based approach, Machine learning-based approach, and framework-based approach. Furthermore, a systematic review of existing data pipeline frameworks and tools is provided, encompassing their strengths, limitations, and real-world use cases. The advantages and disadvantages of each approach, also the challenges and performance metrics that influence their effectuality have been examined. Every data pipeline approach has certain advantages and disadvantages, whether it is framework-based, heuristic-based, or machine learning-based. Each approach is suitable for specific use cases. Hence, it is crucial assess the trade-offs between complexity, performance, cost, and scalability, while selecting a data pipeline approach. In the end, the paper highlights a number of open issues and future investigations directions for data pipeline in the serverless computing, which involve scalability, fault tolerance, data real time processing, data workflow orchestration, function state management with performance and cost in the serverless computing environments.",IT,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00939-0
"SSF-CDW: achieving scalable, secure, and fast OLAP query for encrypted cloud data warehouse","Implementing a cloud data-warehouse to store sensitive data presents challenges, especially for OLAP queries over encrypted multidimensional data. This paper proposes SSF-CDW, a privacy-preserving scheme combining symmetric encryption and CP-ABE with a Redis-based cube-retrieval mechanism. It supports fine-grained access, accelerates ad-hoc & repeated OLAP queries, and shows significant speed-ups over baseline indexing while preserving privacy.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00692-y
"Orchestration in the Cloud-to-Things compute continuum: taxonomy, survey and future directions","IoT use-cases require simultaneous access to heterogeneous sensors and multi-cloud resources. Extending orchestration across the Cloud-to-Things continuum raises new challenges. This survey builds a detailed taxonomy of orchestration requirements, reviews existing research/industrial solutions (e.g., AWS CloudFormation, Kubernetes, MiCADO), identifies gaps, and outlines future research challenges for automated deployment and run-time management across cloud, fog and edge layers.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00516-5
Domain-knowledge-free cloud-IDS with lightweight embedding method,"Cloud attacks are rising as data migrates to virtualised servers. The paper introduces C-IDS, an adaptive cloud intrusion-detection system automatically deployed via Infrastructure-as-Code. Using Seq2Seq BI-LSTM with Bahdanau attention, it learns environment-specific logs and detects anomalies in real time. Experiments on Linux, Windows, Hadoop, OpenStack, Apache and CICIDS2018 datasets show 98.2 % recognition and 94.2 % detection accuracy, demonstrating effective, IaC-driven cloud security.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00707-8
A proposed architecture for network forensic system in large-scale networks,"Cybercrime is increasing at a faster pace and sometimes causes billions of dollars of business- losses so investigating attackers after commitment is of utmost importance and become one of the main concerns of network managers. Network forensics as the process of Collecting, identifying, extracting and analyzing data and systematically monitoring traffic of network is one of the main requirements in detection and tracking of criminals. In this paper, we propose an architecture for network forensic system. Our proposed architecture consists of five main components: collection and indexing, database management, analysis component, SOC communication component and the database. The main difference between our proposed architecture and other systems is in analysis component. This component is composed of four parts: Analysis and investigation subsystem, Reporting subsystem, Alert and visualization subsystem and the malware analysis subsystem. The most important differentiating factors of the proposed system with existing systems are: clustering and ranking of malware, dynamic analysis of malware, collecting and analysis of network flows and anomalous behaviour analysis.",IT,https://arxiv.org/abs/1508.01890
A Framework for Measuring the Quality of Infrastructure-as-Code Scripts,"Infrastructure as Code (IaC) has become integral to modern software development, enabling automated and consistent configuration of computing environments. The rapid proliferation of IaC scripts has highlighted the need for better code quality assessment methods. This paper proposes a new IaC code quality framework specifically showcased for Ansible repositories as a foundation. By analyzing a comprehensive dataset of repositories from Ansible Galaxy, we applied our framework to evaluate code quality across multiple attributes. The analysis of our code quality metrics applied to Ansible Galaxy repositories reveal trends over time indicating improvements in areas such as metadata and error handling, while highlighting declines in others such as sophistication and automation. The framework offers practitioners a systematic tool for assessing and enhancing IaC scripts, fostering standardization and facilitating continuous improvement. It also provides a standardized foundation for further work into IaC code quality.",IT,https://arxiv.org/abs/2502.03127
Polyglot Code Smell Detection for Infrastructure as Code with GLITCH,"This paper presents GLITCH, a new technology-agnostic framework that enables automated polyglot code smell detection for Infrastructure as Code scripts. GLITCH uses an intermediate representation on which different code smell detectors can be defined. It currently supports the detection of nine security smells and nine design & implementation smells in scripts written in Ansible, Chef, Docker, Puppet, or Terraform. Studies conducted with GLITCH not only show that GLITCH can reduce the effort of writing code smell analyses for multiple IaC technologies, but also that it has higher precision and recall than current state-of-the-art tools. A video describing and demonstrating GLITCH is available at: https://youtu.be/E4RhCcZjWbk",IT,https://arxiv.org/abs/2308.09458
A Survey on Software Defined Networking: Architecture for Next Generation Network,"The evolution of software defined networking (SDN) has played a significant role in the development of next-generation networks (NGN). SDN as a programmable network having service provisioning on the fly has induced a keen interest both in academic world and industry. In this article, a comprehensive survey is presented on SDN advancement over conventional network. The paper covers historical evolution in relation to SDN, functional architecture of the SDN and its related technologies, and OpenFlow standards/protocols, including the basic concept of interfacing of OpenFlow with network elements (NEs) such as optical switches. In addition a selective architecture survey has been conducted. Our proposed architecture on software defined heterogeneous network, points towards new technology enabling the opening of new vistas in the domain of network technology, which will facilitate in handling of huge internet traffic and helps infrastructure and service providers to customize their resources dynamically. Besides, current research projects and various activities as being carried out to standardize SDN as NGN by different standard development organizations (SODs) have been duly elaborated to judge how this technology moves towards standardization.",IT,https://arxiv.org/abs/2001.10165
Network Function Virtualization: State-of-the-art and Research Challenges,"Network Function Virtualization (NFV) has drawn significant attention from both industry and academia as an important shift in telecommunication service provisioning. By decoupling Network Functions (NFs) from the physical devices on which they run, NFV has the potential to lead to significant reductions in Operating Expenses (OPEX) and Capital Expenses (CAPEX) and facilitate the deployment of new services with increased agility and faster time-to-value. The NFV paradigm is still in its infancy and there is a large spectrum of opportunities for the research community to develop new architectures, systems and applications, and to evaluate alternatives and trade-offs in developing technologies for its successful deployment. In this paper, after discussing NFV and its relationship with complementary fields of Software Defined Networking (SDN) and cloud computing, we survey the state-of-the-art in NFV, and identify promising research directions in this area. We also overview key NFV projects, standardization efforts, early implementations, use cases and commercial products.",IT,https://arxiv.org/abs/1509.07675
Revolutionizing Datacenter Networks via Reconfigurable Topologies,"With the popularity of cloud computing and data-intensive applications such as machine learning, datacenter networks have become a critical infrastructure for our digital society. Given the explosive growth of datacenter traffic and the slowdown of Moore's law, significant efforts have been made to improve datacenter network performance over the last decade. A particularly innovative solution is reconfigurable datacenter networks (RDCNs): datacenter networks whose topologies dynamically change over time, in either a demand-oblivious or a demand-aware manner. Such dynamic topologies are enabled by recent optical switching technologies and stand in stark contrast to state-of-the-art datacenter network topologies, which are fixed and oblivious to the actual traffic demand. In particular, reconfigurable demand-aware and 'self-adjusting' datacenter networks are motivated empirically by the significant spatial and temporal structures observed in datacenter communication traffic. This paper presents an overview of reconfigurable datacenter networks. In particular, we discuss the motivation for such reconfigurable architectures, review the technological enablers, and present a taxonomy that classifies the design space into two dimensions: static vs. dynamic and demand-oblivious vs. demand-aware. We further present a formal model and discuss related research challenges. Our article comes with complementary video interviews in which three leading experts, Manya Ghobadi, Amin Vahdat, and George Papen, share with us their perspectives on reconfigurable datacenter networks.",IT,https://arxiv.org/abs/2502.16228
Disruption-aware Microservice Re-orchestration for Cost-efficient Multi-cloud Deployments,"Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice colocation constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the default Kubernetes scheduler implementation, while ensuring QoS requirements are consistently met.",IT,https://arxiv.org/abs/2501.16143
"Portable, high-performance containers for HPC","Building and deploying software on high-end computing systems is a challenging task. High performance applications have to reliably run across multiple platforms and environments, and make use of site-specific resources while resolving complicated software-stack dependencies. Containers are a type of lightweight virtualization technology that attempt to solve this problem by packaging applications and their environments into standard units of software that are: portable, easy to build and deploy, have a small footprint, and low runtime overhead. In this work we present an extension to the container runtime of Shifter that provides containerized applications with a mechanism to access GPU accelerators and specialized networking from the host system, effectively enabling performance portability of containers across HPC resources. The presented extension makes possible to rapidly deploy high-performance software on supercomputers from containerized applications that have been developed, built, and tested in non-HPC commodity hardware, e.g. the laptop or workstation of a researcher.",IT,https://arxiv.org/abs/1704.03383
"Serverless Computing: Architecture, Concepts, and Applications","Recently, serverless computing has gained recognition as a leading cloud computing method. Providing a solution that does not require direct server and infrastructure management, this technology has addressed many traditional model problems by eliminating them. Therefore, operational complexity and costs are reduced, allowing developers to concentrate on writing and deploying software without worrying about server management. This chapter examines the advantages, disadvantages, and applications of serverless computing, implementation environments, and reasons for its use. Additionally, integrating this computing paradigm with other technologies is examined to address the challenges of managing, securing, and implementing large amounts of data. This chapter aims to provide a comprehensive view of the potentials and limitations of serverless computing by comparing its applications in different industries and examining the future trends of this technology. Lastly, this chapter provides a comprehensive conclusion of the applications and challenges of serverless computing.",IT,https://arxiv.org/abs/2501.09831
"AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges","Artificial Intelligence for IT operations (AIOps) aims to combine the power of AI with the big data generated by IT Operations processes, particularly in cloud infrastructures, to provide actionable insights with the primary goal of maximizing availability. There are a wide variety of problems to address, and multiple use-cases, where AI capabilities can be leveraged to enhance operational efficiency. Here we provide a review of the AIOps vision, trends challenges and opportunities, specifically focusing on the underlying AI techniques. We discuss in depth the key types of data emitted by IT Operations activities, the scale and challenges in analyzing them, and where they can be helpful. We categorize the key AIOps tasks as - incident detection, failure prediction, root cause analysis and automated actions. We discuss the problem formulation for each task, and then present a taxonomy of techniques to solve these problems. We also identify relatively under explored topics, especially those that could significantly benefit from advances in AI literature. We also provide insights into the trends in this field, and what are the key investment opportunities.",IT,https://arxiv.org/abs/2304.04661
Cloud Resource Allocation with Convex Optimization,"We present a convex optimization framework for overcoming the limitations of Kubernetes Cluster Autoscaler by intelligently allocating diverse cloud resources while minimizing costs and fragmentation. Current Kubernetes scaling mechanisms are restricted to homogeneous scaling of existing node types, limiting cost-performance optimization possibilities. Our matrix-based model captures resource demands, costs, and capacity constraints in a unified mathematical framework. A key contribution is our logarithmic approximation to the indicator function, which enables dynamic node type selection while maintaining problem convexity. Our approach balances cost optimization with operational complexity through interior-point methods. Experiments with real-world Kubernetes workloads demonstrate reduced costs and improved resource utilization compared to conventional Cluster Autoscaler strategies that can only scale up or down existing node pools.",IT,https://arxiv.org/abs/2503.21096
Foundational DevOps Patterns,"Adopting DevOps practices is nowadays a recurring task in the industry. DevOps is a set of practices intended to reduce the friction between the software development (Dev) and the IT operations (Ops), resulting in higher quality software and a shorter development lifecycle. Even though many resources are talking about DevOps practices, they are often inconsistent with each other on the best DevOps practices. Furthermore, they lack the needed detail and structure for beginners to the DevOps field to quickly understand them. In order to tackle this issue, this paper proposes four foundational DevOps patterns: Version Control Everything, Continuous Integration, Deployment Automation, and Monitoring. The patterns are both detailed enough and structured to be easily reused by practitioners and flexible enough to accommodate different needs and quirks that might arise from their actual usage context. Furthermore, the patterns are tuned to the DevOps principle of Continuous Improvement by containing metrics so that practitioners can improve their pattern implementations.",IT,https://arxiv.org/abs/2302.01053
Mitigating Configuration Differences Between Development and Production Environments: A Catalog of Strategies,"Context: The Configuration Management of the development and production environments is an important aspect of IT operations. However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime. Objective: In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments. Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues. Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data. These participants shed some light on the current practices, processes, challenges, or issues they have encountered. Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences. These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies. Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment. In addition, they can also ensure compliance with industry standards and regulations.",IT,https://arxiv.org/abs/2505.09392
Lorentzian-Constrained Holographic Beamforming Optimization in Multi-user Networks with Dynamic Metasurface Antennas,"Dynamic metasurface antennas (DMAs) are promising alternatives to fully digital (FD) architectures, enabling hybrid beamforming via low-cost reconfigurable metasurfaces. In DMAs, holographic beamforming is achieved through tunable elements by Lorentzian-constrained holography (LCH), significantly reducing the need for radio-frequency (RF) chains and analog circuitry. However, the Lorentzian constraints and limited RF chains introduce a trade-off between reduced system complexity and beamforming performance, especially in dense network scenarios. This paper addresses resource allocation in multi-user multiple-input-single-output (MISO) networks under the Signal-to-Interference-plus-Noise Ratio (SINR) constraints, aiming to minimize total transmit power. We propose a holographic beamforming algorithm based on the Generalized Method of Lorentzian-Constrained Holography (GMLCH), which optimizes DMA weights, yielding flexibility for using various LCH techniques to tackle the aforementioned trade-offs. Building upon GMLCH, we further propose a new algorithm, Adaptive Radius Lorentzian Constrained Holography (ARLCH), which achieves optimization of DMA weights with additional degree of freedom in a greater optimization space, and provides lower transmitted power, while improving scalability for higher number of users. Numerical results show that ARLCH reduces power consumption by over 20% compared to benchmarks, with increasing effectiveness as the number of users grows.",IT,https://arxiv.org/abs/2505.08356
Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks,"Sharpness-Aware Minimization (SAM) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. We introduce ZSharp, a refined sharpness-aware optimization method that incorporates layer-wise Z-score normalization followed by percentile-based filtering. This process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. ZSharp retains the standard two-phase SAM structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10, CIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and Vision Transformers. Across all architectures and datasets, ZSharp consistently achieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These results indicate that Z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.",IT,https://arxiv.org/abs/2505.02369
Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification,"To efficiently compress the sign information of images, we address a sign retrieval problem for the block-wise discrete cosine transformation (DCT): reconstruction of the signs of DCT coefficients from their amplitudes. To this end, we propose a fast sign retrieval method on the basis of binary classification machine learning. We first introduce 3D representations of the amplitudes and signs, where we pack amplitudes/signs belonging to the same frequency band into a 2D slice, referred to as the sub-band block. We then retrieve the signs from the 3D amplitudes via binary classification, where each sign is regarded as a binary label. We implement a binary classification algorithm using convolutional neural networks, which are advantageous for efficiently extracting features in the 3D amplitudes. Experimental results demonstrate that our method achieves accurate sign retrieval with an overwhelmingly low computation cost.",IT,https://arxiv.org/abs/2504.21632
On the Secrecy-Sensing Optimization of RIS-assisted Full-Duplex Integrated Sensing and Communication Network,"Integrated sensing and communication (ISAC) has recently emerged as a viable technique for establishing sensing and communication using the same resources. Nonetheless, the operation of ISAC networks is often challenged by the absence of a direct link between the sensing node and the targets, and by the risk of disclosing confidential data to malicious targets when using the same signal for both tasks. In this paper, a robust reconfigurable intelligent surface (RIS)-aided scheme for securing a full-duplex (FD) ISAC network is proposed. The considered network consists of uplink and downlink users served in FD through a multi-antenna dual-functional radar communication base station (BS), which employs co-located multi-antenna communication-radar arrays to detect multiple malicious targets while preserving communication secrecy in their presence. Additionally, the BS utilizes an optimized artificial noise (AN) that serves to disrupt the malicious targets' reception and increase the sensing power. By optimally designing the RIS phase shifts, transmit beamforming, AN covariance, and uplink users' transmit power and combining vectors using an alternating optimization-based algorithm, the network's sensing performance is maximized under secrecy and total power constraints. Numerical results present the proposed scheme's efficacy, particularly when a direct link between the BS and the various nodes/targets is absent.",IT,https://arxiv.org/abs/2504.20912
Active Reconfigurable Intelligent Surface Assisted MIMO: Electromagnetic-Compliant Modeling with Mutual Coupling,"Reconfigurable Intelligent Surfaces (RIS) represent a transformative technology for sixth-generation (6G) wireless communications, but it suffers from a significant limitation, namely the double-fading attenuation. Active RIS has emerged as a promising solution, effectively mitigating the attenuation issues associated with conventional RIS-assisted systems. However, the current academic work on active RIS focuses on the system-level optimization of active RIS, often overlooking the development of models that are compatible with its electromagnetic (EM) and physical properties. The challenge of constructing realistic, EM-compliant models for active RIS-assisted communication, as well as understanding their implications on system-level optimization, remains an open research area. To tackle these problems, in this paper we develop a novel EM-compliant model with mutual coupling (MC) for active RIS-assisted wireless systems by integrating the developed scattering-parameter ($S$-parameter) based active RIS framework with multiport network theory, which facilitates system-level analysis and optimization. To evaluate the performance of the EM-compliant active RIS model, we design the joint optimization scheme based on the transmit beamforming at the transmitter and the reflection coefficient at the active RIS to maximize the achievable rate of EM-compliant active RIS-assisted MIMO system. To tackle the inherent non-convexity of this problem, we employ the Sherman-Morrison inversion and Neumann series (SMaN)-based alternating optimization (AO) algorithm. Simulation results verified that EM property (i.e., MC effect) is an indispensable factor in the optimization process of MIMO systems. Neglecting this effect introduces a substantial performance gap, highlighting its significance in the more pronounced the MC effect is, the greater the gap in achievable rates.",IT,https://arxiv.org/abs/2504.15961
A Tutorial on Six-Dimensional Movable Antenna for 6G Networks: Synergizing Positionable and Rotatable Antennas,"Six-dimensional movable antenna (6DMA) is a new and revolutionary technique that fully exploits the wireless channel spatial variations at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and/or 3D rotations of antennas/antenna surfaces (sub-arrays), thereby improving the performance of wireless networks cost-effectively without the need to deploy additional antennas. It is thus expected that the integration of new 6DMAs into future sixth-generation (6G) wireless networks will fundamentally enhance antenna agility and adaptability, and introduce new degrees of freedom (DoFs) for system design. Despite its great potential, 6DMA faces new challenges to be efficiently implemented in wireless networks, including corresponding architectures, antenna position and rotation optimization, channel estimation, and system design from both communication and sensing perspectives. In this paper, we provide a tutorial on 6DMA-enhanced wireless networks to address the above issues by unveiling associated new channel models, hardware implementations and practical position/rotation constraints, as well as various appealing applications in wireless networks. Moreover, we discuss two special cases of 6DMA, namely, rotatable 6DMA with fixed antenna position and positionable 6DMA with fixed antenna rotation, and highlight their respective design challenges and applications. We further present prototypes developed for 6DMA-enhanced communication along with experimental results obtained with these prototypes. Finally, we outline promising directions for further investigation.",IT,https://arxiv.org/abs/2503.18240
Semantic-aided Parallel Image Transmission Compatible with Practical System,"In this paper, we propose a novel semantic-aided image communication framework for supporting the compatibility with practical separation-based coding architectures. Particularly, the deep learning (DL)-based joint source-channel coding (JSCC) is integrated into the classical separate source-channel coding (SSCC) to transmit the images via the combination of semantic stream and image stream from DL networks and SSCC respectively, which we name as parallel-stream transmission. The positive coding gain stems from the sophisticated design of the JSCC encoder, which leverages the residual information neglected by the SSCC to enhance the learnable image features. Furthermore, a conditional rate adaptation mechanism is introduced to adjust the transmission rate of semantic stream according to residual, rendering the framework more flexible and efficient to bandwidth allocation. We also design a dynamic stream aggregation strategy at the receiver, which provides the composite framework with more robustness to signal-to-noise ratio (SNR) fluctuations in wireless systems compared to a single conventional codec. Finally, the proposed framework is verified to surpass the performance of both traditional and DL-based competitors in a large range of scenarios and meanwhile, maintains lightweight in terms of the transmission and computational complexity of semantic stream, which exhibits the potential to be applied in real systems.",IT,https://arxiv.org/abs/2504.21466
A Reference Architecture for Autonomous Networks: An Agent-Based Approach,"The vision of autonomous systems is becoming increasingly important in many application areas, where the aim is to replace humans with agents. These include autonomous vehicles and other agents' applications in business processes and problem-solving. For networks, the increasing scale and operation and management (O&M) complexity drive the need for autonomous networks (AN). The technical objective of AN is to ensure trustworthy O&M without human intervention for higher efficiency and lower operating costs. However, realizing AN seems more difficult than autonomous vehicles. It encounters challenges of networks' structural and functional complexity, which operate as distributed dynamic systems governed by various technical and economic constraints. A key problem lies in formulating a rigorous development methodology that facilitates a seamless transition from traditional networks to AN. Central to this methodology is the definition of a reference architecture for network agents, which specifies the required functionalities for their realization, regardless of implementation choices. This article proposes a reference architecture characterizing main functional features, illustrating its application with network use cases. It shows how artificial intelligence components can be used to implement the required functionality and its coordination. The latter is achieved through the management and generation of shared domain-specific knowledge stored in long-term memory, ensuring the overall consistency of decisions and their execution. The article concludes with a discussion of architecture specialization for building network layer agents. It also identifies the main technical challenges ahead, such as satisfying essential requirements at development or runtime, as well as the issue of coordinating agents to achieve collective intelligence in meeting overall network goals.",IT,https://arxiv.org/abs/2503.12871
Holistic Network Virtualization and Pervasive Network Intelligence for 6G,"In this tutorial paper, we look into the evolution and prospect of network architecture and propose a novel conceptual architecture for the 6th generation (6G) networks. The proposed architecture has two key elements, i.e., holistic network virtualization and pervasive artificial intelligence (AI). The holistic network virtualization consists of network slicing and digital twin, from the aspects of service provision and service demand, respectively, to incorporate service-centric and user-centric networking. The pervasive network intelligence integrates AI into future networks from the perspectives of networking for AI and AI for networking, respectively. Building on holistic network virtualization and pervasive network intelligence, the proposed architecture can facilitate three types of interplay, i.e., the interplay between digital twin and network slicing paradigms, between model-driven and data-driven methods for network management, and between virtualization and AI, to maximize the flexibility, scalability, adaptivity, and intelligence for 6G networks. We also identify challenges and open issues related to the proposed architecture. By providing our vision, we aim to inspire further discussions and developments on the potential architecture of 6G.",IT,https://arxiv.org/abs/2301.00519
"Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities","With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure.",IT,https://arxiv.org/abs/2412.14538v2
"6G Survey on Challenges, Requirements, Applications, Key Enabling Technologies, Use Cases, AI integration issues and Security aspects","Fifth-generation (5G) wireless networks will likely offer high data rates, increased reliability, and low delay for mobile, personal, and local area networks. Along with the rapid growth of smart wireless sensing and communication technologies, data traffic has increased significantly and existing 5G networks are not able to fully support future massive data traffic for services, storage, and processing. To meet the challenges ahead, research communities and industry are exploring the sixth generation (6G) Terahertz-based wireless network that is expected to be offered to industrial users in just ten years. Gaining knowledge and understanding of the different challenges and facets of 6G is crucial in meeting the requirements of future communication and addressing evolving quality of service (QoS) demands. This survey provides a comprehensive examination of specifications, requirements, applications, and enabling technologies related to 6G. It covers disruptive and innovative, integration of 6G with advanced architectures and networks such as software-defined networks (SDN), network functions virtualization (NFV), Cloud/Fog computing, and Artificial Intelligence (AI) oriented technologies. The survey also addresses privacy and security concerns and provides potential futuristic use cases such as virtual reality, smart healthcare, and Industry 5.0. Furthermore, it identifies the current challenges and outlines future research directions to facilitate the deployment of 6G networks.",IT,https://arxiv.org/abs/2206.00868v2
"6G: The Intelligent Network of Everything -- A Comprehensive Vision, Survey, and Tutorial","The global 6G vision has taken its shape after years of international research and development efforts. This work culminated in ITU-R's Recommendation on ""IMT-2030 Framework"". While the definition phase of technological requirements is currently ongoing, 3GPP's standardization process on 6G networks is expected to start in 2025 and worldwide commercialization around 2030. This article serves as a comprehensive guide to 6G by providing an overall vision, a contemporary survey of the main literature, and an informative tutorial-type presentation style. In our vision, 6G will be based on three fundamental elements: wireless, artificial intelligence (AI), and the Internet of Everything (IoE). Consequently, 6G can ultimately become the Intelligent Network of Everything while serving as an enabling platform for the next major disruption in mobile communication, called mobile intelligence. The potential of mobile intelligence is that anything can be made connected, intelligent, and aware of its environment. This will revolutionize the way how devices, systems, and applications are designed; how they operate and interact with humans and each other; and how they can be used for the benefit of people, society, and the world in general. After high-level visioning, the main details of 6G are discussed, including fundamental elements, disruptive applications, key use cases, main performance requirements, potential technologies, and defining features. A special focus is given to a comprehensive set of potential 6G technologies, each of which is introduced in a tutorial manner. Finally, we speculate on what comes after 6G and sketch the first high-level vision of 7G. All in all, the objective of this article is to provide a thorough guide to 6G in order to serve as a source of knowledge and inspiration for further research and development work in academia, industry, and standardization bodies.",IT,https://arxiv.org/abs/2407.09398v2
O-RAN and 6G: The Future of Wireless Innovation?,"The emergence of 6G technology represents a significant advancement in wireless communications, providing unprecedented speed, extremely low latency, and pioneering applications. In light of this development, an important question arises: Can the Open Radio Access Network (O-RAN), with its emphasis on openness, flexibility, RAN slicing, RAN Intelligent Controller (RIC), and cost-effectiveness, fulfill the complex requirements of 6G? This paper delves into the potential synergy between O-RAN and 6G, illustrating how O-RAN can facilitate customization, reduce expenses, and stimulate innovation in next-generation networks. We also tackle the challenges associated with 6G, such as the need for exceptional performance, integration with non-terrestrial networks, and heightened security. By examining the interaction between O-RAN and 6G, we underscore their joint role in shaping the future of wireless communication. Lastly, we demonstrate the potential of O-RAN through a unique, learning-based spectrum-sharing solution that aligns with the objectives of 6G for efficient spectrum usage.",IT,https://arxiv.org/abs/2411.09959v1
6G: the Wireless Communications Network for Collaborative and AI Applications,"At the dawn of 5G, we take a leap forward and present an original vision of wireless communication beyond its horizon towards 6G: a paradigm-shifting perspective of wireless networks on the cusp of an AI revolution.",IT,https://arxiv.org/abs/1904.03413
An In-Depth Survey on Virtualization Technologies in 6G Integrated Terrestrial and Non-Terrestrial Networks,"6G networks are envisioned to deliver a large diversity of applications and meet stringent quality of service (QoS) requirements. Hence, integrated terrestrial and non-terrestrial networks (TN-NTNs) are anticipated to be key enabling technologies. However, the TN-NTNs integration faces a number of challenges that could be addressed through network virtualization technologies such as Software-Defined Networking (SDN), Network Function Virtualization (NFV) and network slicing. In this survey, we provide a comprehensive review on the adaptation of these networking paradigms in 6G networks. We begin with a brief overview on NTNs and virtualization techniques. Then, we highlight the integral role of Artificial Intelligence in improving network virtualization by summarizing major research areas where AI models are applied. Building on this foundation, the survey identifies the main issues arising from the adaptation of SDN, NFV, and network slicing in integrated TN-NTNs, and proposes a taxonomy of integrated TN-NTNs virtualization offering a thorough review of relevant contributions. The taxonomy is built on a four-level classification indicating for each study the level of TN-NTNs integration, the used virtualization technology, the addressed problem, the type of the study and the proposed solution, which can be based on conventional or AI-enabled methods. Moreover, we present a summary on the simulation tools commonly used in the testing and validation of such networks. Finally, we discuss open issues and give insights on future research directions for the advancement of integrated TN-NTNs virtualization in the 6G era.",IT,https://arxiv.org/abs/2312.01895
Towards Organic 6G Networks: Virtualization and Live Migration of Core Network Functions,"In the context of Industry 4.0, more and more mobile use cases are appearing on industrial factory floors. These use cases place high demands on various quantitative requirements, such as latency, availability, and more. In addition, qualitative requirements such as flexibility are arising. Since virtualization technology is a key enabler for the flexibility that is required by novel use cases and on the way to organic networking as it is addressed by 6G, we investigate container virtualization technology in this paper. We focus on container technology since OS-level virtualization has multiple benefits compared to hardware virtualization, such as VMs. Thus, we discuss several aspects of container based virtualization, e.g. selection of suitable network drivers and orchestration tools, with respect to most important 5GC functions. In addition, the functions have different quantitative or qualitative requirements depending on whether they are stateless or stateful, and whether the specific function is located at either the control or user plane. Therefore, we also analyze the aforementioned live migration concepts for the 5GC functions and evaluate them based on well-defined metrics, such as migration time and process downtime.",IT,https://arxiv.org/abs/2110.12737
Secure Virtual Mobile Small Cells: A Stepping Stone Towards 6G,"As 5th Generation research reaches the twilight, the research community must go beyond 5G and look towards the 2030 connectivity landscape, namely 6G. In this context, this work takes a step towards the 6G vision by proposing a next generation communication platform, which aims to extend the rigid coverage area of fixed deployment networks by considering virtual mobile small cells (MSC) that are created on demand. Relying on emerging computing paradigms such as NFV (Network Function Virtualization) and SDN (Software Defined Networking), these cells can harness radio and networking capability locally reducing protocol signaling latency and overhead. These MSCs constitute an intelligent pool of networking resources that can collaborate to form a wireless network of MSCs providing a communication platform for localized, ubiquitous and reliable connectivity. The technology enablers for implementing the MSC concept are also addressed in terms of virtualization, lightweight wireless security, and energy efficient RF. The benefits of the MSC architecture towards reliable and efficient cell offloading are demonstrated as a use-case.",IT,https://arxiv.org/abs/2105.01373
Towards Sustainability in 6G and beyond: Challenges and Opportunities of Open RAN,"The transition to 6G is expected to bring significant advancements, including much higher data rates, enhanced reliability and ultra-low latency compared to previous generations. Although 6G is anticipated to be 100 times more energy efficient, this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability. Network sustainability encompasses a broader scope, integrating business viability, environmental sustainability, and social responsibility. This paper explores the sustainability requirements for 6G and proposes Open RAN as a key architectural solution. By enabling network diversification, fostering open and continuous innovation, and integrating AI/ML, Open RAN can promote sustainability in 6G. The paper identifies high energy consumption and e-waste generation as critical sustainability challenges and discusses how Open RAN can address these issues through softwarisation, edge computing, and AI integration.",IT,https://arxiv.org/abs/2503.08353v1
A Survey of AIOps for Failure Management in the Era of Large Language Models,"As software systems grow increasingly intricate, Artificial Intelligence for IT Operations (AIOps) methods have been widely used in software system failure management to ensure the high availability and reliability of large-scale distributed software systems. However, these methods still face several challenges, such as lack of cross-platform generality and cross-task flexibility. Fortunately, recent advancements in large language models (LLMs) can significantly address these challenges, and many approaches have already been proposed to explore this field. However, there is currently no comprehensive survey that discusses the differences between LLM-based AIOps and traditional AIOps methods. Therefore, this paper presents a comprehensive survey of AIOps technology for failure management in the LLM era. It includes a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps. Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain, aiming to further its development and application.",IT,https://arxiv.org/abs/2406.11213
AIOps Solutions for Incident Management: Technical Guidelines and A Comprehensive Literature Review,"The management of modern IT systems poses unique challenges, necessitating scalability, reliability, and efficiency in handling extensive data streams. Traditional methods, reliant on manual tasks and rule-based approaches, prove inefficient for the substantial data volumes and alerts generated by IT systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a solution, leveraging advanced analytics like machine learning and big data to enhance incident management. AIOps detects and predicts incidents, identifies root causes, and automates healing actions, improving quality and reducing operational costs. However, despite its potential, the AIOps domain is still in its early stages, decentralized across multiple sectors, and lacking standardized conventions. Research and industrial contributions are distributed without consistent frameworks for data management, target problems, implementation details, requirements, and capabilities. This study proposes an AIOps terminology and taxonomy, establishing a structured incident management procedure and providing guidelines for constructing an AIOps framework. The research also categorizes contributions based on criteria such as incident management tasks, application areas, data sources, and technical approaches. The goal is to provide a comprehensive review of technical and research aspects in AIOps for incident management, aiming to structure knowledge, identify gaps, and establish a foundation for future developments in the field.",IT,https://arxiv.org/abs/2404.01363
AIOps-Driven Enhancement of Log Anomaly Detection in Unsupervised Scenarios,"Artificial intelligence operations (AIOps) play a pivotal role in identifying, mitigating, and analyzing anomalous system behaviors and alerts. However, the research landscape in this field remains limited, leaving significant gaps unexplored. This study introduces a novel hybrid framework through an innovative algorithm that incorporates an unsupervised strategy. This strategy integrates Principal Component Analysis (PCA) and Artificial Neural Networks (ANNs) and uses a custom loss function to substantially enhance the effectiveness of log anomaly detection. The proposed approach encompasses the utilization of both simulated and real-world datasets, including logs from SockShop and Hadoop Distributed File System (HDFS). The experimental results are highly promising, demonstrating significant reductions in pseudo-positives. Moreover, this strategy offers notable advantages, such as the ability to process logs in their raw, unprocessed form, and the potential for further enhancements. The successful implementation of this approach showcases a remarkable reduction in anomalous logs, thus unequivocally establishing the efficacy of the proposed methodology. Ultimately, this study makes a substantial contribution to the advancement of log anomaly detection within AIOps platforms, addressing the critical need for effective and efficient log analysis in modern and complex systems.",IT,https://arxiv.org/abs/2311.02621
Data Silos A Roadblock for AIOps,"Using artificial intelligence to manage IT operations, also known as AIOps, is a trend that has attracted a lot of interest and anticipation in recent years. The challenge in IT operations is to run steady-state operations without disruption as well as support agility"" can be rephrased as ""IT operations face the challenge of maintaining steady-state operations while also supporting agility [11]. AIOps assists in bridging the gap between the demand for IT operations and the ability of humans to meet that demand. However, it is not easy to apply AIOps in current organizational settings. Data Centralization is a major obstacle for adopting AIOps, according to a recent survey by Cisco [1]. The survey, which involved 8,161 senior business leaders from organizations with more than 500 employees, found that 81% of them acknowledged that their data was scattered across different silos within their organizations. This paper illustrates the topic of data silos, their causes, consequences, and solutions.",IT,https://arxiv.org/abs/2312.10039
A Systematic Mapping Study in AIOps,"IT systems of today are becoming larger and more complex, rendering their human supervision more difficult. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to AI and Big Data. However, past AIOps contributions are scattered, unorganized and missing a common terminology convention, which renders their discovery and comparison impractical. In this work, we conduct an in-depth mapping study to collect and organize the numerous scattered contributions to AIOps in a unique reference index. We create an AIOps taxonomy to build a foundation for future contributions and allow an efficient comparison of AIOps papers treating similar problems. We investigate temporal trends and classify AIOps contributions based on the choice of algorithms, data sources and the target components. Our results show a recent and growing interest towards AIOps, specifically to those contributions treating failure-related tasks (62%), such as anomaly detection and root cause analysis.",IT,https://arxiv.org/abs/2012.09108
MapReduce: simplified data processing on large clusters,"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.",IT,https://dl.acm.org/doi/10.1145/1327452.1327492
Dynamo: amazon's highly available key-value store,"Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an ""always-on"" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.",IT,https://dl.acm.org/doi/10.1145/1323293.1294281
Incremental View Maintenance for Property Graph Queries,"Graph processing challenges are common in modern database systems, with the property graph data model gaining widespread adoption. Due to the novelty of the field, graph databases and frameworks typically provide their own query language, such as Cypher for Neo4j, Gremlin for TinkerPop and GraphScript for SAP HANA. These languages often lack a formal background for their data model and semantics. To address this, the openCypher initiative aims to standardise a subset of the Cypher language, for which it currently provides grammar specification and a set of acceptance tests to allow vendors to implement their openCypher compatible engine.",IT,https://dl.acm.org/doi/10.1145/3183713.3183724
Nimrod/G: An Architecture of a Resource Management and Scheduling System in a Global Computational Grid,"The availability of powerful microprocessors and high-speed networks as commodity components has enabled high performance computing on distributed systems (wide-area cluster computing). In this environment, as the resources are usually distributed geographically at various levels (department, enterprise, or worldwide) there is a great challenge in integrating, coordinating and presenting them as a single resource to the user; thus forming a computational grid. Another challenge comes from the distributed ownership of resources with each resource having its own access policy, cost, and mechanism. The proposed Nimrod/G grid-enabled resource management and scheduling system builds on our earlier work on Nimrod and follows a modular and component-based architecture enabling extensibility, portability, ease of development, and interoperability of independently developed components. It uses the Globus toolkit services and can be easily extended to operate with any other emerging grid middleware services. It focuses on the management and scheduling of computations over dynamic resources scattered geographically across the Internet at department, enterprise, or global level with particular emphasis on developing scheduling schemes based on the concept of computational economy for a real test bed, namely, the Globus testbed (GUSTO).",IT,http://arxiv.org/abs/cs/0009021v1
Replica Selection in the Globus Data Grid,"The Globus Data Grid architecture provides a scalable infrastructure for the management of storage resources and data that are distributed across Grid environments. These services are designed to support a variety of scientific applications, ranging from high-energy physics to computational genomics, that require access to large amounts of data (terabytes or even petabytes) with varied quality of service requirements. By layering on a set of core services, such as data transport, security, and replica cataloging, one can construct various higher-level services. In this paper, we discuss the design and implementation of a high-level replica selection service that uses information regarding replica location and user preferences to guide selection from among storage replica alternatives. We first present a basic replica selection service design, then show how dynamic information collected using Globus information service capabilities concerning storage system properties can help improve and optimize the selection process. We demonstrate the use of Condor's ClassAds resource description and matchmaking mechanism as an efficient tool for representing and matching storage resource capabilities and policies against application requirements.",IT,http://arxiv.org/abs/cs/0104002v1
Economic Models for Management of Resources in Grid Computing,"The accelerated development in Grid and peer-to-peer computing has positioned them as promising next generation computing platforms. They enable the creation of Virtual Enterprises (VE) for sharing resources distributed across the world. However, resource management, application development and usage models in these environments is a complex undertaking. This is due to the geographic distribution of resources that are owned by different organizations. The resource owners of each of these resources have different usage or access policies and cost models, and varying loads and availability. In order to address complex resource management issues, we have proposed a computational economy framework for resource allocation and for regulating supply and demand in Grid computing environments. The framework provides mechanisms for optimizing resource provider and consumer objective functions through trading and brokering services. In a real world market, there exist various economic models for setting the price for goods based on supply-and-demand and their value to the user. They include commodity market, posted price, tenders and auctions. In this paper, we discuss the use of these models for interaction between Grid components in deciding resource value and the necessary infrastructure to realize them. In addition to normal services offered by Grid computing systems, we need an infrastructure to support interaction protocols, allocation mechanisms, currency, secure banking, and enforcement services. Furthermore, we demonstrate the usage of some of these economic models in resource brokering through Nimrod/G deadline and cost-based scheduling for two different optimization strategies on the World Wide Grid (WWG) testbed.",IT,http://arxiv.org/abs/cs/0106020v1
Dynamic Management of Virtual Machine and Container Scheduling in Multi-Data Center Cloud Environments,"Efficiently managing virtual resources is a critical component of server virtualization technology. The scheduler is crucial in strategically distributing Virtual Machines (VMs) and containers across diverse computing nodes, responsible for the allocation and the placement of VMs and containers on different computing nodes, and the migration of deployed ones between different nodes. In this thesis, we propose novel solutions in scheduling virtual resources, particularly in the management of VMs and containers deployed across multi-data center cloud environments. The proposed solutions leverage mathematical models, machine learning techniques, and blockchain technology to optimize scheduling decisions, enhance server consolidation, minimize energy consumption, and secure container scheduling. We introduce mathematical models for live VM migration techniques used in simulating and studying live VM migration in cloud systems environments. We present a novel distributed scheduling model that leverages blockchain technology to facilitate efficient sharing of VM status across multiple data centers. This enables prompt Local Area Network (LAN) or Wide Area Network (WAN) scheduling decisions for VMs. Additionally, we employ machine and deep learning techniques in a VM migration prediction service to identify the most suitable live migration method for each VM based on its unique characteristics. Our blockchain-based model reduces the total messages exchanged for the VM migration with percentages ranging from 0.5% to 22% and the total communication delay by 8% to 72% compared to a REST-based distributed model. The proposed blockchain-based distributed model also reduces the number of communication messages by 41.79% to 49.85% and total delay by 2% to 12% compared to a VPN-based centralized model. The Service Level Agreement (SLA) compliance rate of the proposed VM migration prediction service ranges from 18% to 94.9% for different machine learning algorithms and SLA policies. The proposed solution reduces the total migration time by 14% to 79% and the downtime by 64% to 99%. Furthermore, we present a novel two-stage container scheduling solution that addresses node imbalances and efficiently deploys containers as an optimization problem, integrating various objective functions and constraints to enhance server consolidation and minimize energy consumption. The confidentiality of migrated containers is ensured through encryption, and the associated costs of the proposed attributes-based encryption model are incorporated into the optimization constraints. The proposed solution's efficacy is demonstrated in its ability to efficiently deploy containers in multi-data center cloud environments and seamlessly migrate them between hosts within the same data center or across different data centers. The results show optimal consolidation with a reduction in the number of running hosts, ranging from 4% to over 18%. Additionally, the solution promotes minimal total power consumption with savings ranging from 3.5 to 16.25 megawatts, while also ensuring balanced server loads, highlighting the effectiveness of the proposed container scheduling approach.",IT,https://spectrum.library.concordia.ca/id/eprint/994071/
A systematic review on recent methods of scheduling and load balancing in containerized cloud environments,"Containers running microservices must handle many requests, tolerate network failures, and ensure scalability, availability, and high performance. Scheduling and load balancing for containers are two closely related activities. In distributed environments, various methods are used to solve the problems of container scheduling and load balancing. These methods encompass heuristic approaches, including traditional methods, bioinspired optimizations, and learning-based approaches. Learning-based approaches leverage artificial intelligence (AI) and related technologies to comprehend runtime situations and make well-informed decisions during the scheduling process. Additionally, load balancing facilitates resource optimization, leading to improved infrastructure efficiency. Scheduling and load balancing can encompass larger contexts in distributed computing environments. The paper evaluates host provisioning in Open Stack and the impact of cloud computing. Kubernetes outperforms Docker Swarm, and even though Kubernetes installation is complex, it is worth the effort. The proposed approach improves resource allocation, focusing on placement strategies for all three entities. The approach, employing integer linear programming, offers cost savings and easy integration into container orchestration frameworks-a novel algorithm that addresses various costs, contributing to containerized application scheduling in the cloud.",IT,https://www.accentsjournals.org/PaperDirectory/Journal/IJATEE/2024/7/6.pdf
Energy efficient virtual machines placement in cloud datacenters using genetic algorithm and adaptive thresholds,"Cloud computing platform provides on-demand IT services to users and advanced the technology. The purpose of virtualization is to improve the utilization of resources and reduce power consumption. Energy consumption is a major issue faced by data centers management. Virtual machine placement is an effective technique used for this purpose. Different algorithms have been proposed for virtual machine placement in cloud environments. These algorithms have considered different parameters. It is obvious that improving one parameter affects other parameters. There is still a need to reduce energy consumption in cloud data centers. Data centers need solutions that reduce energy consumption without affecting other parameters. There is a need to device solutions to effectively utilize cloud resources and reduce energy consumption. In this article, we present an algorithm for Virtual Machines (VMs) placement in cloud computing. The algorithm uses adaptive thresholding to identify over utilized and underutilized hosts to reduce energy consumption and Service Level Agreement (SLA) violations. The algorithm is validated with simulations and comparative results are presented. In cloud data centers, energy consumption can be reduced by increasing the number of VMs and decreasing physical machines. Activation of the active and idle modes of VMs also decreases energy consumption and enhances resource utilization. Sorting of tasks according to the schedule of processing needs to avoid overload or underload hosts in data centers and reduce energy consumption. Mapping of the task from one VM to other VMs and the assignment of the physical machine to VMs also consumes energy and needs to be addressed. In this article, we present a novel algorithm for VM placement in cloud computing environments. The algorithm is based on Genetic Algorithm (GA) with adaptive threshold. Adaptive thresholds are used to detect over-utilized and underutilized hosts in cloud data centers. Setting static upper and lower thresholds may lead to imbalance load and thus cause more energy consumption and low QoS. The proposed algorithm uses an intermediate value as the average of the upper and lower thresholds to identify hosts with utilization near the lower threshold. The resources of these hosts are allocated to VMs for better resource utilization. Furthermore, GA is used to optimize the allocation process. The objectives are to reduce energy consumption with low SLA violations for better QoS. The algorithm is validated by experiments with different configurations and comparative results are presented.",IT,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0296399
Xen and the Art of Virtualization,"Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe, resource-managed fashion without sacrificing performance or functionality.",IT,https://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf
Live Migration of Virtual Machines,"Migrating operating system instances across distinct physical hosts is a useful tool for administrators of data centers and clusters: it allows a clean separation between hardware and software, and facilitates fault management, load balancing, and low-level system maintenance. By carrying out the majority of migration while OSes continue to run, we achieve impressive performance with minimal service downtimes.",IT,https://www.usenix.org/legacy/event/nsdi05/tech/full_papers/clark/clark.pdf
OpenFlow: Enabling Innovation in Campus Networks,This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use every day. OpenFlow is based on an Ethernet switch with an internal flow-table and a standardized interface to add and remove flow entries.,IT,https://ccr.sigcomm.org/online/files/p69-v38n2n-mckeown.pdf
Bigtable: A Distributed Storage System for Structured Data,"Bigtable is a distributed storage system for managing structured data that is designed to scale to very large size – petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance.",IT,https://research.google.com/archive/bigtable-osdi06.pdf
My VM is Lighter (and Safer) Than Your Container,"Containers are in great demand because they are lightweight compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point that people often run containers inside VMs to achieve proper isolation. In this paper, we ask whether there is truly a strict trade-off between isolation (VMs) and efficiency (containers).",IT,https://www.cs.utexas.edu/~witchel/380L/papers/manco17sosp-lightvm.pdf
DevOps: A Systematic Literature Review,The DevOps approach is a connection of development and operation teams in software development to supply IT solutions faster to the market. This systematic literature review selected 58 out of 842 publications.,IT,https://www.fh-wedel.de/fileadmin/Mitarbeiter/Records/Ruetz_2019_-_DEVOPS_A_SYSTEMATIC_LITERATURE_REVIEW.pdf
Gray Failure: The Achilles' Heel of Cloud-Scale Systems,"Cloud scale provides vast resources to replace failed components, but only if those failures can be detected. Major availability breakdowns in cloud environments tend to be caused by subtle underlying faults – so-called gray failures – rather than obvious fail-stop failures.",IT,https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-1.pdf
Onix: A Distributed Control Platform for Large-Scale Networks,"Computer networks lack a general control paradigm, as traditional networks provide no network-wide management abstractions. As a result, each new function (such as routing) must implement its own state distribution, discovery, and failover mechanisms. We believe this lack of a common control platform has hindered development of flexible, reliable, feature-rich control planes.",IT,https://static.usenix.org/event/osdi10/tech/full_papers/Koponen.pdf
"“Hey, You, Get Off of My Cloud!”: Exploring Information Leakage in Third-Party Clouds","Third-party cloud computing offers the promise of outsourcing computation. Services like Microsoft Azure and Amazon EC2 allow users to instantiate virtual machines (VMs) on demand and purchase precisely the capacity needed when required. In turn, virtualization allows cloud providers to maximize utilization by multiplexing many customer VMs across shared physical infrastructure. However, in this paper, we show that this approach can introduce new vulnerabilities. Using Amazon EC2 as a case study, we demonstrate that it is possible to map the internal cloud infrastructure, identify where a particular target VM is likely located, and then instantiate new VMs until one is co-resident with the target. We then explore how such placement can be used to mount cross-VM side-channel attacks to extract information from a target VM on the same physical server.",IT,https://zxr.cc.acm.org/doi/pdf/10.1145/1653662.1653687
"Ceph: A Scalable, High-Performance Distributed File System","We have developed Ceph, a distributed file system that provides excellent performance, reliability, and scalability. Ceph maximizes separation of data and metadata management by replacing allocation tables with a pseudo-random data distribution function (CRUSH), designed for heterogeneous and dynamic clusters of unreliable object storage devices (OSDs). We leverage device intelligence by distributing data replication, failure detection, and recovery to semi-autonomous OSDs. A dynamic distributed metadata cluster provides extremely efficient metadata management and adapts seamlessly to a wide range of workloads. Performance measurements under varied workloads show that Ceph delivers excellent I/O performance and scalable metadata management, supporting more than 250,000 metadata operations per second.",IT,https://www.usenix.org/legacy/events/osdi06/tech/full_papers/weil/weil.pdf
Remus: High Availability via Asynchronous VM Replication,"Allowing applications to survive hardware failure is an expensive undertaking, generally involving complex recovery software or redundant fail-over hardware. This paper presents Remus, a virtualization-based system that provides high availability by asynchronously replicating the entire state of a running virtual machine to a backup. Remus significantly reduces HA cost by isolating an OS and its applications from hardware failures transparently. It runs a primary VM with a backup VM such that if the primary fails, the backup seamlessly takes over. The system asynchronously propagates state changes from primary to backup at high frequency, buffering network output until state is safely replicated to the backup. Our approach provides OS- and application-agnostic high availability on commodity hardware, incurring only modest performance overhead for many workloads.",IT,https://www.usenix.org/legacy/events/eurosys08/tech/full_papers/cully/cully.pdf
Pivot Tracing: Dynamic Causal Monitoring for Distributed Systems,"Monitoring and troubleshooting large distributed systems is notoriously difficult – problems are complex, varied, and unpredictable. The tools used today (logs, counters, metrics) have two key limitations: what gets recorded is defined a priori, and data is recorded in a component-centric way, making it hard to correlate cross-component events. This paper presents Pivot Tracing, a monitoring framework that addresses both limitations by combining dynamic instrumentation with a novel relational operator called the happened-before join. Pivot Tracing lets users, at runtime, define arbitrary metrics at one point in the system, while filtering and grouping events meaningful at other parts, even across component boundaries. We implemented a Pivot Tracing prototype for Java and evaluated it on a heterogeneous Hadoop cluster (HDFS, HBase, MapReduce, YARN). We show that Pivot Tracing can effectively identify diverse root causes such as software bugs, misconfiguration, and “limping” hardware, enabling dynamic cross-tier analysis with low overhead.",IT,https://cs.brown.edu/~jcmace/papers/pivot-tracing-sosp15.pdf
Power Provisioning for a Warehouse-Sized Computer,"Large-scale Internet services require a computing infrastructure that can be described as a warehouse-sized computing system. The cost of building datacenter facilities to deliver a given power capacity can rival the ongoing energy costs. Thus, there are strong economic incentives to operate facilities close to maximum capacity so that non-recurring facility costs are well amortized. In practice this is difficult due to uncertainties in equipment power ratings and highly variable power usage under real workloads. Effective power provisioning strategies are needed to determine how much equipment can be safely and efficiently hosted within a given power budget. In this paper we present aggregate power usage characteristics of large server populations (up to 15,000 servers) for different application classes over ~6 months. These observations let us evaluate opportunities to maximize utilization of deployed power capacity, and assess risks of over-subscribing it. We find even well-tuned applications have a noticeable gap (7–16%) between achieved and theoretical peak cluster power usage – growing to ~40% at whole-datacenter scale. This “headroom” can be used to deploy extra equipment within the same power budget with minimal risk. We use our model to estimate potential power management savings, finding significant opportunities (greater at cluster-level than rack-level). Finally, we argue systems must be power-efficient across the activity range, not just at peak load.",IT,https://research.google/pubs/pub35290.pdf
"Cloud Computing: Vision, Hype, and Reality for the 5th Utility (2009)","With significant advances in ICT over the last half-century, there is a growing vision that computing will one day be the “5th utility” (after water, electricity, gas, and telephony). This computing utility, like the other four, would provide a basic level of service considered essential to meet everyday needs of the general community. To deliver this vision, numerous computing paradigms have been proposed – the latest being Cloud Computing. Hence, in this paper, we define cloud computing and provide an architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as virtual machines. We also provide insights on market-based resource management strategies that encompass customer-driven service management and computational risk management to sustain SLA-oriented resource allocation. In addition, we offer early thoughts on interconnecting Clouds for dynamically creating global cloud exchanges and markets. We then present representative Cloud platforms (especially industrial offerings), along with our current work toward realizing market-oriented Cloud resource allocation in the Aneka enterprise Cloud technology. Furthermore, we highlight the differences between High-Performance Computing (HPC) workloads and Internet-based service workloads, describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing “Storage Clouds” for high-performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st-century vision.",IT,https://www.sciencedirect.com/science/article/pii/S0167739X09001957/pdfft?md5=79de98205f66884e7191cf9df0f205a1&pid=1-s2.0-S0167739X09001957-main.pdf
The Vision of Autonomic Computing (2003),"A 2001 IBM manifesto observed that a looming software complexity crisis – caused by applications and environments comprising tens of millions of lines of code – threatened to halt computing progress. The manifesto noted the almost impossible difficulty of managing current and planned systems, which require integrating heterogeneous environments into corporate-wide systems extending into the Internet. Autonomic computing, perhaps the most attractive approach to this problem, envisions systems that can manage themselves given high-level objectives from administrators. Systems will self-manage according to administrator goals: new components integrate effortlessly, like a new cell in a body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems.",IT,https://www.research.ibm.com/autonomic/papers/vision/vision.pdf
Ethane: Taking Control of the Enterprise (2007),"This paper presents Ethane, a new network architecture for the enterprise. Ethane allows managers to define a single network-wide, fine-grained policy, which is then enforced directly. Ethane couples extremely simple flow-based Ethernet switches with a centralized controller that manages flow admission and routing. While radical, the design is backwards-compatible with existing hosts and switches. We implemented Ethane in both hardware and software, supporting wired and wireless hosts. Our operational Ethane network has supported 300+ hosts for four months in Stanford’s Computer Science network, and this deployment experience significantly affected Ethane’s design.",IT,https://yuba.stanford.edu/~casado/ethane-sigcomm07.pdf
Chubby: The Lock Service for Loosely-Coupled Systems (2006),"We describe our experiences with the Chubby lock service, which provides coarse-grained locking and reliable (low-volume) storage for a loosely-coupled distributed system. Chubby’s interface is like a distributed file system with advisory locks, but the design emphasis is on availability and reliability rather than high performance. Many Chubby instances have been used for over a year, with several each handling tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences. (For example, Chubby was intended to help with coarse-grained leader election and meta-data storage; systems like GFS and Bigtable use Chubby for master election and as a well-known root for their distributed data structures.) Our experience shows Chubby greatly improved availability in systems that previously required human intervention on faults.",IT,https://www.usenix.org/legacy/events/osdi06/tech/full_papers/burrows/burrows.pdf
ZooKeeper: Wait-free Coordination for Internet-Scale Systems (2010),"In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, it aims to provide a simple and high-performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by ZooKeeper has the wait-free properties of shared registers, combined with an event-driven mechanism (analogous to cache invalidations in file systems) to provide a simple yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides each client with FIFO ordering of its requests and linearizability for all updates to the ZooKeeper state. These design decisions enable a high-throughput pipeline where read requests are often served by local replicas. We show that for typical workloads (read:write ratios of 2:1 up to 100:1), ZooKeeper can handle tens to hundreds of thousands of transactions per second, allowing it to be used extensively by client applications.",IT,https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf
Dremel: Interactive Analysis of Web-Scale Datasets (2010),"Dremel is a scalable, interactive ad hoc query system for analysis of read-only nested data. By combining multilevel execution trees and a columnar data layout, it can run aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe Dremel’s architecture and implementation, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on multi-thousand-node instances of the system.",IT,https://research.google/pubs/pub36632.pdf
Failure Trends in a Large Disk Drive Population (2007),"Over 90% of new information produced globally is stored on magnetic media, most of it on hard disk drives. Despite their importance, relatively little published work exists on disk drive failure patterns and factors affecting drive lifetime. Most available data come either from small accelerated-aging tests or modest field studies; large-population studies seldom collect health signals from in-service drives, which are critical for detailed failure analysis. We present data from detailed observations of a very large disk drive population in a production Internet services deployment – an order of magnitude larger than previous studies. In addition to failure statistics, we analyze correlation between failures and several parameters commonly thought to impact drive longevity. Our analysis identifies several Self-Monitoring (SMART) parameters (e.g. scan errors, reallocation counts, offline reallocation counts, probational counts) that correlate highly with failures. Despite this high correlation, we conclude that models based on SMART parameters alone are unlikely to accurately predict individual drive failures. Surprisingly, we found that temperature and activity levels were much less correlated with drive failures than previously reported.",IT,https://www.usenix.org/legacy/events/fast07/tech/full_papers/pinheiro/pinheiro.pdf
Cloud Computing: State-of-the-Art and Research Challenges (2010),"Cloud computing has recently emerged as a new paradigm for hosting and delivering services over the Internet. Cloud computing is attractive to business owners because it eliminates the need to plan ahead for provisioning and allows enterprises to start small and increase resources only when demand rises. However, despite the huge opportunities cloud computing offers the IT industry, the technology is still in its infancy with many issues to address. In this paper, we present a survey of cloud computing, highlighting key concepts, architectural principles, state-of-the-art implementations, and research challenges. The aim is to better understand cloud design challenges and identify important research directions in this increasingly important area.",IT,https://jisajournal.springeropen.com/articles/10.1007/s13174-010-0007-6
Disaster Recovery in Cloud Computing Systems: An Overview,"Over the years, there has been a heavy reliance on cloud computing as IT has innovated through time. In recent times cloud computing has grown monumentally. Many organizations rely on this technology to perform their business as usual and use it as a backbone of their companies’ IT infrastructure. This paper investigates the organizational adaptation for cloud computing technology – reviewing case studies from various institutions and companies worldwide to provide a detailed analysis of innovative techniques with cloud computing. We investigate the features and delivery approaches cloud computing offers and the potential challenges and constraints we face when adopting cloud computing into the business setting. We also explore the cybersecurity elements associated with cloud computing, focusing on intrusion detection and prevention and understanding how that can be applied in the cloud. Finally, we investigate the future research directions for cloud computing and expand this paper into further articles with experiments and results.",IT,https://thesai.org/Publications/ViewPaper?Volume=11&Issue=9&Code=IJACSA&SerialNo=84
Container Security in Cloud Environments: A Comprehensive Analysis and Future Directions for DevSecOps,"In recent years, the security of containers has become a crucial aspect of modern software applications’ security and integrity. Containers are extensively used due to their lightweight and portable nature, allowing swift and agile deployment across different environments. However, the increasing popularity of containers has led to unique security risks, including vulnerabilities in container images, misconfigured containers, and insecure runtime environments. Containers are often built using public repository images and base image vulnerability is inherited by containers. Container images may contain outdated components or services, including system libraries and dependencies and known vulnerabilities from these components can be exploited. Images downloaded from untrusted sources may include malicious code that compromises other containers running in the same network or the host system. Base images may include unnecessary software or services that increase the attack surface and potential vulnerabilities. Several security measures have been implemented to address these risks, such as container image scanning, container orchestration security, and runtime security monitoring. Implementing a solid security policy and updating containers with the latest patches can significantly improve container security. Given the increasing adoption of containers, organizations must prioritize container security to protect their applications and data. This work presents automated, robust security techniques for continuous integration and continuous development pipelines, and the added overhead is empirically analyzed. Then, we nail down specific research and technological problems the DevSecOps community encounters and appropriate initial fixes. Our results will make it possible to make judgments that are enforced when using DevSecOps techniques in enterprise security and cloud-native applications.",IT,https://www.mdpi.com/2673-4591/59/1/57
Infrastructure as Code: Technology Review and Research Challenges,"The quality of software management in infrastructure operations for application software is important as automation in software operations continues to grow. Infrastructure as Code (IaC) refers to a systematic, technology-supported approach to manage deployment infrastructure for software applications. Sample contexts are general software automation, but also cloud and edge and various software-defined networking applications. DevOps (development and operations) practices, which are already applied in the Infrastructure as Code (IaC) context, need to be extended to cover the whole IaC life cycle from code generation to dynamic, automated control. The ultimate objective would range from IaC generation to full self-adaptation of IaC code in an automated setting. We review available IaC technologies based on a comprehensive comparison framework to capture the state-of-the-art. We also introduce an IaC-specific DevOps process. This serves as a basis to identify open research challenges. A discussion of defect categories is at the centre of this process.",IT,http://www.scitepress.org/Papers/2025/132477/132477.pdf
Autonomic Cloud Computing: Research Perspective,"As the cloud infrastructure grows, it becomes more challenging to manage resources in such a massive, diverse, and distributed setting, despite the fact that cloud computing provides computational capabilities on-demand. Due to resource variability and unpredictability, resource allocation issues arise in a cloud setting. A Quality of Service (QoS) based autonomic resource management strategy automates resource management, delivering trustworthy, dependable, and cost-effective cloud services that efficiently execute workloads. Autonomic cloud computing aims to understand how computing systems may autonomously accomplish user-specified “control” objectives without the need for an administrator and without violating the Service Level Agreement (SLA) in dynamic cloud computing environments. This article presents a research perspective and analysis on autonomous resource allocation in cloud computing, with a focus on QoS- and SLA-aware autonomous resource management. The study also discusses the current status of autonomic resource management in the cloud and highlights key next-generation research directions.",IT,https://arxiv.org/abs/1507.01546
AI-Powered Cloud Orchestration: Automating Multi-Cloud & Hybrid Cloud Workloads,"AI-powered cloud orchestration revolutionizes how enterprises manage and optimize their multi-cloud and hybrid cloud environments. Integrating artificial intelligence into cloud management addresses complexity, manual intervention, and reactive problem-solving challenges that plague traditional orchestration methods. By implementing intelligent algorithms for resource allocation, workload balancing, predictive scaling, security enhancement, and self-healing capabilities, organizations can transform their cloud operations from manually-defined workflows to autonomous systems capable of continuous optimization. These advanced orchestration technologies enable dynamic resource distribution based on usage patterns and forecasted demand while simultaneously identifying cost-saving opportunities through workload consolidation and intelligent scheduling. Security frameworks are significantly strengthened through anomaly detection, predictive threat intelligence, and adaptive access control policies that evolve with changing organizational needs. Perhaps most transformative is the ability of self-healing infrastructure to automatically detect, diagnose, and remediate issues before they cause service disruptions, dramatically reducing the operational burden on technical teams and allowing them to focus on innovation rather than troubleshooting. This technological shift represents a fundamental evolution in cloud management, offering enterprises unprecedented efficiency, reliability, and cost optimization across their distributed computing environments.",IT,https://eajournals.org/ejcsit/vol13-issue-8-2025/ai-powered-cloud-orchestration-automating-multi-cloud-hybrid-cloud-workloads/
"A Multivocal Review of MLOps Practices, Challenges and Open Issues","MLOps has emerged as a key solution to address many socio-technical challenges of bringing ML models to production, such as integrating ML models with non-ML software, continuous monitoring, maintenance, and retraining of deployed models. Despite the utility of MLOps, an integrated body of knowledge regarding MLOps remains elusive because of its extensive scope due to the diversity of ML productionalization challenges it addresses. Whilst the existing literature reviews provide valuable snapshots of specific practices, tools, and research prototypes related to MLOps at various times, they focus on particular facets of MLOps, thus fail to offer a comprehensive and invariant framework that can weave these perspectives into a unified understanding of MLOps. This paper presents a Multivocal Literature Review that systematically analyzes a corpus of 150 peer-reviewed and 48 grey literature to synthesize a unified conceptualization of MLOps and develop a snapshot of its best practices, adoption challenges, and solutions.",IT,https://arxiv.org/abs/2406.09737
Cost modelling and optimisation for cloud: a graph-based approach,"Cloud computing has become popular among individuals and enterprises due to its convenience, scalability, and flexibility. However, a major concern for many cloud service users is the rising cost of cloud resources. Since cloud computing uses a pay-per-use model, costs can add up quickly, and unexpected expenses can arise from a lack of visibility and control. The cost structure gets even more complicated when working with multi-cloud or hybrid environments. Businesses may spend much of their IT budget on cloud computing, and any savings can improve their competitiveness and financial stability. Hence, an efficient cloud cost management is crucial. To overcome this difficulty, new approaches and tools are being developed to provide greater oversight and command over cloud a graph-based approach for modelling cost elements and cloud resources and a potential way to solve the resulting constraint problem of cost optimisation. In this context, we primarily consider utilisation, cost, performance, and availability. The proposed approach is evaluated on three different user scenarios, and results indicate that it could be effective in cost modelling, cost optimisation, and scalability. This approach will eventually help organisations make informed decisions about cloud resource placement and manage the costs of software applications and data workflows deployed in single, hybrid, or multi-cloud environments.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00709-6
Optimization of datacenter selection through a genetic algorithm-driven service broker policy,"Establishing an optimal datacenter selection policy within the cloud environment is paramount to maximize the performance of the cloud services. Service broker policy governs the selection of datacenters for user requests. In our research, we introduce an innovative approach incorporating the genetic algorithm with service broker policy to assist cloud services in identifying the most suitable datacenters for specific userbases. The effectiveness of our proposed genetic algorithm was rigorously evaluated through experiments conducted on CloudAnalyst platform. The results clearly indicate that our proposed algorithm surpasses existing service broker policies and previous research works done in this field in terms of reducing response time and data processing time. The results analysis validates its efficacy and potential for enhancing cloud service performance and reducing the cost of overall cloud infrastructure.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00719-4
Navigating intent-based networking: from user descriptions to deployable configurations,"Network automation development has accompanied network evolution due to its significant role in speeding up and simplifying network operations. Emerging networking and computing paradigms such as information-centric networks, next-generation networks, cloud, and edge computing and recent innovative technologies, such as the Internet of things (IoT), enabled novel network services (such as the Internet of Vehicles (IoV), context-aware applications, virtual reality, and augmented reality) that demand complex configurations and management. Intent-based networking (IBN) is a promising networking paradigm that provides abstract and autonomous network management. IBN promises to simplify configuring networking devices, allowing network engineers and service providers to focus on providing the expected services and continuously verifying that the network operates within the desired status. An IBN process starts by expressing the user requirement in a high-level descriptive format. Then, the IBN system translates these requirements to a low-level deployable format in a process called intent translation. In this work, we formally define the intent translation process and propose a generic intent translation system. Furthermore, we review the research on intent translation published between 2018 and 2022. We analyze and classify the proposed intent translation schemes and discuss the challenges and recent trends in intent translation.",IT,https://link.springer.com/article/10.1007/s00521-025-11193-7
Chaos Engineering: A Multi-Vocal Literature Review,"Organizations, particularly medium and large enterprises, typically today rely heavily on complex, distributed systems to deliver critical services and products. However, the growing complexity of these systems poses challenges in ensuring service availability, performance, and reliability. Traditional resilience testing methods often fail to capture modern systems’ intricate interactions and failure modes. Chaos Engineering addresses these challenges by proactively testing how systems in production behave under turbulent conditions, allowing developers to uncover and resolve potential issues before they escalate into outages. Though chaos engineering has received growing attention from researchers and practitioners alike, we observed a lack of a comprehensive literature review. Hence, we performed a Multivocal Literature Review (MLR) on chaos engineering to fill this research gap by systematically analyzing 88 academic and grey literature sources published from January 2019 to April 2024. We first used the selected sources to derive a unified definition of chaos engineering and to identify key capabilities, components, and adoption drivers. We also developed a taxonomy for chaos engineering and compared the relevant tools using it. Finally, we analyzed the state of the current chaos engineering research and identified several open research issues.",IT,https://arxiv.org/abs/2412.01416
kvm: the Linux Virtual Machine Monitor,"Virtualization is a hot topic in operating systems these days. It is useful in many scenarios: server consolidation, virtual test environments, and for Linux enthusiasts who still cannot decide which distribution is best. Recently, hardware vendors of commodity x86 processors have added virtualization extensions to the instruction set that can be utilized to write relatively simple virtual machine monitors. The Kernel-based Virtual Machine, or kvm, is a new Linux subsystem which leverages these virtualization extensions to add a virtual machine monitor (or hypervisor) capability to Linux. Using kvm, one can create and run multiple virtual machines. These virtual machines appear as normal Linux processes and integrate seamlessly with the rest of the system.",IT,https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf
Impacts of data consistency levels in cloud-based NoSQL for data-intensive applications,"When using database management systems (DBMSs), it is common to distribute instance replicas across multiple locations for disaster recovery and scaling purposes. To efficiently geo-replicate data, it is crucial to ensure the data and its replicas remain consistent with the same and the most up-to-date data. However, DBMSs’ inner characteristics and external factors, such as the replication strategy and network latency, can affect system performance when dealing with data replication, especially when the replicas are deployed far apart. Thus, it is essential to comprehend how achieving high data consistency levels in geo-replicated systems can impact systems performance. This work analyzes various data consistency settings for the widely used NoSQL DBMSs, namely MongoDB, Redis, and Cassandra. The analysis is based on real-world experiments in which DBMS nodes are deployed on cloud platforms in different locations, considering single and multiple region deployments. Based on the results of the experiments, we provide a comprehensive analysis regarding the system throughput and response time when executing reading and writing operations, pointing out scenarios where each DBMS could be better employed. Some of our findings include, for instance, that opting for strong data consistency significantly impacts Cassandra’s reading operations in the single-region deployment, while MongoDB writing operations are most affected in a multi-region scenario. Additionally, all of these DBMSs exhibit statistically significant variations across all scenarios in the multi-region setup when the data consistency is switched from weak to stronger level.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00716-7
Functionality-aware offloading technique for scheduling containerized edge applications in IoT edge computing,"Edge computing (EC) represents a basic functionality to support the efficiency of the future Internet of intelligent things. The EC has promoted container adoption for deploying and managing applications. Current container scheduling techniques on edge infrastructures show multiple limitations. The design scheduler of container applications execute workflow as specified by the container cloud workflow engine assisted by the Kubernetes platform. We model dependency workflow of containerized applications built using microservices as directed acyclic graph (DAG). The structure of DAG allows the system to prepare the scheduling order list of microservices. The Argo workflow is used to prepare the sequence to deploy containerized applications. In addition, edge worker nodes’ resource utilization data enabled assists to select on which edge worker nodes the scheduling will take place. By combining the two mechanism, we termed the scheduling as functionality-aware offloading on scheduling containerized edge applications. We implemented the orchestration prototype and evaluate the performance of the proposed technique under extensive simulations using the ContainerCloudSim simulator with a module that models a lightweight Kubernetes platform in the context of the edge computing infrastructure. To validate our containerized edge inference service and collect data for the simulation setup, we used Raspberry Pis 4, and the cloud core was set up on Amazon Web Services. The workload in the pre-defined workflow using Argo K8s native was performed by calling the pre-trained model (downloaded and stored locally) and then executing the prediction microservice running on Raspberry Pis. The results demonstrate that our proposal outperforms the baseline scheduling offloading technique in edge computing by decreasing the average scheduling time of containerized edge applications by 15%.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-025-00737-w
Explainable AI-based innovative hybrid ensemble model for intrusion detection,"Cybersecurity threats have become more worldly, demanding advanced detection mechanisms with the exponential growth in digital data and network services. Intrusion Detection Systems (IDSs) are crucial in identifying illegitimate access or anomalous behaviour within computer network systems, consequently opposing sensitive information. Traditional IDS approaches often struggle with high false positive rates and the ability to adapt embryonic attack patterns. This work asserts a novel Hybrid Adaptive Ensemble for Intrusion Detection (HAEnID), an innovative and powerful method to enhance intrusion detection, different from the conventional techniques. HAEnID is composed of a string of multi-layered ensemble, which consists of a Stacking Ensemble (SEM), a Bayesian Model Averaging (BMA), and a Conditional Ensemble method (CEM). HAEnID combines the best of these three ensemble techniques for ultimate success in detection with a considerable cut in false alarms. A key feature of HAEnID is an adaptive mechanism that allows ensemble components to change over time as network traffic patterns vary and new threats appear. This way, HAEnID would provide adequate protection as attack vectors change. Furthermore, the model would become more interpretable and explainable using Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). The proposed Ensemble model for intrusion detection on CIC-IDS 2017 achieves excellent accuracy (97-98%), demonstrating effectiveness and consistency across various configurations. Feature selection further enhances performance, with BMA-M (20) reaching 98.79% accuracy. These results highlight the potential of the ensemble model for accurate and reliable intrusion detection and, hence, is a state-of-the-art choice for accuracy and explainability.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00712-x
Privacy-enhanced distributed revocable identity management scheme based self-sovereign identity,"In recent years, the rapid proliferation of digital services and resources on Industrial Internet has imposed higher demands on universality and privacy of identity management. Particularly with the advent of the digital economy, prudent users are urged to maintain control over their digital identity credentials. However, traditional identity management methods have failed to meet this requirement and thus have been prone to raise users’ concerns about potential financial loss. Specifically, conventional identity management systems (IDMS) have been plagued by imperceptible privacy disclosure, which derives from the flaws in single points of failure, excessive disclosure, correlation analysis, traceability, and revocation. The emerging Self-Sovereign Identity (SSI) architecture aims to tackle these issues and is propelling the evolution of privacy-enhanced distributed identity management. To this end, we proposed a privacy-enhanced distributed identity management scheme with sequential aggregate issuance, threshold traceability and revocability in the setting of multiple credential issuers and regulators. We adopted the Decentralized identifiers (DIDs) and verifiable credentials (VCs) based on the SSI architecture to ensure the hierarchical identity authentication. The security and performance analysis shows that our proposal achieves the desired design goals and is feasible for distributed Industrial Internet.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00715-8
Seagull: Intelligent Cloud Bursting for Enterprise Applications,"Enterprises with existing IT infrastructure are beginning to employ a hybrid cloud model where the enterprise uses its own private resources for the majority of its computing, but then “bursts” into the cloud when local resources are insufficient. However, current approaches to cloud bursting cannot be effectively automated because they heavily rely on system administrator knowledge to make decisions. In this paper we describe Seagull, a system designed to facilitate cloud bursting by determining which applications can be transitioned into the cloud most economically, and automating the movement process at the proper time. We further optimize the deployment of applications into the cloud using an intelligent precopying mechanism that proactively replicates virtualized applications, lowering the bursting time from hours to minutes. Our evaluation illustrates how our prototype can reduce cloud costs by more than 45% when bursting to the cloud, and the incremental cost added by precopying applications is offset by a burst time reduction of nearly 95%.",IT,https://none.cs.umass.edu/papers/pdf/usenix12-seagull.pdf
Maglev: A Fast and Reliable Software Network Load Balancer,"Maglev is Google’s network load balancer. It is a large distributed software system that runs on commodity Linux servers. Unlike traditional hardware network load balancers, it does not require a specialized physical rack deployment, and its capacity can be easily adjusted by adding or removing servers. Network routers distribute packets evenly to the Maglev machines via Equal Cost Multipath (ECMP); each Maglev machine then matches the packets to their corresponding services and spreads them evenly to the service endpoints. To accommodate high and ever-increasing traffic, Maglev is specifically optimized for packet processing performance. A single Maglev machine is able to saturate a 10Gbps link with small packets. Maglev is also equipped with consistent hashing and connection tracking features, to minimize the negative impact of unexpected faults and failures on connection-oriented protocols. Maglev has been serving Google’s traffic since 2008. It has sustained the rapid global growth of Google services, and it also provides network load balancing for Google Cloud Platform.",IT,https://research.google.com/pubs/archive/44824.pdf
Monarch: Google’s Planet-Scale In-Memory Time Series Database,"Monarch is a globally-distributed in-memory time series database system in Google. Monarch runs as a multi-tenant service and is used mostly to monitor the availability, correctness, performance, load, and other aspects of billion-user-scale applications and systems at Google. Every second, the system ingests terabytes of time series data into memory and serves millions of queries. Monarch has a regionalized architecture for reliability and scalability, and global query and configuration planes that integrate the regions into a unified system. On top of its distributed architecture, Monarch has flexible configuration, an expressive relational data model, and powerful queries. This paper describes the structure of the system and the novel mechanisms that achieve a reliable and flexible unified system on a regionalized distributed architecture. We also share important lessons learned from a decade’s experience of developing and running Monarch as a service in Google.",IT,https://www.vldb.org/pvldb/vol13/p3181-adams.pdf
Cassandra - A Decentralized Structured Storage System,"Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.",IT,https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf
Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency,"Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere at any time and only pay for what they use and store. In WAS, data is stored durably using both local and geographic replication to facilitate disaster recovery. Currently, WAS storage comes in the form of Blobs (files), Tables (structured storage), and Queues (message delivery). In this paper, we describe the WAS architecture, global namespace, and data model, as well as its resource provisioning, load balancing, and replication systems.",IT,https://www.cs.purdue.edu/homes/csjgwang/CloudNativeDB/AzureStorageSOSP11.pdf
FAWN: A Fast Array of Wimpy Nodes,"This paper presents a new cluster architecture for low-power data-intensive computing. The design couples low-power CPUs with flash storage to create a fast array of wimpy nodes. FAWN reduces per-node power consumption while still achieving high performance by parallelism across many nodes. We prototype a FAWN key-value storage system and show it yields significantly more queries per joule than traditional disk-based clusters. FAWN demonstrates that efficiency at scale can be improved by using large numbers of modest, energy-efficient nodes for certain workloads, highlighting an alternative approach to building energy-proportional data centers.",IT,https://www.cs.cmu.edu/~fawnproj/papers/fawn-sosp2009.pdf
Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,"We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are immutable, partitioned collections of records. RDDs can be rebuilt efficiently if partitions are lost, using lineage information. We implement RDDs in Spark, which provides a high-level programming interface for cluster computing. Using RDDs, many iterative algorithms and interactive data analysis tasks run 10× faster than Hadoop MapReduce by avoiding repeated disk I/O. RDDs enable a wide range of applications while retaining fault tolerance. We evaluate Spark on real cluster workloads and show it achieves excellent performance and recovery properties.",IT,https://www.usenix.org/legacy/event/nsdi12/tech/full_papers/Zaharia.pdf
CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms,"CloudSim is a toolkit that provides a generalized and extensible simulation framework to model Cloud infrastructure and application services. It supports system and behavior modeling of cloud components (data centers, virtual machines, resource provisioning policies, etc.) and allows evaluation of resource allocation and scheduling algorithms in a repeatable manner under different configurations. This paper describes CloudSim’s architecture and capabilities, including modeling of multi-tenant data centers, energy-aware simulations, network topologies, and dynamic workloads. We demonstrate CloudSim’s use by simulating a scheduling algorithm and analyzing its performance. As an open-source tool, CloudSim has become popular for researchers to test new cloud management strategies (e.g., load balancing, VM consolidation, power management) without the expense of real deployments.",IT,https://doi.org/10.1002/spe.995
Apollo: Scalable and Coordinated Scheduling for Cloud-Scale Computing,"This paper presents Apollo, a highly scalable and coordinated scheduling framework, which has been deployed on production clusters at Microsoft. Apollo dynamically schedules hundreds of thousands of jobs across tens of thousands of machines while meeting diverse scheduling objectives. It combines distributed local schedulers for low-latency decisions with a periodic global coordination mechanism to optimize overall placement. Experiments on Microsoft’s production workload show Apollo achieves near-optimal scheduling quality with sub-second average decision time, significantly outperforming prior centralized and decentralized schedulers in both fairness and efficiency. Apollo demonstrates that even at cloud scale, coordinated scheduling can provide excellent performance, challenging the conventional trade-off between scale and optimality.",IT,https://www.usenix.org/conference/osdi14/technical-sessions/presentation/boutin
"Firmament: Fast, Centralized Cluster Scheduling at Scale","This paper describes Firmament, a centralized scheduler that scales to over 10,000 machines while making sub-second placement decisions. Firmament formulates scheduling as a min-cost max-flow problem, enabling global optimization of task assignments. By using incremental graph optimization and multiple optimizations, Firmament achieves fast, high-quality scheduling. Simulation results on Google cluster trace workloads show Firmament improves job turnaround time and fairness compared to state-of-the-art schedulers. A prototype integrated with Kubernetes confirms Firmament can feasibly handle production-scale cluster management. Firmament’s results suggest that contrary to conventional assumptions, centralized scheduling can meet the latency and scale demands of large clusters, paving the way for more intelligent resource management in datacenters.",IT,https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gog
The Vision of Autonomic Computing,"In mid-October 2001, IBM released a manifesto observing that the main obstacle to further progress in the IT industry is a looming software complexity crisis. The company cited applications and environments that weigh in at tens of millions of lines of code and require skilled IT professionals to install, configure, tune, and maintain. The manifesto pointed out that the difficulty of managing today's computing systems goes well beyond the administration of individual software environments. The need to integrate several heterogeneous environments into corporate-wide computing systems, and to extend that beyond company boundaries into the Internet, introduces new levels of complexity. Computing systems' complexity appears to be approaching the limits of human capability, yet the march toward increased interconnectivity and integration rushes ahead unabated. This march could turn the dream of pervasive computing—trillions of computing devices connected to the Internet—into a nightmare. Programming language innovations have extended the size and complexity of systems that architects can design, but relying solely on further innovations in programming methods will not get us through the present complexity crisis. As systems become more interconnected and diverse, architects are less able to anticipate and design interactions among components, leaving such issues to be dealt with at runtime. Soon systems will become too massive and complex for even the most skilled system integrators to install, configure, optimize, maintain, and merge. And there will be no way to make timely, decisive responses to the rapid stream of changing and conflicting demands. The only option remaining is autonomic computing—computing systems that can manage themselves given high-level objectives from administrators. When IBM's senior vice president of research, Paul Horn, introduced this idea to the National Academy of Engineers at Harvard University in a March 2001 keynote address, he deliberately chose a term with a biological connotation. The autonomic nervous system governs our heart rate and body temperature, thus freeing our conscious brain from the burden of dealing with these and many other low-level, yet vital, functions. The term autonomic computing is emblematic of a vast and somewhat tangled hierarchy of natural self-governing systems, many of which consist of myriad interacting, self-governing components that in turn comprise large numbers of interacting, autonomous, self-governing components at the next level down. The enormous range in scale, starting with molecular machines within cells and extending to human markets, societies, and the entire world socioeconomy, mirrors that of computing systems, which run from individual devices to the entire Internet. Thus, we believe it will be profitable to seek inspiration in the self-governance of social and economic systems as well as purely biological ones. Clearly then, autonomic computing is a grand challenge that reaches far beyond a single organization. Its realization will take a concerted, long-term, worldwide effort by researchers in a diversity of fields. A necessary first step is to examine this vision: what autonomic computing systems might look like, how they might function, and what obstacles researchers will face in designing them and understanding their behavior.",IT,https://ieeexplore.ieee.org/document/1160055
f4: Facebook’s Warm BLOB Storage System,"f4 is a new system that lowers the effective replication factor of warm BLOBs (binary large objects) while remaining fault tolerant and able to support their lower throughput demands. Facebook’s photo storage stack originally replicated all images three times for reliability. As images age (“warm” data), f4 replaces one replica with erasure-coded fragments, reducing storage overhead significantly (e.g., 2.1× instead of 3× replication) while tolerating disk, host, rack, and even datacenter failures. f4 consists of heterogeneous storage nodes and a software stack that handles encoding, decoding, and repair. Deployed across Facebook’s data centers, f4 saves dozens of petabytes of storage and seamlessly serves warm photo traffic with acceptable latency. This paper details f4’s design, including its Reed-Solomon encoding parameters, failure domain design, and the caching and tiering mechanisms that ensure user experience is not impacted. f4 demonstrates how large-scale storage systems can exploit erasure coding for substantial cost savings.",IT,https://research.facebook.com/publications/f4-facebook-s-warm-blob-storage-system/
Finding a needle in Haystack: Facebook’s photo storage,"This paper describes Haystack, an object storage system optimized for Facebook’s Photos. Facebook currently stores over 260 billion images, with millions of new photos uploaded every hour. Traditional storage architectures incur excessive metadata overhead and disk seeks per photo. Haystack collapses the storage hierarchy: it stores many photos in one large file (a “haystack”) on each storage volume and maintains an in-memory index mapping photo IDs to byte offsets. This removes almost all disk seeks for reads and writes. In production, Haystack reduced average photo read latency from several hundred milliseconds (previous NFS-based system) to under 50 ms, and cut the number of I/O operations per read from 3+ to 1. Haystack’s simplified design and efficient use of extents enabled Facebook to scale photo storage to petabytes on commodity hardware with minimal operational complexity. The system has been running in production for over two years, serving billions of images daily with high reliability and efficiency.",IT,https://research.facebook.com/publications/finding-a-needle-in-haystack-facebook-s-photo-storage/
DeathStarBench: Open-Source Benchmark Suite for Cloud Microservices,"We introduce DeathStarBench, a comprehensive benchmark suite to evaluate end-to-end performance of cloud microservice applications. It includes five real-world inspired application stacks (e.g., social network, media streaming, e-commerce) built with dozens of microservices using popular frameworks. We utilize DeathStarBench to study microservices’ impact on cloud systems. Our analysis reveals that microservices incur increased communication (e.g., 60% of requests are remote calls) and exhibit highly variable tail latencies due to fan-out and inter-service dependencies. We evaluate how modern RPC frameworks and kernel network stacks handle these workloads, finding existing systems often underperform (e.g., kernel network overhead contributes significantly to 99th percentile latency). We discuss optimizations such as user-space networking and improved concurrency control. DeathStarBench provides a realistic and reproducible way to drive research on microservice-aware operating systems, network optimizations, and scheduling policies, ultimately guiding the design of next-generation cloud platforms better suited for microservices.",IT,https://www.csl.cornell.edu/~delimitrou/papers/2019.asplos.microservices.pdf
Google Cluster Workload Traces: Analysis and Lessons,"Google’s publicly released cluster workload traces provide a rare glimpse into large-scale data center operations. In this study, we analyze a month-long Google trace (May 2011) to characterize workload patterns and draw implications for cluster management. We observe that the workload is highly heterogeneous: a mix of long-running services and short batch jobs, with arrival rates varying diurnally. Resource utilization is low on average (CPU ~40%) but bursts to high levels, indicating significant headroom for consolidation. Task durations are heavy-tailed, with many very short tasks and a few extremely long ones. Failures and interruptions (preemptions) are frequent – over 10% of tasks were evicted or failed – underscoring the need for resilient scheduling. Based on these findings, we discuss lessons: e.g., schedulers should incorporate priority preemption to accommodate urgent jobs, and resource allocation should account for usage variance. Our analysis provides a foundation for researchers to develop more efficient, robust cluster schedulers and resource management policies that reflect production realities.",IT,https://arxiv.org/abs/2308.02358
Site Reliability Engineering: Google’s Approach to Service Reliability,"Google’s Site Reliability Engineering (SRE) is a discipline that applies software engineering to operations, aiming to create ultra-reliable software systems. This article provides an overview of SRE practices that keep Google’s services running at scale. Key concepts include the use of Service Level Objectives (SLOs) and error budgets to balance reliability with feature velocity – if a service meets its reliability target, developers can take more risks until the error budget is exhausted. SRE teams focus on eliminating toil (repetitive manual work) through automation and have authority to improve systems’ reliability by refactoring or optimizing code. We describe Google’s incident management process (emphasizing blameless post-mortems), how SREs conduct capacity planning and demand forecasting, and techniques for testing reliability (like chaos engineering). SRE has a unique culture where risk is managed proactively and failure is embraced as an opportunity to learn. The result is services that achieve high uptime despite continuous deployment of changes. The paper concludes with lessons for organizations adopting SRE, including hiring profiles (software engineers with ops mindset) and the importance of management support for reliability investments.",IT,https://dl.acm.org/doi/book/10.5555/3006357
Fog Computing and Its Role in the Internet of Things,"Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. De ning characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid , Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).",IT,https://conferences.sigcomm.org/sigcomm/2012/paper/mcc/p13.pdf
I-MPaFS: enhancing EDoS attack detection in cloud computing through a data-driven approach,"Cloud computing offers cost-effective IT solutions but is susceptible to security threats, particularly the Economic Denial of Sustainability (EDoS) attack. EDoS exploits cloud elasticity and the pay-per-use billing model, forcing users to incur unnecessary costs. This research introduces the Integrated Model Prediction and Feature Selection (I-MPaFS) framework to address EDoS attacks. I-MPaFS framework enhances an existing dataset to improve performance, using the generated data to build a Random Forest model for EDoS detection. Our investigation employs the UNSW-NB15, CSE-CIC-IDS18 and NSL-KDD datasets, demonstrating the proposed method’s superiority over existing techniques. The model achieved recall scores of 99.45% on the UNSW-NB15 dataset, 98.19% on the CSE-CIC-IDS18 dataset, and 99.82% on the NSL-KDD dataset, highlighting its reliability and efficacy in safeguarding cloud users from financial exploitation. This study contributes to the field by evaluating current EDoS detection methods, introducing the I-MPaFS framework, validating its performance with benchmark datasets, and comparing its effectiveness against state-of-the-art techniques. The findings affirm the significant potential of I-MPaFS in enhancing cloud security and protecting users from EDoS attacks.",IT,https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-024-00699-5
PowerNap: Eliminating Server Idle Power,"Cloud data centers often operate servers at 10-50% average utilization, yet servers consume nearly full power even when doing little work. This wastes energy and reduces efficiency. We propose PowerNap, an approach where servers transition rapidly to a low-power state when idle and resume full-power operation when load returns. We present a PowerNap-enabled server prototype and show it can enter a near-zero-power sleep state in milliseconds. To complement PowerNap, we introduce RAILS (Redundant Array of Inexpensive Load Sharing), a power supply design that efficiently delivers power across the wide dynamic range from full load to sleep with minimal conversion loss. Evaluation using real-world traces indicates that a cluster of PowerNap servers can maintain required performance while eliminating almost all idle power consumption, achieving energy proportionality. For example, a web cluster with PowerNap reduces energy use by 74% compared to today’s always-on servers. We discuss software implications, such as ensuring quick state save/restore and modifying operating system timers. PowerNap demonstrates a practical path to energy-proportional computing by tackling idle power waste through fast, fine-grained power management.",IT,https://dl.acm.org/doi/10.1145/2528521.1508269
The Hadoop Distributed File System,"The Hadoop Distributed File System (HDFS) is designed to reliably store very large files across machines in a large cluster. HDFS stores file data as blocks on multiple DataNodes for fault tolerance, with an overarching NameNode managing namespace metadata and block mapping. This article outlines HDFS’s architecture and features. Key design points include: high throughput rather than low latency, streaming data access patterns (write-once, read-many), and simple coherency model (no updates to files once written). HDFS ensures durability through replication (default 3x copies) and automatically re-replicates blocks on DataNode failure. A secondary NameNode periodically checkpoints metadata for recovery. We highlight HDFS’s approach to rack-aware placement, which improves data reliability and network bandwidth utilization by spreading replicas across racks. HDFS has proven to scale to petabytes of storage and thousands of nodes in production at Yahoo! and Facebook. Its success has made it the foundation of the Apache Hadoop ecosystem, demonstrating how a carefully engineered file system can enable big data processing on commodity hardware.",IT,https://ieeexplore.ieee.org/document/5496972
Apache Hadoop YARN: yet another resource negotiator,"The initial design of Apache Hadoop [1] was tightly focused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agorá---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler. In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environments (including 100% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto YARN viz. Dryad, Giraph, Hoya, Hadoop MapReduce, REEF, Spark, Storm, Tez.",IT,https://dl.acm.org/doi/10.1145/2523616.2523633
Infrastructure as Code for Dynamic Deployments,"Modern DevOps organizations require a high degree of automation to achieve software stability at frequent changes. Further, there is a need for flexible, timely reconfiguration of the infrastructure, e.g., to use pay-per-use infrastructure efficiently based on application load. Infrastructure as Code (IaC) is the DevOps tool to automate infrastructure. However, modern static IaC solutions only support infrastructures that are deployed and do not change afterward. To implementinfrastructures that change dynamically over time, static IaC programs have to be (updated and) re-run, e.g., in a CI/CD pipeline, or configure an external orchestrator that implements the dynamic behavior, e.g., an autoscaler or Kubernetes operator. Both do not capture the dynamic behavior in the IaC program and prevent analyzing and testing the infrastructure configuration jointly with its dynamic behavior. To fill this gap, we envision dynamic IaC, which augments static IaC with the ability to define dynamic behavior within the IaC program. In contrast to static IaC programs, dynamic IaC programs run continuously. They re-evaluate program parts that depend on external signals when these change and automatically adjust the infrastructure accordingly. We implement DIaC as the first dynamic IaC solution and demonstrate it in two realistic use cases of broader relevance. With dynamic IaC, ensuring the program’s correctness is even harder than for static IaC because programs may define many target configurations in contrast to only a few. However, for this reason, it is also more critical. To solve this issue, we propose automated, specialized property-based testing for IaC programs and implement it in ProTI.",IT,https://programming-group.com/assets/pdf/papers/2022_Infrastructure-as-Code-for-Dynamic-Deployments.pdf
"DevSecOps: A Multivocal Literature Review on Current State, Challenges, Practices, Tools, and Metrics","Security appears as a key non-functional requirement of software development but is often ignored and devalued, because of regarding security as an inhibitor to high velocity required in DevOps implementation. DevSecOps approach is created as a security-orientation expansion to DevOps, aimed to integrate security into DevOps by promoting collaboration among development, operation and security teams.This paper aims to analyze the current state of DevSecOps in the literature; and investigate the application of DevSecOps in Global Software Engineering contexts. A Multi-vocal Literature Review on DevSecOps and its global application was conducted, including white (104) and grey (43) literature from 2012 to 2021. A Thematic Analysis was performed to identify, synthesize and analyze the themes within data for reporting the results.Results identify five aspects of DevSecOps in the existing literature: Definitions, Challenges, Practices, Tools/Technologies, and Metrics/Measurement; collect related themes of each aspect; further generate a Challenge-Practice-Tool-Metric (CPTM) model by integrating the themes of latter four aspects. This model captures the current status and existing experience of DevSecOps. Moreover, a missing global dimension of DevSecOps has been identified. This reveals the need for future research on exploring how DevSecOps operates globally, and how it may differ in local or global settings.",IT,https://ssrn.com/abstract=4511782
The Google File System,"We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.",IT,https://research.google.com/archive/gfs-sosp2003.pdf
"Service Mesh: Architectures, Applications, and Implementations","The scalability and flexibility of microservice architecture have led to major changes in cloud-native application architectures. However, the complexity of managing thousands of small services written in different languages and handling the exchange of data between them have caused significant management challenges. Service mesh is a promising solution that could mitigate these problems by introducing an overlay layer on top of the services. In this paper, we first study the architecture and components of service mesh architecture. Then, we review two important service mesh implementations and discuss how the service mesh could be helpful in other areas, including 5G.",IT,https://arxiv.org/abs/2405.13333
Enterprise Architecture Components for Cloud Service Consumers,Enterprise Architecture (EA) and appropriate governance enables cloud computing adoption by consumer organisations. EA is gaining acceptability as an approach for strategic alignment of business and IT and as key enabler for cloud computing. EA practices consist of a range of activities and covers many of the elements necessary for enabling cloud computing. This paper discusses the key architectural components necessary from the perspective of a consumer organization for the adoption of cloud computing and discusses these elements in the context of EA frameworks and governance. The ability to use maturity assessments on these architectural components to determine organizational readiness to achieve cloud benefits is introduced.,IT,http://www.scitepress.org/Papers/2015/54683/54683.pdf
An Updated Performance Comparison of Virtual Machines and Linux Containers,"Cloud computing makes extensive use of virtual machines because they permit workloads to be isolated from one another and for the resource usage to be somewhat controlled. In this paper, we explore the performance of traditional virtual machine (VM) deployments, and contrast them with the use of Linux containers. We use KVM as a representative hypervisor and Docker as a container manager. Our results show that containers result in equal or better performance than VMs in almost all cases. Both VMs and containers require tuning to support I/Ointensive applications. We also discuss the implications of our performance results for future cloud architectures.",IT,https://ieeexplore.ieee.org/document/7095802
Spanner: Google’s Globally-Distributed Database,"Spanner is Google’s scalable, multi-version, globallydistributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.",IT,https://research.google.com/archive/spanner-osdi2012.pdf
Ananta: Cloud Scale Load Balancing,"Layer-4 load balancing is fundamental to creating scale-out web services. We designed and implemented Ananta, a scale-out layer-4 load balancer that runs on commodity hardware and meets the performance, reliability and operational requirements of multi-tenant cloud computing environments. Ananta combines existing techniques in routing and distributed systems in a unique way and splits the components of a load balancer into a consensus-based reliable control plane and a decentralized scale-out data plane. A key component of Ananta is an agent in every host that can take over the packet modification function from the load balancer, thereby enabling the load balancer to naturally scale with the size of the data center. Due to its distributed architecture, Ananta provides direct server return (DSR) and network address translation (NAT) capabilities across layer-2 boundaries. Multiple instances of Ananta have been deployed in the Windows Azure public cloud with combined bandwidth capacity exceeding 1Tbps. It is serving traffic needs of a diverse set of tenants, including the blob, table and relational storage services. With its scale-out data plane we can easily achieve more than 100Gbps throughput for a single public IP address. In this paper, we describe the requirements of a cloud-scale load balancer, the design of Ananta and lessons learnt from its implementation and operation in the Windows Azure public cloud.",IT,https://dl.acm.org/doi/10.1145/2534169.2486026
X-Trace: A Pervasive Network Tracing Framework,"Modern Internet systems often combine different applications (e.g., DNS, web, and database), span different administrative domains, and function in the context of network mechanisms like tunnels, VPNs, NATs, and overlays. Diagnosing these complex systems is a daunting challenge. Although many diagnostic tools exist, they are typically designed for a specific layer (e.g., traceroute) or application, and there is currently no tool for reconstructing a comprehensive view of service behavior. In this paper we propose X-Trace, a tracing framework that provides such a comprehensive view for systems that adopt it. We have implemented X-Trace in several protocols and software systems, and we discuss how it works in three deployed scenarios: DNS resolution, a three-tiered photo-hosting website, and a service accessed through an overlay network.",IT,https://www.usenix.org/conference/nsdi-07/x-trace-pervasive-network-tracing-framework
The Tail at Scale,Software techniques that tolerate latency variability are vital to building responsive large-scale Web services.,IT,https://dl.acm.org/doi/10.1145/2408776.2408794
A Virtual Machine Introspection Based Architecture for Intrusion Detection,"Today's architectures for intrusion detection force the IDS designer to make a difficult choice. If the IDS resides on the host, it has an excellent view of what is happening in that host's software, but is highly susceptible to attack. On the other hand, if the IDS resides in the network, it is more resistant to attack, but has a poor view of what is happeninginside the host, making it more susceptible to evasion. In this paper we present an architecture that retains the visibility of a host-based IDS, but pulls the IDS outside of the host for greater attack resistance. We achieve this through the use of a virtual machine monitor. Using this approach allows us to isolate the IDS from the monitored host but still retain excellent visibility into the host's state. The VMM also offers us the unique ability to completely mediate interactions between the host software and the underlying hardware. We present a detailed study of our architecture, including Livewire, a prototype implementation. We demonstrate Livewire by implementing a suite of simple intrusion detection policies and using them to detect real attacks.",IT,https://www.ndss-symposium.org/wp-content/uploads/2017/09/A-Virtual-Machine-Introspection-Based-Architecture-for-Intrusion-Detection-Tal-Garfinkel.pdf
Disaster Recovery as a Cloud Service: Economic Benefits & Deployment Challenges,"Many businesses rely on Disaster Recovery (DR) services to prevent either manmade or natural disasters from causing expensive service disruptions. Unfortunately, current DR services come either at very high cost, or with only weak guarantees about the amount of data lost or time required to restart operation after a failure. In this work, we argue that cloud computing platforms are well suited for offering DR as a service due to their pay-as-you-go pricing model that can lower costs, and their use of automated virtual platforms that can minimize the recovery time after a failure. To this end, we perform a pricing analysis to estimate the cost of running a public cloud based DR service and show significant cost reductions compared to using privately owned resources. Further, we explore what additional functionality must be exposed by current cloud platforms and describe what challenges remain in order to minimize cost, data loss, and recovery time in cloud based DR services.",IT,https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Wood.pdf
Software-Defined Networking: A Comprehensive Survey,"The Internet has led to the creation of a digital society, where (almost) everything is connected and is accessible from anywhere. However, despite their widespread adoption, traditional IP networks are complex and very hard to manage. It is both difficult to configure the network according to predefined policies, and to reconfigure it to respond to faults, load and changes. To make matters even more difficult, current networks are also vertically integrated: the control and data planes are bundled together. Software-Defined Networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network’s control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound APIs, network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms– with a focus on aspects such as resiliency, scalability, performance, security and dependability– as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.",IT,https://arxiv.org/pdf/1406.0440
PeerPressure: Automated Diagnosis of Misconfigurations in Large-Scale Systems,"Technical support contributes 17% of the total cost of ownership of today’s desktop PCs . An important element of technical support is troubleshooting misconf igured applications. Misconfiguration troubleshooting is particularly challenging, because configuration information is shared and altered by multiple applications. In this paper, we present a novel troubleshooting system: PeerPressure, which uses statistics from a set of sample machines to diagnose the root-cause misconfigurations on a sick machine. This is in contrastwithmethods that require manual identification on a healthy machine for diagnosing misconfigurations . The elimination of this manual operation makes a significant step towards automated misconfiguration troubleshooting. In PeerPressure, we introduce a ranking metric for misconfiguration candidates. This metric is based on empirical Bayesian estimation. We have prototyped a PeerPressure troubleshootingsystem anduseda databaseof87machine configurationsnapshots to evaluate its performance. With 20 real-world troubleshooting cases, PeerPressure can effectively pinpoint the root-cause misconfigurations for 12 of these cases. For the remaining cases, PeerPressure significantly narrows down the number of root-cause candidates by three orders of magnitude.",IT,https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/wang/wang.pdf
Bubble-up: Increasing utilization in modern warehouse scale computers via sensible co-locations,"As much of the world's computing continues to move into the cloud, the overprovisioning of computing resources to ensure the performance isolation of latency-sensitive tasks, such as web search, in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority, latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper, we present Bubble-Up, a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of “pressure” to the memory subsystem on processors in production datacenters, our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at “sensible” co-locations in Google's production datacenters with real-world large-scale applications, we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications.",IT,https://ieeexplore.ieee.org/document/7851476
DCTCP: Efficient Packet Transport for the Commoditized Data Center,"Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry ""background"" flows build up queues at the switches, and thus impact the performance of latency sensitive ""foreground"" traffic. To address these problems, we propose DCTCP, a variant of TCP for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to a simple control mechanism implemented in the host OS. We evaluate DCTCP at 1 and 10Gbps speeds, through benchmark experiments and analysis. In the data center, operating with commodity, shallow buffered switches, we find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.",IT,https://www.researchgate.net/publication/238065260_DCTCP_Efficient_Packet_Transport_for_the_Commoditized_Data_Center
Prominent Security Vulnerabilities in Cloud Computing,"This research study examines the significant security vulnerabilities and threats in cloud computing, analyzes their potential consequences for enterprises, and proposes effective solutions for mitigating these vulnerabilities. This paper discusses the increasing significance of cloud security in a time characterized by rapid data expansion and technological progress. The paper examines prevalent vulnerabilities in cloud computing, including cloud misconfigurations, data leakage, shared technology threats, and insider threats. It emphasizes the necessity of adopting a proactive and comprehensive approach to ensure cloud security. The report places significant emphasis on the shared responsibility paradigm, adherence to industry laws, and the dynamic nature of cybersecurity threats. The situation necessitates the cooperation of researchers, cybersecurity professionals, and enterprises to proactively address these difficulties. This partnership aims to provide a thorough manual for organizations aiming to bolster their cloud security measures and safeguard valuable data in an ever-evolving digital landscape.",IT,https://thesai.org/Downloads/Volume15No2/Paper_81-Prominent_Security_Vulnerabilities_in_Cloud_Computing.pdf
On Scalable Integrity Checking for Secure Cloud Disks,"Merkle hash trees are the standard method to protect the integrity and freshness of stored data. However, hash trees introduce additional compute and I/O costs on the I/O critical path, and prior efforts have not fully characterized these costs. In this paper, we quantify performance overheads of storage-level hash trees in realistic settings. We then design an optimized tree structure called Dynamic Merkle Trees (DMTs) based on an analysis of root causes of overheads. DMTs exploit patterns in workloads to deliver up to a 2.2x throughput and latency improvement over the state of the art. Our novel approach provides a promising new direction to achieve integrity guarantees in storage efficiently and at scale.",IT,https://arxiv.org/abs/2405.03830
Secure Data Sharing in Cloud Computing: A Comprehensive Survey of Two-Factor Authentication and Cryptographic Solutions,"Cloud computing has emerged as a pivotal trend across both commercial and academic sectors, offering substantial storage capabilities to service providers and end-users. However, the security of data within cloud environments remains a paramount concern, primarily due to insufficient access controls. This concern is addressed herein through a systematic literature review, which underscores the shared responsibility model, the ubiquitous nature of data access, and the associated breach risks. The importance of enforcing rigorous security protocols to maintain data integrity, confidentiality, and availability is emphasized, as these are vital to stakeholders relying on cloud-based services. The current work presents an analytical review of two-factor authentication (2FA) and cryptographic measures, advocating for their combined implementation to bolster the security frameworks of cloud systems. The survey meticulously examines existing research on cloud computing vulnerabilities, security mechanisms, and the underlying challenges. It is posited that ongoing enhancements and innovations in security practices are critical for countering evolving threats and safeguarding data in an increasingly digital landscape. An exhaustive evaluation of the latest advancements in cryptography is conducted, aiming to ensure secure and authenticated access control for outsourced and encrypted cloud data across diverse user groups. The findings herein serve as a foundation for researchers to refine and develop robust cloud data storage systems. The survey underscores the need for the creation of advanced encryption algorithms, authentication protocols, and intrusion detection systems. Such developments are instrumental in mitigating risks, establishing trust, and preserving the confidentiality and integrity of data stored in cloud infrastructures.",IT,https://www.iieta.org/download/file/fid/114847
Categorizing Defects in Infrastructure as Code,"Infrastructure as code (IaC) scripts are used to automate the maintenance and con guration of software development and deployment infrastructure. IaC scripts can be complex in nature and can contain hundreds of lines of code, leading to defects that can be di cult to debug and to wide-scale system discrepancies, such as service outages. The use of IaC scripts is getting increasingly popular, yet the nature of defects that occur in these scripts have not been systematically categorized. The goal of this paper is to help software practitioners improve their development process of infrastructure as code (IaC) scripts by analyzing the defect categories in IaC scripts based upon a qualitative analysis of commit messages and issue report descriptions. We collect 12,875 commits that map to 2,424 IaC scripts from four organizations, namely Mirantis, Mozilla, Openstack, and Wikimedia Commons. With 89 raters, we apply the defect type attribute of the orthogonal defect classi cation (ODC)",IT,https://techrep.csc.ncsu.edu/2019/TR-2019-2.pdf
"Space Shuffle: A Scalable, Flexible, and High-Performance Data Center Network","The increasing need of cloud and big data applications requires data center networks to be scalable and bandwidth-rich. Current data center network architectures often use rigid topologies to increase network bandwidth. A major limitation is that they can hardly support incremental network growth. Recent work has been investigating new network architecture and protocols to achieve three important design goals of large data centers, namely, high throughput, routing and forwarding scalability, and flexibility for incremental growth. Unfortunately, existing data center network architectures focus on one or two of the above properties and pay little attention to the others. In this paper, we design a novel flexible data center network architecture, Space Shuffle (S2), which applies greedy routing on multiple ring spaces to achieve high-throughput, scalability, and flexibility. The proposed greedy routing protocol of S2 effectively exploits the path diversity of densely connected topologies and enables key-based routing. Extensive experimental studies showthat S2 provides high bisectional bandwidth and throughput, near-optimal routing path lengths, extremely small forwarding state, fairness among concurrent data flows, and resiliency to network failures.",IT,https://users.soe.ucsc.edu/~qian/papers/TPDS16.pdf
DEFENSE-IN-DEPTH METHODS IN MICROSERVICES ACCESS CONTROL,"More and more application deployments are moving towards leveraging the microservice paradigm in hopes of increased efficiency of operations and more flexible software development. Microservices are not a straightforward successor of existing methods and they introduce a lot of new complexity. Especially security concerns lack analysis in academic literature and new developments have mostly been assessed in grey literature. The thesis explores the solutions to increase the security of microservice applications hosted in virtual private clouds. We start with the assumption that the networking security controls have been bypassed and the adversary is inside the network. We look at the situation through a holistic lens to identify the biggest gaps and how they can be filled in REST service-to-service communications. The solutions are platform agnostic to support the multi-cloud paradigm to reduce operational costs and increase global coverage. Defense-in-depth methods proposed are establishing mutually authenticated TLS connections between services comprising an application and introducing granular access control using cryptographically secure methods. The industry state of the art ways to achieve these are assessed and analyzed comparatively and against good security engineering design principles. Both methodologies and their practical implementations are explored. We assess two distinct models for reference use for secure architecture design in microservices. These models piece lower level pieces into a comprehensive idea of what good microservice security looks like. The architectures can be used as is, as a basis for designing secure application architectures. The thesis introduces security analysis of existing methods of deploying and establishing secure microservice applications, from container level orchestration to high level architectural choices. The work adds to the existing body of knowledge by assessing some of the security concerns enterprises moving towards microservice deployments are facing and by providing a new analysis of industry developments that have not been looked at thoroughly through a security lens in scientific literature.",IT,https://trepo.tuni.fi/bitstream/123456789/27172/4/suomalainen.pdf
"A survey on cloud computing security: Issues, threats, and solutions","Over the internet, the cloud computing reveals a remarkable potential to provide on-demand services to consumers with greater flexibility in a cost effective manner. While moving towards the concept of on-demand service, resource pooling, shifting everything on the distributive environment, security is the major obstacle for this new dreamed vision of computing capability. This survey present a comprehensive overview of the security issues for different factors affecting cloud computing. Furthermore, a detailed discussion on several key topics regarding embedded system, application, storage system, clustering related issues and many more. This paper works on some public cloud and private cloud authorities as well as related security concerns. Additionally, it encompasses the requirements for better security management and suggests 3-tier security architecture. Open issues with discussion in which some new security concepts and recommendations are also provided.",IT,https://www.sciencedirect.com/science/article/abs/pii/S1084804516301990
