Title,Abstract,Link,Discipline,Methodology
EnStack: An Ensemble Stacking Framework of Large Language Models for Enhanced Vulnerability Detection in Source Code,"Automated detection of software vulnerabilities is critical for enhancing security, yet existing methods often struggle with the complexity and diversity of modern codebases. In this paper, we introduce EnStack, a novel ensemble stacking framework that enhances vulnerability detection using natural language processing (NLP) techniques. Our approach synergizes multiple pre-trained large language models (LLMs) specialized in code understanding CodeBERT for semantic analysis, GraphCodeBERT for structural representation, and UniXcoder for cross-modal capabilities. By fine-tuning these models on the Draper VDISC dataset and integrating their outputs through meta-classifiers such as Logistic Regression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack effectively captures intricate code patterns and vulnerabilities that individual models may overlook. The meta-classifiers consolidate the strengths of each LLM, resulting in a comprehensive model that excels in detecting subtle and complex vulnerabilities across diverse programming contexts. Experimental results demonstrate that EnStack significantly outperforms existing methods, achieving notable improvements in accuracy, precision, recall, and F1-score. This work highlights the potential of ensemble LLM approaches in code analysis tasks and offers valuable insights into applying NLP techniques for advancing automated vulnerability detection.",http://arxiv.org/abs/2411.16561v1,CS,Quantitative
An Overview of Cyber Security Funding for Open Source Software,"Context: Many open source software (OSS) projects need more human resources for maintenance, improvements, and sometimes even their survival. This need allegedly applies even to vital OSS projects that can be seen as being a part of the world's critical infrastructures. To address this resourcing problem, new funding instruments for OSS projects have been established in recent years. Objectives: The paper examines two such funding bodies for OSS and the projects they have funded. The focus of both funding bodies is on software security and cyber security in general. Methods: The methodology is based on qualitative thematic analysis. Results: Particularly OSS supply chains, network and cryptography libraries, programming languages, and operating systems and their low-level components have been funded and thus seen as critical in terms of cyber security by the two funding bodies. Conclusions: In addition to the qualitative results presented, the paper makes a contribution by connecting the research branches of critical infrastructure and sustainability of OSS projects. A further contribution is made by connecting the topic examined to recent cyber security regulations. Finally, an important argument is raised that neither cyber security nor sustainability alone can entirely explain the rationales behind the funding decisions made by the two bodies.",http://arxiv.org/abs/2412.05887v2,CS,Qualitative
From Requirements to Test Cases: An NLP-Based Approach for High-Performance ECU Test Case Automation,"Automating test case specification generation is vital for improving the efficiency and accuracy of software testing, particularly in complex systems like high-performance Electronic Control Units (ECUs). This study investigates the use of Natural Language Processing (NLP) techniques, including Rule-Based Information Extraction and Named Entity Recognition (NER), to transform natural language requirements into structured test case specifications. A dataset of 400 feature element documents from the Polarion tool was used to evaluate both approaches for extracting key elements such as signal names and values. The results reveal that the Rule-Based method outperforms the NER method, achieving 95% accuracy for more straightforward requirements with single signals, while the NER method, leveraging SVM and other machine learning algorithms, achieved 77.3% accuracy but struggled with complex scenarios. Statistical analysis confirmed that the Rule-Based approach significantly enhances efficiency and accuracy compared to manual methods. This research highlights the potential of NLP-driven automation in improving quality assurance, reducing manual effort, and expediting test case generation, with future work focused on refining NER and hybrid models to handle greater complexity.",http://arxiv.org/abs/2505.00547v1,CS,Quantitative
Task-parallelism in SWIFT for heterogeneous compute architectures,"This paper highlights the first steps towards enabling graphics processing unit (GPU) acceleration of the smoothed particle hydrodynamics (SPH) solver for cosmology SWIFT and creating a hydrodynamics solver capable of fully leveraging the hardware available on heterogeneous exascale machines composed of central and graphics processing units (CPUs and GPUs). Exploiting the existing task-based parallelism in SWIFT, novel combinations of algorithms are presented which enable SWIFT to function as a truly heterogeneous software leveraging CPUs for memory-bound computations concurrently with GPUs for compute-bound computations in a manner which minimises the effects of CPU-GPU communication latency. These algorithms are validated in extensive testing which shows that the GPU acceleration methodology is capable of delivering up to 3.5x speedups for SWIFTs SPH hydrodynamics computation kernels when including the time required to prepare the computations on the CPU and unpack the results on the CPU. Speedups of 7.5x are demonstrated when not including the CPU data preparation and unpacking times. Whilst these measured speedups are substantial, it is shown that the overall performance of the hydrodynamic solver for a full simulation when accelerated on the GPU of state-of-the-art superchips, is only marginally faster than the code performance when using the Grace Hopper superchips fully parallelised CPU capabilities. This is shown to be mostly due to excessive fine-graining of the tasks prior to offloading on the GPU. Fine-graining introduces significant over-heads associated with task management on the CPU hosting the simulation and also introduces un-necessary duplication of CPU-GPU communications of the same data.",http://arxiv.org/abs/2505.14538v1,CS,Quantitative
Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs,"This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.",http://arxiv.org/abs/2504.19675v1,CS,Mixed
Generating refactored code accurately using reinforcement learning,"Automated source code refactoring, particularly extract method refactoring, is a crucial and frequently employed technique during software development. Despite its importance and frequent use by practitioners, current automated techniques face significant limitations. These approaches often rely on developers to identify the precise bounds of refactoring opportunities in terms of source code statements. Also, they often do not capture the semantic context, resulting in offering no automated means to suggest meaningful method name, for instance. To address these challenges, we propose a novel reinforcement learning-based approach for fine-tuning and aligning code language models to perform automated, intelligent extract method refactoring on Java source code. Our approach fine-tunes sequence-to-sequence generative models and aligns them using the Proximal Policy Optimization (PPO) algorithm. We utilize code compilation and presence of the refactoring in the generated code as reward signals, providing a code-centric optimization process. Our experiments demonstrate that our approach significantly enhances the performance of large language models in code refactoring, as evidenced by both quantitative evaluation metrics such as BLEU, ROUGE, and CodeBLEU, and qualitative measures including syntactical and functional correctness. The supervised fine-tuned model, further aligned with PPO, surpasses traditional supervised fine-tuning by 11.96% and 16.45% in terms of BLEU and CodeBLEU scores, respectively. When subjected to a suite of 122 unit tests, the number of successful tests increased from 41 to 66 for the reinforcement learning aligned fine-tuned Code-T5 model, highlighting the effectiveness of our approach in producing functionally correct refactorings.",http://arxiv.org/abs/2412.18035v1,CS,Mixed
Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models,"Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical settings, presenting opportunities for automated detection to identify patients. This study evaluates natural language processing approaches for detecting PTSD from clinical interview transcripts. We compared general and mental health-specific transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and large language model prompting strategies (zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset. Domain-specific models significantly outperformed general models (Mental-RoBERTa F1=0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural networks achieved the highest performance (F1=0.700). Zero-shot prompting using DSM-5 criteria yielded competitive results without training data (F1=0.657). Performance varied significantly across symptom severity and comorbidity status, with higher accuracy for severe PTSD cases and patients with comorbid depression. Our findings highlight the potential of domain-adapted embeddings and LLMs for scalable screening while underscoring the need for improved detection of nuanced presentations and offering insights for developing clinically viable AI tools for PTSD assessment.",http://arxiv.org/abs/2504.01216v1,CS,Mixed
BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks,"Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.",http://arxiv.org/abs/2505.20997v1,CS,Mixed
"A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis","Multi-modal medical image fusion (MMIF) is increasingly recognized as an essential technique for enhancing diagnostic precision and facilitating effective clinical decision-making within computer-aided diagnosis systems. MMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create detailed, clinically useful images of patient anatomy and pathology. These integrated representations significantly advance diagnostic accuracy, lesion detection, and segmentation. This comprehensive review meticulously surveys the evolution, methodologies, algorithms, current advancements, and clinical applications of MMIF. We present a critical comparative analysis of traditional fusion approaches, including pixel-, feature-, and decision-level methods, and delves into recent advancements driven by deep learning, generative models, and transformer-based architectures. A critical comparative analysis is presented between these conventional methods and contemporary techniques, highlighting differences in robustness, computational efficiency, and interpretability. The article addresses extensive clinical applications across oncology, neurology, and cardiology, demonstrating MMIF's vital role in precision medicine through improved patient-specific therapeutic outcomes. Moreover, the review thoroughly investigates the persistent challenges affecting MMIF's broad adoption, including issues related to data privacy, heterogeneity, computational complexity, interpretability of AI-driven algorithms, and integration within clinical workflows. It also identifies significant future research avenues, such as the integration of explainable AI, adoption of privacy-preserving federated learning frameworks, development of real-time fusion systems, and standardization efforts for regulatory compliance.",http://arxiv.org/abs/2505.14715v1,CS,Quantitative
Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation,"Generating accurate code review comments remains a significant challenge due to the inherently diverse and non-unique nature of the task output. Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks. However, large-scale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues. In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer-grade hardware to improve review comment generation. Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks. To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries. Both of our strategies improve the review comment generation performance, with function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90% BLEU-4 score on the CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25% to 83% performance improvement) on this task. An additional human evaluation study further validates our experimental findings, reflecting real-world developers' perceptions of LLM-generated code review comments based on relevant qualitative metrics.",http://arxiv.org/abs/2411.10129v1,CS,Mixed
When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey,"Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model's capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.",http://arxiv.org/abs/2505.00144v1,CS,Quantitative
Fixseeker: An Empirical Driven Graph-based Approach for Detecting Silent Vulnerability Fixes in Open Source Software,"Open source software vulnerabilities pose significant security risks to downstream applications. While vulnerability databases provide valuable information for mitigation, many security patches are released silently in new commits of OSS repositories without explicit indications of their security impact. This makes it challenging for software maintainers and users to detect and address these vulnerability fixes. There are a few approaches for detecting vulnerability-fixing commits (VFCs) but most of these approaches leverage commit messages, which would miss silent VFCs. On the other hand, there are some approaches for detecting silent VFCs based on code change patterns but they often fail to adequately characterize vulnerability fix patterns, thereby lacking effectiveness. For example, some approaches analyze each hunk in known VFCs, in isolation, to learn vulnerability fix patterns; but vulnerabiliy fixes are often associated with multiple hunks, in which cases correlations of code changes across those hunks are essential for characterizing the vulnerability fixes. To address these problems, we first conduct a large-scale empirical study on 11,900 VFCs across six programming languages, in which we found that over 70% of VFCs involve multiple hunks with various types of correlations. Based on our findings, we propose Fixseeker, a graph-based approach that extracts the various correlations between code changes at the hunk level to detect silent vulnerability fixes. Our evaluation demonstrates that Fixseeker outperforms state-of-the-art approaches across multiple programming languages, achieving a high F1 score of 0.8404 on average in balanced datasets and consistently improving F1 score, AUC-ROC and AUC-PR scores by 32.40%, 1.55% and 8.24% on imbalanced datasets. Our evaluation also indicates the generality of Fixseeker across different repository sizes and commit complexities.",http://arxiv.org/abs/2503.20265v1,CS,Quantitative
Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems,"Resource allocation in High Performance Computing (HPC) environments presents a complex and multifaceted challenge for job scheduling algorithms. Beyond the efficient allocation of system resources, schedulers must account for and optimize multiple performance metrics, including job wait time and system utilization. While traditional rule-based scheduling algorithms dominate the current deployments of HPC systems, the increasing heterogeneity and scale of those systems is expected to challenge the efficiency and flexibility of those algorithms in minimizing job wait time and maximizing utilization. Recent research efforts have focused on leveraging advancements in Reinforcement Learning (RL) to develop more adaptable and intelligent scheduling strategies. Recent RL-based scheduling approaches have explored a range of algorithms, from Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently, hybrid methods that integrate Graph Neural Networks with RL techniques. However, a common limitation across these methods is their reliance on relatively small datasets, and these methods face scalability issues when using large datasets. This study introduces a novel RL-based scheduler utilizing the Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm, which supports large-scale distributed training across multiple workers without requiring parameter synchronization at every step. By eliminating reliance on centralized updates to a shared policy, the DD-PPO scheduler enhances scalability, training efficiency, and sample utilization. The validation dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO performance between traditional and advanced scheduling approaches, and the experimental results demonstrate improved scheduling performance in comparison to both rule-based schedulers and existing RL-based scheduling algorithms.",http://arxiv.org/abs/2505.03946v1,CS,Quantitative
Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing,"Quantum optimization is the most mature quantum computing technology to date, providing a promising approach towards efficiently solving complex combinatorial problems. Methods such as adiabatic quantum computing (AQC) have been employed in recent years on important optimization problems across various domains. In deep learning, deep neural networks (DNN) have reached immense sizes to support new predictive capabilities. Optimization of large-scale models is critical for sustainable deployment, but becomes increasingly challenging with ever-growing model sizes and complexity. While quantum optimization is suitable for solving complex problems, its application to DNN optimization is not straightforward, requiring thorough reformulation for compatibility with commercially available quantum devices. In this work, we explore the potential of adopting AQC for fine-grained pruning-quantization of convolutional neural networks. We rework established heuristics to formulate model compression as a quadratic unconstrained binary optimization (QUBO) problem, and assess the solution space offered by commercial quantum annealing devices. Through our exploratory efforts of reformulation, we demonstrate that AQC can achieve effective compression of practical DNN models. Experiments demonstrate that adiabatic quantum computing (AQC) not only outperforms classical algorithms like genetic algorithms and reinforcement learning in terms of time efficiency but also excels at identifying global optima.",http://arxiv.org/abs/2505.16332v1,CS,Quantitative
RGC-Bent: A Novel Dataset for Bent Radio Galaxy Classification,"We introduce a novel machine learning dataset tailored for the classification of bent radio active galactic nuclei (AGN) in astronomical observations. Bent radio AGN, distinguished by their curved jet structures, provide critical insights into galaxy cluster dynamics, interactions within the intracluster medium, and the broader physics of AGN. Despite their astrophysical significance, the classification of bent radio AGN remains a challenge due to the scarcity of specialized datasets and benchmarks. To address this, we present a dataset, derived from a well-recognized radio astronomy survey, that is designed to support the classification of NAT (Narrow-Angle Tail) and WAT (Wide-Angle Tail) categories, along with detailed data processing steps. We further evaluate the performance of state-of-the-art deep learning models on the dataset, including Convolutional Neural Networks (CNNs), and transformer-based architectures. Our results demonstrate the effectiveness of advanced machine learning models in classifying bent radio AGN, with ConvNeXT achieving the highest F1-scores for both NAT and WAT sources. By sharing this dataset and benchmarks, we aim to facilitate the advancement of research in AGN classification, galaxy cluster environments and galaxy evolution.",http://arxiv.org/abs/2505.19249v1,CS,Mixed
Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge,"The rapid rise of Language Models (LMs) has expanded the capabilities of natural language processing, powering applications from text generation to complex decision-making. While state-of-the-art LMs often boast hundreds of billions of parameters and are primarily deployed in data centers, recent trends show a growing focus on compact models-typically under 10 billion parameters-enabled by techniques such as quantization and other model compression techniques. This shift paves the way for LMs on edge devices, offering potential benefits such as enhanced privacy, reduced latency, and improved data sovereignty. However, the inherent complexity of even these smaller models, combined with the limited computing resources of edge hardware, raises critical questions about the practical trade-offs in executing LM inference outside the cloud. To address these challenges, we present a comprehensive evaluation of generative LM inference on representative CPU-based and GPU-accelerated edge devices. Our study measures key performance indicators-including memory usage, inference speed, and energy consumption-across various device configurations. Additionally, we examine throughput-energy trade-offs, cost considerations, and usability, alongside an assessment of qualitative model performance. While quantization helps mitigate memory overhead, it does not fully eliminate resource bottlenecks, especially for larger models. Our findings quantify the memory and energy constraints that must be considered for practical real-world deployments, offering concrete insights into the trade-offs between model size, inference performance, and efficiency. The exploration of LMs at the edge is still in its early stages. We hope this study provides a foundation for future research, guiding the refinement of models, the enhancement of inference efficiency, and the advancement of edge-centric AI systems.",http://arxiv.org/abs/2503.09114v1,CS,Qualitative
Automated Generation of Commit Messages in Software Repositories,"Commit messages are crucial for documenting software changes, aiding in program comprehension and maintenance. However, creating effective commit messages is often overlooked by developers due to time constraints and varying levels of documentation skills. Our research presents an automated approach to generate commit messages using Machine Learning (ML) and Natural Language Processing (NLP) by developing models that use techniques such as Logistic Regression with TF-IDF and Word2Vec, as well as more sophisticated methods like LSTM. We used the dataset of code changes and corresponding commit messages that was used by Liu et al., which we used to train and evaluate ML/NLP models and was chosen because it is extensively used in previous research, also for comparability in our study. The objective was to explore which ML/NLP techniques generate the most effective, clear, and concise commit messages that accurately reflect the code changes. We split the dataset into training, validation, and testing sets and used these sets to evaluate the performance of each model using qualitative and quantitative evaluation methods. Our results reveal a spectrum of effectiveness among these models, with the highest BLEU score achieved being 16.82, showcasing the models' capability in automating a clear and concise commit message generation. Our paper offers insights into the comparative effectiveness of different machine learning models for automating commit message generation in software development, aiming to enhance the overall practice of code documentation. The source code is available at https://doi.org/10.5281/zenodo.10888106.",http://arxiv.org/abs/2504.12998v1,CS,Mixed
LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming,"Automated Test Case Generation (ATCG) is crucial for evaluating software reliability, particularly in competitive programming where robust algorithm assessments depend on diverse and accurate test cases. However, existing ATCG methods often fail to meet complex specifications or generate effective corner cases, limiting their utility. In this work, we introduce Context-Free Grammars with Counters (CCFGs), a formalism that captures both syntactic and semantic structures in input specifications. Using a fine-tuned CodeT5 model, we translate natural language input specifications into CCFGs, enabling the systematic generation of high-quality test cases. Experiments on the CodeContests dataset demonstrate that CCFG-based test cases outperform baseline methods in identifying incorrect algorithms, achieving significant gains in validity and effectiveness. Our approach provides a scalable and reliable grammar-driven framework for enhancing automated competitive programming evaluations.",http://arxiv.org/abs/2505.15039v1,CS,Quantitative
Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts,"Generative pre-trained models power intelligent software features used by millions of users controlled by developer-written natural language prompts. Despite the impact of prompt-powered software, little is known about its development process and its relationship to programming. In this work, we argue that some prompts are programs and that the development of prompts is a distinct phenomenon in programming known as ""prompt programming"". We develop an understanding of prompt programming using Straussian grounded theory through interviews with 20 developers engaged in prompt development across a variety of contexts, models, domains, and prompt structures. We contribute 15 observations to form a preliminary understanding of current prompt programming practices. For example, rather than building mental models of code, prompt programmers develop mental models of the foundation model (FM)'s behavior on the prompt by interacting with the FM. While prior research shows that experts have well-formed mental models, we find that prompt programmers who have developed dozens of prompts still struggle to develop reliable mental models. Our observations show that prompt programming differs from traditional software development, motivating the creation of prompt programming tools and providing implications for software engineering stakeholders.",http://arxiv.org/abs/2409.12447v2,CS,Qualitative
An Empirical Study on the Classification of Bug Reports with Machine Learning,"Software defects are a major threat to the reliability of computer systems. The literature shows that more than 30% of bug reports submitted in large software projects are misclassified (i.e., are feature requests, or mistakes made by the bug reporter), leading developers to place great effort in manually inspecting them. Machine Learning algorithms can be used for the automatic classification of issue reports. Still, little is known regarding key aspects of training models, such as the influence of programming languages and issue tracking systems. In this paper, we use a dataset containing more than 660,000 issue reports, collected from heterogeneous projects hosted in different issue tracking systems, to study how different factors (e.g., project language, report content) can influence the performance of models in handling classification of issue reports. Results show that using the report title or description does not significantly differ; Support Vector Machine, Logistic Regression, and Random Forest are effective in classifying issue reports; programming languages and issue tracking systems influence classification outcomes; and models based on heterogeneous projects can classify reports from projects not present during training. Based on findings, we propose guidelines for future research, including recommendations for using heterogeneous data and selecting high-performing algorithms.",http://arxiv.org/abs/2503.00660v1,CS,Quantitative
Continuously Learning Bug Locations,"Automatically locating buggy changesets associated with bug reports is crucial in the software development process. Deep Learning (DL)-based techniques show promising results by leveraging structural information from the code and learning links between changesets and bug reports. However, since source code associated with changesets evolves, the performance of such models tends to degrade over time due to concept drift. Aiming to address this challenge, in this paper, we evaluate the potential of using Continual Learning (CL) techniques in multiple sub-tasks setting for bug localization (each of which operates on either stationary or non-stationary data), comparing it against a bug localization technique that leverages the BERT model, a deep reinforcement learning-based technique that leverages the A2C algorithm, and a DL-based function-level interaction model for semantic bug localization. Additionally, we enhanced the CL techniques by using logistic regression to identify and integrate the most significant bug-inducing factors. Our empirical evaluation across seven widely used software projects shows that CL techniques perform better than DL-based techniques by up to 61% in terms of Mean Reciprocal Rank (MRR), 44% in terms of Mean Average Precision (MAP), 83% in terms of top@1, 56% in terms of top@5, and 66% in terms of top@10 metrics in non-stationary setting. Further, we show that the CL techniques we studied are effective at localizing changesets relevant to a bug report while being able to mitigate catastrophic forgetting across the studied tasks and require up to 5x less computational effort during training. Our findings demonstrate the potential of adopting CL for bug localization in non-stationary settings, and we hope it helps to improve bug localization activities in Software Engineering using CL techniques.",http://arxiv.org/abs/2412.11289v1,CS,Quantitative
Stochastic Weight Sharing for Bayesian Neural Networks,"While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.",http://arxiv.org/abs/2505.17856v1,CS,Quantitative
A Distributed Partitioning Software and its Applications,"This article describes a geometric partitioning software that can be used for quick computation of data partitions on many-core HPC machines. It is most suited for dynamic applications with load distributions that vary with time. Partitioning costs were minimized with a lot of care, to tolerate frequent adjustments to the load distribution. The partitioning algorithm uses both geometry as well as statistics collected from the data distribution. The implementation is based on a hybrid programming model that is both distributed and multi-threaded. Partitions are computed by a hierarchical data decomposition, followed by data ordering using space-filling curves and greedy knapsack. This software was primarily used for partitioning 2 and 3 dimensional meshes in scientific computing. It was also used to solve point-location problems and for partitioning general graphs. The experiments described in this paper provide useful performance data for important parallel algorithms on a HPC machine built using a recent many-core processor designed for data-intensive applications by providing large on-chip memory.",http://arxiv.org/abs/2503.02185v1,CS,Quantitative
Predicting At-Risk Programming Students in Small Imbalanced Datasets using Synthetic Data,"This study is part of a larger project focused on measuring, understanding, and improving student engagement in programming education. We investigate whether synthetic data generation can help identify at-risk students earlier in a small, imbalanced dataset from an introductory programming module. The analysis used anonymised records from 379 students, with 15\% marked as failing, and applied several machine learning algorithms. The first experiments showed poor recall for the failing group. However, using synthetic data generation methods led to a significant improvement in performance. Our results suggest that machine learning can help identify at-risk students early in programming courses when combined with synthetic data. This research lays the groundwork for validating and using these models with live student cohorts in the future, to allow for timely and effective interventions that can improve student outcomes. It also includes feature importance analysis to refine formative tasks. Overall, this study contributes to developing practical workflows that help detect disengagement early and improve student success in programming education.",http://arxiv.org/abs/2505.17128v1,CS,Quantitative
Scaled Block Vecchia Approximation for High-Dimensional Gaussian Process Emulation on GPUs,"Emulating computationally intensive scientific simulations is essential to enable uncertainty quantification, optimization, and decision-making at scale. Gaussian Processes (GPs) offer a flexible and data-efficient foundation for statistical emulation, but their poor scalability limits applicability to large datasets. We introduce the Scaled Block Vecchia (SBV) algorithm for distributed GPU-based systems. SBV integrates the Scaled Vecchia approach for anisotropic input scaling with the Block Vecchia (BV) method to reduce computational and memory complexity while leveraging GPU acceleration techniques for efficient linear algebra operations. To the best of our knowledge, this is the first distributed implementation of any Vecchia-based GP variant. Our implementation employs MPI for inter-node parallelism and the MAGMA library for GPU-accelerated batched matrix computations. We demonstrate the scalability and efficiency of the proposed algorithm through experiments on synthetic and real-world workloads, including a 50M point simulation from a respiratory disease model. SBV achieves near-linear scalability on up to 64 A100 and GH200 GPUs, handles 320M points, and reduces energy use relative to exact GP solvers, establishing SBV as a scalable and energy-efficient framework for emulating large-scale scientific models on GPU-based distributed systems.",http://arxiv.org/abs/2504.12004v1,CS,Quantitative
Co-Change Graph Entropy: A New Process Metric for Defect Prediction,"Process metrics, valued for their language independence and ease of collection, have been shown to outperform product metrics in defect prediction. Among these, change entropy (Hassan, 2009) is widely used at the file level and has proven highly effective. Additionally, past research suggests that co-change patterns provide valuable insights into software quality. Building on these findings, we introduce Co-Change Graph Entropy, a novel metric that models co-changes as a graph to quantify co-change scattering. Experiments on eight Apache projects reveal a significant correlation between co-change entropy and defect counts at the file level, with a Pearson correlation coefficient of up to 0.54. In filelevel defect classification, replacing change entropy with co-change entropy improves AUROC in 72.5% of cases and MCC in 62.5% across 40 experimental settings (five machine learning classifiers and eight projects), though these improvements are not statistically significant. However, when co-change entropy is combined with change entropy, AUROC improves in 82.5% of cases and MCC in 65%, with statistically significant gains confirmed via the Friedman test followed by the post-hoc Nemenyi test. These results indicate that co-change entropy complements change entropy, significantly enhancing defect classification performance and underscoring its practical importance in defect prediction.",http://arxiv.org/abs/2504.18511v1,CS,Quantitative
Experimental robustness benchmark of quantum neural network on a superconducting quantum processor,"Quantum machine learning (QML) models, like their classical counterparts, are vulnerable to adversarial attacks, hindering their secure deployment. Here, we report the first systematic experimental robustness benchmark for 20-qubit quantum neural network (QNN) classifiers executed on a superconducting processor. Our benchmarking framework features an efficient adversarial attack algorithm designed for QNNs, enabling quantitative characterization of adversarial robustness and robustness bounds. From our analysis, we verify that adversarial training reduces sensitivity to targeted perturbations by regularizing input gradients, significantly enhancing QNN's robustness. Additionally, our analysis reveals that QNNs exhibit superior adversarial robustness compared to classical neural networks, an advantage attributed to inherent quantum noise. Furthermore, the empirical upper bound extracted from our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the theoretical lower bound, providing strong experimental confirmation of the attack's effectiveness and the tightness of fidelity-based robustness bounds. This work establishes a critical experimental framework for assessing and improving quantum adversarial robustness, paving the way for secure and reliable QML applications.",http://arxiv.org/abs/2505.16714v1,CS,Quantitative
Advances in Artificial Intelligence forDiabetes Prediction: Insights from a Systematic Literature Review,"This systematic review explores the use of machine learning (ML) in predicting diabetes, focusing on datasets, algorithms, training methods, and evaluation metrics. It examines datasets like the Singapore National Diabetic Retinopathy Screening program, REPLACE-BG, National Health and Nutrition Examination Survey, and Pima Indians Diabetes Database. The review assesses the performance of ML algorithms like CNN, SVM, Logistic Regression, and XGBoost in predicting diabetes outcomes. The study emphasizes the importance of interdisciplinary collaboration and ethical considerations in ML-based diabetes prediction models.",http://arxiv.org/abs/2412.14736v1,CS,Quantitative
GENCNIPPET: Automated Generation of Code Snippets for Supporting Programming Questions,"Context: Software developers often ask questions on Technical Q&A forums like Stack Overflow (SO) to seek solutions to their programming-related problems (e.g., errors and unexpected behavior of code). Problem: Many questions miss required code snippets due to the lack of readily available code, time constraints, employer restrictions, confidentiality concerns, or uncertainty about what code to share. Unfortunately, missing but required code snippets prevent questions from getting prompt and appropriate solutions. Objective: We plan to introduce GENCNIPPET, a tool designed to integrate with SO's question submission system. GENCNIPPET will generate relevant code examples (when required) to support questions for their timely solutions. Methodology: We first downloaded the SO April 2024 data dump, which contains 1.94 million questions related to Python that have code snippets and 1.43 million questions related to Java. Then, we filter these questions to identify those that genuinely require code snippets using a state-of-the-art machine learning model. Next, we select questions with positive scores to ensure high-quality data. Our plan is to fine-tune Llama-3 models (e.g., Llama-3-8B), using 80% of the selected questions for training and 10% for validation. The primary reasons for choosing Llama models are their open-source accessibility and robust fine-tuning capabilities, which are essential for deploying a freely accessible tool. GENCNIPPET will be integrated with the SO question submission system as a browser plugin. It will communicate with the fine-tuned model to generate code snippets tailored to the target questions. The effectiveness of the generated code examples will be assessed using automatic evaluation against ground truth, user perspectives, and live (wild) testing in real-world scenarios.",http://arxiv.org/abs/2504.16292v1,CS,Quantitative
MedGNN: Capturing the Links Between Urban Characteristics and Medical Prescriptions,"Understanding how urban socio-demographic and environmental factors relate with health is essential for public health and urban planning. However, traditional statistical methods struggle with nonlinear effects, while machine learning models often fail to capture geographical (nearby areas being more similar) and topological (unequal connectivity between places) effects in an interpretable way. To address this, we propose MedGNN, a spatio-topologically explicit framework that constructs a 2-hop spatial graph, integrating positional and locational node embeddings with urban characteristics in a graph neural network. Applied to MEDSAT, a comprehensive dataset covering over 150 environmental and socio-demographic factors and six prescription outcomes (depression, anxiety, diabetes, hypertension, asthma, and opioids) across 4,835 Greater London neighborhoods, MedGNN improved predictions by over 25% on average compared to baseline methods. Using depression prescriptions as a case study, we analyzed graph embeddings via geographical principal component analysis, identifying findings that: align with prior research (e.g., higher antidepressant prescriptions among older and White populations), contribute to ongoing debates (e.g., greenery linked to higher and NO2 to lower prescriptions), and warrant further study (e.g., canopy evaporation correlated with fewer prescriptions). These results demonstrate MedGNN's potential, and more broadly, of carefully applied machine learning, to advance transdisciplinary public health research.",http://arxiv.org/abs/2504.04739v1,CS,Mixed
Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community,"Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.",http://arxiv.org/abs/2503.22066v3,CS,Quantitative
Aerial Active STAR-RIS-assisted Satellite-Terrestrial Covert Communications,"An integration of satellites and terrestrial networks is crucial for enhancing performance of next generation communication systems. However, the networks are hindered by the long-distance path loss and security risks in dense urban environments. In this work, we propose a satellite-terrestrial covert communication system assisted by the aerial active simultaneous transmitting and reflecting reconfigurable intelligent surface (AASTAR-RIS) to improve the channel capacity while ensuring the transmission covertness. Specifically, we first derive the minimal detection error probability (DEP) under the worst condition that the Warden has perfect channel state information (CSI). Then, we formulate an AASTAR-RIS-assisted satellite-terrestrial covert communication optimization problem (ASCCOP) to maximize the sum of the fair channel capacity for all ground users while meeting the strict covert constraint, by jointly optimizing the trajectory and active beamforming of the AASTAR-RIS. Due to the challenges posed by the complex and high-dimensional state-action spaces as well as the need for efficient exploration in dynamic environments, we propose a generative deterministic policy gradient (GDPG) algorithm, which is a generative deep reinforcement learning (DRL) method to solve the ASCCOP. Concretely, the generative diffusion model (GDM) is utilized as the policy representation of the algorithm to enhance the exploration process by generating diverse and high-quality samples through a series of denoising steps. Moreover, we incorporate an action gradient mechanism to accomplish the policy improvement of the algorithm, which refines the better state-action pairs through the gradient ascent. Simulation results demonstrate that the proposed approach significantly outperforms important benchmarks.",http://arxiv.org/abs/2504.16146v1,CS,Quantitative
Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing,"We introduce a distributed quantum-classical framework that synergizes photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping to achieve parameter-efficient training of classical neural networks. By leveraging universal linear-optical decompositions of $M$-mode interferometers and photon-counting measurement statistics, our architecture generates neural parameters through a hybrid quantum-classical workflow: photonic QNNs with $M(M+1)/2$ trainable parameters produce high-dimensional probability distributions that are mapped to classical network weights via an MPS model with bond dimension $\chi$. Empirical validation on MNIST classification demonstrates that photonic QT achieves an accuracy of $95.50\% \pm 0.84\%$ using 3,292 parameters ($\chi = 10$), compared to $96.89\% \pm 0.31\%$ for classical baselines with 6,690 parameters. Moreover, a ten-fold compression ratio is achieved at $\chi = 4$, with a relative accuracy loss of less than $3\%$. The framework outperforms classical compression techniques (weight sharing/pruning) by 6--12\% absolute accuracy while eliminating quantum hardware requirements during inference through classical deployment of compressed parameters. Simulations incorporating realistic photonic noise demonstrate the framework's robustness to near-term hardware imperfections. Ablation studies confirm quantum necessity: replacing photonic QNNs with random inputs collapses accuracy to chance level ($10.0\% \pm 0.5\%$). Photonic quantum computing's room-temperature operation, inherent scalability through spatial-mode multiplexing, and HPC-integrated architecture establish a practical pathway for distributed quantum machine learning, combining the expressivity of photonic Hilbert spaces with the deployability of classical neural networks.",http://arxiv.org/abs/2505.08474v1,CS,Quantitative
Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey,"Diffusion models have emerged as leading generative models for images and other modalities, but aligning their outputs with human preferences and safety constraints remains a critical challenge. This thesis proposal investigates methods to align diffusion models using reinforcement learning (RL) and reward modeling. We survey recent advances in fine-tuning text-to-image diffusion models with human feedback, including reinforcement learning from human and AI feedback, direct preference optimization, and differentiable reward approaches. We classify these methods based on the type of feedback (human, automated, binary or ranked preferences), the fine-tuning technique (policy gradient, reward-weighted likelihood, direct backpropagation, etc.), and their efficiency and safety outcomes. We compare key algorithms and frameworks, highlighting how they improve alignment with user intent or safety standards, and discuss inter-relationships such as how newer methods build on or diverge from earlier ones. Based on the survey, we identify five promising research directions for the next two years: (1) multi-objective alignment with combined rewards, (2) efficient human feedback usage and active learning, (3) robust safety alignment against adversarial inputs, (4) continual and online alignment of diffusion models, and (5) interpretable and trustworthy reward modeling for generative images. Each direction is elaborated with its problem statement, challenges, related work, and a proposed research plan. The proposal is organized as a comprehensive document with literature review, comparative tables of methods, and detailed research plans, aiming to contribute new insights and techniques for safer and value-aligned diffusion-based generative AI.",http://arxiv.org/abs/2505.17352v1,CS,Quantitative
"Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements","Graphs are essential data structures for modeling complex interactions in domains such as social networks, molecular structures, and biological systems. Graph-level tasks, which predict properties or classes for the entire graph, are critical for applications, such as molecular property prediction and subgraph counting. Graph Neural Networks (GNNs) have shown promise in these tasks, but their evaluations are often limited to narrow datasets, tasks, and inconsistent experimental setups, restricting their generalizability. To address these limitations, we propose a unified evaluation framework for graph-level GNNs. This framework provides a standardized setting to evaluate GNNs across diverse datasets, various graph tasks (e.g., graph classification and regression), and challenging scenarios, including noisy, imbalanced, and few-shot graphs. Additionally, we propose a novel GNN model with enhanced expressivity and generalization capabilities. Specifically, we enhance the expressivity of GNNs through a $k$-path rooted subgraph approach, enabling the model to effectively count subgraphs (e.g., paths and cycles). Moreover, we introduce a unified graph contrastive learning algorithm for graphs across diverse domains, which adaptively removes unimportant edges to augment graphs, thereby significantly improving generalization performance. Extensive experiments demonstrate that our model achieves superior performance against fourteen effective baselines across twenty-seven graph datasets, establishing it as a robust and generalizable model for graph-level tasks.",http://arxiv.org/abs/2501.00773v1,CS,Quantitative
Reliable Vertical Ground Reaction Force Estimation with Smart Insole During Walking,"The vertical ground reaction force (vGRF) and its characteristic weight acceptance and push-off peaks measured during walking are important for gait and biomechanical analysis. Current wearable vGRF estimation methods suffer from drifting errors or low generalization performances, limiting their practical application. This paper proposes a novel method for reliably estimating vGRF and its characteristic peaks using data collected from the smart insole, including inertial measurement unit data and the newly introduced center of the pressed sensor data. These data were fused with machine learning algorithms including artificial neural networks, random forest regression, and bi-directional long-short-term memory. The proposed method outperformed the state-of-the-art methods with the root mean squared error, normalized root mean squared error, and correlation coefficient of 0.024 body weight (BW), 1.79% BW, and 0.997 in intra-participant testing, and 0.044 BW, 3.22% BW, and 0.991 in inter-participant testing, respectively. The difference between the reference and estimated weight acceptance and push-off peak values are 0.022 BW and 0.017 BW with a delay of 1.4% and 1.8% of the gait cycle for the intra-participant testing and 0.044 BW and 0.025 BW with a delay of 1.5% and 2.3% of the gait cycle for the inter-participant testing. The results indicate that the proposed vGRF estimation method has the potential to achieve accurate vGRF measurement during walking in free living environments.",http://arxiv.org/abs/2501.07748v1,CS,Quantitative
A deep solver for backward stochastic Volterra integral equations,"We present the first deep-learning solver for backward stochastic Volterra integral equations (BSVIEs) and their fully-coupled forward-backward variants. The method trains a neural network to approximate the two solution fields in a single stage, avoiding the use of nested time-stepping cycles that limit classical algorithms. For the decoupled case we prove a non-asymptotic error bound composed of an a posteriori residual plus the familiar square root dependence on the time step. Numerical experiments confirm this rate and reveal two key properties: \emph{scalability}, in the sense that accuracy remains stable from low dimension up to 500 spatial variables while GPU batching keeps wall-clock time nearly constant; and \emph{generality}, since the same method handles coupled systems whose forward dynamics depend on the backward solution. These results open practical access to a family of high-dimensional, path-dependent problems in stochastic control and quantitative finance.",http://arxiv.org/abs/2505.18297v1,CS,Quantitative
Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies,"Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid-region information into an augmented state space and cost function, which prevents flexible, dynamic specification of novel avoid-region information at evaluation time. They also rely heavily on well-designed reward and cost functions, limiting scalability to complex or poorly structured environments. We introduce RADT, a decision transformer model for offline, reward-free, goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid regions directly as prompt tokens, allowing any number of avoid regions of arbitrary size to be specified at evaluation time. Using only suboptimal offline trajectories from a random policy, RADT learns reach-avoid behavior through a novel combination of goal and avoid-region hindsight relabeling. We benchmark RADT against 3 existing offline goal-conditioned RL models across 11 tasks, environments, and experimental settings. RADT generalizes in a zero-shot manner to out-of-distribution avoid region sizes and counts, outperforming baselines that require retraining. In one such zero-shot setting, RADT achieves 35.7% improvement in normalized cost over the best retrained baseline while maintaining high goal-reaching success. We apply RADT to cell reprogramming in biology, where it reduces visits to undesirable intermediate gene expression states during trajectories to desired target states, despite stochastic transitions and discrete, structured state dynamics.",http://arxiv.org/abs/2505.19337v2,CS,Quantitative
"Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions","Language models (LMs) are machine learning models designed to predict linguistic patterns by estimating the probability of word sequences based on large-scale datasets, such as text. LMs have a wide range of applications in natural language processing (NLP) tasks, including autocomplete and machine translation. Although larger datasets typically enhance LM performance, scalability remains a challenge due to constraints in computational power and resources. Distributed computing strategies offer essential solutions for improving scalability and managing the growing computational demand. Further, the use of sensitive datasets in training and deployment raises significant privacy concerns. Recent research has focused on developing decentralized techniques to enable distributed training and inference while utilizing diverse computational resources and enabling edge AI. This paper presents a survey on distributed solutions for various LMs, including large language models (LLMs), vision language models (VLMs), multimodal LLMs (MLLMs), and small language models (SLMs). While LLMs focus on processing and generating text, MLLMs are designed to handle multiple modalities of data (e.g., text, images, and audio) and to integrate them for broader applications. To this end, this paper reviews key advancements across the MLLM pipeline, including distributed training, inference, fine-tuning, and deployment, while also identifying the contributions, limitations, and future areas of improvement. Further, it categorizes the literature based on six primary focus areas of decentralization. Our analysis describes gaps in current methodologies for enabling distributed solutions for LMs and outline future research directions, emphasizing the need for novel solutions to enhance the robustness and applicability of distributed LMs.",http://arxiv.org/abs/2503.16585v1,CS,Quantitative
MFC 5.0: An exascale many-physics flow solver,"Many problems of interest in engineering, medicine, and the fundamental sciences rely on high-fidelity flow simulation, making performant computational fluid dynamics solvers a mainstay of the open-source software community. A previous work (Bryngelson et al., Comp. Phys. Comm. (2021)) published MFC 3.0 with numerous physical features, numerics, and scalability. MFC 5.0 is a marked update to MFC 3.0, including a broad set of well-established and novel physical models and numerical methods, and the introduction of XPU acceleration. We exhibit state-of-the-art performance and ideal scaling on the first two exascale supercomputers, OLCF Frontier and LLNL El Capitan. Combined with MFC's single-accelerator performance, MFC achieves exascale computation in practice. New physical features include the immersed boundary method, N-fluid phase change, Euler--Euler and Euler--Lagrange sub-grid bubble models, fluid-structure interaction, hypo- and hyper-elastic materials, chemically reacting flow, two-material surface tension, magnetohydrodynamics (MHD), and more. Numerical techniques now represent the current state-of-the-art, including general relaxation characteristic boundary conditions, WENO variants, Strang splitting for stiff sub-grid flow features, and low Mach number treatments. Weak scaling to tens of thousands of GPUs on OLCF Summit and Frontier and LLNL El Capitan sees efficiencies within 5% of ideal to their full system sizes. Strong scaling results for a 16-times increase in device count show parallel efficiencies over 90% on OLCF Frontier. MFC's software stack has improved, including continuous integration, ensuring code resilience and correctness through over 300 regression tests; metaprogramming, reducing code length and maintaining performance portability; and code generation for computing chemical reactions.",http://arxiv.org/abs/2503.07953v3,CS,Quantitative
Offline and Distributional Reinforcement Learning for Wireless Communications,"The rapid growth of heterogeneous and massive wireless connectivity in 6G networks demands intelligent solutions to ensure scalability, reliability, privacy, ultra-low latency, and effective control. Although artificial intelligence (AI) and machine learning (ML) have demonstrated their potential in this domain, traditional online reinforcement learning (RL) and deep RL methods face limitations in real-time wireless networks. For instance, these methods rely on online interaction with the environment, which might be unfeasible, costly, or unsafe. In addition, they cannot handle the inherent uncertainties in real-time wireless applications. We focus on offline and distributional RL, two advanced RL techniques that can overcome these challenges by training on static datasets and accounting for network uncertainties. We introduce a novel framework that combines offline and distributional RL for wireless communication applications. Through case studies on unmanned aerial vehicle (UAV) trajectory optimization and radio resource management (RRM), we demonstrate that our proposed Conservative Quantile Regression (CQR) algorithm outperforms conventional RL approaches regarding convergence speed and risk management. Finally, we discuss open challenges and potential future directions for applying these techniques in 6G networks, paving the way for safer and more efficient real-time wireless systems.",http://arxiv.org/abs/2504.03804v1,CS,Quantitative
"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases","In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. However, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use. In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering. It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics. The average first-hit rate exceeds that of the second-best method by 25.7%. FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers. For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation. However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning. The source code is publicly available at https://github.com/kramerlab/FocusedRetriever .",http://arxiv.org/abs/2505.09246v1,CS,Quantitative
Deep-Bench: Deep Learning Benchmark Dataset for Code Generation,"Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types. To address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text. GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code. Furthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.",http://arxiv.org/abs/2502.18726v1,CS,Quantitative
MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning,"Molecular editing aims to modify a given molecule to optimize desired chemical properties while preserving structural similarity. However, current approaches typically rely on string-based or continuous representations, which fail to adequately capture the discrete, graph-structured nature of molecules, resulting in limited structural fidelity and poor controllability. In this paper, we propose MolEditRL, a molecular editing framework that explicitly integrates structural constraints with precise property optimization. Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion model pretrained to reconstruct target molecules conditioned on source structures and natural language instructions; (2) an editing-aware reinforcement learning fine-tuning stage that further enhances property alignment and structural preservation by explicitly optimizing editing decisions under graph constraints. For comprehensive evaluation, we construct MolEdit-Instruct, the largest and most property-rich molecular editing dataset, comprising 3 million diverse examples spanning single- and multi-property tasks across 10 chemical attributes. Experimental results demonstrate that MolEditRL significantly outperforms state-of-the-art methods in both property optimization accuracy and structural fidelity, achieving a 74\% improvement in editing success rate while using 98\% fewer parameters.",http://arxiv.org/abs/2505.20131v1,CS,Quantitative
The Evolving Role of Programming and LLMs in the Development of Self-Driving Laboratories,"Machine learning and automation are transforming scientific research, yet the implementation of self-driving laboratories (SDLs) remains costly and complex, and it remains difficult to learn how to use these facilities. To address this, we introduce Claude-Light, a lightweight, remotely accessible instrument designed for prototyping automation algorithms and machine learning workflows. Claude-Light integrates a REST API, a Raspberry Pi-based control system, and an RGB LED with a photometer that measures ten spectral outputs, providing a controlled but realistic experimental environment. This device enables users to explore automation at multiple levels, from basic programming and experimental design to machine learning-driven optimization. We demonstrate the application of Claude-Light in structured automation approaches, including traditional scripting, statistical design of experiments, and active learning methods. Additionally, we explore the role of large language models (LLMs) in laboratory automation, highlighting their use in instrument selection, structured data extraction, function calling, and code generation. While LLMs present new opportunities for streamlining automation, they also introduce challenges related to reproducibility, security, and reliability. We discuss strategies to mitigate these risks while leveraging LLMs for enhanced efficiency in self-driving laboratories. Claude-Light provides a practical and accessible platform for students and researchers to develop automation skills and test algorithms before deploying them in larger-scale SDLs. By lowering the barrier to entry for automation in scientific research, this tool facilitates broader adoption of AI-driven experimentation and fosters innovation in autonomous laboratories.",http://arxiv.org/abs/2504.13870v1,CS,Quantitative
Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda,"Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth"" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key moments during model development that could lead to different future outcomes, uncertainty and harmful effects as these tools are deployed by social media platforms. We conclude by offering a tentative path toward reflexive and responsible development of ML tools for moderating misinformation and other harmful content online.",http://arxiv.org/abs/2210.09014v2,CS,Qualitative
ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows,"Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.",http://arxiv.org/abs/2505.19897v1,CS,Quantitative
A Methodological Framework for Socio-Cognitive Analyses of Collaborative Design of Open Source Software,"Open Source Software (OSS) development challenges traditional software engineering practices. In particular, OSS projects are managed by a large number of volunteers, working freely on the tasks they choose to undertake. OSS projects also rarely rely on explicit system-level design, or on project plans or schedules. Moreover, OSS developers work in arbitrary locations and collaborate almost exclusively over the Internet, using simple tools such as email and software code tracking databases (e.g. CVS). All the characteristics above make OSS development akin to weaving a tapestry of heterogeneous components. The OSS design process relies on various types of actors: people with prescribed roles, but also elements coming from a variety of information spaces (such as email and software code). The objective of our research is to understand the specific hybrid weaving accomplished by the actors of this distributed, collective design process. This, in turn, challenges traditional methodologies used to understand distributed software engineering: OSS development is simply too ""fibrous"" to lend itself well to analysis under a single methodological lens. In this paper, we describe the methodological framework we articulated to analyze collaborative design in the Open Source world. Our framework focuses on the links between the heterogeneous components of a project's hybrid network. We combine ethnography, text mining, and socio-technical network analysis and visualization to understand OSS development in its totality. This way, we are able to simultaneously consider the social, technical, and cognitive aspects of OSS development. We describe our methodology in detail, and discuss its implications for future research on distributed collective practices.",http://arxiv.org/abs/cs/0703009v1,CS,Qualitative
Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data,"Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: https://github.com/Soltanilara/SurveyAutomation.",http://arxiv.org/abs/2505.05752v1,CS,Quantitative
Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding,"Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.",http://arxiv.org/abs/2505.17939v2,CS,Mixed
A Formal Verification Approach to Safeguard Controller Variables from Single Event Upset,"We present a method based on program analysis and formal verification to identify conditionally relevant variables (CRVs) - variables which could lead to violation of safety properties in control software when affected by single event upsets (SEUs). Traditional static analysis can distinguish between relevant and irrelevant variables. However, it would fail to take into account the conditions specific to the control software in question. This can lead to false positives. Our algorithm employs formal verification to avoid false positives. We have conducted experiments that demonstrate that CRVs indeed are fewer in number than what traditional static analysis can detect and that our algorithm is able to identify this fact. The information provided by our algorithm could prove helpful to a compiler while it does register allocation during the compilation of the control software. In turn, this could cause significant reduction in the cost of controller chips.",http://arxiv.org/abs/2505.06648v1,CS,Quantitative
Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey,"Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies.",http://arxiv.org/abs/2504.19660v1,CS,Quantitative
Protocol as Poetry: Case Study on Pak's Protocol Arts,"Protocol art emerges at the confluence of blockchain-based smart contracts and a century-long lineage of conceptual art, participatory art, and algorithmic generative art practices. Yet existing definitions-most notably Primavera De Filippi's ""protocolism""-struggle to demarcate this nascent genre from other art forms in practice. Addressing this definition-to-practice gap, this paper offers a focused case study of pioneering protocol artworks by Pak, an early and influential pseudonymous protocol artist who treats smart contracts as medium and protocol participation as message. Tracing the evolution from early open-edition releases of The Fungible and the dynamic mechanics of Merge to the soul-bound messaging of Censored and the reflective absence of Not Found, we examine how Pak choreographs distributed agency across collectors and autonomous contracts, showing how programmable protocols become a social fabric in artistic meaning-making. Through thematic analysis of Pak's works, we identify seven core characteristics that distinguish protocol art: (1) system-centric rather than object-centric composition, (2) autonomous governance for open-ended control, (3) distributed agency and communal authorship, (4) temporal dynamism and lifecycle aesthetics, (5) economic-driven engagement, (6) poetic message embedding in interaction rituals, and (7) interoperability enabling composability for emergence. We then discuss how these features set protocol art apart from adjacent artistic movements. By developing a theoretical framework grounded in Pak's practice, we contribute to the emerging literature on protocolism while offering design implications for artists shaping this evolving art form.",http://arxiv.org/abs/2505.12393v1,CS,Qualitative
Uncertainty Quantification in SVM prediction,"This paper explores Uncertainty Quantification (UQ) in SVM predictions, particularly for regression and forecasting tasks. Unlike the Neural Network, the SVM solutions are typically more stable, sparse, optimal and interpretable. However, there are only few literature which addresses the UQ in SVM prediction. At first, we provide a comprehensive summary of existing Prediction Interval (PI) estimation and probabilistic forecasting methods developed in the SVM framework and evaluate them against the key properties expected from an ideal PI model. We find that none of the existing SVM PI models achieves a sparse solution. To introduce sparsity in SVM model, we propose the Sparse Support Vector Quantile Regression (SSVQR) model, which constructs PIs and probabilistic forecasts by solving a pair of linear programs. Further, we develop a feature selection algorithm for PI estimation using SSVQR that effectively eliminates a significant number of features while improving PI quality in case of high-dimensional dataset. Finally we extend the SVM models in Conformal Regression setting for obtaining more stable prediction set with finite test set guarantees. Extensive experiments on artificial, real-world benchmark datasets compare the different characteristics of both existing and proposed SVM-based PI estimation methods and also highlight the advantages of the feature selection in PI estimation. Furthermore, we compare both, the existing and proposed SVM-based PI estimation models, with modern deep learning models for probabilistic forecasting tasks on benchmark datasets. Furthermore, SVM models show comparable or superior performance to modern complex deep learning models for probabilistic forecasting task in our experiments.",http://arxiv.org/abs/2505.15429v1,CS,Quantitative
A Linear Approach to Data Poisoning,"We investigate the theoretical foundations of data poisoning attacks in machine learning models. Our analysis reveals that the Hessian with respect to the input serves as a diagnostic tool for detecting poisoning, exhibiting spectral signatures that characterize compromised datasets. We use random matrix theory (RMT) to develop a theory for the impact of poisoning proportion and regularisation on attack efficacy in linear regression. Through QR stepwise regression, we study the spectral signatures of the Hessian in multi-output regression. We perform experiments on deep networks to show experimentally that this theory extends to modern convolutional and transformer networks under the cross-entropy loss. Based on these insights we develop preliminary algorithms to determine if a network has been poisoned and remedies which do not require further training.",http://arxiv.org/abs/2505.15175v2,CS,Quantitative
DPS: Design Pattern Summarisation Using Code Features,"Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries.",http://arxiv.org/abs/2504.11081v1,CS,Quantitative
The Impact of Cut Layer Selection in Split Federated Learning,"Split Federated Learning (SFL) is a distributed machine learning paradigm that combines federated learning and split learning. In SFL, a neural network is partitioned at a cut layer, with the initial layers deployed on clients and remaining layers on a training server. There are two main variants of SFL: SFL-V1 where the training server maintains separate server-side models for each client, and SFL-V2 where the training server maintains a single shared model for all clients. While existing studies have focused on algorithm development for SFL, a comprehensive quantitative analysis of how the cut layer selection affects model performance remains unexplored. This paper addresses this gap by providing numerical and theoretical analysis of SFL performance and convergence relative to cut layer selection. We find that SFL-V1 is relatively invariant to the choice of cut layer, which is consistent with our theoretical results. Numerical experiments on four datasets and two neural networks show that the cut layer selection significantly affects the performance of SFL-V2. Moreover, SFL-V2 with an appropriate cut layer selection outperforms FedAvg on heterogeneous data.",http://arxiv.org/abs/2412.15536v1,CS,Quantitative
The Race to Efficiency: A New Perspective on AI Scaling Laws,"As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the ""efficiency-doubling rate"" parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.",http://arxiv.org/abs/2501.02156v3,CS,Quantitative
Self-Supervised Learning for Image Segmentation: A Comprehensive Survey,"Supervised learning demands large amounts of precisely annotated data to achieve promising results. Such data curation is labor-intensive and imposes significant overhead regarding time and costs. Self-supervised learning (SSL) partially overcomes these limitations by exploiting vast amounts of unlabeled data and creating surrogate (pretext or proxy) tasks to learn useful representations without manual labeling. As a result, SSL has become a powerful machine learning (ML) paradigm for solving several practical downstream computer vision problems, such as classification, detection, and segmentation. Image segmentation is the cornerstone of many high-level visual perception applications, including medical imaging, intelligent transportation, agriculture, and surveillance. Although there is substantial research potential for developing advanced algorithms for SSL-based semantic segmentation, a comprehensive study of existing methodologies is essential to trace advances and guide emerging researchers. This survey thoroughly investigates over 150 recent image segmentation articles, particularly focusing on SSL. It provides a practical categorization of pretext tasks, downstream tasks, and commonly used benchmark datasets for image segmentation research. It concludes with key observations distilled from a large body of literature and offers future directions to make this research field more accessible and comprehensible for readers.",http://arxiv.org/abs/2505.13584v1,CS,Mixed
Matrix Product Sketching via Coordinated Sampling,"We revisit the well-studied problem of approximating a matrix product, $\mathbf{A}^T\mathbf{B}$, based on small space sketches $\mathcal{S}(\mathbf{A})$ and $\mathcal{S}(\mathbf{B})$ of $\mathbf{A} \in \R^{n \times d}$ and $\mathbf{B}\in \R^{n \times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\mathbf{A}$ and $\mathbf{B}$ are sparse, methods based on \emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\epsilon\|\mathbf{A}\|_F\|\mathbf{B}\|_F$, coordinated sampling requires sketches of size $O(s/\epsilon^2)$ when $\mathbf{A}$ and $\mathbf{B}$ have at most $s \leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\epsilon^2)$ and $O(m/\epsilon^2)$ for $\mathbf{A}$ and $\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching.",http://arxiv.org/abs/2501.17836v1,CS,Quantitative
Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning,"We introduce a new algebraic structure for multi-dimensional compositional embeddings, built on directional non-commutative monoidal operators. The core contribution of this work is this novel framework, which exhibits appealing theoretical properties (associativity along each dimension and an interchange law ensuring global consistency) while remaining compatible with modern machine learning architectures. Our construction defines a distinct composition operator circ_i for each axis i, ensuring associative combination along each axis without imposing global commutativity. Importantly, all axis-specific operators commute with one another, enforcing a global interchange law that enables consistent crossaxis compositions. This is, to our knowledge, the first approach that provides a common foundation that generalizes classical sequence-modeling paradigms (e.g., structured state-space models (SSMs) and transformer self-attention) to a unified multi-dimensional framework. For example, specific one-dimensional instances of our framework can recover the familiar affine transformation algebra, vanilla self-attention, and the SSM-style recurrence. The higher-dimensional generalizations naturally support recursive, structure-aware operations in embedding spaces. We outline several potential applications unlocked by this structure-including structured positional encodings in Transformers, directional image embeddings, and symbolic modeling of sequences or grids-indicating that it could inform future deep learning model designs. We formally establish the algebraic properties of our framework and discuss efficient implementations. Finally, as our focus is theoretical, we include no experiments here and defer empirical validation to future work, which we plan to undertake.",http://arxiv.org/abs/2505.15507v1,CS,Quantitative
Neural Network Graph Similarity Computation Based on Graph Fusion,"Graph similarity learning, crucial for tasks such as graph classification and similarity search, focuses on measuring the similarity between two graph-structured entities. The core challenge in this field is effectively managing the interactions between graphs. Traditional methods often entail separate, redundant computations for each graph pair, leading to unnecessary complexity. This paper revolutionizes the approach by introducing a parallel graph interaction method called graph fusion. By merging the node sequences of graph pairs into a single large graph, our method leverages a global attention mechanism to facilitate interaction computations and to harvest cross-graph insights. We further assess the similarity between graph pairs at two distinct levels-graph-level and node-level-introducing two innovative, yet straightforward, similarity computation algorithms. Extensive testing across five public datasets shows that our model not only outperforms leading baseline models in graph-to-graph classification and regression tasks but also sets a new benchmark for performance and efficiency. The code for this paper is open-source and available at https://github.com/LLiRarry/GFM-code.git",http://arxiv.org/abs/2502.18291v1,CS,Quantitative
Data Balancing Strategies: A Survey of Resampling and Augmentation Methods,"Imbalanced data poses a significant obstacle in machine learning, as an unequal distribution of class labels often results in skewed predictions and diminished model accuracy. To mitigate this problem, various resampling strategies have been developed, encompassing both oversampling and undersampling techniques aimed at modifying class proportions. Conventional oversampling approaches like SMOTE enhance the representation of the minority class, whereas undersampling methods focus on trimming down the majority class. Advances in deep learning have facilitated the creation of more complex solutions, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which are capable of producing high-quality synthetic examples. This paper reviews a broad spectrum of data balancing methods, classifying them into categories including synthetic oversampling, adaptive techniques, generative models, ensemble-based strategies, hybrid approaches, undersampling, and neighbor-based methods. Furthermore, it highlights current developments in resampling techniques and discusses practical implementations and case studies that validate their effectiveness. The paper concludes by offering perspectives on potential directions for future exploration in this domain.",http://arxiv.org/abs/2505.13518v1,CS,Mixed
Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks,"Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.",http://arxiv.org/abs/2505.11665v1,CS,Quantitative
RADICE: Causal Graph Based Root Cause Analysis for System Performance Diagnostic,"Root cause analysis is one of the most crucial operations in software reliability regarding system performance diagnostic. It aims to identify the root causes of system performance anomalies, allowing the resolution or the future prevention of issues that can cause millions of dollars in losses. Common existing approaches relying on data correlation or full domain expert knowledge are inaccurate or infeasible in most industrial cases, since correlation does not imply causation, and domain experts may not have full knowledge of complex and real-time systems. In this work, we define a novel causal domain knowledge model representing causal relations about the underlying system components to allow domain experts to contribute partial domain knowledge for root cause analysis. We then introduce RADICE, an algorithm that through the causal graph discovery, enhancement, refinement, and subtraction processes is able to output a root cause causal sub-graph showing the causal relations between the system components affected by the anomaly. We evaluated RADICE with simulated data and reported a real data use case, sharing the lessons we learned. The experiments show that RADICE provides better results than other baseline methods, including causal discovery algorithms and correlation based approaches for root cause analysis.",http://arxiv.org/abs/2501.11545v1,CS,Quantitative
Packaging HEP Heterogeneous Mini-apps for Portable Benchmarking and Facility Evaluation on Modern HPCs,"High Energy Physics (HEP) experiments are making increasing use of GPUs and GPU dominated High Performance Computer facilities. Both the software and hardware of these systems are rapidly evolving, creating challenges for experiments to make informed decisions as to where they wish to devote resources. In its first phase, the High Energy Physics Center for Computational Excellence (HEP-CCE) produced portable versions of a number of heterogeneous HEP mini-apps, such as \ptor, FastCaloSim, Patatrack and the WireCell Toolkit, that exercise a broad range of GPU characteristics, enabling cross platform and facility benchmarking and evaluation. However, these mini-apps still require a significant amount of manual intervention to deploy on a new facility. We present our work in developing turn-key deployments of these mini-apps, where by means of containerization and automated configuration and build techniques such as Spack, we are able to quickly test new hardware, software, environments and entire facilities with minimal user intervention, and then track performance metrics over time.",http://arxiv.org/abs/2505.08933v1,CS,Quantitative
Efficient Graph Embedding at Scale: Optimizing CPU-GPU-SSD Integration,"Graph embeddings provide continuous vector representations of nodes in a graph, which are widely applicable in community detection, recommendations, and various scientific fields. However, existing graph embedding systems either face scalability challenges due to the high cost of RAM and multiple GPUs, or rely on disk storage at the expense of I/O efficiency. In this paper, we propose Legend, a lightweight heterogeneous system for graph embedding that systematically redefines data management across CPU, GPU, and NVMe SSD resources. Legend is built on a foundation of efficient data placement and retrieval strategies tailored to the unique strengths of each hardware. Key innovations include a prefetch-friendly embedding loading strategy, enabling GPUs to directly prefetch data from SSDs with minimal I/O overhead, and a high-throughput GPU-SSD direct access driver optimized for graph embedding tasks. Furthermore, we propose a customized parallel execution strategy to maximize GPU utilization, ensuring efficient handling of billion-scale datasets. Extensive experiments demonstrate that Legend achieves up to 4.8x speedup compared to state-of-the-art systems. Moreover, Legend exhibits comparable performance on a single GPU to that of the state-of-the-art system using 4 GPUs on the billion-scale dataset.",http://arxiv.org/abs/2505.09258v2,CS,Quantitative
"PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics","Despite decades of advancements, the simulation of fluids remains one of the most challenging areas of in scientific computing. Supported by the necessity of gradient information in deep learning, differentiable simulators have emerged as an effective tool for optimization and learning in physics simulations. In this work, we present our fluid simulator PICT, a differentiable pressure-implicit solver coded in PyTorch with Graphics-processing-unit (GPU) support. We first verify the accuracy of both the forward simulation and our derived gradients in various established benchmarks like lid-driven cavities and turbulent channel flows before we show that the gradients provided by our solver can be used to learn complicated turbulence models in 2D and 3D. We apply both supervised and unsupervised training regimes using physical priors to match flow statistics. In particular, we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow purely based on reference statistics. The low-resolution corrector trained with our solver runs substantially faster than the highly resolved references, while keeping or even surpassing their accuracy. Finally, we give additional insights into the physical interpretation of different solver gradients, and motivate a physically informed regularization technique. To ensure that the full potential of PICT can be leveraged, it is published as open source: https://github.com/tum-pbs/PICT.",http://arxiv.org/abs/2505.16992v1,CS,Quantitative
efunc: An Efficient Function Representation without Neural Networks,"Function fitting/approximation plays a fundamental role in computer graphics and other engineering applications. While recent advances have explored neural networks to address this task, these methods often rely on architectures with many parameters, limiting their practical applicability. In contrast, we pursue high-quality function approximation using parameter-efficient representations that eliminate the dependency on neural networks entirely. We first propose a novel framework for continuous function modeling. Most existing works can be formulated using this framework. We then introduce a compact function representation, which is based on polynomials interpolated using radial basis functions, bypassing both neural networks and complex/hierarchical data structures. We also develop memory-efficient CUDA-optimized algorithms that reduce computational time and memory consumption to less than 10% compared to conventional automatic differentiation frameworks. Finally, we validate our representation and optimization pipeline through extensive experiments on 3D signed distance functions (SDFs). The proposed representation achieves comparable or superior performance to state-of-the-art techniques (e.g., octree/hash-grid techniques) with significantly fewer parameters.",http://arxiv.org/abs/2505.21319v1,CS,Quantitative
ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks,"Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a large reasoning transformer-based criticmodel scaling scheme that brings reasoning ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks.",http://arxiv.org/abs/2505.10992v1,CS,Mixed
Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization,"We address the optimization problem of simultaneously minimizing multiple objective functionals over a family of probability distributions. This type of Multi-Objective Distributional Optimization commonly arises in machine learning and statistics, with applications in areas such as multiple target sampling, multi-task learning, and multi-objective generative modeling. To solve this problem, we propose an iterative particle-based algorithm, which we call Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of intermediate empirical distributions, each being represented by a set of particles, which gradually minimize the multiple objective functionals simultaneously. Specifically, MWGraD consists of two key steps at each iteration. First, it estimates the Wasserstein gradient for each objective functional based on the current particles. Then, it aggregates these gradients into a single Wasserstein gradient using dynamically adjusted weights and updates the particles accordingly. In addition, we provide theoretical analysis and present experimental results on both synthetic and real-world datasets, demonstrating the effectiveness of MWGraD.",http://arxiv.org/abs/2505.18765v1,CS,Quantitative
Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses,"Generative AI (GenAI) is rapidly entering computer science education, yet its effects on student learning, skill development, and perceptions remain underexplored. Concerns about overreliance coexist with a gap in research on structured scaffolding to guide tool use in formal courses. This study examines the impact of a dedicated ""AI-Lab"" intervention -- emphasizing guided scaffolding and mindful engagement -- on undergraduate students in Data Structures and Algorithms, Competitive Programming, and first-year engineering courses at Purdue University. Over three semesters, we integrated AI-Lab modules into four mandatory and elective courses, yielding 831 matched pre- and post-intervention survey responses, alongside focus group discussions. Employing a mixed-methods approach, we analyzed quantitative shifts in usage patterns and attitudes as well as qualitative narratives of student experiences. While the overall frequency of GenAI usage for homework or programming projects remained largely stable, we observed large effect sizes in comfort and openness across conceptual, debugging, and homework problems. Notably, usage patterns for debugging also shifted statistically significantly, reflecting students' more mindful and deliberate approach. Focus group discussions corroborated these results, suggesting that the intervention ""bridged the gap"" between naive GenAI usage and more nuanced, reflective integration of AI tools into coursework, ultimately heightening students' awareness of their own skill development. These findings suggest that structured, scaffolded interventions can enable students to harness GenAI's benefits without undermining essential competencies. We offer evidence-based recommendations for educators seeking to integrate GenAI responsibly into computing curricula and identify avenues for future research on GenAI-supported pedagogy.",http://arxiv.org/abs/2505.00100v1,CS,Mixed
Mining Q&A Platforms for Empirical Evidence on Quantum Software Programming,"The rise of quantum computing has driven the need for quantum software engineering, yet its programming landscape remains largely unexplored in empirical research. As quantum technologies advance toward industrial adoption, understanding programming aspects is crucial to addressing software development challenges. This study analyzes 6,935 quantum software programming discussion posts from Stack Exchange platforms (Quantum Computing, Stack Overflow, Software Engineering, and Code Review). Using topic modeling and qualitative analysis, we identified key discussion topics, trends (popular and difficult), tools/frameworks, and practitioner challenges. Twenty topics were identified, including popular ones such as physical theories and mathematical foundations, as well as security and encryption algorithms, while the most difficult were object-oriented programming and parameter control in quantum algorithms. Additionally, we identified nine frameworks that support quantum programming, with Qiskit emerging as the most widely adopted. Our findings also reveal core challenges in quantum software programming, thematically mapped into four areas: theories and mathematical concepts, algorithms and applications, experimental practices and software development, and education and community engagement. This study provides empirical insights that can inform future research, tool development, and educational efforts, supporting the evolution of the quantum software ecosystem.",http://arxiv.org/abs/2503.05240v1,CS,Mixed
Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment,"This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.",http://arxiv.org/abs/2502.15754v1,CS,Mixed
Towards a Science of Causal Interpretability in Deep Learning for Software Engineering,"This dissertation addresses achieving causal interpretability in Deep Learning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show strong performance in automating software tasks, their lack of transparency in causal relationships between inputs and outputs limits full understanding of their capabilities. To build trust in NCMs, researchers and practitioners must explain code predictions. Associational interpretability, which identifies correlations, is often insufficient for tasks requiring intervention and change analysis. To address this, the dissertation introduces DoCode, a novel post hoc interpretability method for NCMs. DoCode uses causal inference to provide programming language-oriented explanations of model predictions. It follows a four-step pipeline: modeling causal problems using Structural Causal Models (SCMs), identifying the causal estimand, estimating effects with metrics like Average Treatment Effect (ATE), and refuting effect estimates. Its framework is extensible, with an example that reduces spurious correlations by grounding explanations in programming language properties. A case study on deep code generation across interpretability scenarios and various deep learning architectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to code syntax changes and their ability to learn certain programming concepts while minimizing confounding bias. The dissertation also examines associational interpretability as a foundation, analyzing software information's causal nature using tools like COMET and TraceXplainer for traceability. It highlights the need to identify code confounders and offers practical guidelines for applying causal interpretability to NCMs, contributing to more trustworthy AI in software engineering.",http://arxiv.org/abs/2505.15023v1,CS,Qualitative
COSMIC: Enabling Full-Stack Co-Design and Optimization of Distributed Machine Learning Systems,"Large-scale machine learning models necessitate distributed systems, posing significant design challenges due to the large parameter space across distinct design stacks. Existing studies often focus on optimizing individual system aspects in isolation. This work challenges this limitation and introduces COSMIC, a full-stack distributed machine learning systems environment enabling end-to-end simulation and agent-based design space exploration. To facilitate efficient exploration and optimization across the entire stack, we introduce Parameter Set Architecture-an abstraction concept analogous to the instruction set architecture-abstracting away configuration complexities of agent-based search methods. Case studies demonstrate COSMIC's ability to consolidate parameters across multiple layers of design abstraction, discovering eight non-obvious high-performance system configurations across four transformer-based models with up to 175 billion parameters. By optimizing across the stack, COSMIC full-stack optimization delivers 1.50-48.41x higher performance compared to the isolated single-stack optimization.",http://arxiv.org/abs/2505.15020v1,CS,Quantitative
SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and Scale Out,"We expand the scope of cache memory to include LEO constellations, which are highly distributed systems with thousands of satellites connected with free-space optics inter-satellite links (ISL) always only one hop from any point on earth. We show how to increase the number of cache hits and improve the speed of inference for the important use case of LLMs. These benefits apply not only to LLMs, both terrestrially hosted and on satellites, but also generalize to any cache distributed over multiple locations that needs to be accessed in a timely manner. We show the benefit of our key value cache (KVC) protocol in simulations and present a proof-of-concept implementation of the protocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a 19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",http://arxiv.org/abs/2505.14427v1,CS,Quantitative
CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges,"Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.",http://arxiv.org/abs/2504.19093v1,CS,Qualitative
Flexible Semantic-Aware Resource Allocation: Serving More Users Through Similarity Range Constraints,"Semantic communication (SemCom) aims to enhance the resource efficiency of next-generation networks by transmitting the underlying meaning of messages, focusing on information relevant to the end user. Existing literature on SemCom primarily emphasizes learning the encoder and decoder through end-to-end deep learning frameworks, with the objective of minimizing a task-specific semantic loss function. Beyond its influence on the physical and application layer design, semantic variability across users in multi-user systems enables the design of resource allocation schemes that incorporate user-specific semantic requirements. To this end, \emph{a semantic-aware resource allocation} scheme is proposed with the objective of maximizing transmission and semantic reliability, ultimately increasing the number of users whose semantic requirements are met. The resulting resource allocation problem is a non-convex mixed-integer nonlinear program (MINLP), which is known to be NP-hard. To make the problem tractable, it is decomposed into a set of sub-problems, each of which is efficiently solved via geometric programming techniques. Finally, simulations demonstrate that the proposed method improves user satisfaction by up to $17.1\%$ compared to state of the art methods based on quality of experience-aware SemCom methods.",http://arxiv.org/abs/2504.20939v1,CS,Quantitative
How Scientists Use Large Language Models to Program,"Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization. As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development. We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university. Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries. We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses.",http://arxiv.org/abs/2502.17348v1,CS,Mixed
IssueCourier: Multi-Relational Heterogeneous Temporal Graph Neural Network for Open-Source Issue Assignment,"Issue assignment plays a critical role in open-source software (OSS) maintenance, which involves recommending the most suitable developers to address the reported issues. Given the high volume of issue reports in large-scale projects, manually assigning issues is tedious and costly. Previous studies have proposed automated issue assignment approaches that primarily focus on modeling issue report textual information, developers' expertise, or interactions between issues and developers based on historical issue-fixing records. However, these approaches often suffer from performance limitations due to the presence of incorrect and missing labels in OSS datasets, as well as the long tail of developer contributions and the changes of developer activity as the project evolves. To address these challenges, we propose IssueCourier, a novel Multi-Relational Heterogeneous Temporal Graph Neural Network approach for issue assignment. Specifically, we formalize five key relationships among issues, developers, and source code files to construct a heterogeneous graph. Then, we further adopt a temporal slicing technique that partitions the graph into a sequence of time-based subgraphs to learn stage-specific patterns. Furthermore, we provide a benchmark dataset with relabeled ground truth to address the problem of incorrect and missing labels in existing OSS datasets. Finally, to evaluate the performance of IssueCourier, we conduct extensive experiments on our benchmark dataset. The results show that IssueCourier can improve over the best baseline up to 45.49% in top-1 and 31.97% in MRR.",http://arxiv.org/abs/2505.11205v1,CS,Quantitative
TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization,"The Key-Value (KV) cache in generative large language models (LLMs) introduces substantial memory overhead. Existing works mitigate this burden by offloading or compressing the KV cache. However, loading the entire cache incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU communication, while aggressive compression causes notable performance degradation. We identify that certain layers in the LLM need to maintain global information and are unsuitable for selective loading. In contrast, other layers primarily focus on a few tokens with dominant activations that potentially incur substantial quantization error. This observation leads to a key insight that loading dominant tokens and quantizing all tokens can complement each other. Building on this insight, we propose a hybrid compression method, TailorKV, which seamlessly integrates quantization and offloading. TailorKV develops an inference framework along with a hardware-friendly implementation that leverages these complementary characteristics. Extensive long-context evaluations exhibit that TailorKV achieves nearly lossless performance under aggressive compression settings, outperforming the state-of-the-art. Particularly, the Llama-3.1-8B with 128k context can be served within a single RTX 3090 GPU, reaching 82 ms per token during decoding.",http://arxiv.org/abs/2505.19586v2,CS,Qualitative
EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective,"Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.",http://arxiv.org/abs/2505.12185v1,CS,Quantitative
RGB-to-Polarization Estimation: A New Task and Benchmark Study,"Polarization images provide rich physical information that is fundamentally absent from standard RGB images, benefiting a wide range of computer vision applications such as reflection separation and material classification. However, the acquisition of polarization images typically requires additional optical components, which increases both the cost and the complexity of the applications. To bridge this gap, we introduce a new task: RGB-to-polarization image estimation, which aims to infer polarization information directly from RGB images. In this work, we establish the first comprehensive benchmark for this task by leveraging existing polarization datasets and evaluating a diverse set of state-of-the-art deep learning models, including both restoration-oriented and generative architectures. Through extensive quantitative and qualitative analysis, our benchmark not only establishes the current performance ceiling of RGB-to-polarization estimation, but also systematically reveals the respective strengths and limitations of different model families -- such as direct reconstruction versus generative synthesis, and task-specific training versus large-scale pre-training. In addition, we provide some potential directions for future research on polarization estimation. This benchmark is intended to serve as a foundational resource to facilitate the design and evaluation of future methods for polarization estimation from standard RGB inputs.",http://arxiv.org/abs/2505.13050v1,CS,Mixed
Aggregating empirical evidence from data strategy studies: a case on model quantization,"Background: As empirical software engineering evolves, more studies adopt data strategies$-$approaches that investigate digital artifacts such as models, source code, or system logs rather than relying on human subjects. Synthesizing results from such studies introduces new methodological challenges. Aims: This study assesses the effects of model quantization on correctness and resource efficiency in deep learning (DL) systems. Additionally, it explores the methodological implications of aggregating evidence from empirical studies that adopt data strategies. Method: We conducted a research synthesis of six primary studies that empirically evaluate model quantization. We applied the Structured Synthesis Method (SSM) to aggregate the findings, which combines qualitative and quantitative evidence through diagrammatic modeling. A total of 19 evidence models were extracted and aggregated. Results: The aggregated evidence indicates that model quantization weakly negatively affects correctness metrics while consistently improving resource efficiency metrics, including storage size, inference latency, and GPU energy consumption$-$a manageable trade-off for many DL deployment contexts. Evidence across quantization techniques remains fragmented, underscoring the need for more focused empirical studies per technique. Conclusions: Model quantization offers substantial efficiency benefits with minor trade-offs in correctness, making it a suitable optimization strategy for resource-constrained environments. This study also demonstrates the feasibility of using SSM to synthesize findings from data strategy-based research.",http://arxiv.org/abs/2505.00816v1,CS,Mixed
A first look at ROS~2 applications written in asynchronous Rust,"The increasing popularity of the Rust programming language in building robotic applications using the Robot Operating System (ROS~2) raises questions about its real-time execution capabilities, particularly when employing asynchronous programming. Existing real-time scheduling and response-time analysis techniques for ROS~2 focus on applications written in C++ and do not address the unique execution models and challenges presented by Rust's asynchronous programming paradigm. In this paper, we analyze the execution model of R2R -- an asynchronous Rust ROS~2 bindings and various asynchronous Rust runtimes, comparing them with the execution model of C++ ROS~2 applications. We propose a structured approach for R2R applications aimed at deterministic real-time operation involving thread prioritization and callback-to-thread mapping schemes. Our experimental evaluation based on measuring end-to-end latencies of a synthetic application shows that the proposed approach is effective and outperforms other evaluated configurations. A more complex autonomous driving case study demonstrates its practical applicability. Overall, the experimental results indicate that our proposed structure achieves bounded response times for time-critical tasks. This paves the way for future work to adapt existing or develop new response-time analysis techniques for R2R applications using our structure.",http://arxiv.org/abs/2505.21323v1,CS,Mixed
How Do Model Export Formats Impact the Development of ML-Enabled Systems? A Case Study on Model Integration,"Machine learning (ML) models are often integrated into ML-enabled systems to provide software functionality that would otherwise be impossible. This integration requires the selection of an appropriate ML model export format, for which many options are available. These formats are crucial for ensuring a seamless integration, and choosing a suboptimal one can negatively impact system development. However, little evidence is available to guide practitioners during the export format selection. We therefore evaluated various model export formats regarding their impact on the development of ML-enabled systems from an integration perspective. Based on the results of a preliminary questionnaire survey (n=17), we designed an extensive embedded case study with two ML-enabled systems in three versions with different technologies. We then analyzed the effect of five popular export formats, namely ONNX, Pickle, TensorFlow's SavedModel, PyTorch's TorchScript, and Joblib. In total, we studied 30 units of analysis (2 systems x 3 tech stacks x 5 formats) and collected data via structured field notes. The holistic qualitative analysis of the results indicated that ONNX offered the most efficient integration and portability across most cases. SavedModel and TorchScript were very convenient to use in Python-based systems, but otherwise required workarounds (TorchScript more than SavedModel). SavedModel also allowed the easy incorporation of preprocessing logic into a single file, which made it scalable for complex deep learning use cases. Pickle and Joblib were the most challenging to integrate, even in Python-based systems. Regarding technical support, all model export formats had strong technical documentation and strong community support across platforms such as Stack Overflow and Reddit. Practitioners can use our findings to inform the selection of ML export formats suited to their context.",http://arxiv.org/abs/2502.00429v1,CS,Mixed
Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis,"The field of deep learning has witnessed significant breakthroughs, spanning various applications, and fundamentally transforming current software capabilities. However, alongside these advancements, there have been increasing concerns about reproducing the results of these deep learning methods. This is significant because reproducibility is the foundation of reliability and validity in software development, particularly in the rapidly evolving domain of deep learning. The difficulty of reproducibility may arise due to several reasons, including having differences from the original execution environment, incompatible software libraries, proprietary data and source code, lack of transparency, and the stochastic nature in some software. A study conducted by the Nature journal reveals that more than 70% of researchers failed to reproduce other researchers experiments and over 50% failed to reproduce their own experiments. Irreproducibility of deep learning poses significant challenges for researchers and practitioners. To address these concerns, this paper presents a systematic approach at analyzing and improving the reproducibility of deep learning models by demonstrating these guidelines using a case study. We illustrate the patterns and anti-patterns involved with these guidelines for improving the reproducibility of deep learning models. These guidelines encompass establishing a methodology to replicate the original software environment, implementing end-to-end training and testing algorithms, disclosing architectural designs, and enhancing transparency in data processing and training pipelines. We also conduct a sensitivity analysis to understand the model performance across diverse conditions. By implementing these strategies, we aim to bridge the gap between research and practice, so that innovations in deep learning can be effectively reproduced and deployed within software.",http://arxiv.org/abs/2505.03165v1,CS,Mixed
Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems,"The rapid proliferation of recommender systems necessitates robust fairness practices to address inherent biases. Assessing fairness, though, is challenging due to constantly evolving metrics and best practices. This paper analyzes how industry practitioners perceive and incorporate these changing fairness standards in their workflows. Through semi-structured interviews with 11 practitioners from technical teams across a range of large technology companies, we investigate industry implementations of fairness in recommendation system products. We focus on current debiasing practices, applied metrics, collaborative strategies, and integrating academic research into practice. Findings show a preference for multi-dimensional debiasing over traditional demographic methods, and a reliance on intuitive rather than academic metrics. This study also highlights the difficulties in balancing fairness with both the practitioner's individual (bottom-up) roles and organizational (top-down) workplace constraints, including the interplay with legal and compliance experts. Finally, we offer actionable recommendations for the recommender system community and algorithmic fairness practitioners, underlining the need to refine fairness practices continually.",http://arxiv.org/abs/2505.19441v1,CS,Qualitative
The Failure of Plagiarism Detection in Competitive Programming,"Plagiarism in programming courses remains a persistent challenge, especially in competitive programming contexts where assignments often have unique, known solutions. This paper examines why traditional code plagiarism detection methods frequently fail in these environments and explores the implications of emerging factors such as generative AI (genAI). Drawing on the author's experience teaching a Competitive Programming 1 (CP1) course over seven semesters at Purdue University (with $\approx 100$ students each term) and completely redesigning the CP1/2/3 course sequence, we provide an academically grounded analysis. We review literature on code plagiarism in computer science education, survey current detection tools (Moss, Kattis, etc.) and methods (manual review, code-authorship interviews), and analyze their strengths and limitations. Experience-based observations are presented to illustrate real-world detection failures and successes. We find that widely-used automated similarity checkers can be thwarted by simple code transformations or novel AI-generated code, while human-centric approaches like oral interviews, though effective, are labor-intensive. The paper concludes with opinions and preliminary recommendations for improving academic integrity in programming courses, advocating for a multi-faceted approach that combines improved detection algorithms, mastery-based learning techniques, and authentic assessment practices to better ensure code originality.",http://arxiv.org/abs/2505.08244v1,CS,Mixed
Benchmark-based Study of CPU/GPU Power-Related Features through JAX and TensorFlow,"Power management has become a crucial focus in the modern computing landscape, considering that {\em energy} is increasingly recognized as a critical resource. This increased the importance of all topics related to {\em energy-aware computing}. This paper presents an experimental study of three prevalent power management techniques that are {\em power limitation, frequency limitation}, and {\em ACPI/P-State governor modes} (OS states related to power consumption). Through a benchmark approach with a set of six computing kernels, we investigate {\em power/performance} trade-off with various hardware units and software frameworks (mainly TensorFlow and JAX). Our experimental results show that {\em frequency limitation} is the most effective technique to improve {\em Energy-Delay Product (EDP)}, which is a convolution of energy and running time. We also observe that running at the highest frequency compared to a reduced one could lead to a reduction of factor $\frac{1}{10}$ in EDP. Another noticeable fact is that frequency management shows a consistent behavior with different CPUs, whereas opposite effects sometimes occur between TensorFlow (TF) and JAX with the same power management settings.",http://arxiv.org/abs/2505.03398v1,CS,Quantitative
negativas: a prototype for searching and classifying sentential negation in speech data,"Negation is a universal feature of natural languages. In Brazilian Portuguese, the most commonly used negation particle is n\~ao, which can scope over nouns or verbs. When it scopes over a verb, n\~ao can occur in three positions: pre-verbal (NEG1), double negation (NEG2), or post-verbal (NEG3), e.g., n\~ao gosto, n\~ao gosto n\~ao, gosto n\~ao (""I do not like it""). From a variationist perspective, these structures are different forms of expressing negation. Pragmatically, they serve distinct communicative functions, such as politeness and modal evaluation. Despite their grammatical acceptability, these forms differ in frequency. NEG1 dominates across Brazilian regions, while NEG2 and NEG3 appear more rarely, suggesting its use is contextually restricted. This low-frequency challenges research, often resulting in subjective, non-generalizable interpretations of verbal negation with n\~ao. To address this, we developed negativas, a tool for automatically identifying NEG1, NEG2, and NEG3 in transcribed data. The tool's development involved four stages: i) analyzing a dataset of 22 interviews from the Falares Sergipanos database, annotated by three linguists, ii) creating a code using natural language processing (NLP) techniques, iii) running the tool, iv) evaluating accuracy. Inter-annotator consistency, measured using Fleiss' Kappa, was moderate (0.57). The tool identified 3,338 instances of n\~ao, classifying 2,085 as NEG1, NEG2, or NEG3, achieving a 93% success rate. However, negativas has limitations. NEG1 accounted for 91.5% of identified structures, while NEG2 and NEG3 represented 7.2% and 1.2%, respectively. The tool struggled with NEG2, sometimes misclassifying instances as overlapping structures (NEG1/NEG2/NEG3).",http://arxiv.org/abs/2504.04275v1,CS,Mixed
\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party,"Verifiable computing (VC) has gained prominence in decentralized machine learning systems, where resource-intensive tasks like deep neural network (DNN) inference are offloaded to external participants due to blockchain limitations. This creates a need to verify the correctness of outsourced computations without re-execution. We propose \texttt{Range-Arithmetic}, a novel framework for efficient and verifiable DNN inference that transforms non-arithmetic operations, such as rounding after fixed-point matrix multiplication and ReLU, into arithmetic steps verifiable using sum-check protocols and concatenated range proofs. Our approach avoids the complexity of Boolean encoding, high-degree polynomials, and large lookup tables while remaining compatible with finite-field-based proof systems. Experimental results show that our method not only matches the performance of existing approaches, but also reduces the computational cost of verifying the results, the computational effort required from the untrusted party performing the DNN inference, and the communication overhead between the two sides.",http://arxiv.org/abs/2505.17623v1,CS,Quantitative
Requirements Coverage-Guided Minimization for Natural Language Test Cases,"As software systems evolve, test suites tend to grow in size and often contain redundant test cases. Such redundancy increases testing effort, time, and cost. Test suite minimization (TSM) aims to eliminate such redundancy while preserving key properties such as requirement coverage and fault detection capability. In this paper, we propose RTM (Requirement coverage-guided Test suite Minimization), a novel TSM approach designed for requirement-based testing (validation), which can effectively reduce test suite redundancy while ensuring full requirement coverage and a high fault detection rate (FDR) under a fixed minimization budget. Based on common practice in critical systems where functional safety is important, we assume test cases are specified in natural language and traced to requirements before being implemented. RTM preprocesses test cases using three different preprocessing methods, and then converts them into vector representations using seven text embedding techniques. Similarity values between vectors are computed utilizing three distance functions. A Genetic Algorithm, whose population is initialized by coverage-preserving initialization strategies, is then employed to identify an optimized subset containing diverse test cases matching the set budget. We evaluate RTM on an industrial automotive system dataset comprising $736$ system test cases and $54$ requirements. Experimental results show that RTM consistently outperforms baseline techniques in terms of FDR across different minimization budgets while maintaining full requirement coverage. Furthermore, we investigate the impact of test suite redundancy levels on the effectiveness of TSM, providing new insights into optimizing requirement-based test suites under practical constraints.",http://arxiv.org/abs/2505.20004v1,CS,Quantitative
Proving the Coding Interview: A Benchmark for Formally Verified Code Generation,"We introduce the Formally Verified Automated Programming Progress Standards, or FVAPPS, a benchmark of 4715 samples for writing programs and proving their correctness, the largest formal verification benchmark, including 1083 curated and quality controlled samples. Previously, APPS provided a benchmark and dataset for programming puzzles to be completed in Python and checked against unit tests, of the kind seen in technical assessments in the software engineering industry. Building upon recent approaches for benchmarks in interactive theorem proving, we generalize the unit tests to Lean 4 theorems given without proof (i.e., using Lean's ""sorry"" keyword). On the 406 theorems of 100 randomly selected samples, Sonnet correctly proves 30% and Gemini correctly proves 18%. We challenge the machine learning and program synthesis communities to solve both each general purpose programming problem and its associated correctness specifications. The benchmark is available at https://huggingface.co/datasets/quinn-dougherty/fvapps.",http://arxiv.org/abs/2502.05714v1,CS,Quantitative
"LLMs Integration in Software Engineering Team Projects: Roles, Impact, and a Pedagogical Design Space for AI Tools in Computing Education","This work takes a pedagogical lens to explore the implications of generative AI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a semester-long 2nd-year undergraduate Software Engineering Team Project. Qualitative findings from survey (39 students) and interviews (eight students) provide insights into the students' views on the impact of GenAI use on their coding experience, learning, and self-efficacy. Our results address a particular gap in understanding the role and implications of GenAI on teamwork, team-efficacy, and team dynamics. The analysis of the learning aspects is distinguished by the application of learning and pedagogy informed lenses to discuss the data. We propose a preliminary design space for GenAI-based programming learning tools highlighting the importance of considering the roles that GenAI can play during the learning process, the varying support-ability patterns that can be applied to each role, and the importance of supporting transparency in GenAI for team members and students in addition to educators.",http://arxiv.org/abs/2410.23069v1,CS,Mixed
One Model to Rank Them All: Unifying Online Advertising with End-to-End Learning,"Modern industrial advertising systems commonly employ Multi-stage Cascading Architectures (MCA) to balance computational efficiency with ranking accuracy. However, this approach presents two fundamental challenges: (1) performance inconsistencies arising from divergent optimization targets and capability differences between stages, and (2) failure to account for advertisement externalities - the complex interactions between candidate ads during ranking. These limitations ultimately compromise system effectiveness and reduce platform profitability. In this paper, we present UniROM, an end-to-end generative architecture that Unifies online advertising Ranking as One Model. UniROM replaces cascaded stages with a single model to directly generate optimal ad sequences from the full candidate ad corpus in location-based services (LBS). The primary challenges associated with this approach stem from high costs of feature processing and computational bottlenecks in modeling externalities of large-scale candidate pools. To address these challenges, UniROM introduces an algorithm and engine co-designed hybrid feature service to decouple user and ad feature processing, reducing latency while preserving expressiveness. To efficiently extract intra- and cross-sequence mutual information, we propose RecFormer with an innovative cluster-attention mechanism as its core architectural component. Furthermore, we propose a bi-stage training strategy that integrates pre-training with reinforcement learning-based post-training to meet sophisticated platform and advertising objectives. Extensive offline evaluations on public benchmarks and large-scale online A/B testing on industrial advertising platform have demonstrated the superior performance of UniROM over state-of-the-art MCAs.",http://arxiv.org/abs/2505.19755v1,CS,Quantitative
Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data,"Linked Data and labelled property graphs (LPG) are two data management approaches with complementary strengths and weaknesses, making their integration beneficial for sharing datasets and supporting software ecosystems. In this paper, we introduce rdf2pg, an extensible framework for mapping RDF data to semantically equivalent LPG formats and data-bases. Utilising this framework, we perform a comparative analysis of three popular graph databases - Virtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages SPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments underline the strengths and limitations of these graph database technologies. Additionally, we highlight the potential of rdf2pg as a versatile tool for enabling polyglot access to knowledge graphs, aligning with established standards of Linked Data and the Semantic Web.",http://arxiv.org/abs/2505.17498v1,CS,Mixed
BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction,"Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments.",http://arxiv.org/abs/2505.20454v1,CS,Mixed
Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset,"The global challenge of sustainable recycling demands automated, fast, and accurate, state-of-the-art (SOTA) material detection systems that act as a bedrock for a circular economy. Democratizing access to these cutting-edge solutions that enable real-time waste analysis is essential for scaling up recycling efforts and fostering the Green Deal. In response, we introduce \textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to accelerate the recovery of critical raw materials through accurate electrolyzer materials classification. The dataset comprises 55 co-registered high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and 424,169 labeled ones. This enables non-invasive spectral analysis of shredded electrolyzer samples, supporting quantitative and qualitative material classification and spectral properties investigation. We evaluate a suite of baseline machine learning (ML) methods alongside SOTA transformer-based deep learning (DL) architectures, including Vision Transformer, SpectralFormer, and the Multimodal Fusion Transformer, to investigate architectural bottlenecks for further efficiency optimisation when deploying transformers in material identification. We implement zero-shot detection techniques and majority voting across pixel-level predictions to establish object-level classification robustness. In adherence to the FAIR data principles, the electrolyzers-HSI dataset and accompanying codebase are openly available at https://github.com/hifexplo/Electrolyzers-HSI and https://rodare.hzdr.de/record/3668, supporting reproducible research and facilitating the broader adoption of smart and sustainable e-waste recycling solutions.",http://arxiv.org/abs/2505.20507v1,CS,Mixed
Requirements for Quality Assurance of AI Models for Early Detection of Lung Cancer,"Lung cancer is the second most common cancer and the leading cause of cancer-related deaths worldwide. Survival largely depends on tumor stage at diagnosis, and early detection with low-dose CT can significantly reduce mortality in high-risk patients. AI can improve the detection, measurement, and characterization of pulmonary nodules while reducing assessment time. However, the training data, functionality, and performance of available AI systems vary considerably, complicating software selection and regulatory evaluation. Manufacturers must specify intended use and provide test statistics, but they can choose their training and test data, limiting standardization and comparability. Under the EU AI Act, consistent quality assurance is required for AI-based nodule detection, measurement, and characterization. This position paper proposes systematic quality assurance grounded in a validated reference dataset, including real screening cases plus phantom data to verify volume and growth rate measurements. Regular updates shall reflect demographic shifts and technological advances, ensuring ongoing relevance. Consequently, ongoing AI quality assurance is vital. Regulatory challenges are also adressed. While the MDR and the EU AI Act set baseline requirements, they do not adequately address self-learning algorithms or their updates. A standardized, transparent quality assessment - based on sensitivity, specificity, and volumetric accuracy - enables an objective evaluation of each AI solution's strengths and weaknesses. Establishing clear testing criteria and systematically using updated reference data lay the groundwork for comparable performance metrics, informing tenders, guidelines, and recommendations.",http://arxiv.org/abs/2502.17639v1,CS,Quantitative
What's DAT? Three Case Studies of Measuring Software Development Productivity at Meta With Diff Authoring Time,"This paper introduces Diff Authoring Time (DAT), a powerful, yet conceptually simple approach to measuring software development productivity that enables rigorous experimentation. DAT is a time based metric, which assess how long engineers take to develop changes, using a privacy-aware telemetry system integrated with version control, the IDE, and the OS. We validate DAT through observational studies, surveys, visualizations, and descriptive statistics. At Meta, DAT has powered experiments and case studies on more than 20 projects. Here, we highlight (1) an experiment on introducing mock types (a 14% DAT improvement), (2) the development of automatic memoization in the React compiler (33% improvement), and (3) an estimate of thousands of DAT hours saved annually through code sharing (> 50% improvement). DAT offers a precise, yet high-coverage measure for development productivity, aiding business decisions. It enhances development efficiency by aligning the internal development workflow with the experiment-driven culture of external product development. On the research front, DAT has enabled us to perform rigorous experimentation on long-standing software engineering questions such as ""do types make development more efficient?""",http://arxiv.org/abs/2503.10977v1,CS,Mixed
Let's Talk About It: Making Scientific Computational Reproducibility Easy,"Computational reproducibility of scientific results, that is, the execution of a computational experiment (e.g., a script) using its original settings (data, code, etc.), should always be possible. However, reproducibility has become a significant challenge, as researchers often face difficulties in accurately replicating experiments due to inconsistencies in documentation, setup configurations, and missing data. This lack of reproducibility may undermine the credibility of scientific results. To address this issue, we propose a conversational, text-based tool that allows researchers to easily reproduce computational experiments (theirs or from others) and package them in a single file that can be re-executed with just a double click on any computer, requiring the installation of a single widely-used software. Researchers interact with the platform in natural language, which our tool processes to automatically create a computational environment able to execute the provided experiment/code. We conducted two studies to evaluate our proposal. In the first study, we gathered qualitative data by executing 18 experiments from the literature. Although in some cases it was not possible to execute the experiment, in most instances, it was necessary to have little or even no interaction for the tool to reproduce the results. We also conducted a user study comparing our tool with an enterprise-level one. During this study, we measured the usability of both tools using the System Usability Scale (SUS) and participants' workload using the NASA Task Load Index (TLX). The results show a statistically significant difference between both tools in favor of our proposal, demonstrating that the usability and workload of our tool are superior to the current state of the art.",http://arxiv.org/abs/2504.10134v1,CS,Mixed
SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding,"Speech disorders such as dysarthria and anarthria can severely impair the patient's ability to communicate verbally. Speech decoding brain-computer interfaces (BCIs) offer a potential alternative by directly translating speech intentions into spoken words, serving as speech neuroprostheses. This paper reports an experimental protocol for Mandarin Chinese speech decoding BCIs, along with the corresponding decoding algorithms. Stereo-electroencephalography (SEEG) and synchronized audio data were collected from eight drug-resistant epilepsy patients as they conducted a word-level reading task. The proposed SEEG and Audio Contrastive Matching (SACM), a contrastive learning-based framework, achieved decoding accuracies significantly exceeding chance levels in both speech detection and speech decoding tasks. Electrode-wise analysis revealed that a single sensorimotor cortex electrode achieved performance comparable to that of the full electrode array. These findings provide valuable insights for developing more accurate online speech decoding BCIs.",http://arxiv.org/abs/2505.19652v1,CS,Quantitative
WindVE: Collaborative CPU-NPU Vector Embedding,"Retrieval-Augmented Generation is a technology that enhances large language models by integrating information retrieval. In the industry, inference services based on LLMs are highly sensitive to cost-performance ratio, prompting the need for improving hardware resource utilization in the inference service. Specifically, vector embedding and retrieval processes take up to 20% of the total latency. Therefore, optimizing the utilization of computational resources in vector embeddings is crucial for enhancing the cost-performance ratio of inference processes, which in turn boosts their product competitiveness.In this paper, we analyze the deployment costs of vector embedding technology in inference services, propose a theoretical formula, and determine through the mathematical expression that increasing the capacity to process concurrent queries is the key to reducing the deployment costs of vector embeddings. Therefore, in this paper, we focus on improving the product's capability to process concurrent queries. To optimize concurrency without sacrificing performance, we have designed a queue manager that adeptly offloads CPU peak queries. This manager utilizes a linear regression model to ascertain the optimal queue depths, a critical parameter that significantly influences the efficacy of the system. We further develop a system named WindVE that uses a CPU-NPU heterogeneous architecture to offload peak concurrent queries, which leverages the performance differences between the two processors to effectively manage traffic surges. Through experiments, we compare WindVE to the state-of-the-art vector embedding framework FlagEmbedding, and achieve a concurrency level up to 22.3% higher than the scheme without offloading.",http://arxiv.org/abs/2504.14941v2,CS,Quantitative
Toward Fair Federated Learning under Demographic Disparities and Data Imbalance,"Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.",http://arxiv.org/abs/2505.09295v1,CS,Quantitative
ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation,"Requirements elicitation and specification remains a labor-intensive, manual process prone to inconsistencies and gaps, presenting a significant challenge in modern software engineering. Emerging studies underscore the potential of employing large language models (LLMs) for automated requirements generation to support requirements elicitation and specification; however, it remains unclear how to implement this effectively. In this work, we introduce ReqBrain, an Al-assisted tool that employs a fine-tuned LLM to generate authentic and adequate software requirements. Software engineers can engage with ReqBrain through chat-based sessions to automatically generate software requirements and categorize them by type. We curated a high-quality dataset of ISO 29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine the most effective base model for ReqBrain. The top-performing model, Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of 91.20 in generating authentic and adequate requirements. Human evaluations further confirmed ReqBrain's effectiveness in generating requirements. Our findings suggest that generative Al, when fine-tuned, has the potential to improve requirements elicitation and specification, paving the way for future extensions into areas such as defect identification, test case generation, and agile user story creation.",http://arxiv.org/abs/2505.17632v1,CS,Quantitative
"The Impact of Generative AI-Powered Code Generation Tools on Software Engineer Hiring: Recruiters' Experiences, Perceptions, and Strategies","The rapid advancements in Generative AI (GenAI) tools, such as ChatGPT and GitHub Copilot, are transforming software engineering by automating code generation tasks. While these tools improve developer productivity, they also present challenges for organizations and hiring professionals in evaluating software engineering candidates' true abilities and potential. Although there is existing research on these tools in both industry and academia, there is a lack of research on how these tools specifically affect the hiring process. Therefore, this study aims to explore recruiters' experiences and perceptions regarding GenAI-powered code generation tools, as well as their challenges and strategies for evaluating candidates. Findings from our survey of 32 industry professionals indicate that although most participants are familiar with such tools, the majority of organizations have not adjusted their candidate evaluation methods to account for candidates' use/knowledge of these tools. There are mixed opinions on whether candidates should be allowed to use these tools during interviews, with many participants valuing candidates who can effectively demonstrate their skills in using these tools. Additionally, most participants believe that it is important to incorporate GenAI-powered code generation tools into computer science curricula and mention the key risks and benefits of doing so.",http://arxiv.org/abs/2409.00875v1,CS,Mixed
Probabilistic Kernel Function for Fast Angle Testing,"In this paper, we study the angle testing problem in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We further apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared to the state-of-the-art graph-based search algorithm HNSW.",http://arxiv.org/abs/2505.20274v1,CS,Quantitative
Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of Different Complexity Levels: An Empirical Analysis,"ChatGPT and other large language models (LLMs) promise to revolutionize software development by automatically generating code from program specifications. We assess the performance of ChatGPT's GPT-3.5-turbo model on LeetCode, a popular platform with algorithmic coding challenges for technical interview practice, across three difficulty levels: easy, medium, and hard. We test three main hypotheses. First, ChatGPT solves fewer problems as difficulty rises (Hypothesis 1). Second, prompt engineering improves ChatGPT's performance, with greater gains on easier problems and diminishing returns on harder ones (Hypothesis 2). Third, ChatGPT performs better in popular languages like Python, Java, and C++ than in less common ones like Elixir, Erlang, and Racket (Hypothesis 3). To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions. These solutions are stored and manually submitted on LeetCode to check their correctness. For Hypothesis 1, results show the GPT-3.5-turbo model successfully solves 92% of easy, 79% of medium, and 51% of hard problems. For Hypothesis 2, prompt engineering yields improvements: 14-29% for Chain of Thought Prompting, 38-60% by providing failed test cases in a second feedback prompt, and 33-58% by switching to GPT-4. From a random subset of problems ChatGPT solved in Python, it also solved 78% in Java, 50% in C++, and none in Elixir, Erlang, or Racket. These findings generally validate all three hypotheses.",http://arxiv.org/abs/2411.07529v1,CS,Mixed
Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines,"This study evaluates the adoption of DevSecOps among small and medium-sized enterprises (SMEs), identifying key challenges, best practices, and future trends. Through a mixed methods approach backed by the Technology Acceptance Model (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data from 405 SME professionals, revealing that while 68% have implemented DevSecOps, adoption is hindered by technical complexity (41%), resource constraints (35%), and cultural resistance (38%). Despite strong leadership prioritization of security (73%), automation gaps persist, with only 12% of organizations conducting security scans per commit. Our findings highlight a growing integration of security tools, particularly API security (63%) and software composition analysis (62%), although container security adoption remains low (34%). Looking ahead, SMEs anticipate artificial intelligence and machine learning to significantly influence DevSecOps, underscoring the need for proactive adoption of AI-driven security enhancements. Based on our findings, this research proposes strategic best practices to enhance CI/CD pipeline security including automation, leadership-driven security culture, and cross-team collaboration.",http://arxiv.org/abs/2503.22612v1,CS,Mixed
Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI,"This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",http://arxiv.org/abs/2505.19443v1,CS,Quantitative
The Dual Horizon: A Rendezvous of Computing and Communication Services at the Optical Layer in Optical Computing-Communication Integrated Network,"With the significant advancements in optical computing platforms recently capable of performing various primitive operations, a seamless integration of optical computing into very fabric of optical communication links is envisioned, paving the way for the advent of \textit{optical computing-communication integrated network}, which provides computing services at the ligthpath scale, alongside the traditional high-capacity communication ones. This necessitates a paradigm shift in optical node architecture, moving away from the conventional optical-bypass design that avoids lightpath interference crossing the same node, toward leveraging such interference for computation. Such new computing capability at the optical layer appears to be a good match with the growing needs of geo-distributed machine learning, where the training of large-scale models and datasets spans geographically diverse nodes, and intermediate results require further aggregation/computation to produce the desired outcomes for the destination node. To address this potential use case, an illustrative example is presented, which highlights the merit of providing in-network optical computing services in comparison with the traditional optical-bypass mode in the context of distributed learning scenarios taking place at two source nodes, and partial results are then optically aggregated to the destination. We then formulate the new \textit{routing, wavelength and computing assignment problem} arisen in serving computing requests, which could be considered as an extension of the traditional routing and wavelength assignment, that is used to accommodate the transmission requests. Simulation results performed on the realistic COST239 topology demonstrate the promising spectral efficiency gains achieved through the \textit{optical computing-communication integrated network} compared to the optical-bypass model.",http://arxiv.org/abs/2505.18753v1,CS,Quantitative
Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions,"The conventional cloud-based large model learning framework is increasingly constrained by latency, cost, personalization, and privacy concerns. In this survey, we explore an emerging paradigm: collaborative learning between on-device small model and cloud-based large model, which promises low-latency, cost-efficient, and personalized intelligent services while preserving user privacy. We provide a comprehensive review across hardware, system, algorithm, and application layers. At each layer, we summarize key problems and recent advances from both academia and industry. In particular, we categorize collaboration algorithms into data-based, feature-based, and parameter-based frameworks. We also review publicly available datasets and evaluation metrics with user-level or device-level consideration tailored to collaborative learning settings. We further highlight real-world deployments, ranging from recommender systems and mobile livestreaming to personal intelligent assistants. We finally point out open research directions to guide future development in this rapidly evolving field.",http://arxiv.org/abs/2504.15300v1,CS,Quantitative
"Diffusion Models on the Edge: Challenges, Optimizations, and Applications","Diffusion models have shown remarkable capabilities in generating high-fidelity data across modalities such as images, audio, and video. However, their computational intensity makes deployment on edge devices a significant challenge. This survey explores the foundational concepts of diffusion models, identifies key constraints of edge platforms, and synthesizes recent advancements in model compression, sampling efficiency, and hardware-software co-design to make diffusion models viable on edge devices. We also review promising applications and suggest future research directions.",http://arxiv.org/abs/2504.15298v1,CS,Quantitative
A Mixed-Methods Study of Open-Source Software Maintainers On Vulnerability Management and Platform Security Features,"In open-source software (OSS), software vulnerabilities have significantly increased. Although researchers have investigated the perspectives of vulnerability reporters and OSS contributor security practices, understanding the perspectives of OSS maintainers on vulnerability management and platform security features is currently understudied. In this paper, we investigate the perspectives of OSS maintainers who maintain projects listed in the GitHub Advisory Database. We explore this area by conducting two studies: identifying aspects through a listing survey ($n_1=80$) and gathering insights from semi-structured interviews ($n_2=22$). Of the 37 identified aspects, we find that supply chain mistrust and lack of automation for vulnerability management are the most challenging, and barriers to adopting platform security features include a lack of awareness and the perception that they are not necessary. Surprisingly, we find that despite being previously vulnerable, some maintainers still allow public vulnerability reporting, or ignore reports altogether. Based on our findings, we discuss implications for OSS platforms and how the research community can better support OSS vulnerability management efforts.",http://arxiv.org/abs/2409.07669v2,CS,Mixed
Understanding Decentralized Social Feed Curation on Mastodon,"As centralized social media platforms face growing concerns, more users are seeking greater control over their social feeds and turning to decentralized alternatives such as Mastodon. The decentralized nature of Mastodon creates unique opportunities for customizing feeds, yet user perceptions and curation strategies on these platforms remain unknown. This paper presents findings from a two-part interview study with 21 Mastodon users, exploring how they perceive, interact with, and manage their current feeds, and how we can better empower users to personalize their feeds on Mastodon. We use the qualitative findings of the first part of the study to guide the creation of Braids, a web-based prototype for feed curation. Results from the second part of our study, using Braids, highlighted opportunities and challenges for future research, particularly in using seamful design to enhance people's acceptance of algorithmic curation and nuanced trade-offs between machine learning-based and rule-based curation algorithms. To optimize user experience, we also discuss the tension between creating new apps and building add-ons in the decentralized social media realm.",http://arxiv.org/abs/2504.18817v1,CS,Qualitative
Workflows and Principles for Collaboration and Communication in Battery Research,"Interdisciplinary collaboration in battery science is required for rapid evaluation of better compositions and materials. However, diverging domain vocabulary and non-compatible experimental results slow down cooperation. We critically assess the current state-of-the-art and develop a structured data management and interpretation system to make data curation sustainable. The techniques we utilize comprise ontologies to give a structure to knowledge, database systems tenable to the FAIR principles, and software engineering to break down data processing into verifiable steps. To demonstrate our approach, we study the applicability of the Galvanostatic Intermittent Titration Technique on various electrodes. Our work is a building block in making automated material science scale beyond individual laboratories to a worldwide connected search for better battery materials.",http://arxiv.org/abs/2505.13566v1,CS,Quantitative
From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions,"Background and Context. The increasing integration of large language models (LLMs) in computing education presents an emerging challenge in understanding how students use LLMs and craft prompts to solve computational tasks. Prior research has used both qualitative and quantitative methods to analyze prompting behavior, but these approaches lack scalability or fail to effectively capture the semantic evolution of prompts. Objective. In this paper, we investigate whether students prompts can be systematically analyzed using propositional logic constraints. We examine whether this approach can identify patterns in prompt evolution, detect struggling students, and provide insights into effective and ineffective strategies. Method. We introduce Prompt2Constraints, a novel method that translates students prompts into logical constraints. The constraints are able to represent the intent of the prompts in succinct and quantifiable ways. We used this approach to analyze a dataset of 1,872 prompts from 203 students solving introductory programming tasks. Findings. We find that while successful and unsuccessful attempts tend to use a similar number of constraints overall, when students fail, they often modify their prompts more significantly, shifting problem-solving strategies midway. We also identify points where specific interventions could be most helpful to students for refining their prompts. Implications. This work offers a new and scalable way to detect students who struggle in solving natural language programming tasks. This work could be extended to investigate more complex tasks and integrated into programming tools to provide real-time support.",http://arxiv.org/abs/2504.18691v1,CS,Mixed
Joint Task Offloading and Channel Allocation in Spatial-Temporal Dynamic for MEC Networks,"Computation offloading and resource allocation are critical in mobile edge computing (MEC) systems to handle the massive and complex requirements of applications restricted by limited resources. In a multi-user multi-server MEC network, the mobility of terminals causes computing requests to be dynamically distributed in space. At the same time, the non-negligible dependencies among tasks in some specific applications impose temporal correlation constraints on the solution as well, leading the time-adjacent tasks to experience varying resource availability and competition from parallel counterparts. To address such dynamic spatial-temporal characteristics as a challenge in the allocation of communication and computation resources, we formulate a long-term delay-energy trade-off cost minimization problem in the view of jointly optimizing task offloading and resource allocation. We begin by designing a priority evaluation scheme to decouple task dependencies and then develop a grouped Knapsack problem for channel allocation considering the current data load and channel status. Afterward, in order to meet the rapid response needs of MEC systems, we exploit the double duel deep Q network (D3QN) to make offloading decisions and integrate channel allocation results into the reward as part of the dynamic environment feedback in D3QN, constituting the joint optimization of task offloading and channel allocation. Finally, comprehensive simulations demonstrate the performance of the proposed algorithm in the delay-energy trade-off cost and its adaptability for various applications.",http://arxiv.org/abs/2505.04272v1,CS,Quantitative
Automated Assessment in Mobile Programming Courses: Leveraging GitHub Classroom and Flutter for Enhanced Student Outcomes,"The growing demand for skilled mobile developers has made mobile programming courses an essential component of computer science curricula. However, these courses face unique challenges due to the complexity of mobile development environments and the graphical, interactive nature of mobile applications. This paper explores the potential of using GitHub Classroom, combined with the Flutter framework, for the automated assessment of mobile programming assignments. By leveraging GitHub Actions for continuous integration and Flutter's robust support for test automation, the proposed approach enables an auto-grading cost-effective solution. We evaluate the feasibility of integrating these tools through an experiment in a Mobile Programming course and present findings from a student survey that assesses their perceptions of the proposed evaluation model. The results are encouraging, showing that the approach is well-received by students.",http://arxiv.org/abs/2504.04230v1,CS,Quantitative
Pipelining Split Learning in Multi-hop Edge Networks,"To support large-scale model training, split learning (SL) enables multiple edge devices/servers to share the intensive training workload. However, most existing works on SL focus solely on two-tier model splitting. Moreover, while some recent works have investigated the model splitting and placement problems for multi-hop SL, these solutions fail to overcome the resource idleness issue, resulting in significant network idle time. In this work, we propose a pipelined SL scheme by addressing the joint optimization problem of model splitting and placement (MSP) in multi-hop edge networks. By applying pipeline parallelism to SL, we identify that the MSP problem can be mapped to a problem of minimizing the weighted sum of a bottleneck cost function (min-max) and a linear cost function (min-sum). Based on graph theory, we devise a bottleneck-aware shortest-path algorithm to obtain the optimal solution. Besides, given the MSP outcomes, we also derive the closed-form solution to the micro-batch size in the pipeline. Finally, we develop an alternating optimization algorithm of MSP and micro-batch size to solve the joint optimization problem to minimize the end-to-end training latency. Extensive simulations have demonstrated the significant advantages of our algorithm compared to existing benchmarks without pipeline parallelism.",http://arxiv.org/abs/2505.04368v1,CS,Quantitative
Evaluating the Performance of Nigerian Lecturers using Multilayer Perceptron,"Evaluating the performance of a lecturer has been essential for enhancing teaching quality, improving student learning outcomes, and strengthening the institution's reputation. The absence of such a system brings about lecturer performance evaluation which was neither comprehensive nor holistic. This system was designed using a web-based platform, created a secure database, and by using a custom dataset, captured some performance metrics which included student evaluation scores, Research Publications, Years of Experience, and Administrative Duties. Multilayer Perceptron (MLP) algorithm was utilized due to its ability to process complex data patterns and generates accurate predictions in a lecturer's performance based on historical data. This research focused on designing multiple performance metrics beyond the standard ones, incorporating student participation, and integrating analytical tools to deliver a comprehensive and holistic evaluation of lecturers' performance and was developed using Object-Oriented Analysis and Design (OOAD) methodology. Lecturers' performance is evaluated by the model, and the evaluation accuracy is about 91% compared with actual performance. Finally, by evaluating the performance of the MLP model, it is concluded that MLP enhanced lecturer performance evaluation by providing accurate predictions, reducing bias, and supporting data-driven decisions, ultimately improving the fairness and efficiency of the evaluation process. The MLP model's performance was evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE), achieved a test loss (MSE) of 256.99 and a MAE of 13.76, and reflected a high level of prediction accuracy. The model also demonstrated an estimated accuracy rate of approximately 96%, validated its effectiveness in predicting lecturer performance.",http://arxiv.org/abs/2505.17143v1,CS,Quantitative
Reduced and mixed precision turbulent flow simulations using explicit finite difference schemes,"The use of reduced and mixed precision computing has gained increasing attention in high-performance computing (HPC) as a means to improve computational efficiency, particularly on modern hardware architectures like GPUs. In this work, we explore the application of mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. We extend the OPS and OpenSBLI frameworks to support customizable precision levels, enabling fine-grained control over precision allocation for different computational tasks. Through a series of numerical experiments on the Taylor-Green vortex benchmark, we demonstrate that mixed precision strategies, such as half-single and single-double combinations, can offer significant performance gains without compromising numerical accuracy. However, pure half-precision computations result in unacceptable accuracy loss, underscoring the need for careful precision selection. Our results show that mixed precision configurations can reduce memory usage and communication overhead, leading to notable speedups, particularly on multi-CPU and multi-GPU systems.",http://arxiv.org/abs/2505.20911v1,CS,Quantitative
Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments,"Federated Learning (FL) is a decentralized machine learning paradigm that enables clients to collaboratively train models while preserving data privacy. However, the coexistence of model and data heterogeneity gives rise to inconsistent representations and divergent optimization dynamics across clients, ultimately hindering robust global performance. To transcend these challenges, we propose Mosaic, a novel data-free knowledge distillation framework tailored for heterogeneous distributed environments. Mosaic first trains local generative models to approximate each client's personalized distribution, enabling synthetic data generation that safeguards privacy through strict separation from real data. Subsequently, Mosaic forms a Mixture-of-Experts (MoE) from client models based on their specialized knowledge, and distills it into a global model using the generated data. To further enhance the MoE architecture, Mosaic integrates expert predictions via a lightweight meta model trained on a few representative prototypes. Extensive experiments on standard image classification benchmarks demonstrate that Mosaic consistently outperforms state-of-the-art approaches under both model and data heterogeneity. The source code has been published at https://github.com/Wings-Of-Disaster/Mosaic.",http://arxiv.org/abs/2505.19699v1,CS,Quantitative
Investigating Youth's Technical and Ethical Understanding of Generative Language Models When Engaging in Construction and Deconstruction Activities,"The widespread adoption of generative artificial intelligence/machine learning (AI/ML) technologies has increased the need to support youth in developing AI/ML literacies. However, most work has centered on preparing young people to use these systems, with less attention to how they can participate in designing and evaluating them. This study investigates how engaging young people in the design and auditing of generative language models (GLMs) may foster the development of their understanding of how these systems work from both technical and ethical perspectives. The study takes an in-pieces approach to investigate novices' conceptions of GLMs. Such an approach supports the analysis of how technical and ethical conceptions evolve and relate to each other. I am currently conducting a series of participatory design workshops with sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from a data-driven perspective that glassboxes how data shapes model performance and (b) audit commercial GLMs by repeatedly and systematically querying them to draw inferences about their behaviors. I will analyze participants' interactions to identify ethical and technical conceptions they may exhibit while designing and auditing GLMs. I will also conduct clinical interviews and use microgenetic knowledge analysis and ordered network analysis to investigate how participants' ethical and technical conceptions of GLMs relate to each other and change after the workshop. The study will contribute (a) evidence of how engaging youth in design and auditing activities may support the development of ethical and technical understanding of GLMs and (b) an inventory of novice design and auditing practices that may support youth's technical and ethical understanding of GLMs.",http://arxiv.org/abs/2504.15132v1,CS,Qualitative
Structuring Competency-Based Courses Through Skill Trees,"Computer science education has seen two important trends. One has been a shift from raw theory towards skills: competency-based teaching. Another has been increasing student numbers, with as a result more automation in teaching. When automating education, it is crucial to properly structure courses, both to manage digitalized educational resources and to facilitate automated coaching algorithms. Currently existing structuring methodologies are focused around theory and not around skills, and are incapable of modeling the dependency links between skills. Because of this, a new didactic framework is needed. This paper presents a new method of structuring educational contents around skills: something that a student is expected to be able to do. It defines Skill Trees that show dependencies between skills, and subsequently couples these to Concept Trees that contain intuitive ideas/notional machines. Due to the algorithmic nature of computer science, this step-wise approach is especially well-suited to this field of education. Next to formal definitions on Skill Trees and Concept Trees, guidelines are given on how to design them and how to plan a course using them. The Skill Trees framework has been applied to improve the structure of a university database course. Student interviews indicated reduced confusion/stress and less study time required for students to meet their desired skill level.",http://arxiv.org/abs/2504.16966v1,CS,Qualitative
QUIC Steps: Evaluating Pacing Strategies in QUIC Implementations,"Pacing is a key mechanism in modern transport protocols, used to regulate packet transmission timing to minimize traffic burstiness, lower latency, and reduce packet loss. Standardized in 2021, QUIC is a UDP-based protocol designed to improve upon the TCP / TLS stack. While the QUIC protocol recommends pacing, and congestion control algorithms like BBR rely on it, the user-space nature of QUIC introduces unique challenges. These challenges include coarse-grained timers, system call overhead, and OS scheduling delays, all of which complicate precise packet pacing. This paper investigates how pacing is implemented differently across QUIC stacks, including quiche, picoquic, and ngtcp2, and evaluates the impact of system-level features like GSO and Linux qdiscs on pacing. Using a custom measurement framework and a passive optical fiber tap, we establish a baseline with default settings and systematically explore the effects of qdiscs, hardware offloading using the ETF qdisc, and GSO on pacing precision and network performance. We also extend and evaluate a kernel patch to enable pacing of individual packets within GSO buffers, combining batching efficiency with precise pacing. Kernel-assisted and purely user-space pacing approaches are compared. We show that pacing with only user-space timers can work well, as demonstrated by picoquic with BBR. With quiche, we identify FQ as a qdisc well-suited for pacing QUIC traffic, as it is relatively easy to use and offers precise pacing based on packet timestamps. Our findings provide new insights into the trade-offs involved in implementing pacing in QUIC and highlight potential optimizations for real-world applications like video streaming and video calls.",http://arxiv.org/abs/2505.09222v1,CS,Quantitative
In-IDE Programming Courses: Learning Software Development in a Real-World Setting,"While learning programming languages is crucial for software engineers, mastering the necessary tools is equally important. To facilitate this, JetBrains recently released the JetBrains Academy plugin, which customizes the IDE for learners, allowing tutors to create courses entirely within IDE. In this work, we provide the first exploratory study of this learning format. We carried out eight one-hour interviews with students and developers who completed at least one course using the plugin, inquiring about their experience with the format, the used IDE features, and the current shortcomings. Our results indicate that learning inside the IDE is overall welcomed by the learners, allowing them to study in a more realistic setting, using features such as debugging and code analysis, which are crucial for real software development. With the collected results and the analysis of the current drawbacks, we aim to contribute to teaching students more practical skills.",http://arxiv.org/abs/2501.17747v1,CS,Qualitative
Aspects of complexity in automotive software systems and their relation to maintainability effort. A case study,"Context: Large embedded systems in vehicles tend to grow in size and complexity, which causes challenges when maintaining these systems. Objective: We explore how developers perceive the relation between maintainability effort and various sources of complexity. Methods: We conduct a case study at Scania AB, a heavy vehicle OEM. The units of analysis are two large software systems and their development teams/organizations. Results: Our results show that maintainability effort is driven by system internal complexity in the form of variant management and complex hardware control tasks. The maintainability is also influenced by emergent complexity caused by the system's longevity and constant growth. Besides these system-internal complexities, maintainability effort is also influenced by external complexities, such as organizational coordination and business needs. During the study, developer trade-off strategies for minimizing maintainability effort emerged. Conclusions: Complexity is a good proxy of maintainability effort, and allows developers to create strategies for managing the maintainability effort. Adequate complexity metrics include both external aspects -- e.g., coordination complexity -- and internal ones -- e.g., McCabe Cyclomatic Complexity.",http://arxiv.org/abs/2505.13135v1,CS,Qualitative
"Algorithms in the Stacks: Investigating automated, for-profit diversity audits in public libraries","Algorithmic systems are increasingly being adopted by cultural heritage institutions like libraries. In this study, we investigate U.S. public libraries' adoption of one specific automated tool -- automated collection diversity audits -- which we see as an illuminating case study for broader trends. Typically developed and sold by commercial book distributors, automated diversity audits aim to evaluate how well library collections reflect demographic and thematic diversity. We investigate how these audits function, whether library workers find them useful, and what is at stake when sensitive, normative decisions about representation are outsourced to automated commercial systems. Our analysis draws on an anonymous survey of U.S. public librarians (n=99), interviews with 14 librarians, a sample of purchasing records, and vendor documentation. We find that many library workers view these tools as convenient, time-saving solutions for assessing and diversifying collections under real and increasing constraints. Yet at the same time, the audits often flatten complex identities into standardized categories, fail to reflect local community needs, and further entrench libraries' infrastructural dependence on vendors. We conclude with recommendations for improving collection diversity audits and reflect on the broader implications for public libraries operating at the intersection of AI adoption, escalating anti-DEI backlash, and politically motivated defunding.",http://arxiv.org/abs/2505.14890v1,CS,Mixed
"Assessing the performance of 8 AI chatbots in bibliographic reference retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate","This study analyzes the performance of eight generative artificial intelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le Chat, and Perplexity -- in their free versions, in the task of generating academic bibliographic references within the university context. A total of 400 references were evaluated across the five major areas of knowledge (Health, Engineering, Experimental Sciences, Social Sciences, and Humanities), based on a standardized prompt. Each reference was assessed according to five key components (authorship, year, title, source, and location), along with document type, publication age, and error count. The results show that only 26.5% of the references were fully correct, 33.8% partially correct, and 39.8% were either erroneous or entirely fabricated. Grok and DeepSeek stood out as the only chatbots that did not generate false references, while Copilot, Perplexity, and Claude exhibited the highest hallucination rates. Furthermore, the chatbots showed a greater tendency to generate book references over journal articles, although the latter had a significantly higher fabrication rate. A high degree of overlap was also detected among the sources provided by several models, particularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal structural limitations in current AI models, highlight the risks of uncritical use by students, and underscore the need to strengthen information and critical literacy regarding the use of AI tools in higher education.",http://arxiv.org/abs/2505.18059v1,CS,Quantitative
Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods,"Recommender systems based on graph neural networks perform well in tasks such as rating and ranking. However, in real-world recommendation scenarios, noise such as user misuse and malicious advertisement gradually accumulates through the message propagation mechanism. Even if existing studies mitigate their effects by reducing the noise propagation weights, the severe sparsity of the recommender system still leads to the low-weighted noisy neighbors being mistaken as meaningful information, and the prediction result obtained based on the polluted nodes is not entirely trustworthy. Therefore, it is crucial to measure the confidence of the prediction results in this highly noisy framework. Furthermore, our evaluation of the existing representative GNN-based recommendation shows that it suffers from overconfidence. Based on the above considerations, we propose a new method to quantify and calibrate the prediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically, we propose a rating calibration method that dynamically adjusts excessive ratings to mitigate overconfidence based on user personalization. We also design a confidence loss function to reduce the overconfidence of negative samples and effectively improve recommendation performance. Experiments on public datasets demonstrate the validity of Conf-GNNRec in prediction confidence and recommendation performance.",http://arxiv.org/abs/2505.16466v1,CS,Quantitative
WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning,"Tabular data, ubiquitous and rich in informational value, is an increasing focus for deep representation learning, yet progress is hindered by studies centered on single tables or isolated databases, which limits model capabilities due to data scale. While collaborative learning approaches such as federated learning, transfer learning, split learning, and tabular foundation models aim to learn from multiple correlated databases, they are challenged by a scarcity of real-world interconnected tabular resources. Current data lakes and corpora largely consist of isolated databases lacking defined inter-database correlations. To overcome this, we introduce WikiDBGraph, a large-scale graph of 100,000 real-world tabular databases from WikiData, interconnected by 17 million edges and characterized by 13 node and 12 edge properties derived from its database schema and data distribution. WikiDBGraph's weighted edges identify both instance- and feature-overlapped databases. Experiments on these newly identified databases confirm that collaborative learning yields superior performance, thereby offering considerable promise for structured foundation model training while also exposing key challenges and future directions for learning from interconnected tabular data.",http://arxiv.org/abs/2505.16635v1,CS,Quantitative
Truth and Trust: Fake News Detection via Biosignals,"Understanding how individuals physiologically respond to false information is crucial for advancing misinformation detection systems. This study explores the potential of using physiological signals, specifically electrodermal activity (EDA) and photoplethysmography (PPG), to classify both the veracity of information and its interaction with user belief. In a controlled laboratory experiment, we collected EDA and PPG signals while participants evaluated the truthfulness of climate-related claims. Each trial was labeled based on the objective truth of the claim and the participant's belief, enabling two classification tasks: binary veracity detection and a novel four-class joint belief-veracity classification. We extracted handcrafted features from the raw signals and trained several machine learning models to benchmark the dataset. Our results show that EDA outperforms PPG, indicating its greater sensitivity to physiological responses related to truth perception. However, performance significantly drops in the joint belief-veracity classification task, highlighting the complexity of modeling the interaction between belief and truth. These findings suggest that while physiological signals can reflect basic truth perception, accurately modeling the intricate relationships between belief and veracity remains a significant challenge. This study emphasizes the importance of multimodal approaches that incorporate psychological, physiological, and cognitive factors to improve fake news detection systems. Our work provides a foundation for future research aimed at enhancing misinformation detection via addressing the complexities of human belief and truth processing.",http://arxiv.org/abs/2505.16702v1,CS,Quantitative
Performance Review on LLM for solving leetcode problems,"This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.",http://arxiv.org/abs/2502.15770v2,CS,Mixed
Less is More: On the Importance of Data Quality for Unit Test Generation,"Unit testing is crucial for software development and maintenance. Effective unit testing ensures and improves software quality, but writing unit tests is time-consuming and labor-intensive. Recent studies have proposed deep learning (DL) techniques or large language models (LLMs) to automate unit test generation. These models are usually trained or fine-tuned on large-scale datasets. Despite growing awareness of the importance of data quality, there has been limited research on the quality of datasets used for test generation. To bridge this gap, we systematically examine the impact of noise on the performance of learning-based test generation models. We first apply the open card sorting method to analyze the most popular and largest test generation dataset, Methods2Test, to categorize eight distinct types of noise. Further, we conduct detailed interviews with 17 domain experts to validate and assess the importance, reasonableness, and correctness of the noise taxonomy. Then, we propose CleanTest, an automated noise-cleaning framework designed to improve the quality of test generation datasets. CleanTest comprises three filters: a rule-based syntax filter, a rule-based relevance filter, and a model-based coverage filter. To evaluate its effectiveness, we apply CleanTest on two widely-used test generation datasets, i.e., Methods2Test and Atlas. Our findings indicate that 43.52% and 29.65% of datasets contain noise, highlighting its prevalence. Finally, we conduct comparative experiments using four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess the impact of noise on test generation performance. The results show that filtering noise positively influences the test generation ability of the models.",http://arxiv.org/abs/2502.14212v1,CS,Mixed
"Don't ""Overthink"" Passage Reranking: Is Reasoning Truly Necessary?","With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers.",http://arxiv.org/abs/2505.16886v1,CS,Qualitative
A Systematic Survey on Debugging Techniques for Machine Learning Systems,"Debugging ML software (i.e., the detection, localization and fixing of faults) poses unique challenges compared to traditional software largely due to the probabilistic nature and heterogeneity of its development process. Various methods have been proposed for testing, diagnosing, and repairing ML systems. However, the big picture informing important research directions that really address the dire needs of developers is yet to unfold, leaving several key questions unaddressed: (1) What faults have been targeted in the ML debugging research that fulfill developers needs in practice? (2) How are these faults addressed? (3) What are the challenges in addressing the yet untargeted faults? In this paper, we conduct a systematic study of debugging techniques for machine learning systems. We first collect technical papers focusing on debugging components in machine learning software. We then map these papers to a taxonomy of faults to assess the current state of fault resolution identified in existing literature. Subsequently, we analyze which techniques are used to address specific faults based on the collected papers. This results in a comprehensive taxonomy that aligns faults with their corresponding debugging methods. Finally, we examine previously released transcripts of interviewing developers to identify the challenges in resolving unfixed faults. Our analysis reveals that only 48 percent of the identified ML debugging challenges have been explicitly addressed by researchers, while 46.9 percent remain unresolved or unmentioned. In real world applications, we found that 52.6 percent of issues reported on GitHub and 70.3% of problems discussed in interviews are still unaddressed by research in ML debugging. The study identifies 13 primary challenges in ML debugging, highlighting a significant gap between the identification of ML debugging issues and their resolution in practice.",http://arxiv.org/abs/2503.03158v1,CS,Qualitative
Revolutionizing Datacenter Networks via Reconfigurable Topologies,"With the popularity of cloud computing and data-intensive applications such as machine learning, datacenter networks have become a critical infrastructure for our digital society. Given the explosive growth of datacenter traffic and the slowdown of Moore's law, significant efforts have been made to improve datacenter network performance over the last decade. A particularly innovative solution is reconfigurable datacenter networks (RDCNs): datacenter networks whose topologies dynamically change over time, in either a demand-oblivious or a demand-aware manner. Such dynamic topologies are enabled by recent optical switching technologies and stand in stark contrast to state-of-the-art datacenter network topologies, which are fixed and oblivious to the actual traffic demand. In particular, reconfigurable demand-aware and 'self-adjusting' datacenter networks are motivated empirically by the significant spatial and temporal structures observed in datacenter communication traffic. This paper presents an overview of reconfigurable datacenter networks. In particular, we discuss the motivation for such reconfigurable architectures, review the technological enablers, and present a taxonomy that classifies the design space into two dimensions: static vs. dynamic and demand-oblivious vs. demand-aware. We further present a formal model and discuss related research challenges. Our article comes with complementary video interviews in which three leading experts, Manya Ghobadi, Amin Vahdat, and George Papen, share with us their perspectives on reconfigurable datacenter networks.",http://arxiv.org/abs/2502.16228v1,CS,Mixed
Large Language Models for Code Generation: The Practitioners Perspective,"Large Language Models (LLMs) have emerged as coding assistants, capable of generating source code from natural language prompts. With the increasing adoption of LLMs in software development, academic research and industry based projects are developing various tools, benchmarks, and metrics to evaluate the effectiveness of LLM-generated code. However, there is a lack of solutions evaluated through empirically grounded methods that incorporate practitioners perspectives to assess functionality, syntax, and accuracy in real world applications. To address this gap, we propose and develop a multi-model unified platform to generate and execute code based on natural language prompts. We conducted a survey with 60 software practitioners from 11 countries across four continents working in diverse professional roles and domains to evaluate the usability, performance, strengths, and limitations of each model. The results present practitioners feedback and insights into the use of LLMs in software development, including their strengths and weaknesses, key aspects overlooked by benchmarks and metrics, and a broader understanding of their practical applicability. These findings can help researchers and practitioners make informed decisions for systematically selecting and using LLMs in software development projects. Future research will focus on integrating more diverse models into the proposed system, incorporating additional case studies, and conducting developer interviews for deeper empirical insights into LLM-driven software development.",http://arxiv.org/abs/2501.16998v1,CS,Mixed
Generative AI in Computer Science Education: Accelerating Python Learning with ChatGPT,"The increasing demand for digital literacy and artificial intelligence (AI) fluency in the workforce has highlighted the need for scalable, efficient programming instruction. This study evaluates the effectiveness of integrating generative AI, specifically OpenAIs ChatGPT, into a self-paced Python programming module embedded within a sixteen-week professional training course on applied generative AI. A total of 86 adult learners with varying levels of programming experience completed asynchronous Python instruction in Weeks three and four, using ChatGPT to generate, interpret, and debug code. Python proficiency and general coding knowledge was assessed across 30 different assessments during the first 13 weeks of the course through timed, code-based evaluations. A mixed-design ANOVA revealed that learners without prior programming experience scored significantly lower than their peers on early assessments. However, following the completion of the accelerated Python instruction module, these group differences were no longer statistically significant,, indicating that the intervention effectively closed initial performance gaps and supported proficiency gains across all learner groups. These findings suggest that generative AI can support accelerated learning outcomes and reduce entry barriers for learners with no prior coding background. While ChatGPT effectively facilitated foundational skill acquisition, the study also highlights the importance of balancing AI assistance with opportunities for independent problem-solving. The results support the potential of AI-augmented instruction as a scalable model for reskilling in the digital economy.",http://arxiv.org/abs/2505.20329v1,CS,Quantitative
Research on feature fusion and multimodal patent text based on graph attention network,"Aiming at the problems of cross-modal feature fusion, low efficiency of long text modeling and lack of hierarchical semantic coherence in patent text semantic mining, this study proposes HGM-Net, a deep learning framework that integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a dynamic mask, contrast and cross-structural similarity constraints on the word, sentence and paragraph hierarchies through HCL. Contrast and cross-structural similarity constraints are constructed at the word and paragraph levels by HCL to strengthen the local semantic and global thematic consistency of patent text; M-GAT models patent classification codes, citation relations and text semantics as heterogeneous graph structures, and achieves dynamic fusion of multi-source features by cross-modal gated attention; MSA adopts a hierarchical sparsity strategy to optimize the computational efficiency of long text modeling at word, phrase, sentence and paragraph granularity. Experiments show that the framework demonstrates significant advantages over existing deep learning methods in tasks such as patent classification and similarity matching, and provides a solution with both theoretical innovation and practical value for solving the problems of patent examination efficiency improvement and technology relevance mining.",http://arxiv.org/abs/2505.20188v1,CS,Quantitative
A Unified Architecture for Efficient Binary and Worst-Case Optimal Join Processing,"Join processing is a fundamental operation in database management systems; however, traditional join algorithms often encounter efficiency challenges when dealing with complex queries that produce intermediate results much larger than the final query output. The emergence of worst-case optimal join (WCOJ) algorithms represents a significant advancement, offering asymptotically better performance by avoiding the enumeration of potentially exploding intermediate results. In this paper, we propose a unified architecture that efficiently supports both traditional binary joins and WCOJ processing. As opposed to the state-of-the-art, which only focuses on either hash-based or sort-based join implementations, our system accommodates both physical implementations of binary joins and WCOJ algorithms. Experimental evaluations demonstrate that our system achieves performance gains of up to 3.1x (on average 1.5x) and 4.8x (on average 1.4x) over the state-of-the-art implementation of Generic Join and Free Join methods, respectively, across acyclic and cyclic queries in standard query benchmarks.",http://arxiv.org/abs/2505.19918v1,CS,Quantitative
A Preliminary Investigation on the Usage of Quantum Approximate Optimization Algorithms for Test Case Selection,"Regression testing is key in verifying that software works correctly after changes. However, running the entire regression test suite can be impractical and expensive, especially for large-scale systems. Test suite optimization methods are highly effective but often become infeasible due to their high computational demands. In previous work, Trovato et al. proposed SelectQA, an approach based on quantum annealing that outperforms the traditional state-of-the-art methods, i.e., Additional Greedy and DIV-GA, in efficiency. This work envisions the usage of Quantum Approximate Optimization Algorithms (QAOAs) for test case selection by proposing QAOA-TCS. QAOAs merge the potential of gate-based quantum machines with the optimization capabilities of the adiabatic evolution. To prove the effectiveness of QAOAs for test case selection, we preliminarily investigate QAOA-TCS leveraging an ideal environment simulation before evaluating it on real quantum machines. Our results show that QAOAs perform better than the baseline algorithms in effectiveness while being comparable to SelectQA in terms of efficiency. These results encourage us to continue our experimentation with noisy environment simulations and real quantum machines.",http://arxiv.org/abs/2504.18955v2,CS,Quantitative
"Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools","Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.",http://arxiv.org/abs/2412.14732v1,CS,Mixed
"Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach","College students are increasingly affected by stress, anxiety, and depression, yet face barriers to traditional mental health care. This study evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor and machine learning (ML) algorithms for real-time stress detection and self-management. In a 12-week randomized controlled trial (n = 117), participants were assigned to a treatment group using mHELP's full suite of interventions or a control group using the app solely for real-time stress logging and weekly psychological assessments. The primary outcome, ""Moments of Stress"" (MS), was assessed via physiological and self-reported indicators and analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly, secondary outcomes of psychological assessments, including the Generalized Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire (PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also analyzed via GLMM. The finding of the objective measure, MS, indicates a substantial decrease in MS among the treatment group compared to the control group, while no notable between-group differences were observed in subjective scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the treatment group exhibited a clinically meaningful decline in GAD-7 and PSS scores. These findings underscore the potential of wearable-enabled mHealth tools to reduce acute stress in college populations and highlight the need for extended interventions and tailored features to address chronic symptoms like depression.",http://arxiv.org/abs/2505.15974v2,CS,Quantitative
RCOMPSs: A Scalable Runtime System for R Code Execution on Manycore Systems,"R has become a cornerstone of scientific and statistical computing due to its extensive package ecosystem, expressive syntax, and strong support for reproducible analysis. However, as data sizes and computational demands grow, native R parallelism support remains limited. This paper presents RCOMPSs, a scalable runtime system that enables efficient parallel execution of R applications on multicore and manycore systems. RCOMPSs adopts a dynamic, task-based programming model, allowing users to write code in a sequential style, while the runtime automatically handles asynchronous task execution, dependency tracking, and scheduling across available resources. We present RCOMPSs using three representative data analysis algorithms, i.e., K-nearest neighbors (KNN) classification, K-means clustering, and linear regression and evaluate their performance on two modern HPC systems: KAUST Shaheen-III and Barcelona Supercomputing Center (BSC) MareNostrum 5. Experimental results reveal that RCOMPSs demonstrates both strong and weak scalability on up to 128 cores per node and across 32 nodes. For KNN and K-means, parallel efficiency remains above 70% in most settings, while linear regression maintains acceptable performance under shared and distributed memory configurations despite its deeper task dependencies. Overall, RCOMPSs significantly enhances the parallel capabilities of R with minimal, automated, and runtime-aware user intervention, making it a practical solution for large-scale data analytics in high-performance environments.",http://arxiv.org/abs/2505.06896v1,CS,Quantitative
Exploring Dynamic Load Balancing Algorithms for Block-Structured Mesh-and-Particle Simulations in AMReX,"Load balancing is critical for successful large-scale high-performance computing (HPC) simulations. With modern supercomputers increasing in complexity and variability, dynamic load balancing is becoming more critical to use computational resources efficiently. In this study, performed during a summer collaboration at Lawrence Berkeley National Laboratory, we investigate various standard dynamic load-balancing algorithms. This includes the time evaluation of a brute-force solve for application in algorithmic evaluation, as well as quality and time evaluations of the Knapsack algorithm, an SFC algorithm, and two novel algorithms: a painter's partition-based SFC algorithm and a combination Knapsack+SFC methodology-based on hardware topology. The results suggest Knapsack and painter's partition-based algorithms should be among the first algorithms evaluated by HPC codes for cases with limited weight deviation and will perform at least slightly better than AMReX's percentage-tracking partitioning strategy across most simulations, although effects diminish as weight variety increases.",http://arxiv.org/abs/2505.15122v1,CS,Quantitative
POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval,"Although Multi-Vector Retrieval (MVR) has achieved the state of the art on many information retrieval (IR) tasks, its performance highly depends on how to decompose queries into smaller pieces, say phrases or tokens. However, optimizing query decomposition for MVR performance is not end-to-end differentiable. Even worse, jointly solving this problem and training the downstream retrieval-based systems, say RAG systems could be highly inefficient. To overcome these challenges, we propose Performance-Oriented Query Decomposer (POQD), a novel query decomposition framework for MVR. POQD leverages one LLM for query decomposition and searches the optimal prompt with an LLM-based optimizer. We further propose an end-to-end training algorithm to alternatively optimize the prompt for query decomposition and the downstream models. This algorithm can achieve superior MVR performance at a reasonable training cost as our theoretical analysis suggests. POQD can be integrated seamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented Generation (RAG) systems. Extensive empirical studies on representative RAG-based QA tasks show that POQD outperforms existing query decomposition strategies in both retrieval performance and end-to-end QA accuracy. POQD is available at https://github.com/PKU-SDS-lab/POQD-ICML25.",http://arxiv.org/abs/2505.19189v1,CS,Quantitative
FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix,"Federated Learning (FL) enables geographically distributed clients to collaboratively train machine learning models by sharing only their local models, ensuring data privacy. However, FL is vulnerable to untargeted attacks that aim to degrade the global model's performance on the underlying data distribution. Existing defense mechanisms attempt to improve FL's resilience against such attacks, but their effectiveness is limited in practical FL environments due to data heterogeneity. On the contrary, we aim to detect and remove the attacks to mitigate their impact. Generalization contribution plays a crucial role in distinguishing untargeted attacks. Our observations indicate that, with limited data, the divergence between embeddings representing different classes provides a better measure of generalization than direct accuracy. In light of this, we propose a novel robust aggregation method, FedGraM, designed to defend against untargeted attacks in FL. The server maintains an auxiliary dataset containing one sample per class to support aggregation. This dataset is fed to the local models to extract embeddings. Then, the server calculates the norm of the Gram Matrix of the embeddings for each local model. The norm serves as an indicator of each model's inter-class separation capability in the embedding space. FedGraM identifies and removes potentially malicious models by filtering out those with the largest norms, then averages the remaining local models to form the global model. We conduct extensive experiments to evaluate the performance of FedGraM. Our empirical results show that with limited data samples used to construct the auxiliary dataset, FedGraM achieves exceptional performance, outperforming state-of-the-art defense methods.",http://arxiv.org/abs/2505.14024v1,CS,Mixed
Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for Multicore Real-Time Systems,"Memory bandwidth regulation and cache partitioning are widely used techniques for achieving predictable timing in real-time computing systems. Combined with partitioned scheduling, these methods require careful co-allocation of tasks and resources to cores, as task execution times strongly depend on available allocated resources. To address this challenge, this paper presents a 0-1 linear program for task-resource co-allocation, along with a multi-objective heuristic designed to minimize resource usage while guaranteeing schedulability under a preemptive EDF scheduling policy. Our heuristic employs a multi-layer framework, where an outer layer explores resource allocations using Pareto-pruned search, and an inner layer optimizes task allocation by solving a knapsack problem using dynamic programming. To evaluate the performance of the proposed optimization algorithm, we profile real-world benchmarks on an embedded AMD UltraScale+ ZCU102 platform, with fine-grained resource partitioning enabled by the Jailhouse hypervisor, leveraging cache set partitioning and MemGuard for memory bandwidth regulation. Experiments based on the benchmarking results show that the proposed 0-1 linear program outperforms existing mixed-integer programs by finding more optimal solutions within the same time limit. Moreover, the proposed multi-objective multi-layer heuristic performs consistently better than the state-of-the-art multi-resource-task co-allocation algorithm in terms of schedulability, resource usage, number of non-dominated solutions, and computational efficiency.",http://arxiv.org/abs/2505.11554v1,CS,Quantitative
Insights from the Frontline: GenAI Utilization Among Software Engineering Students,"Generative AI (genAI) tools (e.g., ChatGPT, Copilot) have become ubiquitous in software engineering (SE). As SE educators, it behooves us to understand the consequences of genAI usage among SE students and to create a holistic view of where these tools can be successfully used. Through 16 reflective interviews with SE students, we explored their academic experiences of using genAI tools to complement SE learning and implementations. We uncover the contexts where these tools are helpful and where they pose challenges, along with examining why these challenges arise and how they impact students. We validated our findings through member checking and triangulation with instructors. Our findings provide practical considerations of where and why genAI should (not) be used in the context of supporting SE students.",http://arxiv.org/abs/2412.15624v1,CS,Qualitative
"Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics","Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed way to conceptualize and evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal trajectory of predictive features coupled with inertia. In addition to our theoretical formulation of EaF metrics, our empirical contributions include: 1/ a pre-registered human subjects experiment, which demonstrates that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; 2/ pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who spent significant efforts to improve but are still stuck with systemic/early-life disadvantages outside their control.",http://arxiv.org/abs/2505.19317v1,CS,Quantitative
Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE,"We revisit the efficacy of simple, real-valued embedding models for knowledge graph completion and introduce RelatE, an interpretable and modular method that efficiently integrates dual representations for entities and relations. RelatE employs a real-valued phase-modulus decomposition, leveraging sinusoidal phase alignments to encode relational patterns such as symmetry, inversion, and composition. In contrast to recent approaches based on complex-valued embeddings or deep neural architectures, RelatE preserves architectural simplicity while achieving competitive or superior performance on standard benchmarks. Empirically, RelatE outperforms prior methods across several datasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680, surpassing all baselines. Additionally, RelatE offers significant efficiency gains, reducing training time by 24%, inference latency by 31%, and peak GPU memory usage by 22% compared to RotatE. Perturbation studies demonstrate improved robustness, with MRR degradation reduced by up to 61% relative to TransE and by up to 19% compared to RotatE under structural edits such as edge removals and relation swaps. Formal analysis further establishes the model's full expressiveness and its capacity to represent essential first-order logical inference patterns. These results position RelatE as a scalable and interpretable alternative to more complex architectures for knowledge graph completion.",http://arxiv.org/abs/2505.18971v1,CS,Quantitative
"Curious, Critical Thinker, Empathetic, and Ethically Responsible: Essential Soft Skills for Data Scientists in Software Engineering","Background. As artificial intelligence and AI-powered systems continue to grow, the role of data scientists has become essential in software development environments. Data scientists face challenges related to managing large volumes of data and addressing the societal impacts of AI algorithms, which require a broad range of soft skills. Goal. This study aims to identify the key soft skills that data scientists need when working on AI-powered projects, with a particular focus on addressing biases that affect society. Method. We conducted a thematic analysis of 87 job postings on LinkedIn and 11 interviews with industry practitioners. The job postings came from companies in 12 countries and covered various experience levels. The interviews featured professionals from diverse backgrounds, including different genders, ethnicities, and sexual orientations, who worked with clients from South America, North America, and Europe. Results. While data scientists share many skills with other software practitioners -- such as those related to coordination, engineering, and management -- there is a growing emphasis on innovation and social responsibility. These include soft skills like curiosity, critical thinking, empathy, and ethical awareness, which are essential for addressing the ethical and societal implications of AI. Conclusion. Our findings indicate that data scientists working on AI-powered projects require not only technical expertise but also a solid foundation in soft skills that enable them to build AI systems responsibly, with fairness and inclusivity. These insights have important implications for recruitment and training within software companies and for ensuring the long-term success of AI-powered systems and their broader societal impact.",http://arxiv.org/abs/2501.02088v2,CS,Qualitative
Optimizing edge AI models on HPC systems with the edge in the loop,"Artificial intelligence and machine learning models deployed on edge devices, e.g., for quality control in Additive Manufacturing (AM), are frequently small in size. Such models usually have to deliver highly accurate results within a short time frame. Methods that are commonly employed in literature start out with larger trained models and try to reduce their memory and latency footprint by structural pruning, knowledge distillation, or quantization. It is, however, also possible to leverage hardware-aware Neural Architecture Search (NAS), an approach that seeks to systematically explore the architecture space to find optimized configurations. In this study, a hardware-aware NAS workflow is introduced that couples an edge device located in Belgium with a powerful High-Performance Computing system in Germany, to train possible architecture candidates as fast as possible while performing real-time latency measurements on the target hardware. The approach is verified on a use case in the AM domain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster inference speed while simultaneously enhancing model quality by a factor of ~1.35, compared to a human-designed baseline.",http://arxiv.org/abs/2505.19995v1,CS,Quantitative
Prime Collective Communications Library -- Technical Report,"This report presents the Prime Collective Communications Library (PCCL), a novel fault-tolerant collective communication library designed for distributed ML workloads over the public internet. PCCL introduces a new programming model that enables dynamic peer joining and failure recovery. The library implements efficient collective operations like all-reduce while providing robust fault tolerance mechanisms that allow the system to continue operating even when peers fail or join during ongoing operations. We demonstrate that PCCL's design enables practical solutions to dynamic membership challenges in workloads with repeated operations and deterministic state advancement. Our implementation passes extensive stress tests across all major operating systems, showing reliable operation even under rapid peer churn and concurrent collective operations. By dispatching to multiple connections, we can efficiently utilize cross-continental long-fat-pipe TCP WAN links, in our experiments achieving up to 45 Gbit/s of bandwidth utilization across Europe and 25 Gbit/s across North America and Europe. PCCL's architecture enables easy implementation of distributed low-communication optimization strategies like DiLoCo, which significantly reduce communication frequency. Combined with quantization, this leads to a significant reduction in the bandwidth required for distributed training workloads. PCCL also allows for concurrent collective operations, which enables optimization strategies like async DiLoCo, which can completely hide communication overhead by implementing one-step delayed parameter updates. PCCL can facilitate exact bit-parity of the shared state across peers in all cases induced by graceful or abrupt peer churn. While PCCL exposes a C99 API, Python bindings are available which are compatible with PyTorch alongside FSDP. PCCL is available under the open source MIT license.",http://arxiv.org/abs/2505.14065v1,CS,Quantitative
Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework,"Electroencephalography-based Emotion Recognition (EEG-ER) has become a growing research area in recent years. Analyzing 216 papers published between 2018 and 2023, we uncover that the field lacks a unified evaluation protocol, which is essential to fairly define the state of the art, compare new approaches and to track the field's progress. We report the main inconsistencies between the used evaluation protocols, which are related to ground truth definition, evaluation metric selection, data splitting types (e.g., subject-dependent or subject-independent) and the use of different datasets. Capitalizing on this state-of-the-art research, we propose a unified evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which enables an easy and efficient evaluation of new methods and datasets. EEGain is a novel open source software framework, offering the capability to compare - and thus define - state-of-the-art results. EEGain includes standardized methods for data pre-processing, data splitting, evaluation metrics, and the ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER, MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In addition, we have assessed and validated EEGain using these six datasets on the four most common publicly available methods (EEGNet, DeepConvNet, ShallowConvNet, TSception). This is a significant step to make research on EEG-ER more reproducible and comparable, thereby accelerating the overall progress of the field.",http://arxiv.org/abs/2505.18175v1,CS,Quantitative
Dual Utilization of Perturbation for Stream Data Publication under Local Differential Privacy,"Stream data from real-time distributed systems such as IoT, tele-health, and crowdsourcing has become an important data source. However, the collection and analysis of user-generated stream data raise privacy concerns due to the potential exposure of sensitive information. To address these concerns, local differential privacy (LDP) has emerged as a promising standard. Nevertheless, applying LDP to stream data presents significant challenges, as stream data often involves a large or even infinite number of values. Allocating a given privacy budget across these data points would introduce overwhelming LDP noise to the original stream data. Beyond existing approaches that merely use perturbed values for estimating statistics, our design leverages them for both perturbation and estimation. This dual utilization arises from a key observation: each user knows their own ground truth and perturbed values, enabling a precise computation of the deviation error caused by perturbation. By incorporating this deviation into the perturbation process of subsequent values, the previous noise can be calibrated. Following this insight, we introduce the Iterative Perturbation Parameterization (IPP) method, which utilizes current perturbed results to calibrate the subsequent perturbation process. To enhance the robustness of calibration and reduce sensitivity, two algorithms, namely Accumulated Perturbation Parameterization (APP) and Clipped Accumulated Perturbation Parameterization (CAPP) are further developed. We prove that these three algorithms satisfy $w$-event differential privacy while significantly improving utility. Experimental results demonstrate that our techniques outperform state-of-the-art LDP stream publishing solutions in terms of utility, while retaining the same privacy guarantee.",http://arxiv.org/abs/2504.14993v1,CS,Mixed
Local consistency and axioms of functional dependence,"Local consistency arises in diverse areas, including Bayesian statistics, relational databases, and quantum foundations. Likewise, the notion of functional dependence arises in all of these areas. We adopt a general approach to study logical inference in a setting that enables both global inconsistency and local consistency. Our approach builds upon pairwise consistent families of K-relations, i.e, relations with tuples annotated with elements of some positive commutative monoid. The framework covers, e.g., families of probability distributions arising from quantum experiments and their possibilistic counterparts. As a first step, we investigate the entailment problem for functional dependencies (FDs) in this setting. Notably, the transitivity rule for FDs is no longer sound, but can be replaced by two novel axiom schemes. We provide a complete axiomatisation and a PTIME algorithm for the entailment problem of unary FDs. In addition, we explore when contextual families over the Booleans have realisations as contextual families over various monoids.",http://arxiv.org/abs/2505.11057v1,CS,Quantitative
LLM Code Customization with Visual Results: A Benchmark on TikZ,"With the rise of AI-based code generation, customizing existing code out of natural language instructions to modify visual results -such as figures or images -has become possible, promising to reduce the need for deep programming expertise. However, even experienced developers can struggle with this task, as it requires identifying relevant code regions (feature location), generating valid code variants, and ensuring the modifications reliably align with user intent. In this paper, we introduce vTikZ, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to customize code while preserving coherent visual outcomes. Our benchmark consists of carefully curated vTikZ editing scenarios, parameterized ground truths, and a reviewing tool that leverages visual feedback to assess correctness. Empirical evaluation with stateof-the-art LLMs shows that existing solutions struggle to reliably modify code in alignment with visual intent, highlighting a gap in current AI-assisted code editing approaches. We argue that vTikZ opens new research directions for integrating LLMs with visual feedback mechanisms to improve code customization tasks in various domains beyond TikZ, including image processing, art creation, Web design, and 3D modeling.",http://arxiv.org/abs/2505.04670v1,CS,Quantitative
Providing Information About Implemented Algorithms Improves Program Comprehension: A Controlled Experiment,"Context: Various approaches aim to support program comprehension by automatically detecting algorithms in source code. However, no empirical evaluations of their helpfulness have been performed. Objective: To empirically evaluate how algorithm labels - which include the algorithm's name and additional information - impact program comprehension in terms of correctness and time. Method: We conducted a controlled experiment with 56 participants, where the experimental group received code with labeled algorithms. The groups completed exercises designed to measure program comprehension as well as a post-questionnaire on label helpfulness, use cases for algorithm recognition, and reasons for self-implementation of algorithms in practice. Results: Annotating source code with algorithm labels significantly improves program comprehension (p=0.040), with a median improvement of 6 points (~23%), but does not affect completion times (p=0.991). Qualitative analysis revealed that a majority of participants perceived the labels as helpful, especially for recognizing the codes intent. Participants also proposed use cases such as error detection, optimization, and library replacement. Reasons for self-implementing algorithms included library inadequacies, performance needs and avoiding dependencies or licensing costs. Conclusion: This study shows that algorithm labels improve program comprehension, especially for developers with medium programming experience. Our qualitative analysis also sheds light on how participants benefit from the labels, further use cases for algorithm recognition and motivations behind self-implementing algorithms.",http://arxiv.org/abs/2504.19225v1,CS,Mixed
Are Autonomous Web Agents Good Testers?,"Despite advances in automated testing, manual testing remains prevalent due to the high maintenance demands associated with test script fragility-scripts often break with minor changes in application structure. Recent developments in Large Language Models (LLMs) offer a potential alternative by powering Autonomous Web Agents (AWAs) that can autonomously interact with applications. These agents may serve as Autonomous Test Agents (ATAs), potentially reducing the need for maintenance-heavy automated scripts by utilising natural language instructions similar to those used by human testers. This paper investigates the feasibility of adapting AWAs for natural language test case execution and how to evaluate them. We contribute with (1) a benchmark of three offline web applications, and a suite of 113 manual test cases, split between passing and failing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and pinATA, two open-source ATA implementations capable of executing test steps, verifying assertions and giving verdicts, and (3) comparative experiments using our benchmark that quantifies our ATAs effectiveness. Finally we also proceed to a qualitative evaluation to identify the limitations of PinATA, our best performing implementation. Our findings reveal that our simple implementation, SeeAct-ATA, does not perform well compared to our more advanced PinATA implementation when executing test cases (50% performance improvement). However, while PinATA obtains around 60% of correct verdict and up to a promising 94% specificity, we identify several limitations that need to be addressed to develop more resilient and reliable ATAs, paving the way for robust, low maintenance test automation. CCS Concepts: $\bullet$ Software and its engineering $\rightarrow$ Software testing and debugging.",http://arxiv.org/abs/2504.01495v1,CS,Mixed
Unified Analysis of Decentralized Gradient Descent: a Contraction Mapping Framework,"The decentralized gradient descent (DGD) algorithm, and its sibling, diffusion, are workhorses in decentralized machine learning, distributed inference and estimation, and multi-agent coordination. We propose a novel, principled framework for the analysis of DGD and diffusion for strongly convex, smooth objectives, and arbitrary undirected topologies, using contraction mappings coupled with a result called the mean Hessian theorem (MHT). The use of these tools yields tight convergence bounds, both in the noise-free and noisy regimes. While these bounds are qualitatively similar to results found in the literature, our approach using contractions together with the MHT decouples the algorithm dynamics (how quickly the algorithm converges to its fixed point) from its asymptotic convergence properties (how far the fixed point is from the global optimum). This yields a simple, intuitive analysis that is accessible to a broader audience. Extensions are provided to multiple local gradient updates, time-varying step sizes, noisy gradients (stochastic DGD and diffusion), communication noise, and random topologies.",http://arxiv.org/abs/2503.14353v1,CS,Qualitative
Multimodal Non-Semantic Feature Fusion for Predicting Segment Access Frequency in Lecture Archives,"This study proposes a multimodal neural network-based approach to predict segment access frequency in lecture archives. These archives, widely used as supplementary resources in modern education, often consist of long, unedited recordings that make it difficult to keep students engaged. Captured directly from face-to-face lectures without post-processing, they lack visual appeal. Meanwhile, the increasing volume of recorded material renders manual editing and annotation impractical. Automatically detecting high-engagement segments is thus crucial for improving accessibility and maintaining learning effectiveness. Our research focuses on real classroom lecture archives, characterized by unedited footage, no additional hardware (e.g., eye-tracking), and limited student numbers. We approximate student engagement using segment access frequency as a proxy. Our model integrates multimodal features from teachers' actions (via OpenPose and optical flow), audio spectrograms, and slide page progression. These features are deliberately chosen for their non-semantic nature, making the approach applicable regardless of lecture language. Experiments show that our best model achieves a Pearson correlation of 0.5143 in 7-fold cross-validation and 69.32 percent average accuracy in a downstream three-class classification task. The results, obtained with high computational efficiency and a small dataset, demonstrate the practical feasibility of our system in real-world educational contexts.",http://arxiv.org/abs/2504.14927v1,CS,Quantitative
"How Scientists Use Jupyter Notebooks: Goals, Quality Attributes, and Opportunities","Computational notebooks are intended to prioritize the needs of scientists, but little is known about how scientists interact with notebooks, what requirements drive scientists' software development processes, or what tactics scientists use to meet their requirements. We conducted an observational study of 20 scientists using Jupyter notebooks for their day-to-day tasks, finding that scientists prioritize different quality attributes depending on their goals. A qualitative analysis of their usage shows (1) a collection of goals scientists pursue with Jupyter notebooks, (2) a set of quality attributes that scientists value when they write software, and (3) tactics that scientists leverage to promote quality. In addition, we identify ways scientists incorporated AI tools into their notebook work. From our observations, we derive design recommendations for improving computational notebooks and future programming systems for scientists. Key opportunities pertain to helping scientists create and manage state, dependencies, and abstractions in their software, enabling more effective reuse of clearly-defined components.",http://arxiv.org/abs/2503.12309v1,CS,Qualitative
Establishing tool support for a concept DSL,"The quality of software products tends to correlate with the quality of the abstractions adopted early in the design process. Acknowledging this tendency has led to the development of various tools and methodologies for modeling systems thoroughly before implementing them. However, creating effective abstract models of domain problems is difficult, especially if the models are also expected to exhibit qualities such as intuitiveness, being seamlessly integrable with other models, or being easily translatable into code. This thesis describes Conceptual, a DSL for modeling the behavior of software systems using self-contained and highly reusable units of functionally known as concepts. The language's syntax and semantics are formalized based on previous work. Additionally, the thesis proposes a strategy for mapping language constructs from Conceptual into the Alloy modeling language. The suggested strategy is then implemented with a simple compiler, allowing developers to access and utilize Alloy's existing analysis tools for program reasoning. The utility and expressiveness of Conceptual is demonstrated qualitatively through several practical case studies. Using the implemented compiler, a few erroneous specifications are identified in the literature. Moreover, the thesis establishes preliminary tool support in the Visual Studio Code IDE.",http://arxiv.org/abs/2503.05849v1,CS,Qualitative
Toward Data-centric Directed Graph Learning: An Entropy-driven Approach,"The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities. In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a data-centric digraph learning paradigm or a model-agnostic hot-and-plug data-centric Knowledge Distillation (KD) module. The core idea is to achieve data-centric ML, guided by our proposed hierarchical encoding theory for structured data. Specifically, EDEN first utilizes directed structural measurements from a topology perspective to construct a coarse-grained Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual information of node profiles to refine knowledge flow in the HKT, enabling data-centric KD supervision within model training. As a general framework, EDEN can also naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets (homophily and heterophily) and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs.",http://arxiv.org/abs/2505.00983v1,CS,Quantitative
FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models,"Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.",http://arxiv.org/abs/2505.21032v1,CS,Qualitative
Evaluating Performance Consistency in Competitive Programming: Educational Implications and Contest Design Insights,"Competitive programming (CP) contests are often treated as interchangeable proxies for algorithmic skill, yet the extent to which results at lower contest tiers anticipate performance at higher tiers, and how closely any tier resembles the ubiquitous online-contest circuit, remains unclear. We analyze ten years (2015--2024) of International Collegiate Programming Contest (ICPC) standings, comprising five long-running superregional championships (Africa \& Arab, Asia East, Asia West, North America, and Northern Eurasia), associated local regionals of North America and Northern Eurasia, and the World Finals. For 366 World Finalist teams (2021--2024) we augment the dataset with pre-contest Codeforces ratings. Pairwise rank alignment is measured with Kendall's $\tau$. Overall, superregional ranks predict World Final ranks only moderately (weighted $\tau=0.407$), but regional-to-superregional consistency varies widely: Northern Eurasia exhibits the strongest alignment ($\tau=0.521$) while Asia West exhibits the weakest ($\tau=0.188$). Internal consistency within a region can exceed its predictive value for Worlds -- e.g., Northern Eurasia and North America regionals vs. superregionals ($\tau=0.666$ and $\tau=0.577$, respectively). Codeforces ratings correlate more strongly with World Final results ($\tau=0.596$) than any single ICPC tier, suggesting that high-frequency online contests capture decisive skill factors that many superregional sets miss. We argue that contest organizers can improve both fairness and pedagogical value by aligning problem style and selection rules with the formats that demonstrably differentiate teams, in particular the Northern-Eurasian model and well-curated online rounds. All data, scripts, and additional analyses are publicly released to facilitate replication and further study.",http://arxiv.org/abs/2505.04143v1,CS,Quantitative
Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition,"Cross-user variability in Human Activity Recognition (HAR) remains a critical challenge due to differences in sensor placement, body dynamics, and behavioral patterns. Traditional methods often fail to capture biomechanical invariants that persist across users, limiting their generalization capability. We propose an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG) framework that integrates anatomical correlation knowledge into a unified graph neural network (GNN) architecture. By modeling three biomechanically motivated relationships together-Interconnected Units, Analogous Units, and Lateral Units-our method encodes domain-invariant features while addressing user-specific variability through Variational Edge Feature Extractor. A Gradient Reversal Layer (GRL) enforces adversarial domain generalization, ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and DSADS datasets demonstrate state-of-the-art performance. Our work bridges biomechanical principles with graph-based adversarial learning by integrating information fusion techniques. This fusion of information underpins our unified and generalized model for cross-user HAR.",http://arxiv.org/abs/2505.06301v1,CS,Quantitative
SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs,"Software logs are messages recorded during the execution of a software system that provide crucial run-time information about events and activities. Although software logs have a critical role in software maintenance and operation tasks, publicly accessible log datasets remain limited, hindering advance in log analysis research and practices. The presence of sensitive information, particularly Personally Identifiable Information (PII) and quasi-identifiers, introduces serious privacy and re-identification risks, discouraging the publishing and sharing of real-world logs. In practice, log anonymization techniques primarily rely on regular expression patterns, which involve manually crafting rules to identify and replace sensitive information. However, these regex-based approaches suffer from significant limitations, such as extensive manual efforts and poor generalizability across diverse log formats and datasets. To mitigate these limitations, we introduce SDLog, a deep learning-based framework designed to identify sensitive information in software logs. Our results show that SDLog overcomes regex limitations and outperforms the best-performing regex patterns in identifying sensitive information. With only 100 fine-tuning samples from the target dataset, SDLog can correctly identify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To the best of our knowledge, this is the first deep learning alternative to regex-based methods in software log anonymization.",http://arxiv.org/abs/2505.14976v1,CS,Quantitative
Channel Fingerprint Construction for Massive MIMO: A Deep Conditional Generative Approach,"Accurate channel state information (CSI) acquisition for massive multiple-input multiple-output (MIMO) systems is essential for future mobile communication networks. Channel fingerprint (CF), also referred to as channel knowledge map, is a key enabler for intelligent environment-aware communication and can facilitate CSI acquisition. However, due to the cost limitations of practical sensing nodes and test vehicles, the resulting CF is typically coarse-grained, making it insufficient for wireless transceiver design. In this work, we introduce the concept of CF twins and design a conditional generative diffusion model (CGDM) with strong implicit prior learning capabilities as the computational core of the CF twin to establish the connection between coarse- and fine-grained CFs. Specifically, we employ a variational inference technique to derive the evidence lower bound (ELBO) for the log-marginal distribution of the observed fine-grained CF conditioned on the coarse-grained CF, enabling the CGDM to learn the complicated distribution of the target data. During the denoising neural network optimization, the coarse-grained CF is introduced as side information to accurately guide the conditioned generation of the CGDM. To make the proposed CGDM lightweight, we further leverage the additivity of network layers and introduce a one-shot pruning approach along with a multi-objective knowledge distillation technique. Experimental results show that the proposed approach exhibits significant improvement in reconstruction performance compared to the baselines. Additionally, zero-shot testing on reconstruction tasks with different magnification factors further demonstrates the scalability and generalization ability of the proposed approach.",http://arxiv.org/abs/2505.07893v1,CS,Quantitative
Discrimination-free Insurance Pricing with Privatized Sensitive Attributes,"Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints on insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that are tailored to the insurance domain, using only privatized sensitive attributes. Notably, our approach ensures statistical guarantees, does not require direct access to sensitive attributes, and adapts to varying transparency requirements, addressing regulatory demands while ensuring fairness in insurance pricing.",http://arxiv.org/abs/2504.11775v1,CS,Quantitative
Do Code LLMs Do Static Analysis?,"This paper investigates code LLMs' capability of static analysis during code intelligence tasks such as code summarization and generation. Code LLMs are now household names for their abilities to do some programming tasks that have heretofore required people. The process that people follow to do programming tasks has long been understood to require static analysis. For example, human programmers navigate the call graph of large programs to comprehend the different parts of those programs. Education in programming includes static analysis under the assumption that better static analysis skills beget better programming. Yet while popular culture is replete with anthropomorphic references such as LLM ""reasoning"", in fact code LLMs could exhibit a wholly alien thought process to humans. This paper studies the specific question of static analysis by code LLMs. We use three different static analysis tasks (callgraph generation, AST generation, and dataflow generation) and three different code intelligence tasks (code generation, summarization, and translation) with two different open-source models (Gemini and GPT-4o) and closed-source models (CodeLlaMA and Jam) as our experiments. We found that LLMs show poor performance on static analysis tasks and that pretraining on the static analysis tasks does not generalize to better performance on the code intelligence tasks.",http://arxiv.org/abs/2505.12118v1,CS,Quantitative
REARANK: Reasoning Re-ranking Agent via Reinforcement Learning,"We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.",http://arxiv.org/abs/2505.20046v1,CS,Quantitative
It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features,"The quality of training data is critical to the performance of machine learning applications in domains like transportation, healthcare, and robotics. Accurate image labeling, however, often relies on time-consuming, expert-driven methods with limited feedback. This research introduces a sketch-based annotation approach supported by large language models (LLMs) to reduce technical barriers and enhance accessibility. Using a synthetic dataset, we examine how sketch recognition features relate to LLM feedback metrics, aiming to improve the reliability and interpretability of LLM-assisted labeling. We also explore how prompting strategies and sketch variations influence feedback quality. Our main contribution is a sketch-based virtual assistant that simplifies annotation for non-experts and advances LLM-driven labeling tools in terms of scalability, accessibility, and explainability.",http://arxiv.org/abs/2505.19419v2,CS,Quantitative
Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data,"Among the factors of production, human capital or skilled manpower is the one that keeps evolving and adapts to changing conditions and resources. This adaptability makes human capital the most crucial factor in ensuring a sustainable growth of industry/sector. As new technologies are developed and adopted, the new generations are required to acquire skills in newer technologies in order to be employable. At the same time professionals are required to upskill and reskill themselves to remain relevant in the industry. There is however no straightforward method to identify the skill needs of the industry at a given point of time. Therefore, this paper proposes a data to decision framework that can successfully identify the desired skill set in a given area by analysing the advertorial data collected from popular online job portals and supplied as input to the framework. The proposed framework uses techniques of statistical analysis, data mining and natural language processing for the purpose. The applicability of the framework is demonstrated on CS&IT job advertisement data from India. The analytical results not only provide useful insights about current state of skill needs in CS&IT industry but also provide practical implications to prospective job applicants, training agencies, and institutions of higher education & professional training.",http://arxiv.org/abs/2503.17424v1,CS,Quantitative
A Black-box Testing Framework for Oracle Quantum Programs,"Oracle quantum programs are a fundamental class of quantum programs that serve as a critical bridge between quantum computing and classical computing. Many important quantum algorithms are built upon oracle quantum programs, making it essential to ensure their correctness during development. While software testing is a well-established approach for improving program reliability, no systematic method has been developed to test oracle quantum programs. This paper proposes a black-box testing framework designed for general oracle quantum programs. We define these programs formally, establish the foundational theory for their testing, and propose a detailed testing framework. We develop a prototype tool and conduct extensive experimental evaluations to evaluate the framework's effectiveness. Our results demonstrate that the proposed framework significantly aids developers in testing oracle quantum programs, providing insights to enhance the reliability of quantum software.",http://arxiv.org/abs/2505.07243v1,CS,Quantitative
A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces,"News outlets' competition for attention in news interfaces has highlighted the need for demographically-aware saliency prediction models. Despite recent advancements in saliency detection applied to user interfaces (UI), existing datasets are limited in size and demographic representation. We present a deep learning framework that enhances the SaRa (Saliency Ranking) model with DeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our framework optimizes three key components: saliency map generation, grid segment scoring, and map normalization. Through a two-fold experiment using eye-tracking (30 participants) and mouse-tracking (375 participants aged 13--70), we analyze attention patterns across demographic groups. Statistical analysis reveals significant age-based variations (p < 0.05, {\epsilon^2} = 0.042), with older users (36--70) engaging more with textual content and younger users (13--35) interacting more with images. Mouse-tracking data closely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI elements that immediately stand out, validating its use in large-scale studies. We conclude that saliency studies should prioritize gathering data from a larger, demographically representative sample and report exact demographic distributions.",http://arxiv.org/abs/2503.17212v1,CS,Quantitative
Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach,"Background and Objective: Precise preoperative planning and effective physician training for coronary interventions are increasingly important. Despite advances in medical imaging technologies, transforming static or limited dynamic imaging data into comprehensive dynamic cardiac models remains challenging. Existing training systems lack accurate simulation of cardiac physiological dynamics. This study develops a comprehensive dynamic cardiac model research framework based on 4D-CTA, integrating digital twin technology, computer vision, and physical model manufacturing to provide precise, personalized tools for interventional cardiology. Methods: Using 4D-CTA data from a 60-year-old female with three-vessel coronary stenosis, we segmented cardiac chambers and coronary arteries, constructed dynamic models, and implemented skeletal skinning weight computation to simulate vessel deformation across 20 cardiac phases. Transparent vascular physical models were manufactured using medical-grade silicone. We developed cardiac output analysis and virtual angiography systems, implemented guidewire 3D reconstruction using binocular stereo vision, and evaluated the system through angiography validation and CABG training applications. Results: Morphological consistency between virtual and real angiography reached 80.9%. Dice similarity coefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory errors below 1.1 mm. The transparent model demonstrated advantages in CABG training, allowing direct visualization while simulating beating heart challenges. Conclusion: Our patient-specific digital-physical twin approach effectively reproduces both anatomical structures and dynamic characteristics of coronary vasculature, offering a dynamic environment with visual and tactile feedback valuable for education and clinical planning.",http://arxiv.org/abs/2505.10902v1,CS,Quantitative
FAIR-QR: Enhancing Fairness-aware Information Retrieval through Query Refinement,"Information retrieval systems such as open web search and recommendation systems are ubiquitous and significantly impact how people receive and consume online information. Previous research has shown the importance of fairness in information retrieval systems to combat the issue of echo chambers and mitigate the rich-get-richer effect. Therefore, various fairness-aware information retrieval methods have been proposed. Score-based fairness-aware information retrieval algorithms, focusing on statistical parity, are interpretable but could be mathematically infeasible and lack generalizability. In contrast, learning-to-rank-based fairness-aware information retrieval algorithms using fairness-aware loss functions demonstrate strong performance but lack interpretability. In this study, we proposed a novel and interpretable framework that recursively refines query keywords to retrieve documents from underrepresented groups and achieve group fairness. Retrieved documents using refined queries will be re-ranked to ensure relevance. Our method not only shows promising retrieval results regarding relevance and fairness but also preserves interpretability by showing refined keywords used at each iteration.",http://arxiv.org/abs/2503.21092v1,CS,Quantitative
Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch,"Countless new machine learning models are published every year and are reported to significantly advance the state-of-the-art in \emph{top-n} recommendation. However, earlier reproducibility studies indicate that progress in this area may be quite limited. Specifically, various widespread methodological issues, e.g., comparisons with untuned baseline models, have led to an \emph{illusion of progress}. In this work, our goal is to examine whether these problems persist in today's research. To this end, we aim to reproduce the latest advancements reported from applying modern Denoising Diffusion Probabilistic Models to recommender systems, focusing on four models published at the top-ranked SIGIR conference in 2023 and 2024. Our findings are concerning, revealing persistent methodological problems. Alarmingly, through experiments, we find that the latest recommendation techniques based on diffusion models, despite their computational complexity and substantial carbon footprint, are consistently outperformed by simpler existing models. Furthermore, we identify key mismatches between the characteristics of diffusion models and those of the traditional \emph{top-n} recommendation task, raising doubts about their suitability for recommendation. We also note that, in the papers we analyze, the generative capabilities of these models are constrained to a minimum. Overall, our results and continued methodological issues call for greater scientific rigor and a disruptive change in the research and publication culture in this area.",http://arxiv.org/abs/2505.09364v2,CS,Quantitative
Explainable AI-Based Interface System for Weather Forecasting Model,"Machine learning (ML) is becoming increasingly popular in meteorological decision-making. Although the literature on explainable artificial intelligence (XAI) is growing steadily, user-centered XAI studies have not extend to this domain yet. This study defines three requirements for explanations of black-box models in meteorology through user studies: statistical model performance for different rainfall scenarios to identify model bias, model reasoning, and the confidence of model outputs. Appropriate XAI methods are mapped to each requirement, and the generated explanations are tested quantitatively and qualitatively. An XAI interface system is designed based on user feedback. The results indicate that the explanations increase decision utility and user trust. Users prefer intuitive explanations over those based on XAI algorithms even for potentially easy-to-recognize examples. These findings can provide evidence for future research on user-centered XAI algorithms, as well as a basis to improve the usability of AI systems in practice.",http://arxiv.org/abs/2504.00795v1,CS,Mixed
Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle,"Context: ChatGPT and other large language models (LLMs) are widely used across healthcare, business, economics, engineering, and software engineering (SE). Despite their popularity, concerns persist about their reliability, especially their error rates across domains and the software development lifecycle (SDLC). Objective: This study synthesizes and quantifies ChatGPT's reported error rates across major domains and SE tasks aligned with SDLC phases. It provides an evidence-based view of where ChatGPT excels, where it fails, and how reliability varies by task, domain, and model version (GPT-3.5, GPT-4, GPT-4-turbo, GPT-4o). Method: A Multivocal Literature Review (MLR) was conducted, gathering data from academic studies, reports, benchmarks, and grey literature up to 2025. Factual, reasoning, coding, and interpretive errors were considered. Data were grouped by domain and SE phase and visualized using boxplots to show error distributions. Results: Error rates vary across domains and versions. In healthcare, rates ranged from 8% to 83%. Business and economics saw error rates drop from ~50% with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%. Programming success reached 87.5%, though complex debugging still showed over 50% errors. In SE, requirements and design phases showed lower error rates (~5-20%), while coding, testing, and maintenance phases had higher variability (10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability. Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error rates varying by domain, task, and SDLC phase. Full reliance without human oversight remains risky, especially in critical settings. Continuous evaluation and critical validation are essential to ensure reliability and trustworthiness.",http://arxiv.org/abs/2504.18858v1,CS,Quantitative
SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity Learning,"Graph similarity computation (GSC) aims to quantify the similarity score between two graphs. Although recent GSC methods based on graph neural networks (GNNs) take advantage of intra-graph structures in message passing, few of them fully utilize the structures presented by edges to boost the representation of their connected nodes. Moreover, previous cross-graph node embedding matching lacks the perception of the overall structure of the graph pair, due to the fact that the node representations from GNNs are confined to the intra-graph structure, causing the unreasonable similarity score. Intuitively, the cross-graph structure represented in the assignment graph is helpful to rectify the inappropriate matching. Therefore, we propose a structure-enhanced graph matching network (SEGMN). Equipped with a dual embedding learning module and a structure perception matching module, SEGMN achieves structure enhancement in both embedding learning and cross-graph matching. The dual embedding learning module incorporates adjacent edge representation into each node to achieve a structure-enhanced representation. The structure perception matching module achieves cross-graph structure enhancement through assignment graph convolution. The similarity score of each cross-graph node pair can be rectified by aggregating messages from structurally relevant node pairs. Experimental results on benchmark datasets demonstrate that SEGMN outperforms the state-of-the-art GSC methods in the GED regression task, and the structure perception matching module is plug-and-play, which can further improve the performance of the baselines by up to 25%.",http://arxiv.org/abs/2411.03624v1,CS,Quantitative
Architecture of Tianyu Software: Relative Photometry as a Case Study,"Tianyu telescope, an one-meter robotic optical survey instrument to be constructed in Lenghu, Qinghai, China, is designed for detecting transiting exoplanets, variable stars and transients. It requires a highly automated, optimally distributed, easily extendable, and highly flexible software to enable the data processing for the raw data at rates exceeding 500MB/s. In this work, we introduce the architecture of the Tianyu pipeline and use relative photometry as a case to demonstrate its high scalability and efficiency. This pipeline is tested on the data collected from Muguang observatory and Xinglong observatory. The pipeline demonstrates high scalability, with most processing stages increasing in throughput as the number of consumers grows. Compared to a single consumer, the median throughput of image calibration, alignment, and flux extraction increases by 41%, 257%, and 107% respectively when using 5 consumers, while image stacking exhibits limited scalability due to I/O constraints. In our tests, the pipeline was able to detect two transiting sources. Besides, the pipeline captures variability in the light curves of nine known and two previously unknown variable sources in the testing data. Meanwhile, the differential photometric precision of the light curves is near the theoretical limitation. These results indicate that this pipeline is suitable for detecting transiting exoplanets and variable stars. This work builds the fundation for further development of Tianyu software. Code of this work is available at https://github.com/ruiyicheng/Tianyu_pipeline.",http://arxiv.org/abs/2505.09107v2,CS,Quantitative
Understanding the Effect of Agile Practice Quality on Software Product Quality,"Agile methods and associated practices have been held to deliver value to software developers and customers. Research studies have reported team productivity and software quality benefits. While such insights are helpful for understanding how agile methods add value during software development, there is need for understanding the intersection of useful practices and outcomes over project duration. This study addresses this opportunity and conducted an observation study of student projects that was complemented by the analysis of demographics data and open responses about the challenges encountered during the use of agile practices. Data from 22 student teams comprising 85 responses were analyzed using quantitative and qualitative approaches, where among our findings we observed that the use of good coding practices and quality management techniques were positively correlated with all dimensions of product quality (e.g., functionality scope and software packaging). Outcomes also reveal that software product quality was predicted by requirements scoping, team planning and communication, and coding practice. However, high levels of team planning and communication were not necessary for all software development activities. When examining project challenges, it was observed that lack of technical skills and poor time management present most challenges to project success. While these challenges may be mitigated by agile practices, such practices may themselves create unease, requiring balance during project implementation.",http://arxiv.org/abs/2412.15761v1,CS,Mixed
A quantitative framework for evaluating architectural patterns in ML systems,"Contemporary intelligent systems incorporate software components, including machine learning components. As they grow in complexity and data volume such machine learning systems face unique quality challenges like scalability and performance. To overcome them, engineers may often use specific architectural patterns, however their impact on ML systems is difficult to quantify. The effect of software architecture on traditional systems is well studied, however more work is needed in the area of machine learning systems. This study proposes a framework for quantitative assessment of architectural patterns in ML systems, focusing on scalability and performance metrics for cost-effective CPU-based inference. We integrate these metrics into a systematic evaluation process for selection of architectural patterns and demonstrate its application through a case study. The approach shown in the paper should enable software architects to objectively analyze and select optimal patterns, addressing key challenges in ML system design.",http://arxiv.org/abs/2501.11543v1,CS,Mixed
"Perspective of Software Engineering Researchers on Machine Learning Practices Regarding Research, Review, and Education","Context: Machine Learning (ML) significantly impacts Software Engineering (SE), but studies mainly focus on practitioners, neglecting researchers. This overlooks practices and challenges in teaching, researching, or reviewing ML applications in SE. Objective: This study aims to contribute to the knowledge, about the synergy between ML and SE from the perspective of SE researchers, by providing insights into the practices followed when researching, teaching, and reviewing SE studies that apply ML. Method: We analyzed SE researchers familiar with ML or who authored SE articles using ML, along with the articles themselves. We examined practices, SE tasks addressed with ML, challenges faced, and reviewers' and educators' perspectives using grounded theory coding and qualitative analysis. Results: We found diverse practices focusing on data collection, model training, and evaluation. Some recommended practices (e.g., hyperparameter tuning) appeared in less than 20\% of literature. Common challenges involve data handling, model evaluation (incl. non-functional properties), and involving human expertise in evaluation. Hands-on activities are common in education, though traditional methods persist. Conclusion: Despite accepted practices in applying ML to SE, significant gaps remain. By enhancing guidelines, adopting diverse teaching methods, and emphasizing underrepresented practices, the SE community can bridge these gaps and advance the field.",http://arxiv.org/abs/2411.19304v1,CS,Qualitative
When Quantum Meets Classical: Characterizing Hybrid Quantum-Classical Issues Discussed in Developer Forums,"Recent advances in quantum computing have sparked excitement that this new computing paradigm could solve previously intractable problems. However, due to the faulty nature of current quantum hardware and quantum-intrinsic noise, the full potential of quantum computing is still years away. Hybrid quantum-classical computing has emerged as a possible compromise that achieves the best of both worlds. In this paper, we look at hybrid quantum-classical computing from a software engineering perspective and present the first empirical study focused on characterizing and evaluating recurrent issues faced by developers of hybrid quantum-classical applications. The study comprised a thorough analysis of 531 real-world issues faced by developers -- including software faults, hardware failures, quantum library errors, and developer mistakes -- documented in discussion threads from forums dedicated to quantum computing. By qualitatively analyzing such forum threads, we derive a comprehensive taxonomy of recurring issues in hybrid quantum-classical applications that can be used by both application and platform developers to improve the reliability of hybrid applications. The study considered how these recurring issues manifest and their causes, determining that hybrid applications are crash-dominant (74% of studied issues) and that errors were predominantly introduced by application developers (70% of issues). We conclude by identifying recurring obstacles for developers of hybrid applications and actionable recommendations to overcome them.",http://arxiv.org/abs/2411.16884v2,CS,Mixed
Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities,"Counterfactual explanations are a widely used approach in Explainable AI, offering actionable insights into decision-making by illustrating how small changes to input data can lead to different outcomes. Despite their importance, evaluating the quality of counterfactual explanations remains an open problem. Traditional quantitative metrics, such as sparsity or proximity, fail to fully account for human preferences in explanations, while user studies are insightful but not scalable. Moreover, relying only on a single overall satisfaction rating does not lead to a nuanced understanding of why certain explanations are effective or not. To address this, we analyze a dataset of counterfactual explanations that were evaluated by 206 human participants, who rated not only overall satisfaction but also seven explanatory criteria: feasibility, coherence, complexity, understandability, completeness, fairness, and trust. Modeling overall satisfaction as a function of these criteria, we find that feasibility (the actionability of suggested changes) and trust (the belief that the changes would lead to the desired outcome) consistently stand out as the strongest predictors of user satisfaction, though completeness also emerges as a meaningful contributor. Crucially, even excluding feasibility and trust, other metrics explain 58% of the variance, highlighting the importance of additional explanatory qualities. Complexity appears independent, suggesting more detailed explanations do not necessarily reduce satisfaction. Strong metric correlations imply a latent structure in how users judge quality, and demographic background significantly shapes ranking patterns. These insights inform the design of counterfactual algorithms that adapt explanatory qualities to user expertise and domain context.",http://arxiv.org/abs/2504.13899v1,CS,Quantitative
NFRs in Medical Imaging,"The diagnostic imaging departments are under great pressure due to a growing workload. The number of required scans is growing and there is a shortage of qualified labor. AI solutions for medical imaging applications have shown great potential. However, very few diagnostic imaging models have been approved for hospital use and even fewer are being implemented at the hospitals. The most common reason why software projects fail is poor requirement engineering, especially non-functional requirements (NFRs) can be detrimental to a project. Research shows that machine learning professionals struggle to work with NFRs and that there is a need to adapt NFR frameworks to machine learning, AI-based, software. This study uses qualitative methods to interact with key stakeholders to identify which types of NFRs are important for medical imaging applications. The study was done on a single Danish hospital and found that NFRs of type Efficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability, and Fairness were important to the stakeholders. Especially Efficiency since the diagnostic imaging department is trying to spend as little time as possible on each scan.",http://arxiv.org/abs/2411.09718v1,CS,Qualitative
Distinguishing LLM-generated from Human-written Code by Contrastive Learning,"Large language models (LLMs), such as ChatGPT released by OpenAI, have attracted significant attention from both industry and academia due to their demonstrated ability to generate high-quality content for various tasks. Despite the impressive capabilities of LLMs, there are growing concerns regarding their potential risks in various fields, such as news, education, and software engineering. Recently, several commercial and open-source LLM-generated content detectors have been proposed, which, however, are primarily designed for detecting natural language content without considering the specific characteristics of program code. This paper aims to fill this gap by proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a contrastive learning framework and a semantic encoder built with UniXcoder. To assess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a large-scale Human and Machine comparison Corpus (HMCorp), which includes 550K pairs of human-written and ChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs). Based on the HMCorp dataset, our qualitative and quantitative analysis of the characteristics of ChatGPT-generated code reveals the challenge and opportunity of distinguishing ChatGPT-generated code from human-written code with their representative features. Our experimental results indicate that CodeGPTSensor can effectively identify ChatGPT-generated code, outperforming all selected baselines.",http://arxiv.org/abs/2411.04704v1,CS,Mixed
"Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph","The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management.",http://arxiv.org/abs/2505.17507v1,CS,Quantitative
Software Design Pattern Model and Data Structure Algorithm Abilities on Microservices Architecture Design in High-tech Enterprises,"This study investigates the impact of software design model capabilities and data structure algorithm abilities on microservices architecture design within enterprises. Utilizing a qualitative methodology, the research involved in-depth interviews with software architects and developers who possess extensive experience in microservices implementation. The findings reveal that organizations emphasizing robust design models and efficient algorithms achieve superior scalability, performance, and flexibility in their microservices architecture. Notably, participants highlighted that a strong foundation in these areas facilitates better service decomposition, optimizes data processing, and enhances system responsiveness. Despite these insights, gaps remain regarding the integration of emerging technologies and the evolving nature of software design practices. This paper contributes to the existing literature by underscoring the critical role of these competencies in fostering effective microservices architectures and suggests avenues for future research to address identified gaps",http://arxiv.org/abs/2411.04143v1,CS,Qualitative
"Charting the Parrot's Song: A Maximum Mean Discrepancy Approach to Measuring AI Novelty, Originality, and Distinctiveness","Current intellectual property frameworks struggle to evaluate the novelty of AI-generated content, relying on subjective assessments ill-suited for comparing effectively infinite AI outputs against prior art. This paper introduces a robust, quantitative methodology grounded in Maximum Mean Discrepancy (MMD) to measure distributional differences between generative processes. By comparing entire output distributions rather than conducting pairwise similarity checks, our approach directly contrasts creative processes--overcoming the computational challenges inherent in evaluating AI outputs against unbounded prior art corpora. Through experiments combining kernel mean embeddings with domain-specific machine learning representations (LeNet-5 for MNIST digits, CLIP for art), we demonstrate exceptional sensitivity: our method distinguishes MNIST digit classes with 95% confidence using just 5-6 samples and differentiates AI-generated art from human art in the AI-ArtBench dataset (n=400 per category; p<0.0001) using as few as 7-10 samples per distribution despite human evaluators' limited discrimination ability (58% accuracy). These findings challenge the ""stochastic parrot"" hypothesis by providing empirical evidence that AI systems produce outputs from semantically distinct distributions rather than merely replicating training data. Our approach bridges technical capabilities with legal doctrine, offering a pathway to modernize originality assessments while preserving intellectual property law's core objectives. This research provides courts and policymakers with a computationally efficient, legally relevant tool to quantify AI novelty--a critical advancement as AI blurs traditional authorship and inventorship boundaries.",http://arxiv.org/abs/2504.08446v1,CS,Quantitative
Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models,"Recent advancements in artificial intelligence (AI) and machine learning have reignited interest in their impact on Computer-based Learning (CBL). AI-driven tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced learning experiences through personalisation and flexibility. ITSs can adapt to individual learning needs and provide customised feedback based on a student's performance, cognitive state, and learning path. Despite these advances, challenges remain in accommodating diverse learning styles and delivering real-time, context-aware feedback. Our research aims to address these gaps by integrating skill-aligned feedback via Retrieval Augmented Generation (RAG) into prompt engineering for Large Language Models (LLMs) and developing an application to enhance learning through personalised tutoring in a computer science programming context. The pilot study evaluated a proposed system using three quantitative metrics: readability score, response time, and feedback depth, across three programming tasks of varying complexity. The system successfully sorted simulated students into three skill-level categories and provided context-aware feedback. This targeted approach demonstrated better effectiveness and adaptability compared to general methods.",http://arxiv.org/abs/2505.02849v1,CS,Quantitative
"User and Recommender Behavior Over Time: Contextualizing Activity, Effectiveness, Diversity, and Fairness in Book Recommendation","Data is an essential resource for studying recommender systems. While there has been significant work on improving and evaluating state-of-the-art models and measuring various properties of recommender system outputs, less attention has been given to the data itself, particularly how data has changed over time. Such documentation and analysis provide guidance and context for designing and evaluating recommender systems, particularly for evaluation designs making use of time (e.g., temporal splitting). In this paper, we present a temporal explanatory analysis of the UCSD Book Graph dataset scraped from Goodreads, a social reading and recommendation platform active since 2006. We measure the book interaction data using a set of activity, diversity, and fairness metrics; we then train a set of collaborative filtering algorithms on rolling training windows to observe how the same measures evolve over time in the recommendations. Additionally, we explore whether the introduction of algorithmic recommendations in 2011 was followed by observable changes in user or recommender system behavior.",http://arxiv.org/abs/2505.04518v1,CS,Quantitative
Software Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents,"Code large language models (CodeLLMs) and agents have shown great promise in tackling complex software engineering tasks.Compared to traditional software engineering methods, CodeLLMs and agents offer stronger abilities, and can flexibly process inputs and outputs in both natural and code. Benchmarking plays a crucial role in evaluating the capabilities of CodeLLMs and agents, guiding their development and deployment. However, despite their growing significance, there remains a lack of comprehensive reviews of benchmarks for CodeLLMs and agents. To bridge this gap, this paper provides a comprehensive review of existing benchmarks for CodeLLMs and agents, studying and analyzing 181 benchmarks from 461 relevant papers, covering the different phases of the software development life cycle (SDLC). Our findings reveal a notable imbalance in the coverage of current benchmarks, with approximately 60% focused on the software development phase in SDLC, while requirements engineering and software design phases receive minimal attention at only 5% and 3%, respectively. Additionally, Python emerges as the dominant programming language across the reviewed benchmarks. Finally, this paper highlights the challenges of current research and proposes future directions, aiming to narrow the gap between the theoretical capabilities of CodeLLMs and agents and their application in real-world scenarios.",http://arxiv.org/abs/2505.05283v2,CS,Quantitative
Let's Influence Algorithms Together: How Millions of Fans Build Collective Understanding of Algorithms and Organize Coordinated Algorithmic Actions,"Previous research pays attention to how users strategically understand and consciously interact with algorithms but mainly focuses on an individual level, making it difficult to explore how users within communities could develop a collective understanding of algorithms and organize collective algorithmic actions. Through a two-year ethnography of online fan activities, this study investigates 43 core fans who always organize large-scale fans collective actions and their corresponding general fan groups. This study aims to reveal how these core fans mobilize millions of general fans through collective algorithmic actions. These core fans reported the rhetorical strategies used to persuade general fans, the steps taken to build a collective understanding of algorithms, and the collaborative processes that adapt collective actions across platforms and cultures. Our findings highlight the key factors that enable computer-supported collective algorithmic actions and extend collective action research into the large-scale domain targeting algorithms.",http://arxiv.org/abs/2409.10670v2,CS,Qualitative
Wearable AR in Everyday Contexts: Insights from a Digital Ethnography of YouTube Videos,"With growing investment in consumer augmented reality (AR) headsets and glasses, wearable AR is moving from niche applications to everyday use. However, current research primarily examines AR in controlled settings, offering limited insights into its use in real-world daily life. To address this gap, we adopt a digital ethnographic approach, analysing 27 hours of 112 YouTube videos featuring early adopters. These videos capture usage ranging from continuous periods of hours to intermittent use over weeks and months. Our analysis shows that currently, wearable AR is primarily used for media consumption and gaming. While productivity is a desired use case, frequent use is constrained by current hardware limitations and the nascent application ecosystem. Users seek continuity in their digital experience, desiring functionalities similar to those on smartphones, tablets, or computers. We propose implications for everyday AR development that promote adoption while ensuring safe, ethical, and socially-aware integration into daily life.",http://arxiv.org/abs/2502.06191v2,CS,Qualitative
Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey,"Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.",http://arxiv.org/abs/2505.01821v2,CS,Quantitative
Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through Fine-Tuning and Reinforcement Learning,"Existing large language models (LLMs) driven search agents typically rely on prompt engineering to decouple the user queries into search plans, limiting their effectiveness in complex scenarios requiring reasoning. Furthermore, they suffer from excessive token consumption due to Python-based search plan representations and inadequate integration of multimedia elements for both input processing and response generation. To address these challenges, we introduce SearchExpert, a training method for LLMs to improve their multimedia search capabilities in response to complex search queries. Firstly, we reformulate the search plan in an efficient natural language representation to reduce token consumption. Then, we propose the supervised fine-tuning for searching (SFTS) to fine-tune LLM to adapt to these representations, together with an automated dataset construction pipeline. Secondly, to improve reasoning-intensive search capabilities, we propose the reinforcement learning from search feedback (RLSF) that takes the search results planned by LLM as the reward signals. Thirdly, we propose a multimedia understanding and generation agent that enables the fine-tuned LLM to process visual input and produce visual output during inference. Finally, we establish an automated benchmark construction pipeline and a human evaluation framework. Our resulting benchmark, SearchExpertBench-25, comprises 200 multiple-choice questions spanning financial and international news scenarios that require reasoning in searching. Experiments demonstrate that SearchExpert outperforms the commercial LLM search method (Perplexity Pro) by 36.60% on the existing FinSearchBench-24 benchmark and 54.54% on our proposed SearchExpertBench-25. Human evaluations further confirm the superior readability.",http://arxiv.org/abs/2505.18831v1,CS,Quantitative
Optimizing Product Provenance Verification using Data Valuation Methods,"Determining and verifying product provenance remains a critical challenge in global supply chains, particularly as geopolitical conflicts and shifting borders create new incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or agriculture grown on illegally cleared land. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes, has emerged as a powerful tool for geographic origin verification. However, the effectiveness of these models is often constrained by data scarcity and suboptimal dataset selection. In this work, we introduce a novel data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied in SIRA. By prioritizing high-informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. We validate our methodology with extensive experiments, demonstrating its potential to significantly enhance provenance verification, mitigate fraudulent trade practices, and strengthen regulatory enforcement of global supply chains.",http://arxiv.org/abs/2502.15177v2,CS,Quantitative
A Powered Prosthetic Hand with Vision System for Enhancing the Anthropopathic Grasp,"The anthropomorphism of grasping process significantly benefits the experience and grasping efficiency of prosthetic hand wearers. Currently, prosthetic hands controlled by signals such as brain-computer interfaces (BCI) and electromyography (EMG) face difficulties in precisely recognizing the amputees' grasping gestures and executing anthropomorphic grasp processes. Although prosthetic hands equipped with vision systems enables the objects' feature recognition, they lack perception of human grasping intention. Therefore, this paper explores the estimation of grasping gestures solely through visual data to accomplish anthropopathic grasping control and the determination of grasping intention within a multi-object environment. To address this, we propose the Spatial Geometry-based Gesture Mapping (SG-GM) method, which constructs gesture functions based on the geometric features of the human hand grasping processes. It's subsequently implemented on the prosthetic hand. Furthermore, we propose the Motion Trajectory Regression-based Grasping Intent Estimation (MTR-GIE) algorithm. This algorithm predicts pre-grasping object utilizing regression prediction and prior spatial segmentation estimation derived from the prosthetic hand's position and trajectory. The experiments were conducted to grasp 8 common daily objects including cup, fork, etc. The experimental results presented a similarity coefficient $R^{2}$ of grasping process of 0.911, a Root Mean Squared Error ($RMSE$) of 2.47\degree, a success rate of grasping of 95.43$\%$, and an average duration of grasping process of 3.07$\pm$0.41 s. Furthermore, grasping experiments in a multi-object environment were conducted. The average accuracy of intent estimation reached 94.35$\%$. Our methodologies offer a groundbreaking approach to enhance the prosthetic hand's functionality and provides valuable insights for future research.",http://arxiv.org/abs/2412.07105v1,CS,Quantitative
On Enhancing Root Cause Analysis with SQL Summaries for Failures in Database Workload Replays at SAP HANA,"Capturing the workload of a database and replaying this workload for a new version of the database can be an effective approach for regression testing. However, false positive errors caused by many factors such as data privacy limitations, time dependency or non-determinism in multi-threaded environment can negatively impact the effectiveness. Therefore, we employ a machine learning based framework to automate the root cause analysis of failures found during replays. However, handling unseen novel issues not found in the training data is one general challenge of machine learning approaches with respect to generalizability of the learned model. We describe how we continue to address this challenge for more robust long-term solutions. From our experience, retraining with new failures is inadequate due to features overlapping across distinct root causes. Hence, we leverage a large language model (LLM) to analyze failed SQL statements and extract concise failure summaries as an additional feature to enhance the classification process. Our experiments show the F1-Macro score improved by 4.77% for our data. We consider our approach beneficial for providing end users with additional information to gain more insights into the found issues and to improve the assessment of the replay results.",http://arxiv.org/abs/2412.13679v1,CS,Quantitative
Continuous Integration Practices in Machine Learning Projects: The Practitioners` Perspective,"Continuous Integration (CI) is a cornerstone of modern software development. However, while widely adopted in traditional software projects, applying CI practices to Machine Learning (ML) projects presents distinctive characteristics. For example, our previous work revealed that ML projects often experience longer build durations and lower test coverage rates compared to their non-ML counterparts. Building on these quantitative findings, this study surveys 155 practitioners from 47 ML projects to investigate the underlying reasons for these distinctive characteristics through a qualitative perspective. Practitioners highlighted eight key differences, including test complexity, infrastructure requirements, and build duration and stability. Common challenges mentioned by practitioners include higher project complexity, model training demands, extensive data handling, increased computational resource needs, and dependency management, all contributing to extended build durations. Furthermore, ML systems' non-deterministic nature, data dependencies, and computational constraints were identified as significant barriers to effective testing. The key takeaway from this study is that while foundational CI principles remain valuable, ML projects require tailored approaches to address their unique challenges. To bridge this gap, we propose a set of ML-specific CI practices, including tracking model performance metrics and prioritizing test execution within CI pipelines. Additionally, our findings highlight the importance of fostering interdisciplinary collaboration to strengthen the testing culture in ML projects. By bridging quantitative findings with practitioners' insights, this study provides a deeper understanding of the interplay between CI practices and the unique demands of ML projects, laying the groundwork for more efficient and robust CI strategies in this domain.",http://arxiv.org/abs/2502.17378v1,CS,Mixed
Generative AI and Empirical Software Engineering: A Paradigm Shift,"The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field. This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.",http://arxiv.org/abs/2502.08108v1,CS,Mixed
Heterogeneous Population Encoding for Multi-joint Regression using sEMG Signals,"Regression-based decoding of continuous movements is essential for human-machine interfaces (HMIs), such as prosthetic control. This study explores a feature-based approach to encoding Surface Electromyography (sEMG) signals, focusing on the role of variability in neural-inspired population encoding. By employing heterogeneous populations of Leaky Integrate-and- Fire (LIF) neurons with varying sizes and diverse parameter distributions, we investigate how population size and variability in encoding parameters, such as membrane time constants and thresholds, influence decoding performance. Using a simple linear readout, we demonstrate that variability improves robustness and generalizability compared to single-neuron encoders. These findings emphasize the importance of optimizing variability and population size for efficient and scalable regression tasks in spiking neural networks (SNNs), paving the way for robust, low-power HMI implementations.",http://arxiv.org/abs/2501.15347v1,CS,Quantitative
Using Artificial Intelligence to Improve Classroom Learning Experience,"This paper explores advancements in Artificial Intelligence technologies to enhance classroom learning, highlighting contributions from companies like IBM, Microsoft, Google, and ChatGPT, as well as the potential of brain signal analysis. The focus is on improving students learning experiences by using Machine Learning algorithms to : identify a student preferred learning style and predict academic dropout risk. A Logistic Regression algorithm is applied for binary classification using six predictor variables, such as assessment scores, lesson duration, and preferred learning style, to accurately identify learning preferences. A case study, with 76,519 candidates and 35 predictor variables, assesses academic dropout risk using Logistic Regression, achieving a test accuracy of 87.39%. In comparison, the Stochastic Gradient Descent classifier achieved an accuracy of 83.1% on the same dataset.",http://arxiv.org/abs/2503.05709v1,CS,Mixed
Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks,"Reducing the memory footprint of Machine Learning (ML) models, especially Deep Neural Networks (DNNs), is imperative to facilitate their deployment on resource-constrained edge devices. However, a notable drawback of DNN models lies in their susceptibility to adversarial attacks, wherein minor input perturbations can deceive them. A primary challenge revolves around the development of accurate, resilient, and compact DNN models suitable for deployment on resource-constrained edge devices. This paper presents the outcomes of a compact DNN model that exhibits resilience against both black-box and white-box adversarial attacks. This work has achieved this resilience through training with the QKeras quantization-aware training framework. The study explores the potential of QKeras and an adversarial robustness technique, Jacobian Regularization (JR), to co-optimize the DNN architecture through per-layer JR methodology. As a result, this paper has devised a DNN model employing this co-optimization strategy based on Stochastic Ternary Quantization (STQ). Its performance was compared against existing DNN models in the face of various white-box and black-box attacks. The experimental findings revealed that, the proposed DNN model had small footprint and on average, it exhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T) benchmarks when challenged with white-box and black-box attacks, respectively, on the CIFAR-10 image and Google Speech Commands audio datasets.",http://arxiv.org/abs/2503.08973v1,CS,Quantitative
What Happened in This Pipeline? Diffing Build Logs with CiDiff,"Continuous integration (CI) is widely used by developers to ensure the quality and reliability of their software projects. However, diagnosing a CI regression is a tedious process that involves the manual analysis of lengthy build logs. In this paper, we explore how textual differencing can support the debugging of CI regressions. As off-the-shelf diff algorithms produce suboptimal results, in this work we introduce a new diff algorithm specifically tailored to build logs called CiDiff. We evaluate CiDiff against several baselines on a novel dataset of 17 906 CI regressions, performing an accuracy study, a quantitative study and a user-study. Notably, our algorithm reduces the number of lines to inspect by about 60 % in the median case, with reasonable overhead compared to the state-of-practice LCS-diff. Finally, our algorithm is preferred by the majority of participants in 70 % of the regression cases, whereas LCS-diff is preferred in only 5 % of the cases.",http://arxiv.org/abs/2504.18182v1,CS,Quantitative
Exploring and Evaluating Interplays of BPpy with Deep Reinforcement Learning and Formal Methods,"We explore and evaluate the interactions between Behavioral Programming (BP) and a range of Artificial Intelligence (AI) and Formal Methods (FM) techniques. Our goal is to demonstrate that BP can serve as an abstraction that integrates various techniques, enabling a multifaceted analysis and a rich development process. Specifically, the paper examines how the BPpy framework, a Python-based implementation of BP, is enhanced by and enhances various FM and AI tools. We assess how integrating BP with tools such as Satisfiability Modulo Theory (SMT) solvers, symbolic and probabilistic model checking, and Deep Reinforcement Learning (DRL) allow us to scale the abilities of BP to model complex systems. Additionally, we illustrate how developers can leverage multiple tools within a single modeling and development task. The paper provides quantitative and qualitative evidence supporting the feasibility of our vision to create a comprehensive toolbox for harnessing AI and FM methods in a unified development framework.",http://arxiv.org/abs/2501.15480v1,CS,Mixed
Deep Learning-Based Identification of Inconsistent Method Names: How Far Are We?,"Concise and meaningful method names are crucial for program comprehension and maintenance. However, method names may become inconsistent with their corresponding implementations, causing confusion and errors. Several deep learning (DL)-based approaches have been proposed to identify such inconsistencies, with initial evaluations showing promising results. However, these evaluations typically use a balanced dataset, where the number of inconsistent and consistent names are equal. This setup, along with flawed dataset construction, leads to false positives, making reported performance less reliable in real-world scenarios, where most method names are consistent. In this paper, we present an empirical study that evaluates state-of-the-art DL-based methods for identifying inconsistent method names. We create a new benchmark by combining automatic identification from commit histories and manual developer inspections, reducing false positives. We evaluate five representative DL approaches (one retrieval-based and four generation-based) on this benchmark. Our results show that performance drops substantially when moving from the balanced dataset to the new benchmark. We further conduct quantitative and qualitative analyses to understand the strengths and weaknesses of the approaches. Retrieval-based methods perform well on simple methods and those with popular name sub-tokens but fail due to inefficient representation techniques. Generation-based methods struggle with inaccurate similarity calculations and immature name generation. Based on these findings, we propose improvements using contrastive learning and large language models (LLMs). Our study suggests that significant improvements are needed before these DL approaches can be effectively applied to real-world software systems.",http://arxiv.org/abs/2501.12617v1,CS,Mixed
GBSVR: Granular Ball Support Vector Regression,"Support Vector Regression (SVR) and its variants are widely used to handle regression tasks, however, since their solution involves solving an expensive quadratic programming problem, it limits its application, especially when dealing with large datasets. Additionally, SVR uses an epsilon-insensitive loss function which is sensitive to outliers and therefore can adversely affect its performance. We propose Granular Ball Support Vector Regression (GBSVR) to tackle problem of regression by using granular ball concept. These balls are useful in simplifying complex data spaces for machine learning tasks, however, to the best of our knowledge, they have not been sufficiently explored for regression problems. Granular balls group the data points into balls based on their proximity and reduce the computational cost in SVR by replacing the large number of data points with far fewer granular balls. This work also suggests a discretization method for continuous-valued attributes to facilitate the construction of granular balls. The effectiveness of the proposed approach is evaluated on several benchmark datasets and it outperforms existing state-of-the-art approaches",http://arxiv.org/abs/2503.10539v1,CS,Quantitative
ACE: A Cardinality Estimator for Set-Valued Queries,"Cardinality estimation is a fundamental functionality in database systems. Most existing cardinality estimators focus on handling predicates over numeric or categorical data. They have largely omitted an important data type, set-valued data, which frequently occur in contemporary applications such as information retrieval and recommender systems. The few existing estimators for such data either favor high-frequency elements or rely on a partial independence assumption, which limits their practical applicability. We propose ACE, an Attention-based Cardinality Estimator for estimating the cardinality of queries over set-valued data. We first design a distillation-based data encoder to condense the dataset into a compact matrix. We then design an attention-based query analyzer to capture correlations among query elements. To handle variable-sized queries, a pooling module is introduced, followed by a regression model (MLP) to generate final cardinality estimates. We evaluate ACE on three datasets with varying query element distributions, demonstrating that ACE outperforms the state-of-the-art competitors in terms of both accuracy and efficiency.",http://arxiv.org/abs/2503.14929v1,CS,Quantitative
The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses,"This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings.",http://arxiv.org/abs/2504.07422v1,CS,Quantitative
Spatially Directional Dual-Attention GAT for Spatial Fluoride Health Risk Modeling,"Environmental exposure to fluoride is a major public health concern, particularly in regions with naturally elevated fluoride concentrations. Accurate modeling of fluoride-related health risks, such as dental fluorosis, requires spatially aware learning frameworks capable of capturing both geographic and semantic heterogeneity. In this work, we propose Spatially Directional Dual-Attention Graph Attention Network (SDD-GAT), a novel spatial graph neural network designed for fine-grained health risk prediction. SDD-GAT introduces a dual-graph architecture that disentangles geographic proximity and attribute similarity, and incorporates a directional attention mechanism that explicitly encodes spatial orientation and distance into the message passing process. To further enhance spatial coherence, we introduce a spatial smoothness regularization term that enforces consistency in predictions across neighboring locations. We evaluate SDD-GAT on a large-scale dataset covering over 50,000 fluoride monitoring samples and fluorosis records across Guizhou Province, China. Results show that SDD-GAT significantly outperforms traditional models and state-of-the-art GNNs in both regression and classification tasks, while also exhibiting improved spatial autocorrelation as measured by Moran's I. Our framework provides a generalizable foundation for spatial health risk modeling and geospatial learning under complex environmental settings.",http://arxiv.org/abs/2504.09416v1,CS,Quantitative
Evaluation of the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis in 19 Programming Languages,"This bachelor's thesis examines the capabilities of ChatGPT 4 in code generation across 19 programming languages. The study analyzed solution rates across three difficulty levels, types of errors encountered, and code quality in terms of runtime and memory efficiency through a quantitative experiment. A total of 188 programming problems were selected from the LeetCode platform, and ChatGPT 4 was given three attempts to produce a correct solution with feedback. ChatGPT 4 successfully solved 39.67% of all tasks, with success rates decreasing significantly as problem complexity increased. Notably, the model faced considerable challenges with hard problems across all languages. ChatGPT 4 demonstrated higher competence in widely used languages, likely due to a larger volume and higher quality of training data. The solution rates also revealed a preference for languages with low abstraction levels and static typing. For popular languages, the most frequent error was ""Wrong Answer,"" whereas for less popular languages, compiler and runtime errors prevailed, suggesting frequent misunderstandings and confusion regarding the structural characteristics of these languages. The model exhibited above-average runtime efficiency in all programming languages, showing a tendency toward statically typed and low-abstraction languages. Memory efficiency results varied significantly, with above-average performance in 14 languages and below-average performance in five languages. A slight preference for low-abstraction languages and a leaning toward dynamically typed languages in terms of memory efficiency were observed. Future research should include a larger number of tasks, iterations, and less popular languages. Additionally, ChatGPT 4's abilities in code interpretation and summarization, debugging, and the development of complex, practical code could be analyzed further. ---- Diese Bachelorarbeit untersucht die F\""ahigkeiten von ChatGPT 4 zur Code-Generierung in 19 Programmiersprachen. Betrachtet wurden die L\""osungsraten zwischen drei Schwierigkeitsgraden, die aufgetretenen Fehlerarten und die Qualit\""at des Codes hinsichtlich der Laufzeit- und Speichereffizienz in einem quantitativen Experiment. Dabei wurden 188 Programmierprobleme der Plattform LeetCode entnommen, wobei ChatGPT 4 jeweils drei Versuche hatte, mittels Feedback eine korrekte L\""osung zu generieren. ChatGPT 4 l\""oste 39,67 % aller Aufgaben erfolgreich, wobei die Erfolgsrate mit zunehmendem Schwierigkeitsgrad deutlich abnahm und bei komplexen Problemen in allen Sprachen signifikante Schwierigkeiten auftraten. Das Modell zeigte eine h\""ohere Kompetenz in weit verbreiteten Sprachen, was wahrscheinlich auf eine gr\""o{ }ere Menge und h\""ohere Qualit\""at der Trainingsdaten zur\""uckzuf\""uhren ist. Bez\""uglich der L\""osungsraten zeigte das Modell zudem eine Pr\""aferenz f\""ur Sprachen mit niedrigem Abstraktionsniveau und statischer Typisierung. Bei Sprachen hoher Popularit\""at trat der Fehler Wrong Answer am h\""aufigsten auf, w\""ahrend bei weniger popul\""aren Sprachen Compiler- und Laufzeitfehler \""uberwogen, was auf h\""aufige Missverst\""andnisse und Verwechslungen bez\""uglich der spezifischen strukturellen Eigenschaften dieser Sprachen zur\""uckzuf\""uhren ist. ChatGPT 4 demonstrierte in allen Programmiersprachen eine \""uberdurchschnittliche Laufzeiteffizienz und tendierte diesbez\""uglich erneut zu statisch typisierten und niedrig abstrahierten Sprachen. Die Werte zur Speichereffizienz variierten erheblich, wobei in 14 Sprachen \""uberdurchschnittliche und in f\""unf Sprachen unterdurchschnittliche Werte erzielt wurden. Es zeigte sich diesbez\""uglich eine leichte Tendenz zugunsten von niedrig abstrahierten sowie eine Pr\""aferenz zu dynamisch typisierten Sprachen. Zuk\""unftige Forschung sollte eine h\""ohere Anzahl an Aufgaben, Iterationen und unpopul\""aren Sprachen einbeziehen. Dar\""uber hinaus k\""onnten die F\""ahigkeiten von ChatGPT 4 in der Code-Interpretation und -Zusammenfassung, im Debugging und in der Entwicklung komplexer, praxisbezogener Codes analysiert werden.",http://arxiv.org/abs/2501.02338v1,CS,Quantitative
Direct Advantage Regression: Aligning LLMs with Online AI Reward,"Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines.",http://arxiv.org/abs/2504.14177v1,CS,Quantitative
Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation,"We introduce a Dual-Flow Generative Ranking Network (DFGR) for recommendation scenarios. This architecture utilizes only raw user behavior sequence information together with a small amount of basic information describing the behaviors to address the limitations of Deep Learning Recommendation Models (DLRMs) that rely on extensive manual feature engineering. DFGR employs a dual-flow mechanism to optimize interaction modeling, ensuring efficient training and inference through end-to-end token processing. It duplicates the original user behavior sequence into a real flow and a fake flow based on whether the action information used is authentic and then defines a novel interaction method between the real flow and the fake flow within the QKV module of the self-attention mechanism. This design reduces computational overhead and improves both training efficiency and inference performance compared to Meta's HSTU-based model which can be considered the current state-of-the-art (SOTA) model in generative ranking. Our experiments in open-source and real industrial datasets show that DFGR outperforms DLRM, which can be regarded as an industrial online baseline that uses extensive feature engineering, Meta's HSTU approaches, and common recommendation architectures such as DIN, DCN, DIEN, and DeepFM. We also investigate optimal parameter allocation strategies under computational constraints, establishing DFGR as an efficient and effective next-generation generative ranking paradigm.",http://arxiv.org/abs/2505.16752v2,CS,Quantitative
NRevisit: A Cognitive Behavioral Metric for Code Understandability Assessment,"Measuring code understandability is both highly relevant and exceptionally challenging. This paper proposes a dynamic code understandability assessment method, which estimates a personalized code understandability score from the perspective of the specific programmer handling the code. The method consists of dynamically dividing the code unit under development or review in code regions (invisible to the programmer) and using the number of revisits (NRevisit) to each region as the primary feature for estimating the code understandability score. This approach removes the uncertainty related to the concept of a ""typical programmer"" assumed by static software code complexity metrics and can be easily implemented using a simple, low-cost, and non-intrusive desktop eye tracker or even a standard computer camera. This metric was evaluated using cognitive load measured through electroencephalography (EEG) in a controlled experiment with 35 programmers. Results show a very high correlation ranging from rs = 0.9067 to rs = 0.9860 (with p nearly 0) between the scores obtained with different alternatives of NRevisit and the ground truth represented by the EEG measurements of programmers' cognitive load, demonstrating the effectiveness of our approach in reflecting the cognitive effort required for code comprehension. The paper also discusses possible practical applications of NRevisit, including its use in the context of AI-generated code, which is already widely used today.",http://arxiv.org/abs/2504.18345v1,CS,Quantitative
The Design Space of in-IDE Human-AI Experience,"Nowadays, integration of AI-driven tools within Integrated Development Environments (IDEs) is reshaping the software development lifecycle. Existing research highlights that users expect these tools to be efficient, context-aware, accurate, user-friendly, customizable, and secure. However, a major gap remains in understanding developers' needs and challenges, particularly when interacting with AI systems in IDEs and from the perspectives of different user groups. In this work, we address this gap through structured interviews with 35 developers from three different groups: Adopters, Churners, and Non-Users of AI in IDEs to create a comprehensive Design Space of in-IDE Human-AI Experience. Our results highlight key areas of Technology Improvement, Interaction, and Alignment in in-IDE AI systems, as well as Simplifying Skill Building and Programming Tasks. Our key findings stress the need for AI systems that are more personalized, proactive, and reliable. We also emphasize the importance of context-aware and privacy-focused solutions and better integration with existing workflows. Furthermore, our findings show that while Adopters appreciate advanced features and non-interruptive integration, Churners emphasize the need for improved reliability and privacy. Non-Users, in contrast, focus on skill development and ethical concerns as barriers to adoption. Lastly, we provide recommendations for industry practitioners aiming to enhance AI integration within developer workflows.",http://arxiv.org/abs/2410.08676v1,CS,Qualitative
Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?,"Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.",http://arxiv.org/abs/2504.20679v1,CS,Mixed
The Code Barrier: What LLMs Actually Understand?,"Understanding code represents a core ability needed for automating software development tasks. While foundation models like LLMs show impressive results across many software engineering challenges, the extent of their true semantic understanding beyond simple token recognition remains unclear. This research uses code obfuscation as a structured testing framework to evaluate LLMs' semantic understanding capabilities. We methodically apply controlled obfuscation changes to source code and measure comprehension through two complementary tasks: generating accurate descriptions of obfuscated code and performing deobfuscation, a skill with important implications for reverse engineering applications. Our testing approach includes 13 cutting-edge models, covering both code-specialized (e.g., StarCoder2) and general-purpose (e.g., GPT-4o) architectures, evaluated on a benchmark created from CodeNet and consisting of filtered 250 Java programming problems and their solutions. Findings show a statistically significant performance decline as obfuscation complexity increases, with unexpected resilience shown by general-purpose models compared to their code-focused counterparts. While some models successfully identify obfuscation techniques, their ability to reconstruct the underlying program logic remains constrained, suggesting limitations in their semantic representation mechanisms. This research introduces a new evaluation approach for assessing code comprehension in language models and establishes empirical baselines for advancing research in security-critical code analysis applications such as reverse engineering and adversarial code analysis.",http://arxiv.org/abs/2504.10557v1,CS,Quantitative
oneDAL Optimization for ARM Scalable Vector Extension: Maximizing Efficiency for High-Performance Data Science,"The evolution of ARM-based architectures, particularly those incorporating Scalable Vector Extension (SVE), has introduced transformative opportunities for high-performance computing (HPC) and machine learning (ML) workloads. The Unified Acceleration Foundation's (UXL) oneAPI Data Analytics Library (oneDAL) is a widely adopted library for accelerating ML and data analytics workflows, but its reliance on Intel's proprietary Math Kernel Library (MKL) has traditionally limited its compatibility to x86platforms. This paper details the porting of oneDAL to ARM architectures with SVE support, using OpenBLAS as an alternative backend to overcome architectural and performance challenges. Beyond porting, the research introduces novel ARM-specific optimizations, including custom sparse matrix routines, vectorized statistical functions, and a Scalable Vector Extension (SVE)-optimized Support Vector Machine (SVM) algorithm. The SVM enhancements leverage SVE's flexible vector lengths and predicate driven execution, achieving notable performance gains of 22% for the Boser method and 5% for the Thunder method. Benchmarks conducted on ARM SVE-enabled AWSGraviton3 instances showcase up to 200x acceleration in ML training and inference tasks compared to the original scikit-learn implementation on the ARM platform. Moreover, the ARM-optimized oneDAL achieves performance parity with, and in some cases exceeds, the x86 oneDAL implementation (MKL backend) on IceLake x86 systems, which are nearly twice as costly as AWSGraviton3 ARM instances. These findings highlight ARM's potential as a high-performance, energyefficient platform for dataintensive ML applications. By expanding cross-architecture compatibility and contributing to the opensource ecosystem, this work reinforces ARM's position as a competitive alternative in the HPC and ML domains, paving the way for future advancements in dataintensive computing.",http://arxiv.org/abs/2504.04241v1,CS,Quantitative
SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale,"Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.",http://arxiv.org/abs/2505.20094v1,CS,Quantitative
Ethnography and Machine Learning: Synergies and New Directions,"Ethnography (social scientific methods that illuminate how people understand, navigate and shape the real world contexts in which they live their lives) and machine learning (computational techniques that use big data and statistical learning models to perform quantifiable tasks) are each core to contemporary social science. Yet these tools have remained largely separate in practice. This chapter draws on a growing body of scholarship that argues that ethnography and machine learning can be usefully combined, particularly for large comparative studies. Specifically, this paper (a) explains the value (and challenges) of using machine learning alongside qualitative field research for certain types of projects, (b) discusses recent methodological trends to this effect, (c) provides examples that illustrate workflow drawn from several large projects, and (d) concludes with a roadmap for enabling productive coevolution of field methods and machine learning.",http://arxiv.org/abs/2412.06087v1,CS,Mixed
"Model-Guided Fieldwork: A Practical, Methodological and Philosophical Investigation in the use of Ethnomethodology for Engineering Software Requirements","Ethnomethodological fieldwork has long been acknowledged as a potentially valuable way of informing the design of technology. However, there is relatively little methodological support for this activity, particularly in relation to the systematic approaches to development advocated in mainstream software and requirements engineering. This thesis focuses on the use of ethnomethodological fieldwork for the engineering of software requirements. Firstly, it proposes an approach, dubbed ""Model Guided Fieldwork,"" to support a fieldworker in making observations that may contribute to a technological development process. It does this by supplementing the normal debriefing sessions that a fieldworker and a technologist might have, with a lightweight iterative system modelling exercise, in such a way that the fieldwork and modelling can be mutually guiding. Secondly, the thesis presents an application of this approach in a high-profile e-Science project. This case study provides an opportunity to examine the relationship between ethnomethodological ethnography and requirements engineering empirically. Thirdly, the thesis addresses a number of theoretical and philosophical concerns relating to its project. This consists in a number of clarifications and counterarguments which aim to better situate ethnomethodological fieldwork as a method of requirements elicitation. In these three regards the thesis constitutes a practical methodological and philosophical investigation into the topic at hand.",http://arxiv.org/abs/2411.09303v1,CS,Mixed
Malinowski in the Age of AI: Can large language models create a text game based on an anthropological classic?,"Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4 have shown remarkable abilities in a wide range of tasks such as summarizing texts and assisting in coding. Scientific research has demonstrated that these models can also play text-adventure games. This study aims to explore whether LLMs can autonomously create text-based games based on anthropological classics, evaluating also their effectiveness in communicating knowledge. To achieve this, the study engaged anthropologists in discussions to gather their expectations and design inputs for an anthropologically themed game. Through iterative processes following the established HCI principle of 'design thinking', the prompts and the conceptual framework for crafting these games were refined. Leveraging GPT3.5, the study created three prototypes of games centered around the seminal anthropological work of the social anthropologist's Bronislaw Malinowski's ""Argonauts of the Western Pacific"" (1922). Subsequently, evaluations were conducted by inviting senior anthropologists to playtest these games, and based on their inputs, the game designs were refined. The tests revealed promising outcomes but also highlighted key challenges: the models encountered difficulties in providing in-depth thematic understandings, showed suspectibility to misinformation, tended towards monotonic responses after an extended period of play, and struggled to offer detailed biographical information. Despite these limitations, the study's findings open up new research avenues at the crossroads of artificial intelligence, machine learning, LLMs, ethnography, anthropology and human-computer interaction.",http://arxiv.org/abs/2410.20536v1,CS,Qualitative
Teaching and Learning Ethnography for Software Engineering Contexts,"Ethnography has become one of the established methods for empirical research on software engineering. Although there is a wide variety of introductory books available, there has been no material targeting software engineering students particularly, until now. In this chapter we provide an introduction to teaching and learning ethnography for faculty teaching ethnography to software engineering graduate students and for the students themselves of such courses. The contents of the chapter focuses on what we think is the core basic knowledge for newbies to ethnography as a research method. We complement the text with proposals for exercises, tips for teaching, and pitfalls that we and our students have experienced. The chapter is designed to support part of a course on empirical software engineering and provides pointers and literature for further reading.",http://arxiv.org/abs/2407.04596v1,CS,Mixed
Towards Continuous Systematic Literature Review in Software Engineering,"Context: New scientific evidence continuously arises with advances in Software Engineering (SE) research. Conventionally, Systematic Literature Reviews (SLRs) are not updated or updated intermittently, leaving gaps between updates, during which time the SLR may be missing crucial new evidence. Goal: We propose and evaluate a concept and process called Continuous Systematic Literature Review (CSLR) in SE. Method: To elaborate on the CSLR concept and process, we performed a synthesis of evidence by conducting a meta-ethnography, addressing knowledge from varied research areas. Furthermore, we conducted a case study to evaluate the CSLR process. Results: We describe the resulting CSLR process in BPMN format. The case study results provide indications on the importance and feasibility of applying CSLR in practice to continuously update SLR evidence in SE. Conclusion: The CSLR concept and process provide a feasible and systematic way to continuously incorporate new evidence into SLRs, supporting trustworthy and up-to-date evidence for SLRs in SE.",http://arxiv.org/abs/2206.04177v1,CS,Qualitative
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence","In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review that highlights the cross-referencing of these angles by discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical facts, current world circumstances, and possible ensuing realities.",http://arxiv.org/abs/2204.07612v2,CS,Mixed
A City upon a Hill: Casting Light on a Real Experimental Process,"Context: The overall scientific community is proposing measures to improve the reproducibility and replicability of experiments. Reproducibility is relatively easy to achieve. However, replicability is considerably more complex in both the sciences and Empirical Software Engineering (ESE). Several strategies, e.g., replication packages and families of experiments, have been proposed to improve replication in ESE, with limited success. We wonder whether the failures are due to some mismatch, i.e., the researchers' needs are not satisfied by the proposed replication procedures. Objectives: Find out how experimental researchers conduct \textit{experiments in practice}. Methods: We carried out an ethnography study within a SE Research Group. Our main activity was to observe/approach the experimental researchers in their day-to-day settings for two years. Their preferred literature and experimental materials were studied. We used individual and group interviews to gain understanding and examine unclear topics in-depth. Results: We have created conceptual and process models that represent how experimentation is really conducted in the Research Group. Models fit the community's procedures and terminology at a high level, but they become particular in their minute details. Conclusion: The actual experimental process differs from textbooks in several points, namely: (1) Number and diversity of activities, (2) existence of different roles, (3) the granularity of the concepts used by the roles, and (4) the viewpoints that different sub-areas or families of experiments have about the overall process.",http://arxiv.org/abs/2108.12800v1,CS,Mixed
Case Survey Studies in Software Engineering Research,"Background: Given the social aspects of Software Engineering (SE), in the last twenty years, researchers from the field started using research methods common in social sciences such as case study, ethnography, and grounded theory. More recently, case survey, another imported research method, has seen its increasing use in SE studies. It is based on existing case studies reported in the literature and intends to harness the generalizability of survey and the depth of case study. However, little is known on how case survey has been applied in SE research, let alone guidelines on how to employ it properly. Aims: This article aims to provide a better understanding of how case survey has been applied in Software Engineering research. Method: To address this knowledge gap, we performed a systematic mapping study and analyzed 12 Software Engineering studies that used the case survey method. Results: Our findings show that these studies presented a heterogeneous understanding of the approach ranging from secondary studies to primary inquiries focused on a large number of instances of a research phenomenon. They have not applied the case survey method consistently as defined in the seminal methodological papers. Conclusions: We conclude that a set of clearly defined guidelines are needed on how to use case survey in SE research, to ensure the quality of the studies employing this approach and to provide a set of clearly defined criteria to evaluate such work.",http://arxiv.org/abs/2007.13592v1,CS,Mixed
Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits,"Reinforcement learning with outcome-based feedback faces a fundamental challenge: when rewards are only observed at trajectory endpoints, how do we assign credit to the right actions? This paper provides the first comprehensive analysis of this problem in online RL with general function approximation. We develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the coverability coefficient of the underlying MDP. By leveraging general function approximation, our approach works effectively in large or infinite state spaces where tabular methods fail, requiring only that value functions and reward functions can be represented by appropriate function classes. Our results also characterize when outcome-based feedback is statistically separated from per-step rewards, revealing an unavoidable exponential separation for certain MDPs. For deterministic MDPs, we show how to eliminate the completeness assumption, dramatically simplifying the algorithm. We further extend our approach to preference-based feedback settings, proving that equivalent statistical efficiency can be achieved even under more limited information. Together, these results constitute a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning.",http://arxiv.org/abs/2505.20268v1,CS,Quantitative
Learning with Expected Signatures: Theory and Applications,"The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This ""model-free"" embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance.",http://arxiv.org/abs/2505.20465v1,CS,Quantitative
Scalable and adaptive prediction bands with kernel sum-of-squares,"Conformal Prediction (CP) is a popular framework for constructing prediction bands with valid coverage in finite samples, while being free of any distributional assumption. A well-known limitation of conformal prediction is the lack of adaptivity, although several works introduced practically efficient alternate procedures. In this work, we build upon recent ideas that rely on recasting the CP problem as a statistical learning problem, directly targeting coverage and adaptivity. This statistical learning problem is based on reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS) methods. First, we extend previous results with a general representer theorem and exhibit the dual formulation of the learning problem. Crucially, such dual formulation can be solved efficiently by accelerated gradient methods with several hundreds or thousands of samples, unlike previous strategies based on off-the-shelf semidefinite programming algorithms. Second, we introduce a new hyperparameter tuning strategy tailored specifically to target adaptivity through bounds on test-conditional coverage. This strategy, based on the Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune kernel lengthscales in our framework, but has broader applicability since it could be used in any CP algorithm where the score function is learned. Finally, extensive experiments are conducted to show how our method compares to related work. All figures can be reproduced with the accompanying code.",http://arxiv.org/abs/2505.21039v1,CS,Quantitative
Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge,"Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.",http://arxiv.org/abs/2505.20658v1,CS,Quantitative
Methodology for GPU Frequency Switching Latency Measurement,"The development of exascale and post-exascale HPC and AI systems integrates thousands of CPUs and specialized accelerators, making energy optimization critical as power costs rival hardware expenses. To reduce consumption, frequency and voltage scaling techniques are widely used, but their effectiveness depends on adapting to application demands in real-time. However, frequency scaling incurs a switching latency, impacting the responsiveness of dynamic tuning approaches. We propose a methodology to systematically evaluate the frequency switching latency of accelerators, with an implementation for CUDA. Our approach employs an artificial iterative workload designed for precise runtime measurements at different frequencies. The methodology consists of three phases: (1) measuring workload execution time across target frequencies, (2) determining switching latency by tracking the transition from an initial to a target frequency, and (3) filtering out outliers caused by external factors such as CUDA driver management or CPU interruptions. A robust statistical system ensures accuracy while minimizing execution time. We demonstrate this methodology on three Nvidia GPUs - GH200, A100, and RTX Quadro 6000 - revealing significant variations in switching latency. These findings are crucial for designing energy-efficient runtime systems, helping determine optimal frequency change rates and avoiding transitions with excessive overhead.",http://arxiv.org/abs/2502.20075v1,CS,Quantitative
Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models,"This paper studies the task of estimating heterogeneous treatment effects in causal panel data models, in the presence of covariate effects. We propose a novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models, that employs flexible model structures and powerful neural network architectures to cohesively deal with the underlying heterogeneity and nonlinearity of both panel units and covariate effects. The proposed CoDEAL integrates nonlinear covariate effect components (parameterized by a feed-forward neural network) with nonlinear factor structures (modeled by a multi-output autoencoder) to form a heterogeneous causal panel model. The nonlinear covariate component offers a flexible framework for capturing the complex influences of covariates on outcomes. The nonlinear factor analysis enables CoDEAL to effectively capture both cross-sectional and temporal dependencies inherent in the data panel. This latent structural information is subsequently integrated into a customized matrix completion algorithm, thereby facilitating more accurate imputation of missing counterfactual outcomes. Moreover, the use of a multi-output autoencoder explicitly accounts for heterogeneity across units and enhances the model interpretability of the latent factors. We establish theoretical guarantees on the convergence of the estimated counterfactuals, and demonstrate the compelling performance of the proposed method using extensive simulation studies and a real data application.",http://arxiv.org/abs/2505.20536v1,CS,Quantitative
Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy,"Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce non-determinism unlike traditional or machine learning software, requiring new approaches to verifying correctness beyond simple output comparisons or statistical accuracy over test datasets. This paper presents a taxonomy for LLM test case design, informed by both the research literature, our experience, and open-source tools that represent the state of practice. We identify key variation points that impact test correctness and highlight open challenges that the research, industry, and open-source communities must address as LLMs become integral to software systems. Our taxonomy defines four facets of LLM test case design, addressing ambiguity in both inputs and outputs while establishing best practices. It distinguishes variability in goals, the system under test, and inputs, and introduces two key oracle types: atomic and aggregated. Our mapping indicates that current tools insufficiently account for these variability points, highlighting the need for closer collaboration between academia and practitioners to improve the reliability and reproducibility of LLM testing.",http://arxiv.org/abs/2503.00481v1,CS,Quantitative
GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation,"Current personalized recommender systems predominantly rely on static offline data for algorithm design and evaluation, significantly limiting their ability to capture long-term user preference evolution and social influence dynamics in real-world scenarios. To address this fundamental challenge, we propose a high-fidelity social simulation platform integrating human-like cognitive agents and dynamic social interactions to realistically simulate user behavior evolution under recommendation interventions. Specifically, the system comprises a population of Sim-User Agents, each equipped with a five-layer cognitive architecture that encapsulates key psychological mechanisms, including episodic memory, affective state transitions, adaptive preference learning, and dynamic trust-risk assessments. In particular, we innovatively introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine grounded in psychological and sociological theories, enabling more realistic user decision-making processes. Furthermore, we construct a multilayer heterogeneous social graph (GGBond Graph) supporting dynamic relational evolution, effectively modeling users' evolving social ties and trust dynamics based on interest similarity, personality alignment, and structural homophily. During system operation, agents autonomously respond to recommendations generated by typical recommender algorithms (e.g., Matrix Factorization, MultVAE, LightGCN), deciding whether to consume, rate, and share content while dynamically updating their internal states and social connections, thereby forming a stable, multi-round feedback loop. This innovative design transcends the limitations of traditional static datasets, providing a controlled, observable environment for evaluating long-term recommender effects.",http://arxiv.org/abs/2505.21154v1,CS,Quantitative
Debugging and Runtime Analysis of Neural Networks with VLMs (A Case Study),"Debugging of Deep Neural Networks (DNNs), particularly vision models, is very challenging due to the complex and opaque decision-making processes in these networks. In this paper, we explore multi-modal Vision-Language Models (VLMs), such as CLIP, to automatically interpret the opaque representation space of vision models using natural language. This in turn, enables a semantic analysis of model behavior using human-understandable concepts, without requiring costly human annotations. Key to our approach is the notion of semantic heatmap, that succinctly captures the statistical properties of DNNs in terms of the concepts discovered with the VLM and that are computed off-line using a held-out data set. We show the utility of semantic heatmaps for fault localization -- an essential step in debugging -- in vision models. Our proposed technique helps localize the fault in the network (encoder vs head) and also highlights the responsible high-level concepts, by leveraging novel differential heatmaps, which summarize the semantic differences between the correct and incorrect behaviour of the analyzed DNN. We further propose a lightweight runtime analysis to detect and filter-out defects at runtime, thus improving the reliability of the analyzed DNNs. The runtime analysis works by measuring and comparing the similarity between the heatmap computed for a new (unseen) input and the heatmaps computed a-priori for correct vs incorrect DNN behavior. We consider two types of defects: misclassifications and vulnerabilities to adversarial attacks. We demonstrate the debugging and runtime analysis on a case study involving a complex ResNet-based classifier trained on the RIVAL10 dataset.",http://arxiv.org/abs/2503.17416v1,CS,Mixed
HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion,"Transparent and reflective objects pose significant challenges for depth sensors, resulting in incomplete depth information that adversely affects downstream robotic perception and manipulation tasks. To address this issue, we propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba architectures. The encoder is constructed based on a dual-branch Transformer-CNN framework, while the multi-scale fusion module leverages a Transformer-Mamba architecture, which also serves as the foundation for the decoder design. We introduce a novel multimodal fusion module grounded in self-attention mechanisms and state space models, marking the first application of the Mamba architecture in the field of transparent object depth completion and revealing its promising potential. Additionally, we design an innovative multi-scale fusion module for the decoder that combines channel attention, spatial attention, and multi-scale feature extraction techniques to effectively integrate multi-scale features through a down-fusion strategy. Extensive evaluations on multiple public datasets demonstrate that our model achieves state-of-the-art(SOTA) performance, validating the effectiveness of our approach.",http://arxiv.org/abs/2505.20904v1,CS,Quantitative
Simple yet Effective Graph Distillation via Clustering,"Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using Fr\'echet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.",http://arxiv.org/abs/2505.20807v1,CS,Quantitative
TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs,"DeepSeek R1 has significantly advanced complex reasoning for large language models (LLMs). While recent methods have attempted to replicate R1's reasoning capabilities in multimodal settings, they face limitations, including inconsistencies between reasoning and final answers, model instability and crashes during long-chain exploration, and low data learning efficiency. To address these challenges, we propose TACO, a novel reinforcement learning algorithm for visual reasoning. Building on Generalized Reinforcement Policy Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly couples reasoning with answer consistency to ensure answers are grounded in thoughtful reasoning. We also introduce the Rollback Resample Strategy, which adaptively removes problematic samples and reintroduces them to the sampler, enabling stable long-chain exploration and future learning opportunities. Additionally, TACO employs an adaptive learning schedule that focuses on moderate difficulty samples to optimize data efficiency. Furthermore, we propose the Test-Time-Resolution-Scaling scheme to address performance degradation due to varying resolutions during reasoning while balancing computational overhead. Extensive experiments on in-distribution and out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning LVLMs leads to significant performance improvements.",http://arxiv.org/abs/2505.20777v1,CS,Quantitative
MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding,"Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in effectiveness. In this work, we propose MUSEG, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios. View our project at https://github.com/THUNLP-MT/MUSEG.",http://arxiv.org/abs/2505.20715v1,CS,Quantitative
"GitBugs: Bug Reports for Duplicate Detection, Retrieval Augmented Generation, Triage, and More","Bug reports provide critical insights into software quality, yet existing datasets often suffer from limited scope, outdated content, or insufficient metadata for machine learning. To address these limitations, we present GitBugs-a comprehen- sive and up-to-date dataset comprising over 150,000 bug reports from nine actively maintained open-source projects, including Firefox, Cassandra, and VS Code. GitBugs aggregates data from Github, Bugzilla and Jira issue trackers, offering standardized categorical fields for classification tasks and predefined train/test splits for duplicate bug detection. In addition, it includes ex- ploratory analysis notebooks and detailed project-level statistics, such as duplicate rates and resolution times. GitBugs supports various software engineering research tasks, including duplicate detection, retrieval augmented generation, resolution prediction, automated triaging, and temporal analysis. The openly licensed dataset provides a valuable cross-project resource for bench- marking and advancing automated bug report analysis. Access the data and code at https://github.com/av9ash/gitbugs/.",http://arxiv.org/abs/2504.09651v1,CS,Quantitative
Convexified Message-Passing Graph Neural Networks,"Graph Neural Networks (GNNs) have become prominent methods for graph representation learning, demonstrating strong empirical results on diverse graph prediction tasks. In this paper, we introduce Convexified Message Passing Graph Neural Networks (CGNNs), a novel and general framework that combines the power of message-passing GNNs with the tractability of convex optimization. By mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs transform training into a convex optimization problem, which can be solved efficiently and optimally by projected gradient methods. This convexity further allows the statistical properties of CGNNs to be analyzed accurately and rigorously. For two-layer CGNNs, we establish rigorous generalization guarantees, showing convergence to the performance of the optimal GNN. To scale to deeper architectures, we adopt a principled layer-wise training strategy. Experiments on benchmark datasets show that CGNNs significantly exceed the performance of leading GNN models, achieving 10 to 40 percent higher accuracy in most cases, underscoring their promise as a powerful and principled method with strong theoretical foundations. In rare cases where improvements are not quantitatively substantial, the convex models either slightly exceed or match the baselines, stressing their robustness and wide applicability. Though over-parameterization is often employed to enhance performance in nonconvex models, we show that our CGNNs framework yields shallow convex models that can surpass these models in both accuracy and resource efficiency.",http://arxiv.org/abs/2505.18289v1,CS,Quantitative
Interpretable Credit Default Prediction with Ensemble Learning and SHAP,"This study focuses on the problem of credit default prediction, builds a modeling framework based on machine learning, and conducts comparative experiments on a variety of mainstream classification algorithms. Through preprocessing, feature engineering, and model training of the Home Credit dataset, the performance of multiple models including logistic regression, random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and recall is evaluated. The results show that the ensemble learning method has obvious advantages in predictive performance, especially in dealing with complex nonlinear relationships between features and data imbalance problems. It shows strong robustness. At the same time, the SHAP method is used to analyze the importance and dependency of features, and it is found that the external credit score variable plays a dominant role in model decision making, which helps to improve the model's interpretability and practical application value. The research results provide effective reference and technical support for the intelligent development of credit risk control systems.",http://arxiv.org/abs/2505.20815v1,CS,Quantitative
Is Single-View Mesh Reconstruction Ready for Robotics?,"This paper evaluates single-view mesh reconstruction models for creating digital twin environments in robot manipulation. Recent advances in computer vision for 3D reconstruction from single viewpoints present a potential breakthrough for efficiently creating virtual replicas of physical environments for robotics contexts. However, their suitability for physics simulations and robotics applications remains unexplored. We establish benchmarking criteria for 3D reconstruction in robotics contexts, including handling typical inputs, producing collision-free and stable reconstructions, managing occlusions, and meeting computational constraints. Our empirical evaluation using realistic robotics datasets shows that despite success on computer vision benchmarks, existing approaches fail to meet robotics-specific requirements. We quantitively examine limitations of single-view reconstruction for practical robotics implementation, in contrast to prior work that focuses on multi-view approaches. Our findings highlight critical gaps between computer vision advances and robotics needs, guiding future research at this intersection.",http://arxiv.org/abs/2505.17966v1,CS,Quantitative
A Structured Literature Review on Traditional Approaches in Current Natural Language Processing,"The continued rise of neural networks and large language models in the more recent past has altered the natural language processing landscape, enabling new approaches towards typical language tasks and achieving mainstream success. Despite the huge success of large language models, many disadvantages still remain and through this work we assess the state of the art in five application scenarios with a particular focus on the future perspectives and sensible application scenarios of traditional and older approaches and techniques. In this paper we survey recent publications in the application scenarios classification, information and relation extraction, text simplification as well as text summarization. After defining our terminology, i.e., which features are characteristic for traditional techniques in our interpretation for the five scenarios, we survey if such traditional approaches are still being used, and if so, in what way they are used. It turns out that all five application scenarios still exhibit traditional models in one way or another, as part of a processing pipeline, as a comparison/baseline to the core model of the respective paper, or as the main model(s) of the paper. For the complete statistics, see https://zenodo.org/records/13683801",http://arxiv.org/abs/2505.12970v1,CS,Quantitative
Disambiguation in Conversational Question Answering in the Era of LLM: A Survey,"Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable language systems.",http://arxiv.org/abs/2505.12543v1,CS,Quantitative
Making Temporal Betweenness Computation Faster and Restless,"Bu{ } et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n 3 T 2 ) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M ) space and runs in time where M is the number of temporal edges, thus significantly improving the algorithm of Bu{ } et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Bu{ } et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM )) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n 2 M T 2 ). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.",http://arxiv.org/abs/2501.12708v1,CS,Mixed
Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management,"Precision agriculture, enhanced by artificial intelligence (AI), offers promising tools such as remote sensing, intelligent irrigation, fertilization management, and crop simulation to improve agricultural efficiency and sustainability. Reinforcement learning (RL), in particular, has outperformed traditional methods in optimizing yields and resource management. However, widespread AI adoption is limited by gaps between algorithmic recommendations and farmers' practical experience, local knowledge, and traditional practices. To address this, our study emphasizes Human-AI Interaction (HAII), focusing on transparency, usability, and trust in RL-based farm management. We employ a well-established trust framework - comprising ability, benevolence, and integrity - to develop a novel mathematical model quantifying farmers' confidence in AI-based fertilization strategies. Surveys conducted with farmers for this research reveal critical misalignments, which are integrated into our trust model and incorporated into a multi-objective RL framework. Unlike prior methods, our approach embeds trust directly into policy optimization, ensuring AI recommendations are technically robust, economically feasible, context-aware, and socially acceptable. By aligning technical performance with human-centered trust, this research supports broader AI adoption in agriculture.",http://arxiv.org/abs/2505.10803v1,CS,Quantitative
Are We Learning the Right Features? A Framework for Evaluating DL-Based Software Vulnerability Detection Solutions,"Recent research has revealed that the reported results of an emerging body of DL-based techniques for detecting software vulnerabilities are not reproducible, either across different datasets or on unseen samples. This paper aims to provide the foundation for properly evaluating the research in this domain. We do so by analyzing prior work and existing vulnerability datasets for the syntactic and semantic features of code that contribute to vulnerability, as well as features that falsely correlate with vulnerability. We provide a novel, uniform representation to capture both sets of features, and use this representation to detect the presence of both vulnerability and spurious features in code. To this end, we design two types of code perturbations: feature preserving perturbations (FPP) ensure that the vulnerability feature remains in a given code sample, while feature eliminating perturbations (FEP) eliminate the feature from the code sample. These perturbations aim to measure the influence of spurious and vulnerability features on the predictions of a given vulnerability detection solution. To evaluate how the two classes of perturbations influence predictions, we conducted a large-scale empirical study on five state-of-the-art DL-based vulnerability detectors. Our study shows that, for vulnerability features, only ~2% of FPPs yield the undesirable effect of a prediction changing among the five detectors on average. However, on average, ~84% of FEPs yield the undesirable effect of retaining the vulnerability predictions. For spurious features, we observed that FPPs yielded a drop in recall up to 29% for graph-based detectors. We present the reasons underlying these results and suggest strategies for improving DNN-based vulnerability detectors. We provide our perturbation-based evaluation framework as a public resource to enable independent future evaluation of vulnerability detectors.",http://arxiv.org/abs/2501.13291v5,CS,Quantitative
GSAT: Graph Structure Attention Networks,"Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.",http://arxiv.org/abs/2505.21288v1,CS,Quantitative
Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs,"Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines.",http://arxiv.org/abs/2505.20948v1,CS,Mixed
Multi-Armed Bandits Meet Large Language Models,"Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.",http://arxiv.org/abs/2505.13355v1,CS,Quantitative
Deep Assessment of Code Review Generation Approaches: Beyond Lexical Similarity,"Code review is a standard practice for ensuring the quality of software projects, and recent research has focused extensively on automated code review. While significant advancements have been made in generating code reviews, the automated assessment of these reviews remains less explored, with existing approaches and metrics often proving inaccurate. Current metrics, such as BLEU, primarily rely on lexical similarity between generated and reference reviews. However, such metrics tend to underestimate reviews that articulate the expected issues in ways different from the references. In this paper, we explore how semantic similarity between generated and reference reviews can enhance the automated assessment of code reviews. We first present a benchmark called \textit{GradedReviews}, which is constructed by collecting real-world code reviews from open-source projects, generating reviews using state-of-the-art approaches, and manually assessing their quality. We then evaluate existing metrics for code review assessment using this benchmark, revealing their limitations. To address these limitations, we propose two novel semantic-based approaches for assessing code reviews. The first approach involves converting both the generated review and its reference into digital vectors using a deep learning model and then measuring their semantic similarity through Cosine similarity. The second approach generates a prompt based on the generated review and its reference, submits this prompt to ChatGPT, and requests ChatGPT to rate the generated review according to explicitly defined criteria. Our evaluation on the \textit{GradedReviews} benchmark indicates that the proposed semantic-based approaches significantly outperform existing state-of-the-art metrics in assessing generated code review, improving the correlation coefficient between the resulting scores and human scores from 0.22 to 0.47.",http://arxiv.org/abs/2501.05176v1,CS,Quantitative
The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents,"The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.",http://arxiv.org/abs/2505.14727v1,CS,Quantitative
From Precision to Perception: User-Centred Evaluation of Keyword Extraction Algorithms for Internet-Scale Contextual Advertising,"Keyword extraction is a foundational task in natural language processing, underpinning countless real-world applications. A salient example is contextual advertising, where keywords help predict the topical congruence between ads and their surrounding media contexts to enhance advertising effectiveness. Recent advances in artificial intelligence, particularly large language models, have improved keyword extraction capabilities but also introduced concerns about computational cost. Moreover, although the end-user experience is of vital importance, human evaluation of keyword extraction performances remains under-explored. This study provides a comparative evaluation of three prevalent keyword extraction algorithms that vary in complexity: TF-IDF, KeyBERT, and Llama 2. To evaluate their effectiveness, a mixed-methods approach is employed, combining quantitative benchmarking with qualitative assessments from 552 participants through three survey-based experiments. Findings indicate a slight user preference for KeyBERT, which offers a favourable balance between performance and computational efficiency compared to the other two algorithms. Despite a strong overall preference for gold-standard keywords, differences between the algorithmic outputs are not statistically significant, highlighting a long-overlooked gap between traditional precision-focused metrics and user-perceived algorithm efficiency. The study highlights the importance of user-centred evaluation methodologies and proposes analytical tools to support their implementation.",http://arxiv.org/abs/2504.21667v1,CS,Mixed
"Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models","The biases exhibited by text-to-image (TTI) models are often treated as independent, though in reality, they may be deeply interrelated. Addressing bias along one dimension - such as ethnicity or age - can inadvertently affect another, like gender, either mitigating or exacerbating existing disparities. Understanding these interdependencies is crucial for designing fairer generative models, yet measuring such effects quantitatively remains a challenge. To address this, we introduce BiasConnect, a novel tool for analyzing and quantifying bias interactions in TTI models. BiasConnect uses counterfactual interventions along different bias axes to reveal the underlying structure of these interactions and estimates the effect of mitigating one bias axis on another. These estimates show strong correlation (+0.65) with observed post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an intersectional bias mitigation algorithm guided by user-defined target distributions and priority weights. InterMit achieves lower bias (0.33 vs. 0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields superior image quality compared to traditional techniques. Although our implementation is training-free, InterMit is modular and can be integrated with many existing debiasing approaches for TTI models, making it a flexible and extensible solution.",http://arxiv.org/abs/2505.17280v1,CS,Quantitative
A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules,"Developing new molecular compounds is crucial to address pressing challenges, from health to environmental sustainability. However, exploring the molecular space to discover new molecules is difficult due to the vastness of the space. Here we introduce CoCoGraph, a collaborative and constrained graph diffusion model capable of generating molecules that are guaranteed to be chemically valid. Thanks to the constraints built into the model and to the collaborative mechanism, CoCoGraph outperforms state-of-the-art approaches on standard benchmarks while requiring up to an order of magnitude fewer parameters. Analysis of 36 chemical properties also demonstrates that CoCoGraph generates molecules with distributions more closely matching real molecules than current models. Leveraging the model's efficiency, we created a database of 8.2M million synthetically generated molecules and conducted a Turing-like test with organic chemistry experts to further assess the plausibility of the generated molecules, and potential biases and limitations of CoCoGraph.",http://arxiv.org/abs/2505.16365v1,CS,Quantitative
It's High Time: A Survey of Temporal Information Retrieval and Question Answering,"Time plays a critical role in how information is generated, retrieved, and interpreted. In this survey, we provide a comprehensive overview of Temporal Information Retrieval and Temporal Question Answering, two research areas aimed at handling and understanding time-sensitive information. As the amount of time-stamped content from sources like news articles, web archives, and knowledge bases increases, systems must address challenges such as detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. These challenges are critical across many dynamic and time-sensitive domains, from news and encyclopedias to science, history, and social media. We review both traditional approaches and modern neural methods, including those that use transformer models and Large Language Models (LLMs). We also review recent advances in temporal language modeling, multi-hop reasoning, and retrieval-augmented generation (RAG), alongside benchmark datasets and evaluation strategies that test temporal robustness, recency awareness, and generalization.",http://arxiv.org/abs/2505.20243v1,CS,Quantitative
"Place Recognition: A Comprehensive Review, Current Challenges and Future Directions","Place recognition is a cornerstone of vehicle navigation and mapping, which is pivotal in enabling systems to determine whether a location has been previously visited. This capability is critical for tasks such as loop closure in Simultaneous Localization and Mapping (SLAM) and long-term navigation under varying environmental conditions. In this survey, we comprehensively review recent advancements in place recognition, emphasizing three representative methodological paradigms: Convolutional Neural Network (CNN)-based approaches, Transformer-based frameworks, and cross-modal strategies. We begin by elucidating the significance of place recognition within the broader context of autonomous systems. Subsequently, we trace the evolution of CNN-based methods, highlighting their contributions to robust visual descriptor learning and scalability in large-scale environments. We then examine the emerging class of Transformer-based models, which leverage self-attention mechanisms to capture global dependencies and offer improved generalization across diverse scenes. Furthermore, we discuss cross-modal approaches that integrate heterogeneous data sources such as Lidar, vision, and text description, thereby enhancing resilience to viewpoint, illumination, and seasonal variations. We also summarize standard datasets and evaluation metrics widely adopted in the literature. Finally, we identify current research challenges and outline prospective directions, including domain adaptation, real-time performance, and lifelong learning, to inspire future advancements in this domain. The unified framework of leading-edge place recognition methods, i.e., code library, and the results of their experimental evaluations are available at https://github.com/CV4RA/SOTA-Place-Recognitioner.",http://arxiv.org/abs/2505.14068v2,CS,Quantitative
Semantic Correspondence: Unified Benchmarking and a Strong Baseline,"Establishing semantic correspondence is a challenging task in computer vision, aiming to match keypoints with the same semantic information across different images. Benefiting from the rapid development of deep learning, remarkable progress has been made over the past decade. However, a comprehensive review and analysis of this task remains absent. In this paper, we present the first extensive survey of semantic correspondence methods. We first propose a taxonomy to classify existing methods based on the type of their method designs. These methods are then categorized accordingly, and we provide a detailed analysis of each approach. Furthermore, we aggregate and summarize the results of methods in literature across various benchmarks into a unified comparative table, with detailed configurations to highlight performance variations. Additionally, to provide a detailed understanding on existing methods for semantic matching, we thoroughly conduct controlled experiments to analyse the effectiveness of the components of different methods. Finally, we propose a simple yet effective baseline that achieves state-of-the-art performance on multiple benchmarks, providing a solid foundation for future research in this field. We hope this survey serves as a comprehensive reference and consolidated baseline for future development. Code is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.",http://arxiv.org/abs/2505.18060v3,CS,Quantitative
A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety,"Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions.",http://arxiv.org/abs/2505.17342v1,CS,Quantitative
Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning,"Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.",http://arxiv.org/abs/2505.17249v1,CS,Quantitative
Statistical Modeling and Uncertainty Estimation of LLM Inference Systems,"Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements. Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments. Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance. Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space. To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \ml for robust statistical prediction and uncertainty estimation in LLM inference workloads. Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \ml predictions. We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor. Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization. Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios.",http://arxiv.org/abs/2505.09319v1,CS,Quantitative
"Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications","Integrating Large Language Models (LLMs) and Evolutionary Computation (EC) represents a promising avenue for advancing artificial intelligence by combining powerful natural language understanding with optimization and search capabilities. This manuscript explores the synergistic potential of LLMs and EC, reviewing their intersections, complementary strengths, and emerging applications. We identify key opportunities where EC can enhance LLM training, fine-tuning, prompt engineering, and architecture search, while LLMs can, in turn, aid in automating the design, analysis, and interpretation of ECs. The manuscript explores the synergistic integration of EC and LLMs, highlighting their bidirectional contributions to advancing artificial intelligence. It first examines how EC techniques enhance LLMs by optimizing key components such as prompt engineering, hyperparameter tuning, and architecture search, demonstrating how evolutionary methods automate and refine these processes. Secondly, the survey investigates how LLMs improve EC by automating metaheuristic design, tuning evolutionary algorithms, and generating adaptive heuristics, thereby increasing efficiency and scalability. Emerging co-evolutionary frameworks are discussed, showcasing applications across diverse fields while acknowledging challenges like computational costs, interpretability, and algorithmic convergence. The survey concludes by identifying open research questions and advocating for hybrid approaches that combine the strengths of EC and LLMs.",http://arxiv.org/abs/2505.15741v1,CS,Quantitative
AutoData: A Multi-Agent System for Open Web Data Collection,"The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. While existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. Current data-collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. To address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. In addition, AutoData is designed with a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. Besides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. Comprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods. Case studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability. Our source code and dataset are available at https://github.com/GraphResearcher/AutoData.",http://arxiv.org/abs/2505.15859v1,CS,Quantitative
A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data,"False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short in jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the computational complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.",http://arxiv.org/abs/2505.20688v1,CS,Quantitative
Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation,"This paper explores the parallels between Thompson's ""Reflections on Trusting Trust"" and modern challenges in LLM-based code generation. We examine how Thompson's insights about compiler backdoors take on new relevance in the era of large language models, where the mechanisms for potential exploitation are even more opaque and difficult to analyze. Building on this analogy, we discuss how the statistical nature of LLMs creates novel security challenges in code generation pipelines. As a potential direction forward, we propose an ensemble-based validation approach that leverages multiple independent models to detect anomalous code patterns through cross-model consensus. This perspective piece aims to spark discussion about trust and validation in AI-assisted software development.",http://arxiv.org/abs/2502.16279v1,CS,Quantitative
FedORGP: Guiding Heterogeneous Federated Learning with Orthogonality Regularization on Global Prototypes,"Federated Learning (FL) has emerged as an essential framework for distributed machine learning, especially with its potential for privacy-preserving data processing. However, existing FL frameworks struggle to address statistical and model heterogeneity, which severely impacts model performance. While Heterogeneous Federated Learning (HtFL) introduces prototype-based strategies to address the challenges, current approaches face limitations in achieving optimal separation of prototypes. This paper presents FedORGP, a novel HtFL algorithm designed to improve global prototype separation through orthogonality regularization, which not only encourages intra-class prototype similarity but also significantly expands the inter-class angular separation. With the guidance of the global prototype, each client keeps its embeddings aligned with the corresponding prototype in the feature space, promoting directional independence that integrates seamlessly with the cross-entropy (CE) loss. We provide theoretical proof of FedORGP's convergence under non-convex conditions. Extensive experiments demonstrate that FedORGP outperforms seven state-of-the-art baselines, achieving up to 10.12\% accuracy improvement in scenarios where statistical and model heterogeneity coexist.",http://arxiv.org/abs/2502.16119v2,CS,Quantitative
A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data,"In compressible fluid flow, reconstructing shocks, discontinuities, rarefactions, and their interactions from sparse measurements is an important inverse problem with practical applications. Moreover, physics-informed machine learning has recently become an increasingly popular approach for performing reconstructions tasks. In this work we explore a machine learning recipe, known as GraphGPS, for reconstructing canonical compressible flows known as 3D Riemann problems from sparse observations, in a physics-informed manner. The GraphGPS framework combines the benefits of positional encodings, local message-passing of graphs, and global contextual awareness, and we explore the latter two components through an ablation study. Furthermore, we modify the aggregation step of message-passing such that it is aware of shocks and discontinuities, resulting in sharper reconstructions of these features. Additionally, we modify message-passing such that information flows strictly from known nodes only, which results in computational savings, better training convergence, and no degradation of reconstruction accuracy. We also show that the GraphGPS framework outperforms numerous machine learning benchmarks.",http://arxiv.org/abs/2505.21421v1,CS,Mixed
Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts,"Despite rapid advances in large language models (LLMs), their integration with traditional supervised machine learning (ML) techniques that have proven applicability to medical data remains underexplored. This is particularly true for psychiatric applications, where narrative data often exhibit nuanced linguistic and contextual complexity, and can benefit from the combination of multiple models with differing characteristics. In this study, we introduce an ensemble framework for automatically classifying Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using narrative transcripts. Our approach integrates three complementary models: LLaMA3, an open-source LLM that captures long-range semantic structure; RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical narratives; and a Support Vector Machine (SVM) classifier trained using TF-IDF-based lexical features. These models are aggregated through a majority voting mechanism to enhance predictive robustness. The dataset includes 441 instances, including 352 for training and 89 for validation. Empirical results show that the ensemble outperforms individual models, achieving an F$_1$ score of 0.71 (95\% CI: [0.60-0.80]). Compared to the best-performing individual model (SVM), the ensemble improved recall while maintaining competitive precision. This indicates the strong sensitivity of the ensemble in identifying ADHD-related linguistic cues. These findings demonstrate the promise of hybrid architectures that leverage the semantic richness of LLMs alongside the interpretability and pattern recognition capabilities of traditional supervised ML, offering a new direction for robust and generalizable psychiatric text classification.",http://arxiv.org/abs/2505.21324v1,CS,Mixed
Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation,"Transformer-based 3D human pose estimation methods suffer from high computational costs due to the quadratic complexity of self-attention with respect to sequence length. Additionally, pose sequences often contain significant redundancy between frames. However, recent methods typically fail to improve model capacity while effectively eliminating sequence redundancy. In this work, we introduce the Spectral Compression Transformer (SCT) to reduce sequence length and accelerate computation. The SCT encoder treats hidden features between blocks as Temporal Feature Signals (TFS) and applies the Discrete Cosine Transform, a Fourier transform-based technique, to determine the spectral components to be retained. By filtering out certain high-frequency noise components, SCT compresses the sequence length and reduces redundancy. To further enrich the input sequence with prior structural information, we propose the Line Pose Graph (LPG) based on line graph theory. The LPG generates skeletal position information that complements the input 2D joint positions, thereby improving the model's performance. Finally, we design a dual-stream network architecture to effectively model spatial joint relationships and the compressed motion trajectory within the pose sequence. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our model achieves state-of-the-art performance with improved computational efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE of 37.7mm while maintaining a low computational cost. Furthermore, we perform ablation studies on each module to assess its effectiveness. The code and models will be released.",http://arxiv.org/abs/2505.21309v1,CS,Quantitative
Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling,"Event cameras offer high temporal resolution and power efficiency, making them well-suited for edge AI applications. However, their high event rates present challenges for data transmission and processing. Subsampling methods provide a practical solution, but their effect on downstream visual tasks remains underexplored. In this work, we systematically evaluate six hardware-friendly subsampling methods using convolutional neural networks for event video classification on various benchmark datasets. We hypothesize that events from high-density regions carry more task-relevant information and are therefore better suited for subsampling. To test this, we introduce a simple causal density-based subsampling method, demonstrating improved classification accuracy in sparse regimes. Our analysis further highlights key factors affecting subsampling performance, including sensitivity to hyperparameters and failure cases in scenarios with large event count variance. These findings provide insights for utilization of hardware-efficient subsampling strategies that balance data efficiency and task accuracy. The code for this paper will be released at: https://github.com/hesamaraghi/event-camera-subsampling-methods.",http://arxiv.org/abs/2505.21187v1,CS,Quantitative
Instance Data Condensation for Image Super-Resolution,"Deep learning based image Super-Resolution (ISR) relies on large training datasets to optimize model generalization; this requires substantial computational and storage resources during training. While dataset condensation has shown potential in improving data efficiency and privacy for high-level computer vision tasks, it has not yet been fully exploited for ISR. In this paper, we propose a novel Instance Data Condensation (IDC) framework specifically for ISR, which achieves instance-level data condensation through Random Local Fourier Feature Extraction and Multi-level Feature Distribution Matching. This aims to optimize feature distributions at both global and local levels and obtain high-quality synthesized training content with fine detail. This framework has been utilized to condense the most commonly used training dataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic dataset offers comparable or (in certain cases) even better performance compared to the original full dataset and excellent training stability when used to train various popular ISR models. To the best of our knowledge, this is the first time that a condensed/synthetic dataset (with a 10% data volume) has demonstrated such performance. The source code and the synthetic dataset have been made available at https://github.com/.",http://arxiv.org/abs/2505.21099v1,CS,Quantitative
RAN Tester UE: An Automated Declarative UE Centric Security Testing Platform,"Cellular networks require strict security procedures and measures across various network components, from core to radio access network (RAN) and end-user devices. As networks become increasingly complex and interconnected, as in O-RAN deployments, they are exposed to a numerous security threats. Therefore, ensuring robust security is critical for O-RAN to protect network integrity and safeguard user data. This requires rigorous testing methodologies to mitigate threats. This paper introduces an automated, adaptive, and scalable user equipment (UE) based RAN security testing framework designed to address the shortcomings of existing RAN testing solutions. Experimental results on a 5G software radio testbed built with commercial off-the-shelf hardware and open source software validate the efficiency and reproducibility of sample security test procedures developed on the RAN Tester UE framework.",http://arxiv.org/abs/2505.10812v1,CS,Quantitative
TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data,"Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.",http://arxiv.org/abs/2505.21027v1,CS,Quantitative
Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation,"Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.",http://arxiv.org/abs/2505.20992v1,CS,Quantitative
Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3,"RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity tracers due to the correlation between their metal abundances and light curve morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs, there is a pressing need for scalable methods to estimate their metallicities from photometric data. We introduce a unified deep learning framework that estimates metallicities for both fundamental-mode (RRab) and first-overtone (RRc) RRLs using Gaia G-band light curves. This approach extends our previous work on RRab stars to include RRc stars, aiming for high predictive accuracy and broad generalization across both pulsation types. The model is based on a Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic regression. Our pipeline includes preprocessing steps such as phase folding, smoothing, and sample weighting, and uses photometric metallicities from the literature as training targets. The architecture is designed to handle morphological differences between RRab and RRc light curves without requiring separate models. On held-out validation sets, our GRU model achieves strong performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401; for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results show the effectiveness of deep learning for large-scale photometric metallicity estimation and support its application to studies of stellar populations and Galactic structure.",http://arxiv.org/abs/2505.20947v1,CS,Quantitative
Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization,"A widely believed explanation for the remarkable generalization capacities of overparameterized neural networks is that the optimization algorithms used for training induce an implicit bias towards benign solutions. To grasp this theoretically, recent works examine gradient descent and its variants in simplified training settings, often assuming vanishing learning rates. These studies reveal various forms of implicit regularization, such as $\ell_1$-norm minimizing parameters in regression and max-margin solutions in classification. Concurrently, empirical findings show that moderate to large learning rates exceeding standard stability thresholds lead to faster, albeit oscillatory, convergence in the so-called Edge-of-Stability regime, and induce an implicit bias towards minima of low sharpness (norm of training loss Hessian). In this work, we argue that a comprehensive understanding of the generalization performance of gradient descent requires analyzing the interaction between these various forms of implicit regularization. We empirically demonstrate that the learning rate balances between low parameter norm and low sharpness of the trained model. We furthermore prove for diagonal linear networks trained on a simple regression task that neither implicit bias alone minimizes the generalization error. These findings demonstrate that focusing on a single implicit bias is insufficient to explain good generalization, and they motivate a broader view of implicit regularization that captures the dynamic trade-off between norm and sharpness induced by non-negligible learning rates.",http://arxiv.org/abs/2505.21423v1,CS,Quantitative
Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks,"Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.",http://arxiv.org/abs/2505.21426v1,CS,Quantitative
The Limits of Preference Data for Post-Training,"Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.",http://arxiv.org/abs/2505.19964v1,CS,Qualitative
Large Language Models in Code Co-generation for Safe Autonomous Vehicles,"Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.",http://arxiv.org/abs/2505.19658v1,CS,Qualitative
TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization,"Adversarial transferability remains a critical challenge in evaluating the robustness of deep neural networks. In security-critical applications, transferability enables black-box attacks without access to model internals, making it a key concern for real-world adversarial threat assessment. While Vision Transformers (ViTs) have demonstrated strong adversarial performance, existing attacks often fail to transfer effectively across architectures, especially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models. In this paper, we introduce \textbf{TESSER} -- a novel adversarial attack framework that enhances transferability via two key strategies: (1) \textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients based on token-wise importance derived from intermediate feature activations, and (2) \textit{Spectral Smoothness Regularization (SSR)}, which suppresses high-frequency noise in perturbations using a differentiable Gaussian prior. These components work in tandem to generate perturbations that are both semantically meaningful and spectrally smooth. Extensive experiments on ImageNet across 12 diverse architectures demonstrate that TESSER achieves +10.9\% higher attack succes rate (ASR) on CNNs and +7.2\% on ViTs compared to the state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER significantly improves robustness against defended models, achieving 53.55\% ASR on adversarially trained CNNs. Qualitative analysis shows strong alignment between TESSER's perturbations and salient visual regions identified via Grad-CAM, while frequency-domain analysis reveals a 12\% reduction in high-frequency energy, confirming the effectiveness of spectral regularization.",http://arxiv.org/abs/2505.19613v1,CS,Mixed
ViewCraft3D: High-Fidelity and View-Consistent 3D Vector Graphics Synthesis,"3D vector graphics play a crucial role in various applications including 3D shape retrieval, conceptual design, and virtual reality interactions due to their ability to capture essential structural information with minimal representation. While recent approaches have shown promise in generating 3D vector graphics, they often suffer from lengthy processing times and struggle to maintain view consistency. To address these limitations, we propose ViewCraft3D (VC3D), an efficient method that leverages 3D priors to generate 3D vector graphics. Specifically, our approach begins with 3D object analysis, employs a geometric extraction algorithm to fit 3D vector graphics to the underlying structure, and applies view-consistent refinement process to enhance visual quality. Our comprehensive experiments demonstrate that VC3D outperforms previous methods in both qualitative and quantitative evaluations, while significantly reducing computational overhead. The resulting 3D sketches maintain view consistency and effectively capture the essential characteristics of the original objects.",http://arxiv.org/abs/2505.19492v1,CS,Mixed
Soft-CAM: Making black box models self-explainable for high-stakes decisions,"Convolutional neural networks (CNNs) are widely used for high-stakes applications like medicine, often surpassing human performance. However, most explanation methods rely on post-hoc attribution, approximating the decision-making process of already trained black-box models. These methods are often sensitive, unreliable, and fail to reflect true model reasoning, limiting their trustworthiness in critical applications. In this work, we introduce SoftCAM, a straightforward yet effective approach that makes standard CNN architectures inherently interpretable. By removing the global average pooling layer and replacing the fully connected classification layer with a convolution-based class evidence layer, SoftCAM preserves spatial information and produces explicit class activation maps that form the basis of the model's predictions. Evaluated on three medical datasets, SoftCAM maintains classification performance while significantly improving both the qualitative and quantitative explanation compared to existing post-hoc methods. Our results demonstrate that CNNs can be inherently interpretable without compromising performance, advancing the development of self-explainable deep learning for high-stakes decision-making.",http://arxiv.org/abs/2505.17748v1,CS,Mixed
Paired and Unpaired Image to Image Translation using Generative Adversarial Networks,"Image to image translation is an active area of research in the field of computer vision, enabling the generation of new images with different styles, textures, or resolutions while preserving their characteristic properties. Recent architectures leverage Generative Adversarial Networks (GANs) to transform input images from one domain to another. In this work, we focus on the study of both paired and unpaired image translation across multiple image domains. For the paired task, we used a conditional GAN model, and for the unpaired task, we trained it using cycle consistency loss. We experimented with different types of loss functions, multiple Patch-GAN sizes, and model architectures. New quantitative metrics - precision, recall, and FID score - were used for analysis. In addition, a qualitative study of the results of different experiments was conducted.",http://arxiv.org/abs/2505.16310v1,CS,Mixed
Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO,"Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.",http://arxiv.org/abs/2505.21457v1,CS,Quantitative
Reward-Aware Proto-Representations in Reinforcement Learning,"In recent years, the successor representation (SR) has attracted increasing attention in reinforcement learning (RL), and it has been used to address some of its key challenges, such as exploration, credit assignment, and generalization. The SR can be seen as representing the underlying credit assignment structure of the environment by implicitly encoding its induced transition dynamics. However, the SR is reward-agnostic. In this paper, we discuss a similar representation that also takes into account the reward dynamics of the problem. We study the default representation (DR), a recently proposed representation with limited theoretical (and empirical) analysis. Here, we lay some of the theoretical foundation underlying the DR in the tabular case by (1) deriving dynamic programming and (2) temporal-difference methods to learn the DR, (3) characterizing the basis for the vector space of the DR, and (4) formally extending the DR to the function approximation case through default features. Empirically, we analyze the benefits of the DR in many of the settings in which the SR has been applied, including (1) reward shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our results show that, compared to the SR, the DR gives rise to qualitatively different, reward-aware behaviour and quantitatively better performance in several settings.",http://arxiv.org/abs/2505.16217v1,CS,Mixed
Systemic Flakiness: An Empirical Analysis of Co-Occurring Flaky Test Failures,"Flaky tests produce inconsistent outcomes without code changes, creating major challenges for software developers. An industrial case study reported that developers spend 1.28% of their time repairing flaky tests at a monthly cost of $2,250. We discovered that flaky tests often exist in clusters, with co-occurring failures that share the same root causes, which we call systemic flakiness. This suggests that developers can reduce repair costs by addressing shared root causes, enabling them to fix multiple flaky tests at once rather than tackling them individually. This study represents an inflection point by challenging the deep-seated assumption that flaky test failures are isolated occurrences. We used an established dataset of 10,000 test suite runs from 24 Java projects on GitHub, spanning domains from data orchestration to job scheduling. It contains 810 flaky tests, which we levered to perform a mixed-method empirical analysis of co-occurring flaky test failures. Systemic flakiness is significant and widespread. We performed agglomerative clustering of flaky tests based on their failure co-occurrence, finding that 75% of flaky tests across all projects belong to a cluster, with a mean cluster size of 13.5 flaky tests. Instead of requiring 10,000 test suite runs to identify systemic flakiness, we demonstrated a lightweight alternative by training machine learning models based on static test case distance measures. Through manual inspection of stack traces, conducted independently by four authors and resolved through negotiated agreement, we identified intermittent networking issues and instabilities in external dependencies as the predominant causes of systemic flakiness.",http://arxiv.org/abs/2504.16777v1,CS,Mixed
SMOTExT: SMOTE meets Large Language Models,"Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.",http://arxiv.org/abs/2505.13434v1,CS,Mixed
IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar,"This article provides an overview of IG Parser, a software that facilitates qualitative content analysis of formal (e.g., legal) rules or informal (e.g., social) norms, and strategies (such as conventions) -- referred to as institutions -- that govern social systems and operate configurally to describe institutional systems. To this end, the IG Parser employs a distinctive syntax that ensures rigorous encoding of natural language, while automating the transformation into various formats that support the downstream analysis using diverse analytical techniques. The conceptual core of the IG Parser is an associated syntax, IG Script, that operationalizes the conceptual foundations of the Institutional Grammar, and more specifically the Institutional Grammar 2.0, an analytical paradigm for institutional analysis. This article presents the IG Parser, including its conceptual foundations, the syntax specification of IG Script, and its architectural principles. This overview is augmented with selective illustrative examples that highlight its use and the associated benefits.",http://arxiv.org/abs/2505.13393v2,CS,Qualitative
Causal Posterior Estimation,"We present Causal Posterior Estimation (CPE), a novel method for Bayesian inference in simulator models, i.e., models where the evaluation of the likelihood function is intractable or too computationally expensive, but where one can simulate model outputs given parameter values. CPE utilizes a normalizing flow-based (NF) approximation to the posterior distribution which carefully incorporates the conditional dependence structure induced by the graphical representation of the model into the neural network. Thereby it is possible to improve the accuracy of the approximation. We introduce both discrete and continuous NF architectures for CPE and propose a constant-time sampling procedure for the continuous case which reduces the computational complexity of drawing samples to O(1) as for discrete NFs. We show, through an extensive experimental evaluation, that by incorporating the conditional dependencies induced by the graphical model directly into the neural network, rather than learning them from data, CPE is able to conduct highly accurate posterior inference either outperforming or matching the state of the art in the field.",http://arxiv.org/abs/2505.21468v1,CS,Quantitative
PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning,"Large-scale deep neural networks (DNN) exhibit excellent performance for various tasks. As DNNs and datasets grow, distributed training becomes extremely time-consuming and demands larger clusters. A main bottleneck is the resulting gradient aggregation overhead. While gradient compression and sparse collective communication techniques are commonly employed to alleviate network load, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy. This paper introduces PacTrain, a novel framework that accelerates distributed training by combining pruning with sparse gradient compression. Active pruning of the neural network makes the model weights and gradients sparse. By ensuring the global knowledge of the gradient sparsity among all distributed training workers, we can perform lightweight compression communication without harming accuracy. We show that the PacTrain compression scheme achieves a near-optimal compression strategy while remaining compatible with the all-reduce primitive. Experimental evaluations show that PacTrain improves training throughput by 1.25 to 8.72 times compared to state-of-the-art compression-enabled systems for representative vision and language models training tasks under bandwidth-constrained conditions.",http://arxiv.org/abs/2505.18563v1,CS,Quantitative
QMIO: A tightly integrated hybrid HPCQC system,"High-Performance Computing (HPC) systems are the most powerful tools that we currently have to solve complex scientific simulations. Quantum computing (QC) has the potential to enhance HPC systems by accelerating the execution of specific kernels that can be offloaded to a Quantum Processing Unit (QPU), granting them new capabilities, improving the speed of computation, or reducing energy consumption. In this paper, we present QMIO: a state-of-the-art hybrid HPCQC system, which tightly integrates HPC and QC. We describe its hardware and software components, the integration middleware, and the lessons learned during the design, implementation, and operation of the system.",http://arxiv.org/abs/2505.19267v1,CS,Quantitative
Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning,"To address the challenges of high resource dynamism and intensive task concurrency in microservice systems, this paper proposes an adaptive resource scheduling method based on the A3C reinforcement learning algorithm. The scheduling problem is modeled as a Markov Decision Process, where policy and value networks are jointly optimized to enable fine-grained resource allocation under varying load conditions. The method incorporates an asynchronous multi-threaded learning mechanism, allowing multiple agents to perform parallel sampling and synchronize updates to the global network parameters. This design improves both policy convergence efficiency and model stability. In the experimental section, a real-world dataset is used to construct a scheduling scenario. The proposed method is compared with several typical approaches across multiple evaluation metrics, including task delay, scheduling success rate, resource utilization, and convergence speed. The results show that the proposed method delivers high scheduling performance and system stability in multi-task concurrent environments. It effectively alleviates the resource allocation bottlenecks faced by traditional methods under heavy load, demonstrating its practical value for intelligent scheduling in microservice systems.",http://arxiv.org/abs/2505.00299v1,CS,Quantitative
HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling,"Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",http://arxiv.org/abs/2505.20836v1,CS,Quantitative
Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees,"In many real-world applications of regression, conditional probability estimation, and uncertainty quantification, exploiting symmetries rooted in physics or geometry can dramatically improve generalization and sample efficiency. While geometric deep learning has made significant empirical advances by incorporating group-theoretic structure, less attention has been given to statistical learning guarantees. In this paper, we introduce an equivariant representation learning framework that simultaneously addresses regression, conditional probability estimation, and uncertainty quantification while providing first-of-its-kind non-asymptotic statistical learning guarantees. Grounded in operator and group representation theory, our framework approximates the spectral decomposition of the conditional expectation operator, building representations that are both equivariant and disentangled along independent symmetry subgroups. Empirical evaluations on synthetic datasets and real-world robotics applications confirm the potential of our approach, matching or outperforming existing equivariant baselines in regression while additionally providing well-calibrated parametric uncertainty estimates.",http://arxiv.org/abs/2505.19809v2,CS,Quantitative
Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning,"Similarity manifests in various forms, including semantic similarity that is particularly important, serving as an approximation of human object categorization based on e.g. shared functionalities and evolutionary traits. It also offers practical advantages in computational modeling via lexical structures such as WordNet with constant and interpretable similarity. As in the domain of deep vision, there is still not enough focus on the phenomena regarding the similarity perception emergence. We introduce Deep Similarity Inspector (DSI) -- a systematic framework to inspect how deep vision networks develop their similarity perception and its alignment with semantic similarity. Our experiments show that both Convolutional Neural Networks' (CNNs) and Vision Transformers' (ViTs) develop a rich similarity perception during training with 3 phases (initial similarity surge, refinement, stabilization), with clear differences between CNNs and ViTs. Besides the gradual mistakes elimination, the mistakes refinement phenomenon can be observed.",http://arxiv.org/abs/2505.21338v1,CS,Quantitative
A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment,"This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.",http://arxiv.org/abs/2505.21414v1,CS,Mixed
Federated Instrumental Variable Analysis via Federated Generalized Method of Moments,"Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.",http://arxiv.org/abs/2505.21012v1,CS,Quantitative
Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals,"AI-Generated (AIG) content has become increasingly widespread by recent advances in generative models and the easy-to-use tools that have significantly lowered the technical barriers for producing highly realistic audio, images, and videos through simple natural language prompts. In response, platforms are adopting provable provenance with platforms recommending AIG to be self-disclosed and signaled to users. However, these indicators may be often missed, especially when they rely solely on visual cues and make them ineffective to users with different sensory abilities. To address the gap, we conducted semi-structured interviews (N=28) with 15 sighted and 13 BLV participants to examine their interaction with AIG content through self-disclosed AI indicators. Our findings reveal diverse mental models and practices, highlighting different strengths and weaknesses of content-based (e.g., title, description) and menu-aided (e.g., AI labels) indicators. While sighted participants leveraged visual and audio cues, BLV participants primarily relied on audio and existing assistive tools, limiting their ability to identify AIG. Across both groups, they frequently overlooked menu-aided indicators deployed by platforms and rather interacted with content-based indicators such as title and comments. We uncovered usability challenges stemming from inconsistent indicator placement, unclear metadata, and cognitive overload. These issues were especially critical for BLV individuals due to the insufficient accessibility of interface elements. We provide practical recommendations and design implications for future AIG indicators across several dimensions.",http://arxiv.org/abs/2505.16057v1,CS,Qualitative
Artifacts of Idiosyncracy in Global Street View Data,"Street view data is increasingly being used in computer vision applications in recent years. Machine learning datasets are collected for these applications using simple sampling techniques. These datasets are assumed to be a systematic representation of cities, especially when densely sampled. Prior works however, show that there are clear gaps in coverage, with certain cities or regions being covered poorly or not at all. Here we demonstrate that a cities' idiosyncracies, such as city layout, may lead to biases in street view data for 28 cities across the globe, even when they are densely covered. We quantitatively uncover biases in the distribution of coverage of street view data and propose a method for evaluation of such distributions to get better insight in idiosyncracies in a cities' coverage. In addition, we perform a case study of Amsterdam with semi-structured interviews, showing how idiosyncracies of the collection process impact representation of cities and regions and allowing us to address biases at their source.",http://arxiv.org/abs/2505.11046v1,CS,Mixed
Paradigm shift on Coding Productivity Using GenAI,"Generative AI (GenAI) applications are transforming software engineering by enabling automated code co-creation. However, empirical evidence on GenAI's productivity effects in industrial settings remains limited. This paper investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q) within telecommunications and FinTech domains. Through surveys and interviews with industrial domain-experts, we identify primary productivity-influencing factors, including task complexity, coding skills, domain knowledge, and GenAI integration. Our findings indicate that GenAI tools enhance productivity in routine coding tasks (e.g., refactoring and Javadoc generation) but face challenges in complex, domain-specific activities due to limited context-awareness of codebases and insufficient support for customized design rules. We highlight new paradigms for coding transfer, emphasizing iterative prompt refinement, immersive development environment, and automated code evaluation as essential for effective GenAI usage.",http://arxiv.org/abs/2504.18404v1,CS,Mixed
Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews,"Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\% prediction accuracy for interviews with 100\% completeness, 79,6\% accuracy for interviews with 80\% completeness, 58,9\% accuracy for 60\% completeness, and 45,7\% accuracy for 40\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models.",http://arxiv.org/abs/2504.11977v1,CS,Qualitative
Toward a Human-Centered AI-assisted Colonoscopy System in Australia,"While AI-assisted colonoscopy promises improved colorectal cancer screening, its success relies on effective integration into clinical practice, not just algorithmic accuracy. This paper, based on an Australian field study (observations and gastroenterologist interviews), highlights a critical disconnect: current development prioritizes machine learning model performance, overlooking essential aspects of user interface design, workflow integration, and overall user experience. Industry interactions reveal a similar emphasis on data and algorithms. To realize AI's full potential, the HCI community must champion user-centered design, ensuring these systems are usable, support endoscopist expertise, and enhance patient outcomes.",http://arxiv.org/abs/2503.20790v1,CS,Qualitative
"""Sorry for bugging you so much."" Exploring Developers' Behavior Towards Privacy-Compliant Implementation","While protecting user data is essential, software developers often fail to fulfill privacy requirements. However, the reasons why they struggle with privacy-compliant implementation remain unclear. Is it due to a lack of knowledge, or is it because of insufficient support? To provide foundational insights in this field, we conducted a qualitative 5-hour programming study with 30 professional software developers implementing 3 privacy-sensitive programming tasks that were designed with GDPR compliance in mind. To explore if and how developers implement privacy requirements, participants were divided into 3 groups: control, privacy prompted, and privacy expert-supported. After task completion, we conducted follow-up interviews. Alarmingly, almost all participants submitted non-GDPR-compliant solutions (79/90). In particular, none of the 3 tasks were solved privacy-compliant by all 30 participants, with the non-prompted group having the lowest number of 3 out of 30 privacy-compliant solution attempts. Privacy prompting and expert support only slightly improved participants' submissions, with 6/30 and 8/30 privacy-compliant attempts, respectively. In fact, all participants reported severe issues addressing common privacy requirements such as purpose limitation, user consent, or data minimization. Counterintuitively, although most developers exhibited minimal confidence in their solutions, they rarely sought online assistance or contacted the privacy expert, with only 4 out of 10 expert-supported participants explicitly asking for compliance confirmation. Instead, participants often relied on existing implementations and focused on implementing functionality and security first.",http://arxiv.org/abs/2504.06697v2,CS,Qualitative
High-Performance ARM-on-ARM Virtualization for Multicore SystemC-TLM-Based Virtual Platforms,"The increasing complexity of hardware and software requires advanced development and test methodologies for modern systems on chips. This paper presents a novel approach to ARM-on-ARM virtualization within SystemC-based simulators using Linux's KVM to achieve high-performance simulation. By running target software natively on ARM-based hosts with hardware-based virtualization extensions, our method eliminates the need for instruction-set simulators, which significantly improves performance. We present a multicore SystemC-TLM-based CPU model that can be used as a drop-in replacement for an instruction-set simulator. It places no special requirements on the host system, making it compatible with various environments. Benchmark results show that our ARM-on-ARM-based virtual platform achieves up to 10 x speedup over traditional instruction-set-simulator-based models on compute-intensive workloads. Depending on the benchmark, speedups increase to more than 100 x.",http://arxiv.org/abs/2505.12987v1,CS,Quantitative
Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks,"Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.",http://arxiv.org/abs/2505.16901v1,CS,Quantitative
Designing and Implementing Robust Test Automation Frameworks using Cucumber BDD and Java,"Modern software development demands rapid, reliable testing methods to maintain high quality in increasingly complex systems. This paper details a comprehensive approach to designing and implementing robust test automation frameworks by leveraging Cucumber BDD with Java. By utilizing Cucumber BDD natural language syntax, the framework enables clear communication between technical and non-technical team members, ensuring that requirements are accurately translated into executable tests. Java, renowned for its versatility and extensive libraries, serves as the backbone for creating scalable, maintainable, and efficient test scripts. The framework described herein focuses on modular architecture, facilitating re usability and streamlined maintenance across diverse application domains. It systematically addresses challenges such as test data management, dynamic environment handling, and integration with continuous integration/continuous delivery pipelines. Empirical evaluations demonstrate that this integrated approach not only reduces manual testing effort but also significantly enhances defect detection and overall software reliability. The methodology encourages the adoption of best practices in test design, including clear documentation, iterative development, and automated reporting.",http://arxiv.org/abs/2505.17168v1,CS,Quantitative
"AI Safety in the Eyes of the Downstream Developer: A First Look at Concerns, Practices, and Challenges","Pre-trained models (PTMs) have become a cornerstone of AI-based software, allowing for rapid integration and development with minimal training overhead. However, their adoption also introduces unique safety challenges, such as data leakage and biased outputs, that demand rigorous handling by downstream developers. While previous research has proposed taxonomies of AI safety concerns and various mitigation strategies, how downstream developers address these issues remains unexplored. This study investigates downstream developers' concerns, practices and perceived challenges regarding AI safety issues during AI-based software development. To achieve this, we conducted a mixed-method study, including interviews with 18 participants, a survey of 86 practitioners, and an analysis of 874 AI incidents from the AI Incident Database. Our results reveal that while developers generally demonstrate strong awareness of AI safety concerns, their practices, especially during the preparation and PTM selection phases, are often inadequate. The lack of concrete guidelines and policies leads to significant variability in the comprehensiveness of their safety approaches throughout the development lifecycle, with additional challenges such as poor documentation and knowledge gaps, further impeding effective implementation. Based on our findings, we offer suggestions for PTM developers, AI-based software developers, researchers, and policy makers to enhance the integration of AI safety measures.",http://arxiv.org/abs/2503.19444v2,CS,Mixed
Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing,"Multitemporal hyperspectral unmixing can capture dynamical evolution of materials. Despite its capability, current methods emphasize variability of endmembers while neglecting dynamics of abundances, which motivates our adoption of neural ordinary differential equations to model abundances temporally. However, this motivation is hindered by two challenges: the inherent complexity in defining, modeling and solving problem, and the absence of theoretical support. To address above challenges, in this paper, we propose a multitemporal latent dynamical (MiLD) unmixing framework by capturing dynamical evolution of materials with theoretical validation. For addressing multitemporal hyperspectral unmixing, MiLD consists of problem definition, mathematical modeling, solution algorithm and theoretical support. We formulate multitemporal unmixing problem definition by conducting ordinary differential equations and developing latent variables. We transfer multitemporal unmixing to mathematical model by dynamical discretization approaches, which describe the discreteness of observed sequence images with mathematical expansions. We propose algorithm to solve problem and capture dynamics of materials, which approximates abundance evolution by neural networks. Furthermore, we provide theoretical support by validating the crucial properties, which verifies consistency, convergence and stability theorems. The major contributions of MiLD include defining problem by ordinary differential equations, modeling problem by dynamical discretization approach, solving problem by multitemporal unmixing algorithm, and presenting theoretical support. Our experiments on both synthetic and real datasets have validated the utility of our work",http://arxiv.org/abs/2505.20902v1,CS,Quantitative
Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data,"This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.",http://arxiv.org/abs/2505.07372v1,CS,Quantitative
Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation,"The increasing complexity of embedded hardware platforms poses significant challenges for real-time workloads. Architectural features such as Intel RDT, Arm QoS, and Arm MPAM are either unavailable on commercial embedded platforms or designed primarily for server environments optimized for average-case performance and might fail to deliver the expected real-time guarantees. Arm DynamIQ Shared Unit (DSU) includes isolation features-among others, hardware per-way cache partitioning-that can improve the real-time guarantees of complex embedded multicore systems and facilitate real-time analysis. However, the DSU also targets average cases, and its real-time capabilities have not yet been evaluated. This paper presents the first comprehensive analysis of three real-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and NVIDIA Orin platforms. We integrate support for the DSU at the operating system and hypervisor level and conduct a large-scale evaluation using both synthetic and real-world benchmarks with varying types and intensities of interference. Our results make extensive use of performance counters and indicate that, although effective, the quality of partitioning and isolation provided by the DSU depends on the type and the intensity of the interfering workloads. In addition, we uncover and analyze in detail the correlation between benchmarks and different types and intensities of interference.",http://arxiv.org/abs/2503.17038v3,CS,Quantitative
Towards Trustworthy Federated Learning with Untrusted Participants,"Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.",http://arxiv.org/abs/2505.01874v1,CS,Quantitative
Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI,"Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.",http://arxiv.org/abs/2505.15031v1,CS,Quantitative
Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution,"AI-driven software development has rapidly advanced with the emergence of software development agents that leverage large language models (LLMs) to tackle complex, repository-level software engineering tasks. These agents go beyond just generation of final code; they engage in multi-step reasoning, utilize various tools for code modification and debugging, and interact with execution environments to diagnose and iteratively resolve issues. However, most existing evaluations focus primarily on static analyses of final code outputs, yielding limited insights into the agents' dynamic problem-solving processes. To fill this gap, we conduct an in-depth empirical study on 3,977 solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked agents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our exploratory analysis shows that Python execution errors during the issue resolution phase correlate with lower resolution rates and increased reasoning overheads. We have identified the most prevalent errors -- such as ModuleNotFoundError and TypeError -- and highlighted particularly challenging errors like OSError and database-related issues (e.g., IntegrityError) that demand significantly more debugging effort. Furthermore, we have discovered 3 bugs in the SWE-Bench platform that affect benchmark fairness and accuracy; these issues have been reported to and confirmed by the maintainers. To promote transparency and foster future research, we publicly share our datasets and analysis scripts.",http://arxiv.org/abs/2503.12374v2,CS,Quantitative
Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach,"Convolutional neural networks (CNNs) have become widely adopted in gravitational wave (GW) detection pipelines due to their ability to automatically learn hierarchical features from raw strain data. However, the physical meaning of these learned features remains underexplored, limiting the interpretability of such models. In this work, we propose a hybrid architecture that combines a CNN-based feature extractor with a random forest (RF) classifier to improve both detection performance and interpretability. Unlike prior approaches that directly connect classifiers to CNN outputs, our method introduces four physically interpretable metrics - variance, signal-to-noise ratio (SNR), waveform overlap, and peak amplitude - computed from the final convolutional layer. These are jointly used with the CNN output in the RF classifier to enable more informed decision boundaries. Tested on long-duration strain datasets, our hybrid model outperforms a baseline CNN model, achieving a relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10 events per month. Notably, it also shows improved detection of low-SNR signals (SNR $\le$ 10), which are especially vulnerable to misclassification in noisy environments. Feature attribution via the RF model reveals that both CNN-extracted and handcrafted features contribute significantly to classification decisions, with learned variance and CNN outputs ranked among the most informative. These findings suggest that physically motivated post-processing of CNN feature maps can serve as a valuable tool for interpretable and efficient GW detection, bridging the gap between deep learning and domain knowledge.",http://arxiv.org/abs/2505.20357v1,CS,Quantitative
Practical Pipeline-Aware Regression Test Optimization for Continuous Integration,"Massive, multi-language, monolithic repositories form the backbone of many modern, complex software systems. To ensure consistent code quality while still allowing fast development cycles, Continuous Integration (CI) is commonly applied. However, operating CI at such scale not only leads to a single point of failure for many developers, but also requires computational resources that may reach feasibility limits and cause long feedback latencies. To address these issues, developers commonly split test executions across multiple pipelines, running small and fast tests in pre-submit stages while executing long-running and flaky tests in post-submit pipelines. Given the long runtimes of many pipelines and the substantial proportion of passing test executions (98% in our pre-submit pipelines), there not only a need but also potential for further improvements by prioritizing and selecting tests. However, many previously proposed regression optimization techniques are unfit for an industrial context, because they (1) rely on complex and difficult-to-obtain features like per-test code coverage that are not feasible in large, multi-language environments, (2) do not automatically adapt to rapidly changing systems where new tests are continuously added or modified, and (3) are not designed to distinguish the different objectives of pre- and post-submit pipelines: While pre-submit testing should prioritize failing tests, post-submit pipelines should prioritize tests that indicate non-flaky changes by transitioning from pass to fail outcomes or vice versa. To overcome these issues, we developed a lightweight and pipeline-aware regression test optimization approach that employs Reinforcement Learning models trained on language-agnostic features. We evaluated our approach on a large industry dataset collected over a span of 20 weeks of CI test executions. When predicting...",http://arxiv.org/abs/2501.11550v1,CS,Quantitative
Machine Learning Models for Reinforced Concrete Pipes Condition Prediction: The State-of-the-Art Using Artificial Neural Networks and Multiple Linear Regression in a Wisconsin Case Study,"The aging sewer infrastructure in the U.S., covering 2.1 million kilometers, encounters increasing structural issues, resulting in around 75,000 yearly sanitary sewer overflows that present serious economic, environmental, and public health hazards. Conventional inspection techniques and deterministic models do not account for the unpredictable nature of sewer decline, whereas probabilistic methods depend on extensive historical data, which is frequently lacking or incomplete. This research intends to enhance predictive accuracy for the condition of sewer pipelines through machine learning models artificial neural networks (ANNs) and multiple linear regression (MLR) by integrating factors such as pipe age, material, diameter, environmental influences, and PACP ratings. ANNs utilized ReLU activation functions and Adam optimization, whereas MLR applied regularization to address multicollinearity, with both models assessed through metrics like RMSE, MAE, and R2. The findings indicated that ANNs surpassed MLR, attaining an R2 of 0.9066 compared to MLRs 0.8474, successfully modeling nonlinear relationships while preserving generalization. MLR, on the other hand, offered enhanced interpretability by pinpointing significant predictors such as residual buildup. As a result, pipeline degradation is driven by pipe length, age, and pipe diameter as key predictors, while depth, soil type, and segment show minimal influence in this analysis. Future studies ought to prioritize hybrid models that merge the accuracy of ANNs with the interpretability of MLR, incorporating advanced methods such as SHAP analysis and transfer learning to improve scalability in managing infrastructure and promoting environmental sustainability.",http://arxiv.org/abs/2502.00363v1,CS,Quantitative
Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM Virtual Platforms,"In today's technology-driven world, early-stage software development and testing are crucial. Virtual Platforms (VPs) have become indispensable tools for this purpose as they serve as a platform to execute and debug the unmodified target software at an early design stage. With the increasing complexity of software, especially in areas like Artificial Intelligence (AI) applications, VPs need to provide high simulation speed to ensure the target software executes within a reasonable time. Hybrid simulation, which combines virtual models with real hardware, can improve the performance of VPs. This paper introduces a novel approach for integrating real Peripheral Component Interconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI devices enable high performance, easy integration, and allow introspection for analysis and optimization. To illustrate the practical application of our approach, we present a case study where we integrate Google Coral's Edge Tensor Processing Unit (TPU) into an ARM-based VP. The integration allows efficient execution of AI workloads, accelerating simulation speeds by up to 480x while eliminating the need for complex virtual device models. Beyond accelerating AI-workload execution, our framework enables driver development, regression testing across architectures, and device communication analysis. Our findings demonstrate that embedding PCI devices into SystemC simulations significantly enhances",http://arxiv.org/abs/2505.15590v1,CS,Mixed
Measuring Social Influence with Networked Synthetic Control,"Measuring social influence is difficult due to the lack of counter-factuals and comparisons. By combining machine learning-based modeling and network science, we present general properties of social value, a recent measure for social influence using synthetic control applicable to political behavior. Social value diverges from centrality measures on in that it relies on an external regressor to predict an output variable of interest, generates a synthetic measure of influence, then distributes individual contribution based on a social network. Through theoretical derivations, we show the properties of SV under linear regression with and without interaction, across lattice networks, power-law networks, and random graphs. A reduction in computation can be achieved for any ensemble model. Through simulation, we find that the generalized friendship paradox holds -- that in certain situations, your friends have on average more influence than you do.",http://arxiv.org/abs/2505.13334v1,CS,Quantitative
Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning,"Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.",http://arxiv.org/abs/2505.20648v1,CS,Quantitative
Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation,"Inference for Large Language Models (LLMs) is computationally demanding. To reduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to store intermediate activations, enabling GPUs to perform only the incremental computation required for each new token. This approach significantly lowers the computational overhead for token generation. However, the memory required for KV caching grows rapidly, often exceeding the capacity of GPU memory. A cost-effective alternative is to offload KV cache to CPU memory, which alleviates GPU memory pressure but shifts the bottleneck to the limited bandwidth of the PCIe connection between the CPU and GPU. Existing methods attempt to address these issues by overlapping GPU computation with I/O or employing CPU-GPU heterogeneous execution, but they are hindered by excessive data movement and dependence on CPU capabilities. In this paper, we introduce an efficient CPU-GPU I/O-aware LLM inference method that avoids transferring the entire KV cache from CPU to GPU by recomputing partial KV cache from activations while concurrently transferring the remaining KV cache via PCIe bus. This approach overlaps GPU recomputation with data transfer to minimize idle GPU time and maximize inference performance. Our method is fully automated by integrating a profiler module that utilizes input characteristics and system hardware information, a scheduler module to optimize the distribution of computation and communication workloads, and a runtime module to efficiently execute the derived execution plan. Experimental results show that our method achieves up to 35.8% lower latency and 46.2% higher throughput during decoding compared to state-of-the-art approaches.",http://arxiv.org/abs/2411.17089v1,CS,Quantitative
Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty,"Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.",http://arxiv.org/abs/2505.17281v1,CS,Quantitative
Fragmented Layer Grouping in GUI Designs Through Graph Learning Based on Multimodal Information,"Automatically constructing GUI groups of different granularities constitutes a critical intelligent step towards automating GUI design and implementation tasks. Specifically, in the industrial GUI-to-code process, fragmented layers may decrease the readability and maintainability of generated code, which can be alleviated by grouping semantically consistent fragmented layers in the design prototypes. This study aims to propose a graph-learning-based approach to tackle the fragmented layer grouping problem according to multi-modal information in design prototypes. Our graph learning module consists of self-attention and graph neural network modules. By taking the multimodal fused representation of GUI layers as input, we innovatively group fragmented layers by classifying GUI layers and regressing the bounding boxes of the corresponding GUI components simultaneously. Experiments on two real-world datasets demonstrate that our model achieves state-of-the-art performance. A further user study is also conducted to validate that our approach can assist an intelligent downstream tool in generating more maintainable and readable front-end code.",http://arxiv.org/abs/2412.05555v1,CS,Quantitative
Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling,"Cyclic peptides offer inherent advantages in pharmaceuticals. For example, cyclic peptides are more resistant to enzymatic hydrolysis compared to linear peptides and usually exhibit excellent stability and affinity. Although deep generative models have achieved great success in linear peptide design, several challenges prevent the development of computational methods for designing diverse types of cyclic peptides. These challenges include the scarcity of 3D structural data on target proteins and associated cyclic peptide ligands, the geometric constraints that cyclization imposes, and the involvement of non-canonical amino acids in cyclization. To address the above challenges, we introduce CpSDE, which consists of two key components: AtomSDE, a generative structure prediction model based on harmonic SDE, and ResRouter, a residue type predictor. Utilizing a routed sampling algorithm that alternates between these two models to iteratively update sequences and structures, CpSDE facilitates the generation of cyclic peptides. By employing explicit all-atom and bond modeling, CpSDE overcomes existing data limitations and is proficient in designing a wide variety of cyclic peptides. Our experimental results demonstrate that the cyclic peptides designed by our method exhibit reliable stability and affinity.",http://arxiv.org/abs/2505.21452v1,CS,Quantitative
Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning,"Early-stage startup investment is a high-risk endeavor characterized by scarce data and uncertain outcomes. Traditional machine learning approaches often require large, labeled datasets and extensive fine-tuning, yet remain opaque and difficult for domain experts to interpret or improve. In this paper, we propose a transparent and data-efficient investment decision framework powered by memory-augmented large language models (LLMs) using in-context learning (ICL). Central to our method is a natural language policy embedded directly into the LLM prompt, enabling the model to apply explicit reasoning patterns and allowing human experts to easily interpret, audit, and iteratively refine the logic. We introduce a lightweight training process that combines few-shot learning with an in-context learning loop, enabling the LLM to update its decision policy iteratively based on structured feedback. With only minimal supervision and no gradient-based optimization, our system predicts startup success far more accurately than existing benchmarks. It is over 20x more precise than random chance, which succeeds 1.9% of the time. It is also 7.1x more precise than the typical 5.6% success rate of top-tier venture capital (VC) firms.",http://arxiv.org/abs/2505.21427v1,CS,Quantitative
Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models,"The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",http://arxiv.org/abs/2505.17974v1,CS,Quantitative
The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training,"Recent large language models (LLMs) exhibit impressive reasoning but often over-think, generating excessively long responses that hinder efficiency. We introduce DIET ( DIfficulty-AwarE Training), a framework that systematically cuts these ""token calories"" by integrating on-the-fly problem difficulty into the reinforcement learning (RL) process. DIET dynamically adapts token compression strategies by modulating token penalty strength and conditioning target lengths on estimated task difficulty, to optimize the performance-efficiency trade-off. We also theoretically analyze the pitfalls of naive reward weighting in group-normalized RL algorithms like GRPO, and propose Advantage Weighting technique, which enables stable and effective implementation of these difficulty-aware objectives. Experimental results demonstrate that DIET significantly reduces token counts while simultaneously improving reasoning performance. Beyond raw token reduction, we show two crucial benefits largely overlooked by prior work: (1) DIET leads to superior inference scaling. By maintaining high per-sample quality with fewer tokens, it enables better scaling performance via majority voting with more samples under fixed computational budgets, an area where other methods falter. (2) DIET enhances the natural positive correlation between response length and problem difficulty, ensuring verbosity is appropriately allocated, unlike many existing compression methods that disrupt this relationship. Our analyses provide a principled and effective framework for developing more efficient, practical, and high-performing LLMs.",http://arxiv.org/abs/2505.19217v1,CS,Quantitative
StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs,"As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.",http://arxiv.org/abs/2505.20139v1,CS,Quantitative
HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs,"Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.",http://arxiv.org/abs/2505.21140v1,CS,Quantitative
Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics,"Since the disruption in LLM technology brought about by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation remains a popular field of research, code evaluation using LLMs remains a problem with no conclusive solution. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.",http://arxiv.org/abs/2503.23989v1,CS,Quantitative
Understanding GEMM Performance and Energy on NVIDIA Ada Lovelace: A Machine Learning-Based Analytical Approach,"Analytical framework for predicting General Matrix Multiplication (GEMM) performance on modern GPUs, focusing on runtime, power consumption, and energy efficiency. Our study employs two approaches: a custom-implemented tiled matrix multiplication kernel for fundamental analysis, and NVIDIA's CUTLASS library for comprehensive performance data collection across advanced configurations. Using the NVIDIA RTX 4070 as our experimental platform, we developed a Random Forest-based prediction model with multi-output regression capability. Through analysis of both naive tiled matrix multiplication with varying tile sizes (1 to 32) and 16,128 CUTLASS GEMM operations across diverse configurations, we identified critical performance patterns related to matrix dimensions, thread block configurations, and memory access patterns. Our framework achieved exceptional accuracy with an R^2 score of 0.98 for runtime prediction (mean error 15.57%) and 0.78 for power prediction (median error 5.42%). The system successfully predicts performance across matrix sizes, demonstrating robust scaling behavior. Our results show that optimal tile size selection can improve performance by up to 3.2x while reducing power consumption by 22% compared to baseline configurations. Analysis of shared memory utilization and SM occupancy reveals that tile sizes of 16x16 achieve the best balance between parallelism and resource usage. The implementation of our framework, including prediction models and analysis tools, is available as an open-source project at GPPerf [https://github.com/pavlyhalim/GPPerf].",http://arxiv.org/abs/2411.16954v1,CS,Quantitative
Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach,"This paper focuses on the design of spatial experiments to optimize the amount of information derived from the experimental data and enhance the accuracy of the resulting causal effect estimator. We propose a surrogate function for the mean squared error (MSE) of the estimator, which facilitates the use of classical graph cut algorithms to learn the optimal design. Our proposal offers three key advances: (1) it accommodates moderate to large spatial interference effects; (2) it adapts to different spatial covariance functions; (3) it is computationally efficient. Theoretical results and numerical experiments based on synthetic environments and a dispatch simulator that models a city-scale ridesharing market, further validate the effectiveness of our design. A python implementation of our method is available at https://github.com/Mamba413/CausalGraphCut.",http://arxiv.org/abs/2505.20130v1,CS,Quantitative
Learnable Kernel Density Estimation for Graphs,"This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and complexity. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.",http://arxiv.org/abs/2505.21285v1,CS,Quantitative
Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement,"Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.",http://arxiv.org/abs/2505.18131v1,CS,Quantitative
On the Structure and Semantics of Identifier Names Containing Closed Syntactic Category Words,"Identifier names are crucial components of code, serving as primary clues for developers to understand program behavior. This paper investigates the linguistic structure of identifier names by extending the concept of grammar patterns; representations of the part-of-speech (PoS) sequences that underlie identifier phrases. The specific focus is on closed syntactic categories (e.g., prepositions, conjunctions, determiners), which are rarely studied in software engineering despite their central role in general natural language. The Closed Category Identifier Dataset (CCID) is presented, a new manually annotated dataset of 1,275 identifiers drawn from 30 open-source systems. The relationship between closed-category grammar patterns and program behavior is analyzed using grounded theory coding, statistical, and pattern analysis. The results reveal recurring structures that developers use to express control flow, data transformation, temporal reasoning, and behavioral roles through naming. This study contributes an empirical foundation for understanding how developers adapt linguistic resources to encode behavior in source code. By analyzing closed-category terms and their associated grammar patterns, the work highlights a previously underexplored dimension of identifier semantics and identifies promising directions for future research in naming support, comprehension, and education.",http://arxiv.org/abs/2505.18444v1,CS,Mixed
Machine Learning Evaluation Metric Discrepancies across Programming Languages and Their Components: Need for Standardization,"This study evaluates metrics for tasks such as classification, regression, clustering, correlation analysis, statistical tests, segmentation, and image-to-image (I2I) translation. Metrics were compared across Python libraries, R packages, and Matlab functions to assess their consistency and highlight discrepancies. The findings underscore the need for a unified roadmap to standardize metrics, ensuring reliable and reproducible ML evaluations across platforms. This study examined a wide range of evaluation metrics across various tasks and found only some to be consistent across platforms, such as (i) Accuracy, Balanced Accuracy, Cohens Kappa, F-beta Score, MCC, Geometric Mean, AUC, and Log Loss in binary classification; (ii) Accuracy, Cohens Kappa, and F-beta Score in multi-class classification; (iii) MAE, MSE, RMSE, MAPE, Explained Variance, Median AE, MSLE, and Huber in regression; (iv) Davies-Bouldin Index and Calinski-Harabasz Index in clustering; (v) Pearson, Spearman, Kendall's Tau, Mutual Information, Distance Correlation, Percbend, Shepherd, and Partial Correlation in correlation analysis; (vi) Paired t-test, Chi-Square Test, ANOVA, Kruskal-Wallis Test, Shapiro-Wilk Test, Welchs t-test, and Bartlett's test in statistical tests; (vii) Accuracy, Precision, and Recall in 2D segmentation; (viii) Accuracy in 3D segmentation; (ix) MAE, MSE, RMSE, and R-Squared in 2D-I2I translation; and (x) MAE, MSE, and RMSE in 3D-I2I translation. Given observation of discrepancies in a number of metrics (e.g. precision, recall and F1 score in binary classification, WCSS in clustering, multiple statistical tests, and IoU in segmentation, amongst multiple metrics), this study concludes that ML evaluation metrics require standardization and recommends that future research use consistent metrics for different tasks to effectively compare ML techniques and solutions.",http://arxiv.org/abs/2411.12032v2,CS,Mixed
Recurrent CircuitSAT Sampling for Sequential Circuits,"In this work, we introduce a novel GPU-accelerated circuit satisfiability (CircuitSAT) sampling technique for sequential circuits. This work is motivated by the requirement in constrained random verification (CRV) to generate input stimuli to validate the functionality of digital hardware circuits. A major challenge in CRV is generating inputs for sequential circuits, along with the appropriate number of clock cycles required to meet design constraints. Traditional approaches often use Boolean satisfiability (SAT) samplers to generate inputs by unrolling state transitions over a fixed number of clock cycles. However, these methods do not guarantee that a solution exists for the given number of cycles. Consequently, producing input stimuli together with the required clock cycles is essential for thorough testing and verification. Our approach converts the logical constraints and temporal behavior of sequential circuits into a recurrent CircuitSAT problem, optimized via gradient descent to efficiently explore a diverse set of valid solutions, including their associated number of clock cycles. By operating directly on the circuit structure, our method reinterprets the sampling process as a supervised multi-output regression task. This differentiable framework enables independent element-wise operations on each tensor element, facilitating parallel execution during learning. As a result, we achieve GPU-accelerated sampling with substantial runtime improvements (up to 105.1x) over state-of-the-art heuristic samplers. We demonstrate the effectiveness of our method through extensive evaluations on circuit problems from the ISCAS-89 and ITC'99 benchmark suites.",http://arxiv.org/abs/2502.21226v2,CS,Quantitative
TabPFN: One Model to Rule Them All?,"Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a transformer-based deep learning model for regression and classification on tabular data, which they claim ""outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time."" Furthermore, they have called TabPFN a ""foundation model"" for tabular data, as it can support ""data generation, density estimation, learning reusable embeddings and fine-tuning"". If these statements are well-supported, TabPFN may have the potential to supersede existing modeling approaches on a wide range of statistical tasks, mirroring a similar revolution in other areas of artificial intelligence that began with the advent of large language models. In this paper, we provide a tailored explanation of how TabPFN works for a statistics audience, by emphasizing its interpretation as approximate Bayesian inference. We also provide more evidence of TabPFN's ""foundation model"" capabilities: We show that an out-of-the-box application of TabPFN vastly outperforms specialized state-of-the-art methods for semi-supervised parameter estimation, prediction under covariate shift, and heterogeneous treatment effect estimation. We further show that TabPFN can outperform LASSO at sparse regression and can break a robustness-efficiency trade-off in classification. All experiments can be reproduced using the code provided at https://github.com/qinglong-tian/tabpfn_study (https://github.com/qinglong-tian/tabpfn_study).",http://arxiv.org/abs/2505.20003v1,CS,Quantitative
A fast sound power prediction tool for genset noise using machine learning,"This paper investigates the application of machine learning regression algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian Process Regression (GPR) for predicting sound power levels of gensets, offering significant value for marketing and sales teams during the early bidding process. When engine sizes and genset enclosure dimensions are tentative, and measured noise data is unavailable, these algorithms enable reliable noise level estimation for unbuilt gensets. The study utilizes high fidelity datasets from over 100 experiments conducted at Cummins Acoustics Technology Center (ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using readily available information from the bidding and initial design stages, KRR predicts sound power with an average accuracy of within 5 dBA. While HR and GPR show slightly higher prediction errors, all models effectively capture the overall noise trends across various genset configurations. These findings present a promising method for early-stage noise estimation in genset design.",http://arxiv.org/abs/2505.20079v1,CS,Quantitative
Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising,"In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly ""expertly targeted"" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing that advisors incorporate diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Finally, we explore the broader implications of human discretion for long-term outcomes and equity, using heterogeneous treatment effect estimation. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.",http://arxiv.org/abs/2505.13325v1,CS,Mixed
How Does Users' App Knowledge Influence the Preferred Level of Detail and Format of Software Explanations?,"Context and Motivation: Due to their increasing complexity, everyday software systems are becoming increasingly opaque for users. A frequently adopted method to address this difficulty is explainability, which aims to make systems more understandable and usable. Question/problem: However, explanations can also lead to unnecessary cognitive load. Therefore, adapting explanations to the actual needs of a user is a frequently faced challenge. Principal ideas/results: This study investigates factors influencing users' preferred the level of detail and the form of an explanation (e.g., short text or video tutorial) in software. We conducted an online survey with 58 participants to explore relationships between demographics, software usage, app-specific knowledge, as well as their preferred explanation form and level of detail. The results indicate that users prefer moderately detailed explanations in short text formats. Correlation analyses revealed no relationship between app-specific knowledge and the preferred level of detail of an explanation, but an influence of demographic aspects (like gender) on app-specific knowledge and its impact on application confidence were observed, pointing to a possible mediated relationship between knowledge and preferences for explanations. Contribution: Our results show that explanation preferences are weakly influenced by app-specific knowledge but shaped by demographic and psychological factors, supporting the development of adaptive explanation systems tailored to user expertise. These findings support requirements analysis processes by highlighting important factors that should be considered in user-centered methods such as personas.",http://arxiv.org/abs/2502.06549v1,CS,Quantitative
Ethical Aspects of the Use of Social Robots in Elderly Care -- A Systematic Qualitative Review,"Background: The use of social robotics in elderly care is increasingly discussed as one way of meeting emerging care needs due to scarce resources. While many potential benefits are associated with robotic care technologies, there is a variety of ethical challenges. To support steps towards a responsible implementation and use, this review develops an overview on ethical aspects of the use of social robots in elderly care from a decision-makers' perspective. Methods: Electronic databases were queried using a comprehensive search strategy based on the key concepts of ""ethical aspects"", ""social robotics"" and ""elderly care"". Abstract and title screening was conducted by two authors independently. Full-text screening was conducted by one author following a joint consolidation phase. Data was extracted using MAXQDA24 by one author, based on a consolidated coding framework. Analysis was performed through modified qualitative content analysis. Results: A total of 1,518 publications were screened, and 248 publications were included. We have organized our analysis in a scheme of ethical hazards, ethical opportunities and unsettled questions, identifying at least 60 broad ethical aspects affecting three different stakeholder groups. While some ethical issues are well-known and broadly discussed our analysis shows a plethora of potentially relevant aspects, often only marginally recognized, that are worthy of consideration from a practical perspective. Discussion: The findings highlight the need for a contextual and detailed evaluation of implementation scenarios. To make use of the vast knowledge of the ethical discourse, we hypothesize that decision-makers need to understand the specific nature of this discourse to be able to engage in careful ethical deliberation.",http://arxiv.org/abs/2505.09224v1,CS,Qualitative
Wildfire Detection Using Vision Transformer with the Wildfire Dataset,"The critical need for sophisticated detection techniques has been highlighted by the rising frequency and intensity of wildfires in the US, especially in California. In 2023, wildfires caused 130 deaths nationwide, the highest since 1990. In January 2025, Los Angeles wildfires which included the Palisades and Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused loss of human lives. The devastation underscores the urgent need for effective detection and prevention strategies. Deep learning models, such as Vision Transformers (ViTs), can enhance early detection by processing complex image data with high accuracy. However, wildfire detection faces challenges, including the availability of high-quality, real-time data. Wildfires often occur in remote areas with limited sensor coverage, and environmental factors like smoke and cloud cover can hinder detection. Additionally, training deep learning models is computationally expensive, and issues like false positives/negatives and scaling remain concerns. Integrating detection systems with real-time alert mechanisms also poses difficulties. In this work, we used the wildfire dataset consisting of 10.74 GB high-resolution images categorized into 'fire' and 'nofire' classes is used for training the ViT model. To prepare the data, images are resized to 224 x 224 pixels, converted into tensor format, and normalized using ImageNet statistics.",http://arxiv.org/abs/2505.17395v1,CS,Quantitative
A Dataset of Performance Measurements and Alerts from Mozilla (Data Artifact),"Performance regressions in software systems can lead to significant financial losses and degraded user satisfaction, making their early detection and mitigation critical. Despite the importance of practices that capture performance regressions early, there is a lack of publicly available datasets that comprehensively capture real-world performance measurements, expert-validated alerts, and associated metadata such as bugs and testing conditions. To address this gap, we introduce a unique dataset to support various research studies in performance engineering, anomaly detection, and machine learning. This dataset was collected from Mozilla Firefox's performance testing infrastructure and comprises 5,655 performance time series, 17,989 performance alerts, and detailed annotations of resulting bugs collected from May 2023 to May 2024. By publishing this dataset, we provide researchers with an invaluable resource for studying performance trends, developing novel change point detection methods, and advancing performance regression analysis across diverse platforms and testing environments. The dataset is available at https://doi.org/10.5281/zenodo.14642238",http://arxiv.org/abs/2503.16332v1,CS,Quantitative
Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means,"This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.",http://arxiv.org/abs/2505.17836v1,CS,Quantitative
Assessing Quantum Extreme Learning Machines for Software Testing in Practice,"Machine learning has been extensively applied for various classical software testing activities such as test generation, minimization, and prioritization. Along the same lines, recently, there has been interest in applying quantum machine learning to software testing. For example, Quantum Extreme Learning Machines (QELMs) were recently applied for testing classical software of industrial elevators. However, most studies on QELMs, whether in software testing or other areas, used ideal quantum simulators that fail to account for the noise in current quantum computers. While ideal simulations offer insight into QELM's theoretical capabilities, they do not enable studying their performance on current noisy quantum computers. To this end, we study how quantum noise affects QELM in three industrial and real-world classical software testing case studies, providing insights into QELMs' robustness to noise. Such insights assess QELMs potential as a viable solution for industrial software testing problems in today's noisy quantum computing. Our results show that QELMs are significantly affected by quantum noise, with a performance drop of 250% in regression tasks and 50% in classification tasks. Although introducing noise during both ML training and testing phases can improve results, the reduction is insufficient for practical applications. While error mitigation techniques can enhance noise resilience, achieving an average 3.0% performance drop in classification, but their effectiveness varies by context, highlighting the need for QELM-tailored error mitigation strategies.",http://arxiv.org/abs/2410.15494v3,CS,Quantitative
Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification,"This paper presents a Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC), especially effective in handling non-stationary environments, data scarcity and noise perturbations. We introduce a versatile wavelet probabilistic module designed to extract and analyse the probabilistic features, which can seamlessly integrate with a variety of neural network architectures. This probabilistic module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). Such formulation extends the application of wavelet probabilistic neural networks to deep neural networks for MTSC. The AWPG constructs an ensemble probabilistic model addressing different data scarcities and non-stationarity; it adaptively selects the optimal ones and generates probabilistic features for APTCN. The APTCN analyses the correlations of the features and forms a comprehensive feature space with existing MTSC models for classification. Here, we instantiate the proposed module to work in parallel with a Long Short-Term Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN), demonstrating its broad applicability in time series analysis. The WPRCN is evaluated on 30 diverse MTS datasets and outperforms all the benchmark algorithms on average accuracy and rank, exhibiting pronounced strength in handling scarce data and physiological data subject to perturbations and non-stationarities.",http://arxiv.org/abs/2505.17307v1,CS,Quantitative
The emergence of sparse attention: impact of data distribution and benefits of repetition,"Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",http://arxiv.org/abs/2505.17863v1,CS,Mixed
Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference,"Uncertainty quantification (UQ) in scientific machine learning is increasingly critical as neural networks are widely adopted to tackle complex problems across diverse scientific disciplines. For physics-informed neural networks (PINNs), a prominent model in scientific machine learning, uncertainty is typically quantified using Bayesian or dropout methods. However, both approaches suffer from a fundamental limitation: the prior distribution or dropout rate required to construct honest confidence sets cannot be determined without additional information. In this paper, we propose a novel method within the framework of extended fiducial inference (EFI) to provide rigorous uncertainty quantification for PINNs. The proposed method leverages a narrow-neck hyper-network to learn the parameters of the PINN and quantify their uncertainty based on imputed random errors in the observations. This approach overcomes the limitations of Bayesian and dropout methods, enabling the construction of honest confidence sets based solely on observed data. This advancement represents a significant breakthrough for PINNs, greatly enhancing their reliability, interpretability, and applicability to real-world scientific and engineering challenges. Moreover, it establishes a new theoretical framework for EFI, extending its application to large-scale models, eliminating the need for sparse hyper-networks, and significantly improving the automaticity and robustness of statistical inference.",http://arxiv.org/abs/2505.19136v1,CS,Mixed
LaSER: How Learning Can Guide the Evolution of Equations,"Evolution and learning are two distinct yet complementary forms of adaptation. While evolutionary processes operate across generations via the selection of genotypes, learning occurs within the lifetime of an individual, shaping behavior through phenotypic adjustment. The Baldwin effect describes how lifetime learning can improve evolutionary search without altering inherited structures. While this has proven effective in areas like neuroevolution, where gradient-based learning is often used to fine-tune weights or behaviors produced by evolution, it remains underexplored in systems that evolve non-differentiable symbolic structures like Genetic Programming (GP). GP evolves explicit syntax trees that represent equations, offering strong interpretability but limited generalization due to the burden of discovering both useful representations and precise mappings. Here, we show for the first time that integrating a simple form of supervised learning, applied at the semantic or behavioral level during evaluation, can effectively guide the evolution of equations in GP. To achieve this, we propose a new GP pipeline, LaSER (Latent Semantic Evolutionary Regression), where each GP individual generates a semantic representation that is passed to a supervised learner. The quality of the learned mapping is used to assign fitness, without modifying the underlying syntax tree or evolutionary process. Across standard symbolic regression benchmarks, in terms of generalization ability, LaSER significantly outperforms traditional GP and, in several cases, matches or exceeds popular machine learning regressors, while preserving the symbolic interpretability. By separating evolution from learning, LaSER offers a practical route to integrating GP with modern ML workflows, and opens new avenues for research at the intersection of evolutionary computation and representation learning.",http://arxiv.org/abs/2505.17309v1,CS,Quantitative
Unlocking FedNL: Self-Contained Compute-Optimized Implementation,"Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.",http://arxiv.org/abs/2410.08760v2,CS,Quantitative
Reformulating Regression Test Suite Optimization using Quantum Annealing -- an Empirical Study,"Maintaining software quality is crucial in the dynamic landscape of software development. Regression testing ensures that software works as expected after changes are implemented. However, re-executing all test cases for every modification is often impractical and costly, particularly for large systems. Although very effective, traditional test suite optimization techniques are often impractical in resource-constrained scenarios, as they are computationally expensive. Hence, quantum computing solutions have been developed to improve their efficiency but have shown drawbacks in terms of effectiveness. We propose reformulating the regression test case selection problem to use quantum computation techniques better. Our objectives are (i) to provide more efficient solutions than traditional methods and (ii) to improve the effectiveness of previously proposed quantum-based solutions. We propose SelectQA, a quantum annealing approach that can outperform the quantum-based approach BootQA in terms of effectiveness while obtaining results comparable to those of the classic Additional Greedy and DIV-GA approaches. Regarding efficiency, SelectQA outperforms DIV-GA and has similar results with the Additional Greedy algorithm but is exceeded by BootQA.",http://arxiv.org/abs/2411.15963v2,CS,Quantitative
Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation,"Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at https://github.com/RUCAIBox/RioRAG.",http://arxiv.org/abs/2505.20825v1,CS,Quantitative
Anomaly detection in radio galaxy data with trainable COSFIRE filters,"Detecting anomalies in radio astronomy is challenging due to the vast amounts of data and the rarity of labeled anomalous examples. Addressing this challenge requires efficient methods capable of identifying unusual radio galaxy morphologies without relying on extensive supervision. This work introduces an innovative approach to anomaly detection based on morphological characteristics of the radio sources using trainable COSFIRE (Combination of Shifted Filter Responses) filters as an efficient alternative to complex deep learning methods. The framework integrates COSFIRE descriptors with an unsupervised Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy morphologies. Evaluations on a radio galaxy benchmark data set demonstrate strong performance, with the COSFIRE-based approach achieving a geometric mean (G-Mean) score of 79%, surpassing the 77% achieved by a computationally intensive deep learning autoencoder. By characterizing normal patterns and detecting deviations, this semi-supervised methodology overcomes the need for anomalous examples in the training set, a major limitation of traditional supervised methods. This approach shows promise for next-generation radio telescopes, where fast processing and the ability to discover unknown phenomena are crucial.",http://arxiv.org/abs/2505.18643v1,CS,Quantitative
Reinforcement Speculative Decoding for Fast Ranking,"Large Language Models (LLMs) have been widely adopted in ranking systems such as information retrieval (IR) systems and recommender systems (RSs). To alleviate the latency of auto-regressive decoding, some studies explore the single (first) token decoding for ranking approximation, but they suffer from severe degradation in tail positions. Although speculative decoding (SD) methods can be a remedy with verification at different positions, they face challenges in ranking systems due to their left-to-right decoding paradigm. Firstly, ranking systems require strict latency constraints, but verification rounds in SD methods remain agnostic; Secondly, SD methods usually discard listwise ranking knowledge about unaccepted items in previous rounds, hindering future multi-token prediction, especially when candidate tokens are the unaccepted items. In this paper, we propose a Reinforcement Speculative Decoding method for fast ranking inference of LLMs. To meet the ranking systems' latency requirement, we propose an up-to-down decoding paradigm that employs an agent to iteratively modify the ranking sequence under a constrained budget. Specifically, we design a ranking-tailored policy optimization, actively exploring optimal multi-round ranking modification policy verified by LLMs via reinforcement learning (RL). To better approximate the target LLM under the constrained budget, we trigger the agent fully utilizing the listwise ranking knowledge about all items verified by LLMs across different rounds in RL, enhancing the modification policy of the agent. More importantly, we demonstrate the theoretical robustness and advantages of our paradigm and implementation. Experiments on both IR and RS tasks show the effectiveness of our proposed method.",http://arxiv.org/abs/2505.20316v1,CS,Quantitative
LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation,"Enhancing the efficiency of high-quality image generation using Diffusion Models (DMs) is a significant challenge due to the iterative nature of the process. Flow Matching (FM) is emerging as a powerful generative modeling paradigm based on a simulation-free training objective instead of a score-based one used in DMs. Typical FM approaches rely on a Gaussian distribution prior, which induces curved, conditional probability paths between the prior and target data distribution. These curved paths pose a challenge for the Ordinary Differential Equation (ODE) solver, requiring a large number of inference calls to the flow prediction network. To address this issue, we present Learned Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for training FM-based image generation models using a better-suited prior distribution learned via a regression-based auxiliary model. By initializing the ODE solver with a prior closer to the target data distribution, LeDiFlow enables the learning of more computationally tractable probability paths. These paths directly translate to fewer solver steps needed for high-quality image generation at inference time. Our method utilizes a State-Of-The-Art (SOTA) transformer architecture combined with latent space sampling and can be trained on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably outperforms the respective FM baselines. For instance, when operating directly on pixels, our model accelerates inference by up to 3.75x compared to the corresponding pixel-space baseline. Simultaneously, our latent FM model enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy (CMMD) metric against its respective baseline.",http://arxiv.org/abs/2505.20723v1,CS,Quantitative
A high-performance and portable implementation of the SISSO method for CPUs and GPUs,"SISSO (sure-independence screening and sparsifying operator) is an artificial intelligence (AI) method based on symbolic regression and compressed sensing widely used in materials science research. SISSO++ is its C++ implementation that employs MPI and OpenMP for parallelization, rendering it well-suited for high-performance computing (HPC) environments. As heterogeneous hardware becomes mainstream in the HPC and AI fields, we chose to port the SISSO++ code to GPUs using the Kokkos performance-portable library. Kokkos allows us to maintain a single codebase for both Nvidia and AMD GPUs, significantly reducing the maintenance effort. In this work, we summarize the necessary code changes we did to achieve hardware and performance portability. This is accompanied by performance benchmarks on Nvidia and AMD GPUs. We demonstrate the speedups obtained from using GPUs across the three most time-consuming parts of our code.",http://arxiv.org/abs/2502.20072v1,CS,Quantitative
Min-Max Correlation Clustering via Neighborhood Similarity,"We present an efficient algorithm for the min-max correlation clustering problem. The input is a complete graph where edges are labeled as either positive $(+)$ or negative $(-)$, and the objective is to find a clustering that minimizes the $\ell_{\infty}$-norm of the disagreement vector over all vertices. We resolve this problem with an efficient $(3 + \epsilon)$-approximation algorithm that runs in nearly linear time, $\tilde{O}(|E^+|)$, where $|E^+|$ denotes the number of positive edges. This improves upon the previous best-known approximation guarantee of 4 by Heidrich, Irmai, and Andres, whose algorithm runs in $O(|V|^2 + |V| D^2)$ time, where $|V|$ is the number of nodes and $D$ is the maximum degree in the graph. Furthermore, we extend our algorithm to the massively parallel computation (MPC) model and the semi-streaming model. In the MPC model, our algorithm runs on machines with memory sublinear in the number of nodes and takes $O(1)$ rounds. In the streaming model, our algorithm requires only $\tilde{O}(|V|)$ space, where $|V|$ is the number of vertices in the graph. Our algorithms are purely combinatorial. They are based on a novel structural observation about the optimal min-max instance, which enables the construction of a $(3 + \epsilon)$-approximation algorithm using $O(|E^+|)$ neighborhood similarity queries. By leveraging random projection, we further show these queries can be computed in nearly linear time.",http://arxiv.org/abs/2502.12519v1,CS,Qualitative
SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability Detection,"Large Language Models (LLMs) have shown promise in software engineering tasks, but evaluating their effectiveness in vulnerability detection is challenging due to the lack of high-quality datasets. Most existing datasets are limited to function-level labels, ignoring finer-grained vulnerability patterns and crucial contextual information. Also, poor data quality such as mislabeling, inconsistent annotations, and duplicates can lead to inflated performance and weak generalization. Moreover, by including only the functions, these datasets miss broader program context, like data/control dependencies and interprocedural interactions, that are essential for accurately understanding real-world security flaws. Without this context, detection models are evaluated under unrealistic assumptions. To address these limitations, this paper introduces SecVulEval, a benchmark designed to support fine-grained evaluation of LLMs and other detection methods with rich contextual information. SecVulEval focuses on real-world C/C++ vulnerabilities at the statement level. This granularity enables more precise evaluation of a model's ability to localize vulnerabilities, beyond simple binary classification at the function level. By incorporating rich contextual information, SecVulEval sets a new standard for vulnerability detection benchmarks in realistic scenarios. This benchmark includes 25,440 function samples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We evaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on our dataset shows that the models are still far from accurately predicting vulnerable statements in a given function. The best-performing Claude-3.7-Sonnet model achieves 23.83% F1-score for detecting vulnerable statements with correct reasoning. Finally, we analyze the LLM outputs and provide insights into their behavior in vulnerability detection for C/C++.",http://arxiv.org/abs/2505.19828v1,CS,Quantitative
Quantum Modeling of Spatial Contiguity Constraints,"Quantum computing has demonstrated potential for solving complex optimization problems; however, its application to spatial regionalization remains underexplored. Spatial contiguity, a fundamental constraint requiring spatial entities to form connected components, significantly increases the complexity of regionalization problems, which are typically challenging for quantum modeling. This paper proposes novel quantum formulations based on a flow model that enforces spatial contiguity constraints. Our scale-aware approach employs a Discrete Quadratic Model (DQM), solvable directly on quantum annealing hardware for small-scale datasets. In addition, it designs a hybrid quantum-classical approach to manage larger-scale problems within existing hardware limitations. This work establishes a foundational framework for integrating quantum methods into practical spatial optimization tasks.",http://arxiv.org/abs/2505.12608v1,CS,Quantitative
Qimax: Efficient quantum simulation via GPU-accelerated extended stabilizer formalism,"Simulating Clifford and near-Clifford circuits using the extended stabilizer formalism has become increasingly popular, particularly in quantum error correction. Compared to the state-vector approach, the extended stabilizer formalism can solve the same problems with fewer computational resources, as it operates on stabilizers rather than full state vectors. Most existing studies on near-Clifford circuits focus on balancing the trade-off between the number of ancilla qubits and simulation accuracy, often overlooking performance considerations. Furthermore, in the presence of high-rank stabilizers, performance is limited by the sequential property of the stabilizer formalism. In this work, we introduce a parallelized version of the extended stabilizer formalism, enabling efficient execution on multi-core devices such as GPU. Experimental results demonstrate that, in certain scenarios, our Python-based implementation outperforms state-of-the-art simulators such as Qiskit and Pennylane.",http://arxiv.org/abs/2505.03307v1,CS,Quantitative
Fast Fixes and Faulty Drivers: An Empirical Analysis of Regression Bug Fixing Times in the Linux Kernel,"Regression bugs refer to situations in which something that worked previously no longer works currently. Such bugs have been pronounced in the Linux kernel. The paper focuses on regression bug tracking in the kernel by considering the time required to fix regression bugs. The dataset examined is based on the regzbot automation framework for tracking regressions in the Linux kernel. According to the results, (i) regression bug fixing times have been faster than previously reported; between 2021 and 2024, on average, it has taken less than a month to fix regression bugs. It is further evident that (ii) device drivers constitute the most prone subsystem for regression bugs, and also the fixing times vary across the kernel's subsystems. Although (iii) most commits fixing regression bugs have been reviewed, tested, or both, the kernel's code reviewing and manual testing practices do not explain the fixing times. Likewise, (iv) there is only a weak signal that code churn might contribute to explaining the fixing times statistically. Finally, (v) some subsystems exhibit strong effects for explaining the bug fixing times statistically, although overall statistical performance is modest but not atypical to the research domain. With these empirical results, the paper contributes to the efforts to better understand software regressions and their tracking in the Linux kernel.",http://arxiv.org/abs/2411.02091v1,CS,Quantitative
LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability,"Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes the input program into manageable blocks; breaking down the complex compilation process into smaller, simpler verifiable steps by organizing it as a verifiable LLM workflow by external tests; and a feedback mechanism for self-correction. Supported by formal proofs of translation composability, LEGO-Compiler demonstrates high accuracy on multiple datasets, including over 99% on ExeBench and 97.9% on industrial-grade AnsiBench. Additionally, LEGO-Compiler has also acheived near one order-of-magnitude improvement on compilable code size scalability. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.",http://arxiv.org/abs/2505.20356v1,CS,Quantitative
An Efficient Implementation of Guard-Based Synchronization for an Object-Oriented Programming Language,"In the shared variable model of concurrency, guarded atomic actions restrict the possible interference between processes by regions of atomic execution. The guard specifies the condition for entering an atomic region. That is a convenient model for the specification and verification of concurrent programs, but has eschewed efficient execution so far. This article shows how guarded atomic actions, when attached to objects, can be implemented highly efficiently using a combination of coroutines, operating-system worker threads, and dedicated management of object queues and stacks. The efficiency of an experimental language, Lime, is shown to compare favourably with that of C/Pthreads, Go, Erlang, Java, and Haskell on synthetic benchmarks.",http://arxiv.org/abs/2505.20850v1,CS,Quantitative
Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare,"The rise of electronic health records (EHRs) has unlocked new opportunities for medical research, but privacy regulations and data heterogeneity remain key barriers to large-scale machine learning. Federated learning (FL) enables collaborative modeling without sharing raw data, yet faces challenges in harmonizing diverse clinical datasets. This paper presents a two-step data alignment strategy integrating ontologies and large language models (LLMs) to support secure, privacy-preserving FL in healthcare, demonstrating its effectiveness in a real-world project involving semantic mapping of EHR data.",http://arxiv.org/abs/2505.20020v1,CS,Quantitative
ContribChain: A Stress-Balanced Blockchain Sharding Protocol with Node Contribution Awareness,"Existing blockchain sharding protocols have focused on eliminating imbalanced workload distributions. However, even with workload balance, disparities in processing capabilities can lead to differential stress among shards, resulting in transaction backlogs in certain shards. Therefore, achieving stress balance among shards in the dynamic and heterogeneous environment presents a significant challenge of blockchain sharding. In this paper, we propose ContribChain, a blockchain sharding protocol that can automatically be aware of node contributions to achieve stress balance. We calculate node contribution values based on the historical behavior to evaluate the performance and security of nodes. Furthermore, we propose node allocation algorithm NACV and account allocation algorithm P-Louvain, which both match shard performance with workload to achieve stress balance. Finally, we conduct extensive experiments to compare our work with state-of-the-art baselines based on real Ethereum transactions. The evaluation results show that P-Louvain reduces allocation execution time by 86% and the cross-shard transaction ratio by 7.5%. Meanwhile, ContribChain improves throughput by 35.8% and reduces the cross-shard transaction ratio by 16%.",http://arxiv.org/abs/2505.06899v1,CS,Quantitative
"VerifyThisBench: Generating Code, Specifications, and Proofs All at Once","Large language models (LLMs) have demonstrated remarkable progress in code generation, but many existing benchmarks are approaching saturation and offer little guarantee on the trustworthiness of the generated programs, offering limited insight into deeper reasoning capabilities. We introduce VerifyThisBench, a new benchmark designed to evaluate LLMs on end-to-end program verification tasks that require interpreting natural language problem descriptions, formulating formal specifications, generating code, and constructing correctness proofs. Our evaluation reveals that even state-of-the-art (SOTA) models, such as o3-mini, achieve a pass rate of less than 4%, with many outputs failing to compile. To reduce task complexity, we further propose VerifyThisBenchXS, a variant in which partial implementations or proofs are provided. We systematically assess SOTA models on both benchmarks, uncovering key strengths and limitations in their formal reasoning and verification capabilities.",http://arxiv.org/abs/2505.19271v1,CS,Quantitative
SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training,"The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.",http://arxiv.org/abs/2505.11594v1,CS,Quantitative
Replication Packages in Software Engineering Secondary Studies: A Systematic Mapping,"Context: Systematic reviews (SRs) summarize state-of-the-art evidence in science, including software engineering (SE). Objective: Our objective is to evaluate how SRs report replication packages and to provide a comprehensive list of these packages. Method: We examined 528 secondary studies published between 2013 and 2023 to analyze the availability and reporting of replication packages. Results: Our findings indicate that only 25.4% of the reviewed studies include replication packages. Encouragingly, the situation is gradually improving, as our regression analysis shows significant increase in the availability of replication packages over time. However, in 2023, just 50.6% of secondary studies provided a replication package while an even lower percentage, 29.1% had used a permanent repository with a digital object identifier (DOI) for storage. Conclusion: To enhance transparency and reproducibility in SE research, we advocate for the mandatory publication of replication packages in secondary studies.",http://arxiv.org/abs/2504.12646v1,CS,Quantitative
"A Real-Time, Auto-Regression Method for In-Situ Feature Extraction in Hydrodynamics Simulations","Hydrodynamics simulations are powerful tools for studying fluid behavior under physical forces, enabling extraction of features that reveal key flow characteristics. Traditional post-analysis methods offer high accuracy but incur significant computational and I/O costs. In contrast, in-situ methods reduce data movement by analyzing data during the simulation, yet often compromise either accuracy or performance. We propose a lightweight auto-regression algorithm for real-time in-situ feature extraction. It applies curve-fitting to temporal and spatial data, reducing data volume and minimizing simulation overhead. The model is trained incrementally using mini-batches, ensuring responsiveness and low computational cost. To facilitate adoption, we provide a flexible library with simple APIs for easy integration into existing workflows. We evaluate the method on simulations of material deformation and white dwarf (WD) mergers, extracting features such as shock propagation and delay-time distribution. Results show high accuracy (94.44%-99.60%) and low performance impact (0.11%-4.95%) demonstrating the method's effectiveness for accurate and efficient in-situ analysis.",http://arxiv.org/abs/2504.10632v1,CS,Quantitative
SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow,"Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating ""trajectories"". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment.",http://arxiv.org/abs/2504.08696v2,CS,Quantitative
Enhancing Trust in AI Marketplaces: Evaluating On-Chain Verification of Personalized AI models using zk-SNARKs,"The rapid advancement of artificial intelligence (AI) has brought about sophisticated models capable of various tasks ranging from image recognition to natural language processing. As these models continue to grow in complexity, ensuring their trustworthiness and transparency becomes critical, particularly in decentralized environments where traditional trust mechanisms are absent. This paper addresses the challenge of verifying personalized AI models in such environments, focusing on their integrity and privacy. We propose a novel framework that integrates zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs) with Chainlink decentralized oracles to verify AI model performance claims on blockchain platforms. Our key contribution lies in integrating zk-SNARKs with Chainlink oracles to securely fetch and verify external data to enable trustless verification of AI models on a blockchain. Our approach addresses the limitations of using unverified external data for AI verification on the blockchain while preserving sensitive information of AI models and enhancing transparency. We demonstrate our methodology with a linear regression model predicting Bitcoin prices using on-chain data verified on the Sepolia testnet. Our results indicate the framework's efficacy, with key metrics including proof generation taking an average of 233.63 seconds and verification time of 61.50 seconds. This research paves the way for transparent and trustless verification processes in blockchain-enabled AI ecosystems, addressing key challenges such as model integrity and model privacy protection. The proposed framework, while exemplified with linear regression, is designed for broader applicability across more complex AI models, setting the stage for future advancements in transparent AI verification.",http://arxiv.org/abs/2504.04794v1,CS,Quantitative
"Extracting Practical, Actionable Energy Insights from Supercomputer Telemetry and Logs","As supercomputers grow in size and complexity, power efficiency has become a critical challenge, particularly in understanding GPU power consumption within modern HPC workloads. This work addresses this challenge by presenting a data co-analysis approach using system data collected from the Polaris supercomputer at Argonne National Laboratory. We focus on GPU utilization and power demands, navigating the complexities of large-scale, heterogeneous datasets. Our approach, which incorporates data preprocessing, post-processing, and statistical methods, condenses the data volume by 94% while preserving essential insights. Through this analysis, we uncover key opportunities for power optimization, such as reducing high idle power costs, applying power strategies at the job-level, and aligning GPU power allocation with workload demands. Our findings provide actionable insights for energy-efficient computing and offer a practical, reproducible approach for applying existing research to optimize system performance.",http://arxiv.org/abs/2505.14796v1,CS,Quantitative
Constructive community race: full-density spiking neural network model drives neuromorphic computing,"The local circuitry of the mammalian brain is a focus of the search for generic computational principles because it is largely conserved across species and modalities. In 2014 a model was proposed representing all neurons and synapses of the stereotypical cortical microcircuit below $1\,\text{mm}^2$ of brain surface. The model reproduces fundamental features of brain activity but its impact remained limited because of its computational demands. For theory and simulation, however, the model was a breakthrough because it removes uncertainties of downscaling, and larger models are less densely connected. This sparked a race in the neuromorphic computing community and the model became a de facto standard benchmark. Within a few years real-time performance was reached and surpassed at significantly reduced energy consumption. We review how the computational challenge was tackled by different simulation technologies and derive guidelines for the next generation of benchmarks and other domains of science.",http://arxiv.org/abs/2505.21185v1,CS,Quantitative
What Could Possibly Go Wrong: Undesirable Patterns in Collective Development,"Software development, often perceived as a technical endeavor, is fundamentally a social activity requiring collaboration among team members. Acknowledging this, the software development community has devised strategies to address possible collaboration-related shortcomings. Various studies have attempted to capture the social dynamics within software engineering. In these studies, the authors developed methods to identify numerous teamwork issues and proposed various approaches to address them. However, certain teamwork issues remain unstudied, necessitating a comprehensive bottom-up exploration from practitioner's perceptions to common patterns. This paper introduces the concept of undesirable patterns in collective development, referring to potential teamwork problems that may escalate if unaddressed. Through 38 in-depth exploratory interviews, we identify and classify 42 patterns, revealing their origins and consequences. Subsequent surveys, 436 and 968 participants each, explore the significance and frequency of the undesirable patterns, and evaluate potential tools and features to manage these patterns. The study contributes a nuanced understanding of undesirable patterns, evaluating their impact and proposing pragmatic tools and features for industrial application. The findings provide a valuable foundation for further in-depth studies and the development of tools to enhance collaborative software engineering practices.",http://arxiv.org/abs/2409.01312v1,CS,Mixed
Prototypical Leadership in Agile Software Development,"Leadership in agile teams is a collective responsibility where team members share leadership work based on expertise and skills. However, the understanding of leadership in this context is limited. This study explores the under-researched area of prototypical leadership, aiming to understand if and how leaders who are perceived as more representative of the team are more effective leaders. Qualitative interviews were conducted with eleven members of six agile software teams in five Swedish companies from various industries and sizes. In this study, the effectiveness of leadership was perceived as higher when it emerged from within the team or when leaders aligned with the group. In addition, leaders in managerial roles that align with the team's shared values and traits were perceived as more effective, contributing to overall team success.",http://arxiv.org/abs/2409.11685v1,CS,Qualitative
SoK: Enhancing Privacy-Preserving Software Development from a Developers' Perspective,"In software development, privacy preservation has become essential with the rise of privacy concerns and regulations such as GDPR and CCPA. While several tools, guidelines, methods, methodologies, and frameworks have been proposed to support developers embedding privacy into software applications, most of them are proofs-of-concept without empirical evaluations, making their practical applicability uncertain. These solutions should be evaluated for different types of scenarios (e.g., industry settings such as rapid software development environments, teams with different privacy knowledge, etc.) to determine what their limitations are in various industry settings and what changes are required to refine current solutions before putting them into industry and developing new developer-supporting approaches. For that, a thorough review of empirically evaluated current solutions will be very effective. However, the existing secondary studies that examine the available developer support provide broad overviews but do not specifically analyze empirically evaluated solutions and their limitations. Therefore, this Systematic Literature Review (SLR) aims to identify and analyze empirically validated solutions that are designed to help developers in privacy-preserving software development. The findings will provide valuable insights for researchers to improve current privacy-preserving solutions and for practitioners looking for effective and validated solutions to embed privacy into software development.",http://arxiv.org/abs/2504.20350v2,CS,Quantitative
KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems,"Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads. This paper introduces KAITIAN, a novel distributed communication framework designed to bridge this gap. KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. Crucially, it incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics. Implemented as an extension to PyTorch and rigorously evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy. KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications.",http://arxiv.org/abs/2505.10183v1,CS,Quantitative
User Personas Improve Social Sustainability by Encouraging Software Developers to Deprioritize Antisocial Features,"Sustainable software development involves creating software in a manner that meets present goals without undermining our ability to meet future goals. In a software engineering context, sustainability has at least four dimensions: ecological, economic, social, and technical. No interventions for improving social sustainability in software engineering have been tested in rigorous lab-based experiments, and little evidence-based guidance is available. The purpose of this study is to evaluate the effectiveness of two interventions-stakeholder maps and persona models-for improving social sustainability through software feature prioritization. We conducted a randomized controlled factorial experiment with 79 undergraduate computer science students. Participants were randomly assigned to one of four groups and asked to prioritize a backlog of prosocial, neutral, and antisocial user stories for a shopping mall's digital screen display and facial recognition software. Participants received either persona models, a stakeholder map, both, or neither. We compared the differences in prioritization levels assigned to prosocial and antisocial user stories using Cumulative Link Mixed Model regression. Participants who received persona models gave significantly lower priorities to antisocial user stories but no significant difference was evident for prosocial user stories. The effects of the stakeholder map were not significant. The interaction effects were not significant. Providing aspiring software professionals with well-crafted persona models causes them to de-prioritize antisocial software features. The impact of persona modelling on sustainable software development therefore warrants further study with more experience professionals. Moreover, the novel methodological strategy of assessing social sustainability behavior through backlog prioritization appears feasible in lab-based settings.",http://arxiv.org/abs/2412.10672v1,CS,Quantitative
DFPL: Decentralized Federated Prototype Learning Across Heterogeneous Data Distributions,"Federated learning is a distributed machine learning paradigm that enables the collaborative training of multiple clients through centralized model aggregation. However, standard federated learning relies on a centralized server, making it vulnerable to server failures. While existing solutions utilize blockchain technology to implement Decentralized Federated Learning (DFL), the statistical heterogeneity of data distributions among clients severely degrades the DFL's performance. Driven by this issue, this paper proposes a decentralized federated prototype learning framework, named DFPL, which significantly improves the performance of distributed machine learning across heterogeneous data distributions. Specifically, our framework introduces prototype learning into DFL to address statistical heterogeneity, which greatly reduces the number of parameters exchanged between clients. Additionally, blockchain is embedded into our framework, enabling the training and mining processes to be implemented at each client. From a theoretical perspective, we provide convergence guarantee of DFPL by combining resource allocation for training and mining. The experiments highlight the superiority of our DFPL framework in communication efficiency and test performance across three benchmark datasets with heterogeneous data distributions.",http://arxiv.org/abs/2505.04947v1,CS,Quantitative
SVA-ICL: Improving LLM-based Software Vulnerability Assessment via In-Context Learning and Information Fusion,"Context: Software vulnerability assessment (SVA) is critical for identifying, evaluating, and prioritizing security weaknesses in software applications. Objective: Despite the increasing application of large language models (LLMs) in various software engineering tasks, their effectiveness in SVA remains underexplored. Method: To address this gap, we introduce a novel approach SVA-ICL, which leverages in-context learning (ICL) to enhance LLM performance. Our approach involves the selection of high-quality demonstrations for ICL through information fusion, incorporating both source code and vulnerability descriptions. For source code, we consider semantic, lexical, and syntactic similarities, while for vulnerability descriptions, we focus on textual similarity. Based on the selected demonstrations, we construct context prompts and consider DeepSeek-V2 as the LLM for SVA-ICL. Results: We evaluate the effectiveness of SVA-ICL using a large-scale dataset comprising 12,071 C/C++ vulnerabilities. Experimental results demonstrate that SVA-ICL outperforms state-of-the-art SVA baselines in terms of Accuracy, F1-score, and MCC measures. Furthermore, ablation studies highlight the significance of component customization in SVA-ICL, such as the number of demonstrations, the demonstration ordering strategy, and the optimal fusion ratio of different modalities. Conclusion: Our findings suggest that leveraging ICL with information fusion can effectively improve the effectiveness of LLM-based SVA, warranting further research in this direction.",http://arxiv.org/abs/2505.10008v1,CS,Quantitative
Mutation-Guided LLM-based Test Generation at Meta,"This paper describes Meta's ACH system for mutation-guided LLM-based test generation. ACH generates relatively few mutants (aka simulated faults), compared to traditional mutation testing. Instead, it focuses on generating currently undetected faults that are specific to an issue of concern. From these currently uncaught faults, ACH generates tests that can catch them, thereby `killing' the mutants and consequently hardening the platform against regressions. We use privacy concerns to illustrate our approach, but ACH can harden code against {\em any} type of regression. In total, ACH was applied to 10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also deploys an LLM-based equivalent mutant detection agent that achieves a precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple pre-processing). ACH was used by Messenger and WhatsApp test-a-thons where engineers accepted 73% of its tests, judging 36% to privacy relevant. We conclude that ACH hardens code against specific concerns and that, even when its tests do not directly tackle the specific concern, engineers find them useful for their other benefits.",http://arxiv.org/abs/2501.12862v1,CS,Quantitative
MigrationBench: Repository-Level Code Migration Benchmark from Java 8,"With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on code generation and issue-resolution tasks. In contrast, we introduce a new coding benchmark MigrationBench with a distinct focus: code migration. MigrationBench aims to serve as a comprehensive benchmark for migration from Java $8$ to the latest long-term support (LTS) versions (Java $17$, $21$), including a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves $62.33\%$ and $27.33\%$ success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3 and https://github.com/amazon-science/MigrationBench respectively.",http://arxiv.org/abs/2505.09569v2,CS,Quantitative
Automated Statistical Testing and Certification of a Reliable Model-Coupling Server for Scientific Computing,"Sequence-based specification and usage-driven statistical testing are designed for rigorous and cost-effective software development, offering a semi-formal approach to assessing the behavior of complex systems and interactions between various components. This approach is particularly valuable for scientific computing applications in which comprehensive tests are needed to prevent flawed results or conclusions. As scientific discovery becomes increasingly more complex, domain scientists couple multiple scientific computing models or simulations to solve intricate multiphysics and multiscale problems. These model-coupling applications use a hardwired coupling program or a flexible web service to link and combine different models. In this paper, we focus on the quality assurance of the more elastic web service via a combination of rigorous specification and testing methods. The application of statistical testing exposes problems ignored by pre-written unit tests and highlights areas in the code where failures might occur. We certify the model-coupling server controller with a derived reliability statistic, offering a quantitative measure to support a claim of its robustness.",http://arxiv.org/abs/2505.09769v1,CS,Quantitative
LAMMPS: A Case Study For Applying Modern Software Engineering to an Established Research Software Package,We review various changes made in recent years to the software development process of the LAMMPS simulation software package and the software itself. We discuss how those changes have impacted the effort and workflow required to develop and maintain a software package that has been in existence for more than 30 years and where a significant part of the code base is contributed by external developers. We also look into how those changes have affected the code quality and ease of modifying and extending the software while at the same time its audience has changed from a cohort with a generally strong software development background to a group containing many researchers with limited software development skills. We explore how this contributes to LAMMPS' significant growth in popularity in that time. We close with an outlook on future steps.,http://arxiv.org/abs/2505.06877v1,CS,Quantitative
Exploring the Susceptibility to Fraud of Monetary Incentive Mechanisms for Strengthening FOSS Projects,"Free and open source software (FOSS) is ubiquitous on modern IT systems, accelerating the speed of software engineering over the past decades. With its increasing importance and historical reliance on uncompensated contributions, questions have been raised regarding the continuous maintenance of FOSS and its implications from a security perspective. In recent years, different funding programs have emerged to provide external incentives to reinforce community FOSS' sustainability. Past research primarily focused on analyses what type of projects have been funded and for what reasons. However, it has neither been considered whether there is a need for such external incentives, nor whether the incentive mechanisms, especially with the development of decentralized approaches, are susceptible to fraud. In this study, we explore the need for funding through a literature review and compare the susceptibility to fraud of centralized and decentralized incentive programs by performing case studies on the Sovereign Tech Fund (STF) and the tea project. We find non-commercial incentives to fill an important gap, ensuring longevity and sustainability of projects. Furthermore, we find the STF to be able to achieve a high resilience against fraud attempts, while tea is highly susceptible to fraud, as evidenced by revelation of an associated sybil attack on npm. Our results imply that special considerations must be taken into account when utilizing quantitative repository metrics regardless whether spoofing is expected.",http://arxiv.org/abs/2505.05897v1,CS,Quantitative
Taming Offload Overheads in a Massively Parallel Open-Source RISC-V MPSoC: Analysis and Optimization,"Heterogeneous multi-core architectures combine on a single chip a few large, general-purpose host cores, optimized for single-thread performance, with (many) clusters of small, specialized, energy-efficient accelerator cores for data-parallel processing. Offloading a computation to the many-core acceleration fabric implies synchronization and communication overheads which can hamper overall performance and efficiency, particularly for small and fine-grained parallel tasks. In this work, we present a detailed, cycle-accurate quantitative analysis of the offload overheads on Occamy, an open-source massively parallel RISC-V based heterogeneous MPSoC. We study how the overheads scale with the number of accelerator cores. We explore an approach to drastically reduce these overheads by co-designing the hardware and the offload routines. Notably, we demonstrate that by incorporating multicast capabilities into the Network-on-Chip of a large (200+ cores) accelerator fabric we can improve offloaded application runtimes by as much as 2.3x, restoring more than 70% of the ideally attainable speedups. Finally, we propose a quantitative model to estimate the runtime of selected applications accounting for the offload overheads, with an error consistently below 15%.",http://arxiv.org/abs/2505.05911v1,CS,Quantitative
Measuring the Impact of Technical Debt on Development Effort in Software Projects,"Technical debt refers to the trade-offs between code quality and faster delivery, impacting future development with increased complexity, bugs, and costs. This study empirically analyzes the additional work effort caused by technical debt in software projects, focusing on feature implementations. I explore how delaying technical debt repayment through refactoring influences long-term work effort. Using data from open-source and enterprise projects, I correlate technical debt with practical work effort, drawing from issue trackers and version control systems. Our goal is to provide a framework for managing technical debt, aiding developers, project managers, and stakeholders in understanding and mitigating its impact on productivity and costs.",http://arxiv.org/abs/2502.16277v1,CS,Quantitative
Bayesian Hierarchical Models for Quantitative Estimates for Performance metrics applied to Saddle Search Algorithms,"The increasing use of high-throughput computational chemistry demands rigorous methods for evaluating algorithm performance. We present a Bayesian hierarchical modeling paradigm (brms/Stan) for analyzing key performance metrics: function evaluations, computation time, and success/failure. This framework accounts for variability across different systems and functionals, providing reliable uncertainty estimates beyond subjective visual assessments or frequentist limitations. We applied this to compare conjugate gradient (CG) and L-BFGS algorithms for the Dimer method's rotation phase (EON, with/without removal of external rotations/translations) on a benchmark of 500 initial saddle search approximations, analyzing over 2000 runs. Our results show CG rotations generally outperform L-BFGS, exhibiting a statistically credible, small reduction in PES calls and significantly higher odds of successful convergence. Conversely, enabling rotation removal incurred a substantial PES call penalty without a corresponding credible improvement in success odds in this implementation. These findings, from our novel Bayesian hierarchical modeling application, suggest CG may be preferable for Dimer rotational optimization in similar contexts. This robust statistical framework highlights benefits for revisiting optimization strategies, quantifying uncertainty, and facilitating improved high-throughput computational chemistry methods.",http://arxiv.org/abs/2505.13621v1,CS,Quantitative
Accessibility Recommendations for Designing Better Mobile Application User Interfaces for Seniors,"Seniors represent a growing user base for mobile applications; however, many apps fail to adequately address their accessibility challenges and usability preferences. To investigate this issue, we conducted an exploratory focus group study with 16 senior participants, from which we derived an initial set of user personas highlighting key accessibility and personalisation barriers. These personas informed the development of a model-driven engineering toolset, which was used to generate adaptive mobile app prototypes tailored to seniors' needs. We then conducted a second focus group study with 22 seniors to evaluate these prototypes and validate our findings. Based on insights from both studies, we developed a refined set of personas and a series of accessibility and personalisation recommendations grounded in empirical data, prior research, accessibility standards, and developer resources, aimed at supporting software practitioners in designing more inclusive mobile applications.",http://arxiv.org/abs/2504.12690v1,CS,Mixed
On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles,"Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.",http://arxiv.org/abs/2505.02274v1,CS,Quantitative
"Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization","Commenting code is a crucial activity in software development, as it aids in facilitating future maintenance and updates. To enhance the efficiency of writing comments and reduce developers' workload, researchers has proposed various automated code summarization (ACS) techniques to automatically generate comments/summaries for given code units. However, these ACS techniques primarily focus on generating summaries for code units at the method level. There is a significant lack of research on summarizing higher-level code units, such as file-level and module-level code units, despite the fact that summaries of these higher-level code units are highly useful for quickly gaining a macro-level understanding of software components and architecture. To fill this gap, in this paper, we conduct a systematic study on how to use LLMs for commenting higher-level code units, including file level and module level. These higher-level units are significantly larger than method-level ones, which poses challenges in handling long code inputs within LLM constraints and maintaining efficiency. To address these issues, we explore various summarization strategies for ACS of higher-level code units, which can be divided into three types: full code summarization, reduced code summarization, and hierarchical code summarization. The experimental results suggest that for summarizing file-level code units, using the full code is the most effective approach, with reduced code serving as a cost-efficient alternative. However, for summarizing module-level code units, hierarchical code summarization becomes the most promising strategy. In addition, inspired by the research on method-level ACS, we also investigate using the LLM as an evaluator to evaluate the quality of summaries of higher-level code units. The experimental results demonstrate that the LLM's evaluation results strongly correlate with human evaluations.",http://arxiv.org/abs/2503.10737v1,CS,Quantitative
Exploring the Untapped: Student Perceptions and Participation in OSS,"Open Source Software (OSS) projects offer valuable opportunities to train the next generation of software engineers while benefiting projects and society as a whole. While research has extensively explored student participation in OSS and its use in software engineering education, student participation in OSS is still low, and the perspectives of students who have never contributed remain underexplored. This study aims to investigate the relationship between students' interest in contributing to OSS and their perceptions of barriers and motivational factors. We developed a theoretical model to understand the relationship between students' perceptions of OSS and their interest in contributing. We then surveyed students majoring in computer science and related fields (N=241). Using structural equation modeling techniques, we tested the model and found that intrinsic and internalized extrinsic motivations are positively associated with interest in contributing to OSS projects, while the impact of extrinsic motivation varies by gender. Comparatively, we found no significant relationship between barriers and interest in contributing. Students suggested several ways to make projects more attractive, including increasing awareness of the importance of OSS. Our findings can help communities better prepare to integrate students and encourage educators to enhance interest in OSS by linking participation to specific motivational factors.",http://arxiv.org/abs/2504.17051v1,CS,Quantitative
Open Source Software Lifecycle Classification: Developing Wrangling Techniques for Complex Sociotechnical Systems,"Open source software is a rapidly evolving center for distributed work, and understanding the characteristics of this work across its different contexts is vital for informing policy, economics, and the design of enabling software. The steep increase in open source projects and corporate participation have transformed a peripheral, cottage industry component of the global technology ecosystem into a large, infinitely complex ""technology parts supplier"" wired into every corner of contemporary life. The lack of theory and tools for breaking this complexity down into identifiable project types or strategies for understanding them more systematically is incommensurate with current industry, society, and developer needs. This paper reviews previous attempts to classify open source software and other organizational ecosystems, using open source scientific software ecosystems in contrast with those found in corporatized open source software. It then examines the divergent and sometimes conflicting purposes that may exist for classifying open source projects and how these competing interests impede our progress in developing a comprehensive understanding of how open source software projects and companies operate. Finally, we will present an empirical, mixed-methods study demonstrating how to classify open-source projects by their lifecycle position. This is the first step forward, advancing our scientific and practical knowledge of open source software through the lens of dynamic and evolving open source genres. It concludes with examples and a proposed path forward.",http://arxiv.org/abs/2504.16670v1,CS,Quantitative
On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices,"AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up industrial survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers' perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns.",http://arxiv.org/abs/2504.16485v1,CS,Quantitative
A Survey for What Developers Require in AI-powered Tools that Aid in Component Selection in CBSD,"Although it has been more than four decades that the first components-based software development (CBSD) studies were conducted, there is still no standard method or tool for component selection which is widely accepted by the industry. The gulf between industry and academia contributes to the lack of an accepted tool. We conducted a mixed methods survey of nearly 100 people engaged in component-based software engineering practice or research to better understand the problems facing industry, how these needs could be addressed, and current best practices employed in component selection. We also sought to identify and prioritize quality criteria for component selection from an industry perspective. In response to the call for CBSD component selection tools to incorporate recent technical advances, we also explored the perceptions of professionals about AI-driven tools, present and envisioned.",http://arxiv.org/abs/2504.13751v1,CS,Mixed
A Unified QoS-Aware Multiplexing Framework for Next Generation Immersive Communication with Legacy Wireless Applications,"Immersive communication, including emerging augmented reality, virtual reality, and holographic telepresence, has been identified as a key service for enabling next-generation wireless applications. To align with legacy wireless applications, such as enhanced mobile broadband or ultra-reliable low-latency communication, network slicing has been widely adopted. However, attempting to statistically isolate the above types of wireless applications through different network slices may lead to throughput degradation and increased queue backlog. To address these challenges, we establish a unified QoS-aware framework that supports immersive communication and legacy wireless applications simultaneously. Based on the Lyapunov drift theorem, we transform the original long-term throughput maximization problem into an equivalent short-term throughput maximization weighted by virtual queue length. Moreover, to cope with the challenges introduced by the interaction between large-timescale network slicing and short-timescale resource allocation, we propose an adaptive adversarial slicing (Ad2S) scheme for networks with invarying channel statistics. To track the network channel variations, we also propose a measurement extrapolation-Kalman filter (ME-KF)-based method and refine our scheme into Ad2S-non-stationary refinement (Ad2S-NR). Through extended numerical examples, we demonstrate that our proposed schemes achieve 3.86 Mbps throughput improvement and 63.96% latency reduction with 24.36% convergence time reduction. Within our framework, the trade-off between total throughput and user service experience can be achieved by tuning systematic parameters.",http://arxiv.org/abs/2504.21444v2,CS,Quantitative
Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era,"Software developers balance a variety of different tasks in a workweek, yet the allocation of time often differs from what they consider ideal. Identifying and addressing these deviations is crucial for organizations aiming to enhance the productivity and well-being of the developers. In this paper, we present the findings from a survey of 484 software developers at Microsoft, which aims to identify the key differences between how developers would like to allocate their time during an ideal workweek versus their actual workweek. Our analysis reveals significant deviations between a developer's ideal workweek and their actual workweek, with a clear correlation: as the gap between these two workweeks widens, we observe a decline in both productivity and satisfaction. By examining these deviations in specific activities, we assess their direct impact on the developers' satisfaction and productivity. Additionally, given the growing adoption of AI tools in software engineering, both in the industry and academia, we identify specific tasks and areas that could be strong candidates for automation. In this paper, we make three key contributions: 1) We quantify the impact of workweek deviations on developer productivity and satisfaction 2) We identify individual tasks that disproportionately affect satisfaction and productivity 3) We provide actual data-driven insights to guide future AI automation efforts in software engineering, aligning them with the developers' requirements and ideal workflows for maximizing their productivity and satisfaction.",http://arxiv.org/abs/2502.15287v1,CS,Quantitative
Gender Influence on Student Teams' Online Communication in Software Engineering Education,"Collaboration is crucial in Software Engineering (SE), yet factors like gender bias can shape team dynamics and behaviours. This study examines an eight-week project involving 39 SE students across eight teams contributing to GitHub projects. Using a mixed-methods approach, we analysed Slack communications to identify gender differences, comparing how they influence learning gains. We found higher help-seeking and leadership behaviours in the all-woman team, while men responded more slowly. Although communication did not affect final grades, we identified statistical significance correlating communications with students' understanding of software development. With some students putting more effort into collaboration, future work can investigate diversity and inclusion training to balance these efforts. The observed link between team engagement and a higher understanding of software development highlights the potential for teaching strategies that promote help-seeking. These findings could guide efforts to address challenges student SE teams face when using communication platforms and foster more equitable collaborative learning in Software Engineering Education.",http://arxiv.org/abs/2502.14653v1,CS,Quantitative
Mind the Gap: The Missing Features of the Tools to Support User Studies in Software Engineering,"User studies are paramount for advancing science. However, researchers face several barriers when performing them despite the existence of supporting tools. In this work, we study how existing tools and their features cope with previously identified barriers. Moreover, we propose new features for the barriers that lack support. We validated our proposal with 102 researchers, achieving statistically significant positive support for all but one feature. We study the current gap between tools and barriers, using features as the bridge. We show there is a significant lack of support for several barriers, as some have no single tool to support them.",http://arxiv.org/abs/2504.08647v1,CS,Quantitative
Innovating the software engineering class through multi-team development,"Often software engineering classes have the student concentrate on designing and planning the project but stop short of actual student team development of code. This leads to criticism by employers of new graduates that they are missing skills in working in teams and coordinating multiple overlapping changes to a code base. Additionally, students that are not actively experiencing team development are unprepared to understand and modify existing legacy-code bases written by others. This paper presents a new approach to teaching undergraduate software engineering that emphasizes not only software engineering methodology but also experiencing development as a member of a team and modifying a legacy code base. Our innovative software engineering course begins with learning the fundamentals of software engineering, followed by examining an existing framework of a social media application. The students are then grouped into multiple software teams, each focusing on a different aspect of the app. The separate teams must define requirements, design, and provide documentation on the services. Using an Agile development approach, the teams incrementally add to the code base and demonstrate features as the application evolves. Subsequent iterations of the class pick up the prior students code base, providing experience working with a legacy code base. Preliminary results of using this approach at the university are presented in this paper including quantitative analysis. Analysis of student software submissions to the cloud-based code repository shows student engagement and contributions over the span of the course. Positive student evaluations show the effectiveness of applying the principles of software engineering to the development of a complex solution in a team environment. Keywords: Software engineering, teaching, college computer science, innovative methods, agile.",http://arxiv.org/abs/2502.02578v1,CS,Quantitative
Show Me Why It's Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison,"Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants' task completion time by 36% and 33% while also improving their confidence by 50% and 20%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39% compared with other patch ranking methods and is generalizable to different APR tools.",http://arxiv.org/abs/2503.00618v1,CS,Quantitative
Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering,"Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide...",http://arxiv.org/abs/2502.06193v3,CS,Quantitative
Minimizing speculation overhead in a parallel recognizer for regular texts,"Speculative data-parallel algorithms for language recognition have been widely experimented for various types of finite-state automata (FA), deterministic (DFA) and nondeterministic (NFA), often derived from regular expressions (RE). Such an algorithm cuts the input string into chunks, independently recognizes each chunk in parallel by means of identical FAs, and at last joins the chunk results and checks overall consistency. In chunk recognition, it is necessary to speculatively start the FAs in any state, thus causing an overhead that reduces the speedup compared to a serial algorithm. Existing data-parallel DFA-based recognizers suffer from the excessive number of starting states, and the NFA-based ones suffer from the number of nondeterministic transitions. Our data-parallel algorithm is based on the new FA type called reduced interface DFA (RI-DFA), which minimizes the speculation overhead without incurring in the penalty of nondeterministic transitions or of impractically enlarged DFA machines. The algorithm is proved to be correct and theoretically efficient, because it combines the state-reduction of an NFA with the speed of deterministic transitions, thus improving on both DFA-based and NFA-based existing implementations. The practical applicability of the RI-DFA approach is confirmed by a quantitative comparison of the number of starting states for a large public benchmark of complex FAs. On multi-core computing architectures, the RI-DFA recognizer is much faster than the NFA-based one on all benchmarks, while it matches the DFA-based one on some benchmarks and performs much better on some others. The extra time cost needed to construct an RI-DFA compared to a DFA is moderate and is compatible with a practical use.",http://arxiv.org/abs/2412.14975v3,CS,Quantitative
An Empirical Study of the Impact of Federated Learning on Machine Learning Model Accuracy,"Federated Learning (FL) enables distributed ML model training on private user data at the global scale. Despite the potential of FL demonstrated in many domains, an in-depth view of its impact on model accuracy remains unclear. In this paper, we investigate, systematically, how this learning paradigm can affect the accuracy of state-of-the-art ML models for a variety of ML tasks. We present an empirical study that involves various data types: text, image, audio, and video, and FL configuration knobs: data distribution, FL scale, client sampling, and local and global computations. Our experiments are conducted in a unified FL framework to achieve high fidelity, with substantial human efforts and resource investments. Based on the results, we perform a quantitative analysis of the impact of FL, and highlight challenging scenarios where applying FL degrades the accuracy of the model drastically and identify cases where the impact is negligible. The detailed and extensive findings can benefit practical deployments and future development of FL.",http://arxiv.org/abs/2503.20768v2,CS,Quantitative
Whisper D-SGD: Correlated Noise Across Agents for Differentially Private Decentralized Learning,"Decentralized learning enables distributed agents to train a shared machine learning model through local computation and peer-to-peer communication. Although each agent retains its dataset locally, the communication of local models can still expose private information to adversaries. To mitigate these threats, local differential privacy (LDP) injects independent noise per agent, but it suffers a larger utility gap than central differential privacy (CDP). We introduce Whisper D-SGD, a novel covariance-based approach that generates correlated privacy noise across agents, unifying several state-of-the-art methods as special cases. By leveraging network topology and mixing weights, Whisper D-SGD optimizes the noise covariance to achieve network-wide noise cancellation. Experimental results show that Whisper D-SGD cancels more noise than existing pairwise-correlation schemes, substantially narrowing the CDP-LDP gap and improving model performance under the same privacy guarantees.",http://arxiv.org/abs/2501.14644v1,CS,Quantitative
StRuCom: A Novel Dataset of Structured Code Comments in Russian,"Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.",http://arxiv.org/abs/2505.11026v1,CS,Quantitative
Artificial Intelligence for Software Architecture: Literature Review and the Road Ahead,"This paper presents a forward-looking vision for artificial intelligence-driven software architecture that addresses longstanding challenges in design and evolution. Although artificial intelligence has achieved notable success in software engineering, its explicit application to software architecture remains under-explored. Traditional practices, heavily reliant on expert knowledge and complex trade-off reasoning, tend to be manual and error-prone, thereby compromising system quality and maintainability. Building on recent advances, we examine how artificial intelligence can automate architectural design, support quantitative trade-off analyses, and continuously update architectural documentation. Our approach combines a systematic review of state-of-the-art applications with insights from industry practitioners. The resulting roadmap outlines 14 current artificial intelligence contributions to software architecture, identifies six artificial intelligence-specific challenges in supporting architectural tasks, and reveals six avenues for future improvement, charting a course for future research and practical implementations.",http://arxiv.org/abs/2504.04334v1,CS,Quantitative
Understanding the relationships between the perceptions of burnout and instability in Software Engineering,"Changes are inherent in software development, often increasing developers' perception of instability. Understanding the relationship between human factors and Software Engineering processes is crucial to mitigating and preventing issues. One such factor is burnout, a recognized disease that impacts productivity, turnover, and, most importantly, developers' well-being. Investigating the link between instability and burnout can help organizations implement strategies to improve developers' work conditions and performance. This study aims to identify and describe the relationship between perceived instability and burnout among software developers. A cross-sectional survey was conducted with 411 respondents, using convenience sampling and self-selection. In addition to analyzing variable relationships, confirmatory factor analysis was applied. Key findings include: (1) A significant positive relationship between burnout (exhaustion and cynicism) and team, technological, and task instability; (2) A weak negative relationship between efficacy and technological/team instability, with no correlation to task instability; (3) Exhaustion was the most frequently reported burnout symptom, while task instability was the most perceived type of instability. These results are valuable for both industry and academia, providing insights to reduce burnout and instability among software engineers. Future research can further explore the impact of instability, offering new perspectives on monitoring and mitigating its effects in software development.",http://arxiv.org/abs/2502.10249v1,CS,Quantitative
Towards an Understanding of Context Utilization in Code Intelligence,"Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially enhance model performance. Such contextual signals may be obtained directly or indirectly from sources such as API documentation or intermediate representations like abstract syntax trees can significantly improve the effectiveness of code intelligence. Despite growing academic interest, there is a lack of systematic analysis of context in code intelligence. To address this gap, we conduct an extensive literature review of 146 relevant studies published between September 2007 and August 2024. Our investigation yields four main contributions. (1) A quantitative analysis of the research landscape, including publication trends, venues, and the explored domains; (2) A novel taxonomy of context types used in code intelligence; (3) A task-oriented analysis investigating context integration strategies across diverse code intelligence tasks; (4) A critical evaluation of evaluation methodologies for context-aware methods. Based on these findings, we identify fundamental challenges in context utilization in current code intelligence systems and propose a research roadmap that outlines key opportunities for future research.",http://arxiv.org/abs/2504.08734v1,CS,Quantitative
Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey,"Artificial Intelligence (AI) is rapidly expanding and integrating more into daily life to automate tasks, guide decision making, and enhance efficiency. However, complex AI models, which make decisions without providing clear explanations (known as the ""black-box problem""), currently restrict trust and widespread adoption of AI. Explainable Artificial Intelligence (XAI) has emerged to address the black-box problem of making AI systems more interpretable and transparent so stakeholders can trust, verify, and act upon AI-based outcomes. Researchers have developed various techniques to foster XAI in the Software Development Lifecycle. However, there are gaps in applying XAI techniques in the Software Engineering phases. Literature review shows that 68% of XAI in Software Engineering research is focused on maintenance as opposed to 8% on software management and requirements. In this paper, we present a comprehensive survey of the applications of XAI methods such as concept-based explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), rule extraction, attention mechanisms, counterfactual explanations, and example-based explanations to the different phases of the Software Development Life Cycle (SDLC), including requirements elicitation, design and development, testing and deployment, and evolution. To the best of our knowledge, this paper presents the first comprehensive survey of XAI techniques for every phase of the Software Development Life Cycle (SDLC). This survey aims to promote explainable AI in Software Engineering and facilitate the practical application of complex AI models in AI-driven software development.",http://arxiv.org/abs/2505.07058v1,CS,Quantitative
How the Misuse of a Dataset Harmed Semantic Clone Detection,"BigCloneBench is a well-known and widely used large-scale dataset for the evaluation of recall of clone detection tools. It has been beneficial for research on clone detection and has become a standard in evaluating the performance of clone detection tools. More recently, it has also been widely used as a dataset to evaluate machine learning approaches to semantic clone detection or code similarity detection for functional or semantic similarity. This paper demonstrates that BigCloneBench is problematic to use as ground truth for learning or evaluating semantic code similarity, and highlights the aspects of BigCloneBench that affect the ground truth quality. A manual investigation of a statistically significant random sample of 406 Weak Type-3/Type-4 clone pairs revealed that 93% of them do not have a similar functionality and are therefore mislabelled. In a literature review of 179 papers that use BigCloneBench as a dataset, we found 139 papers that used BigCloneBench to evaluate semantic clone detection and where the results are threatened in their validity by the mislabelling. As such, these papers often report high F1 scores (e.g., above 0.9), which indicates overfitting to dataset-specific artefacts rather than genuine semantic similarity detection. We emphasise that using BigCloneBench remains valid for the intended purpose of evaluating syntactic or textual clone detection of Type-1, Type-2, and Type-3 clones. We acknowledge the important contributions of BigCloneBench to two decades of traditional clone detection research. However, the usage of BigCloneBench beyond the intended purpose without careful consideration of its limitations has led to misleading results and conclusions, and potentially harmed the field of semantic clone detection.",http://arxiv.org/abs/2505.04311v1,CS,Quantitative
Efficient Incremental Code Coverage Analysis for Regression Test Suites,"Code coverage analysis has been widely adopted in the continuous integration of open-source and industry software repositories to monitor the adequacy of regression test suites. However, computing code coverage can be costly, introducing significant overhead during test execution. Plus, re-collecting code coverage for the entire test suite is usually unnecessary when only a part of the coverage data is affected by code changes. While regression test selection (RTS) techniques exist to select a subset of tests whose behaviors may be affected by code changes, they are not compatible with code coverage analysis techniques -- that is, simply executing RTS-selected tests leads to incorrect code coverage results. In this paper, we present the first incremental code coverage analysis technique, which speeds up code coverage analysis by executing a minimal subset of tests to update the coverage data affected by code changes. We implement our technique in a tool dubbed iJaCoCo, which builds on Ekstazi and JaCoCo -- the state-of-the-art RTS and code coverage analysis tools for Java. We evaluate iJaCoCo on 1,122 versions from 22 open-source repositories and show that iJaCoCo can speed up code coverage analysis time by an average of 1.86x and up to 8.20x compared to JaCoCo.",http://arxiv.org/abs/2410.21798v1,CS,Quantitative
Efficient and Optimal No-Regret Caching under Partial Observation,"Online learning algorithms have been successfully used to design caching policies with sublinear regret in the total number of requests, with no statistical assumption about the request sequence. Most existing algorithms involve computationally expensive operations and require knowledge of all past requests. However, this may not be feasible in practical scenarios like caching at a cellular base station. Therefore, we study the caching problem in a more restrictive setting where only a fraction of past requests are observed, and we propose a randomized caching policy with sublinear regret based on the classic online learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy is the first to attain the asymptotically optimal regret bound while ensuring asymptotically constant amortized time complexity in the partial observability setting of requests. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under synthetic and real-world request traces.",http://arxiv.org/abs/2503.02758v1,CS,Quantitative
On the Limits of Distributed Quantum Computing,"Quantum advantage is well-established in centralized computing, where quantum algorithms can solve certain problems exponentially faster than classical ones. In the distributed setting, significant progress has been made in bandwidth-limited networks, where quantum distributed networks have shown computational advantages over classical counterparts. However, the potential of quantum computing in networks that are constrained only by large distances is not yet understood. We focus on the LOCAL model of computation (Linial, FOCS 1987), a distributed computational model where computational power and communication bandwidth are unconstrained, and its quantum generalization. In this brief survey, we summarize recent progress on the quantum-LOCAL model outlining its limitations with respect to its classical counterpart: we discuss emerging techniques, and highlight open research questions that could guide future efforts in the field.",http://arxiv.org/abs/2503.11394v1,CS,Quantitative
Conditioning on Local Statistics for Scalable Heterogeneous Federated Learning,"Federated learning is a distributed machine learning approach where multiple clients collaboratively train a model without sharing their local data, which contributes to preserving privacy. A challenge in federated learning is managing heterogeneous data distributions across clients, which can hinder model convergence and performance due to the need for the global model to generalize well across diverse local datasets. We propose to use local characteristic statistics, by which we mean some statistical properties calculated independently by each client using only their local training dataset. These statistics, such as means, covariances, and higher moments, are used to capture the characteristics of the local data distribution. They are not shared with other clients or a central node. During training, these local statistics help the model learn how to condition on the local data distribution, and during inference, they guide the client's predictions. Our experiments show that this approach allows for efficient handling of heterogeneous data across the federation, has favorable scaling compared to approaches that directly try to identify peer nodes that share distribution characteristics, and maintains privacy as no additional information needs to be communicated.",http://arxiv.org/abs/2503.00378v1,CS,Quantitative
Designing a Syllabus for a Course on Empirical Software Engineering,"Increasingly, courses on Empirical Software Engineering research methods are being offered in higher education institutes across the world, mostly at the M.Sc. and Ph.D. levels. While the need for such courses is evident and in line with modern software engineering curricula, educators designing and implementing such courses have so far been reinventing the wheel; every course is designed from scratch with little to no reuse of ideas or content across the community. Due to the nature of the topic, it is rather difficult to get it right the first time when defining the learning objectives, selecting the material, compiling a reader, and, more importantly, designing relevant and appropriate practical work. This leads to substantial effort (through numerous iterations) and poses risks to the course quality. This chapter attempts to support educators in the first and most crucial step in their course design: creating the syllabus. It does so by consolidating the collective experience of the authors as well as of members of the Empirical Software Engineering community; the latter was mined through two working sessions and an online survey. Specifically, it offers a list of the fundamental building blocks for a syllabus, namely course aims, course topics, and practical assignments. The course topics are also linked to the subsequent chapters of this book, so that readers can dig deeper into those chapters and get support on teaching specific research methods or cross-cutting topics. Finally, we guide educators on how to take these building blocks as a starting point and consider a number of relevant aspects to design a syllabus to meet the needs of their own program, students, and curriculum.",http://arxiv.org/abs/2503.11291v1,CS,Quantitative
"Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair","The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness. We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.",http://arxiv.org/abs/2505.13103v2,CS,Quantitative
EvoSort: A Genetic-Algorithm-Based Adaptive Parallel Sorting Framework for Large-Scale High Performance Computing,"In today's era of big data, sorting enormous datasets is a major challenge. We present EvoSort, an adaptive parallel sorting framework that employs a Genetic Algorithm (GA) to automatically discover and refine critical parameters, including insertion sort and fallback thresholds, tile size, and mergesort vs Least Significant Digit (LSD) radix sort. EvoSort integrates parallel sorting primitives and adapts continuously to input data and system architecture, ensuring optimal performance. Experiments on up to 10 billion elements show that EvoSort consistently outperforms NumPy sorting by factors from three to over 90 times. EvoSort exemplifies a powerful auto-tuning solution for large-scale data processing.",http://arxiv.org/abs/2505.18681v1,CS,Quantitative
Towards Efficient Multi-Scale Deformable Attention on NPU,"Multi-scale deformable attention (MSDA) is a flexible and powerful feature extraction mechanism for visual tasks, but its random-access grid sampling strategy poses significant optimization challenges, especially on domain-specific accelerators such as NPUs. In this work, we present a co-design approach that systematically rethinks memory access and computation strategies for MSDA on the Ascend NPU architecture. With this co-design approach, our implementation supports both efficient forward and backward computation, is fully adapted for training workloads, and incorporates a suite of hardware-aware optimizations. Extensive experiments show that our solution achieves up to $5.9\times$ (forward), $8.9\times$ (backward), and $7.3\times$ (end-to-end training) speedup over the grid sample-based baseline, and $1.9\times$, $2.4\times$, and $2.0\times$ acceleration over the latest vendor library, respectively.",http://arxiv.org/abs/2505.14022v1,CS,Quantitative
Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI,"Timely and efficient dissemination of service information is critical in compute-first networking systems, where user requests arrive dynamically and computing resources are constrained. In such systems, the access point (AP) plays a key role in forwarding user requests to a server based on its latest received service information. This paper considers a single-source, single-destination system and introduces an Age-and-Value-Aware (AVA) metric that jointly captures both the timeliness and the task relevance of service information. Unlike traditional freshness-based metrics, AVA explicitly incorporates variations in server-side service capacity and AP forwarding decisions, allowing more context-aware update evaluation. Building upon AVA, we propose a reinforcement learning-based update policy that learns to selectively transmit service information updates to the AP. It aims to maximize overall task success while minimizing unnecessary communications. Extensive simulations under diverse user request patterns and varying service capacities demonstrate that AVA reduces the update frequency by over 90% on average compared to baselines, with reductions reaching 98% in certain configurations. Crucially, this reduction is achieved without compromising the accuracy of task execution or the quality of decision making.",http://arxiv.org/abs/2505.06025v1,CS,Quantitative
Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications,"We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReXCastro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm_smi_lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also, we found gaps in the AMD tooling used on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms-1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.",http://arxiv.org/abs/2505.05623v2,CS,Quantitative
Augmenting Software Bills of Materials with Software Vulnerability Description: A Preliminary Study on GitHub,"Software Bills of Material (SBOMs) are becoming a consolidated, often enforced by governmental regulations, way to describe software composition. However, based on recent studies, SBOMs suffer from limited support for their consumption and lack information beyond simple dependencies, especially regarding software vulnerabilities. This paper reports the results of a preliminary study in which we augmented SBOMs of 40 open-source projects with information about Common Vulnerabilities and Exposures (CVE) exposed by project dependencies. Our augmented SBOMs have been evaluated by submitting pull requests and by asking project owners to answer a survey. Although, in most cases, augmented SBOMs were not directly accepted because owners required a continuous SBOM update, the received feedback shows the usefulness of the suggested SBOM augmentation.",http://arxiv.org/abs/2503.13998v1,CS,Quantitative
"CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation","Modern software development demands code that is maintainable, testable, and scalable by organizing the implementation into modular components with iterative reuse of existing codes. We formalize this iterative, multi-turn paradigm as codeflow and introduce CodeFlowBench, the first benchmark designed to comprehensively evaluate LLMs' ability to perform codeflow, namely implementing new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5,258 problems from Codeforces and is continuously updated via an automated pipeline, which decomposes each problem into subproblems with unit tests based on dependency tree analysis and dataflow analysis. We further propose a novel evaluation framework featured dual assessment protocol and structural metrics derived from dependency trees. Extensive experiments on 16 popular LLMs reveal significant performance degradation in multi-turn scenarios. For instance, o1-mini retains only 20.8% Pass@1 in multi-turn scenario versus 37.8% in single-turn scenario. More fine-grained analysis illustrates that model performance inversely correlates with dependency complexity. These findings not only highlight the critical challenges for supporting real-world workflows, but also establish CodeFlowBench as an essential tool for advancing code generation research.",http://arxiv.org/abs/2504.21751v2,CS,Quantitative
Latent Space Class Dispersion: Effective Test Data Quality Assessment for DNNs,"High-quality test datasets are crucial for assessing the reliability of Deep Neural Networks (DNNs). Mutation testing evaluates test dataset quality based on their ability to uncover injected faults in DNNs as measured by mutation score (MS). At the same time, its high computational cost motivates researchers to seek alternative test adequacy criteria. We propose Latent Space Class Dispersion (LSCD), a novel metric to quantify the quality of test datasets for DNNs. It measures the degree of dispersion within a test dataset as observed in the latent space of a DNN. Our empirical study shows that LSCD reveals and quantifies deficiencies in the test dataset of three popular benchmarks pertaining to image classification tasks using DNNs. Corner cases generated using automated fuzzing were found to help enhance fault detection and improve the overall quality of the original test sets calculated by MS and LSCD. Our experiments revealed a high positive correlation (0.87) between LSCD and MS, significantly higher than the one achieved by the well-studied Distance-based Surprise Coverage (0.25). These results were obtained from 129 mutants generated through pre-training mutation operators, with statistical significance and a high validity of corner cases. These observations suggest that LSCD can serve as a cost-effective alternative to expensive mutation testing, eliminating the need to generate mutant models while offering comparably valuable insights into test dataset quality for DNNs.",http://arxiv.org/abs/2503.18799v1,CS,Mixed
Cost-driven prunings for iterative solving of constrained routing problem with SRLG-disjoint protection,"The search for the optimal pair of active and protection paths in a network with Shared Risk Link Groups (SRLG) is a challenging but high-value problem in the industry that is inevitable in ensuring reliable connections on the modern Internet. We propose a new approach to solving this problem, with a novel use of statistical analysis of the distribution of paths with respect to their cost, which is an integral part of our innovation. The key idea in our algorithm is to employ iterative updates of cost bounds, allowing efficient pruning of suboptimal paths. This idea drives an efficacious exploration of the search space. We benchmark our algorithms against the state-of-the-art algorithms that exploit the alternative strategy of conflicting links exclusion, showing that our approach has the advantage of finding more feasible connections within a set time limit.",http://arxiv.org/abs/2503.08262v1,CS,Quantitative
Beyond the Classroom: Bridging the Gap Between Academia and Industry with a Hands-on Learning Approach,"Modern software systems require various capabilities to meet architectural and operational demands, such as the ability to scale automatically and recover from sudden failures. Self-adaptive software systems have emerged as a critical focus in software design and operation due to their capacity to autonomously adapt to changing environments. However, educating students on this topic is scarce in academia, and a survey among practitioners identified that the lack of knowledgeable individuals has hindered its adoption in the industry. In this paper, we present our experience teaching a course on self-adaptive software systems that integrates theoretical knowledge and hands-on learning with industry-relevant technologies. To close the gap between academic education and industry practices, we incorporated guest lectures from experts and showcases featuring industry professionals as judges, improving technical and communication skills for our students. Feedback based on surveys from 21 students indicates significant improvements in their understanding of self-adaptive systems. The empirical analysis of the developed course demonstrates the effectiveness of the proposed course syllabus and teaching methodology. In addition, we provide a summary of the educational challenges of running this unique course, including balancing theory and practice, addressing the diverse backgrounds and motivations of students, and integrating the industry-relevant technologies. We believe these insights can provide valuable guidance for educating students in other emerging topics within software engineering.",http://arxiv.org/abs/2504.10726v1,CS,Quantitative
Emotional Strain and Frustration in LLM Interactions in Software Engineering,"Large Language Models (LLMs) are increasingly integrated into various daily tasks in Software Engineering such as coding and requirement elicitation. Despite their various capabilities and constant use, some interactions can lead to unexpected challenges (e.g. hallucinations or verbose answers) and, in turn, cause emotions that develop into frustration. Frustration can negatively impact engineers' productivity and well-being if they escalate into stress and burnout. In this paper, we assess the impact of LLM interactions on software engineers' emotional responses, specifically strains, and identify common causes of frustration when interacting with LLMs at work. Based on 62 survey responses from software engineers in industry and academia across various companies and universities, we found that a majority of our respondents experience frustrations or other related emotions regardless of the nature of their work. Additionally, our results showed that frustration mainly stemmed from issues with correctness and less critical issues such as adaptability to context or specific format. While such issues may not cause frustration in general, artefacts that do not follow certain preferences, standards, or best practices can make the output unusable without extensive modification, causing frustration over time. In addition to the frustration triggers, our study offers guidelines to improve the software engineers' experience, aiming to minimise long-term consequences on mental health.",http://arxiv.org/abs/2504.10050v3,CS,Quantitative
QuCheck: A Property-based Testing Framework for Quantum Programs in Qiskit,"Property-based testing has been previously proposed for quantum programs in Q# with QSharpCheck; however, this implementation was limited in functionality, lacked extensibility, and was evaluated on a narrow range of programs using a single property. To address these limitations, we propose QuCheck, an enhanced property-based testing framework in Qiskit. By leveraging Qiskit and the broader Python ecosystem, QuCheck facilitates property construction, introduces flexible input generators and assertions, and supports expressive preconditions. We assessed its effectiveness through mutation analysis on five quantum programs (2-10 qubits), varying the number of properties, inputs, and measurement shots to assess their impact on fault detection and demonstrate the effectiveness of property-based testing across a range of conditions. Results show a strong positive correlation between the mutation score (a measure of fault detection) and number of properties evaluated, with a moderate negative correlation between the false positive rate and number of measurement shots. Among the most thorough test configurations, those evaluating three properties achieved a mean mutation score ranging from 0.90 to 0.92 across all five algorithms, with the false positive rate between 0 and 0.04. QuCheck identified 36.0% more faults than QSharpCheck, with execution time reduced by 81.1%, despite one false positive. These findings underscore the viability of property-based testing for verifying quantum systems.",http://arxiv.org/abs/2503.22641v1,CS,Quantitative
How Accurately Do Large Language Models Understand Code?,"Large Language Models (LLMs) are increasingly used in post-development tasks such as code repair and testing. A key factor in these tasks' success is the model's deep understanding of code. However, the extent to which LLMs truly understand code remains largely unevaluated. Quantifying code comprehension is challenging due to its abstract nature and the lack of a standardized metric. Previously, this was assessed through developer surveys, which are not feasible for evaluating LLMs. Existing LLM benchmarks focus primarily on code generation, fundamentally different from code comprehension. Additionally, fixed benchmarks quickly become obsolete as they become part of the training data. This paper presents the first large-scale empirical investigation into LLMs' ability to understand code. Inspired by mutation testing, we use an LLM's fault-finding ability as a proxy for its deep code understanding. This approach is based on the insight that a model capable of identifying subtle functional discrepancies must understand the code well. We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization. Next, we apply semantic-preserving code mutations (SPMs) to the faulty programs and test whether the LLMs still locate the faults, verifying their confidence in code understanding. We evaluate nine popular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs. We find that LLMs lose the ability to debug the same bug in 78% of faulty programs when SPMs are applied, indicating a shallow understanding of code and reliance on features irrelevant to semantics. We also find that LLMs understand code earlier in the program better than later. This suggests that LLMs' code comprehension remains tied to lexical and syntactic features due to tokenization designed for natural languages, which overlooks code semantics.",http://arxiv.org/abs/2504.04372v2,CS,Quantitative
CertainSync: Rateless Set Reconciliation with Certainty,"Set reconciliation is a fundamental task in distributed systems, particularly in blockchain networks, where it enables synchronization of transaction pools among peers and facilitates block dissemination. Traditional set reconciliation schemes are either statistical, offering success probability as a function of communication overhead and symmetric difference size, or require parametrization and estimation of that size, which can be error-prone. We present CertainSync, a novel reconciliation framework that, to the best of our knowledge, is the first to guarantee successful set reconciliation without any parametrization or estimators. The framework is rateless and adapts to the unknown symmetric difference size. Reconciliation is guaranteed whenever the communication overhead reaches a lower bound derived from the symmetric difference size and universe size. Our framework builds on recent constructions of Invertible Bloom Lookup Tables (IBLTs), ensuring successful element listing as long as the number of elements is bounded. We provide a theoretical analysis proving the certainty of reconciliation for multiple constructions. Our approach is validated by simulations, showing the ability to synchronize sets with efficient communication costs while maintaining guarantees compared to baseline schemes. To further reduce overhead in large universes such as blockchain networks, CertainSync is extended with a universe reduction technique. We compare and validate this extension, UniverseReduceSync, against the basic framework using real Ethereum transaction hash data. Results show a trade-off between lower communication costs and maintaining guarantees, offering a comprehensive solution for diverse reconciliation scenarios.",http://arxiv.org/abs/2504.08314v1,CS,Quantitative
Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding,"Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.",http://arxiv.org/abs/2505.07768v1,CS,Quantitative
Unveiling Hybrid Cyclomatic Complexity: A Comprehensive Analysis and Evaluation as an Integral Feature in Automatic Defect Prediction Models,"The complex software systems developed nowadays require assessing their quality and proneness to errors. Reducing code complexity is a never-ending problem, especially in today's fast pace of software systems development. Therefore, the industry needs to find a method to determine the qualities of a software system, the degree of difficulty in developing new functionalities, or the system's proneness to errors. One way of measuring and predicting the quality attributes of a software system is to analyse the software metrics values for it and the relationships between them. More precisely, we should study the metrics that measure and determine the degree of complexity of the code. This paper aims to analyse a novel complexity metric, Hybrid Cyclomatic Complexity (HCC) and its efficiency as a feature in a defect prediction model. The main idea behind this new metric is that inherited complexity should play a role in the complexity of a class, hence the need for a metric that calculates the total complexity of a class, taking into account the complexities of its descendants. Moreover, we will present a comparative study between the HCC metric and its two components, the inherited complexity and the actual complexity of a class in the object-oriented context. Since we want this metric to be as valuable as possible, the experiments will use data from open-source projects. One of the conclusions that can be drawn from these experiments is that inherited complexity is not correlated with class complexity. Therefore, HCC can be considered a valid metric from this point of view. Moreover, the evaluation of the efficiency of the prediction models shows us a similar efficiency for HCC and the inherited complexity. Additionally, there is a need for a clear distinction between a class's complexity and its inherited complexity when defining complexity metrics.",http://arxiv.org/abs/2504.00477v1,CS,Quantitative
OSS-Bench: Benchmark Generator for Coding LLMs,"In light of the rapid adoption of AI coding assistants, LLM-assisted development has become increasingly prevalent, creating an urgent need for robust evaluation of generated code quality. Existing benchmarks often require extensive manual effort to create static datasets, rely on indirect or insufficiently challenging tasks, depend on non-scalable ground truth, or neglect critical low-level security evaluations, particularly memory-safety issues. In this work, we introduce OSS-Bench, a benchmark generator that automatically constructs large-scale, live evaluation tasks from real-world open-source software. OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth. In our evaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql), profiles 17 diverse LLMs, revealing insights such as intra-family behavioral patterns and inconsistencies between model size and performance. Our results demonstrate that OSS-Bench mitigates overfitting by leveraging the evolving complexity of OSS and highlights LLMs' limited understanding of low-level code security via extended fuzzing experiments. Overall, OSS-Bench offers a practical and scalable framework for benchmarking the real-world coding capabilities of LLMs.",http://arxiv.org/abs/2505.12331v2,CS,Quantitative
Wavelet-Based CSI Reconstruction for Improved Wireless Security Through Channel Reciprocity,"The reciprocity of channel state information (CSI) collected by two devices communicating over a wireless channel has been leveraged to provide security solutions to resource-limited IoT devices. Despite the extensive research that has been done on this topic, much of the focus has been on theoretical and simulation analysis. However, these security solutions face key implementation challenges, mostly pertaining to limitations of IoT hardware and variations of channel conditions, limiting their practical adoption. To address this research gap, we revisit the channel reciprocity assumption from an experimental standpoint using resource-constrained devices. Our experimental study reveals a significant degradation in channel reciprocity for low-cost devices due to the varying channel conditions. Through experimental investigations, we first identify key practical causes for the degraded channel reciprocity. We then propose a new wavelet-based CSI reconstruction technique using wavelet coherence and time-lagged cross-correlation to construct CSI data that are consistent between the two participating devices, resulting in significant improvement in channel reciprocity. Additionally, we propose a secret-key generation scheme that exploits the wavelet-based CSI reconstruction, yielding significant increase in the key generation rates. Finally, we propose a technique that exploits CSI temporal variations to enhance device authentication resiliency through effective detection of replay attacks.",http://arxiv.org/abs/2504.08078v1,CS,Quantitative
TickIt: Leveraging Large Language Models for Automated Ticket Escalation,"In large-scale cloud service systems, support tickets serve as a critical mechanism for resolving customer issues and maintaining service quality. However, traditional manual ticket escalation processes encounter significant challenges, including inefficiency, inaccuracy, and difficulty in handling the high volume and complexity of tickets. While previous research has proposed various machine learning models for ticket classification, these approaches often overlook the practical demands of real-world escalations, such as dynamic ticket updates, topic-specific routing, and the analysis of ticket relationships. To bridge this gap, this paper introduces TickIt, an innovative online ticket escalation framework powered by Large Language Models. TickIt enables topic-aware, dynamic, and relationship-driven ticket escalations by continuously updating ticket states, assigning tickets to the most appropriate support teams, exploring ticket correlations, and leveraging category-guided supervised fine-tuning to continuously improve its performance. By deploying TickIt in ByteDance's cloud service platform Volcano Engine, we validate its efficacy and practicality, marking a significant advancement in the field of automated ticket escalation for large-scale cloud service systems.",http://arxiv.org/abs/2504.08475v1,CS,Quantitative
A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation,"Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \ul{S}emantic \ul{T}argeting for \ul{A}nalytical \ul{R}epair (\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\textsc{MINT}) and optimization methods (\textsc{SGD}), \textsc{STAR} integrates their strengths while mitigating their limitations. \textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \textsc{STAR} demonstrates superior effectiveness. Additionally, \textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \textsc{STAR} outperforms prior work by a significant margin.",http://arxiv.org/abs/2503.12899v2,CS,Quantitative
"Quality In, Quality Out: Investigating Training Data's Role in AI Code Generation","Deep Learning-based code generators have seen significant advancements in recent years. Tools such as GitHub Copilot are used by thousands of developers with the main promise of a boost in productivity. However, researchers have recently questioned their impact on code quality showing, for example, that code generated by DL-based tools may be affected by security vulnerabilities. Since DL models are trained on large code corpora, one may conjecture that low-quality code they output is the result of low-quality code they have seen during training. However, there is very little empirical evidence documenting this phenomenon. Indeed, most of previous work look at the frequency with which commercial code generators recommend low-quality code without the possibility of relating this to their training set. We investigate the extent to which low-quality code instances seen during training affect the quality of the code generated at inference time. We start by fine-tuning a pre-trained DL model on a large-scale dataset being representative of those usually adopted in the training of code generators. We show that 4.98% of functions in this dataset exhibit one or more quality issues related to security, maintainability, best practices, etc. We use the fine-tuned model to generate 551k Python functions, showing that 5.85% of them are affected by at least one quality issue. We then remove from the training set the low-quality functions, and use the cleaned dataset to fine-tune a second model which has been used to generate the same 551k Python functions. We show that the model trained on the cleaned dataset exhibits similar performance in terms of functional correctness as compared to the original model while, however, generating a statistically significant lower number of low-quality functions (2.16%). Our study empirically documents the importance of high-quality training data for code generators.",http://arxiv.org/abs/2503.11402v1,CS,Quantitative
Compendium Manager: a tool for coordination of workflow management instances for bulk data processing in Python,"Compendium Manager is a command-line tool written in Python to automate the provisioning, launch, and evaluation of bioinformatics pipelines. Although workflow management tools such as Snakemake and Nextflow enable users to automate the processing of samples within a single sequencing project, integrating many datasets in bulk requires launching and monitoring hundreds or thousands of pipelines. We present the Compendium Manager, a lightweight command-line tool to enable launching and monitoring analysis pipelines at scale. The tool can gauge progress through a list of projects, load results into a shared database, and record detailed processing metrics for later evaluation and reproducibility.",http://arxiv.org/abs/2505.11385v1,CS,Quantitative
Critical Considerations on Effort-aware Software Defect Prediction Metrics,"Background. Effort-aware metrics (EAMs) are widely used to evaluate the effectiveness of software defect prediction models, while accounting for the effort needed to analyze the software modules that are estimated defective. The usual underlying assumption is that this effort is proportional to the modules' size measured in LOC. However, the research on module analysis (including code understanding, inspection, testing, etc.) suggests that module analysis effort may be better correlated to code attributes other than size. Aim. We investigate whether assuming that module analysis effort is proportional to other code metrics than LOC leads to different evaluations. Method. We show mathematically that the choice of the code measure used as the module effort driver crucially influences the resulting evaluations. To illustrate the practical consequences of this, we carried out a demonstrative empirical study, in which the same model was evaluated via EAMs, assuming that effort is proportional to either McCabe's complexity or LOC. Results. The empirical study showed that EAMs depend on the underlying effort model, and can give quite different indications when effort is modeled differently. It is also apparent that the extent of these differences varies widely. Conclusions. Researchers and practitioners should be aware that the reliability of the indications provided by EAMs depend on the nature of the underlying effort model. The EAMs used until now appear to be actually size-aware, rather than effort-aware: when analysis effort does not depend on size, these EAMs can be misleading.",http://arxiv.org/abs/2504.19181v1,CS,Quantitative
Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models,"We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to \mathbb{R}$ that depends only on the projection of its input onto a $K$-dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most $m$ distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the realizable and independent noise settings, our algorithm incurs complexity $d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper bound, we show that if for some subspace degree-$m$ distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity $d^{\Omega(m)}$. As an application, we give the first efficient learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The resulting algorithm has complexity $\mathrm{poly}(d) 2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work.",http://arxiv.org/abs/2505.21475v1,CS,Mixed
A Software Engineering Capstone Course Facilitated By GitHub Templates,"How can instructors facilitate spreading out the work in a software engineering or computer science capstone course across time and among team members? Currently teams often compromise the quality of their learning experience by frantically working before each deliverable. Some team members further compromise their own learning, and that of their colleagues, by not contributing their fair share to the team effort. To mitigate these problems, we propose using a GitHub template that contains all the initial infrastructure a team needs, including the folder structure, text-based template documents and template issues. In addition, we propose each team begins the year by identifying specific quantifiable individual productivity metrics for monitoring, such as the count of meetings attended, issues closed and number of commits. Initial data suggests that these steps may have an impact. In 2022/23 we observed 24% of commits happening on the due dates. After partially introducing the above ideas in 2023/24, this number improved to 18%. To measure the fairness we introduce a fairness measure based on the disparity between number of commits between all pairs of teammates. Going forward we propose an experiment where commit data and interview data is compared between teams that use the proposed interventions and those that do not.",http://arxiv.org/abs/2410.12114v1,CS,Mixed
An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks,"Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, other existing automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts. In this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SWE-Judge first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges to produce a final correctness score through ensembling. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess. These benchmarks span three SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SWE-Judge consistently achieves a higher correlation with human judgments, with improvements ranging from 5.9% to 183.8% over existing automatic metrics. Furthermore, SWE-Judge reaches agreement levels with human annotators that are comparable to inter-annotator agreement in code generation and program repair tasks. These findings underscore SWE-Judge's potential as a scalable and reliable alternative to human evaluation.",http://arxiv.org/abs/2505.20854v1,CS,Quantitative
A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements,"In this work, we present a novel approach to process the DIC measurements of multiple biaxial stretching protocols. In particular, we develop a optimization-based approach, which calculates the smoothed nodal displacements using a moving least-squares algorithm subject to positive strain constraints. As such, physically consistent displacement and strain fields are obtained. Then, we further deploy a data-driven workflow to heterogeneous material modeling from these physically consistent DIC measurements, by estimating a nonlocal constitutive law together with the material microstructure. To demonstrate the applicability of our approach, we apply it in learning a material model and fiber orientation field from DIC measurements of a porcine tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC data processing approach can significantly improve the accuracy of modeling biological materials.",http://arxiv.org/abs/2505.18784v1,CS,Quantitative
"Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning","Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived primarily from anthropogenic sources. The recent increase in river-discharged nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR) lagoon. Although nitrate is an optically inactive (i.e., colourless) constituent, previous studies have demonstrated there is an indirect, non-causal relationship between water surface nitrate and water-leaving reflectance that is mediated through optically active water quality parameters such as total suspended solids and coloured dissolved organic matter. This work aims to advance our understanding of this relationship with an effort to measure time-series nitrate and simultaneous hyperspectral reflectance at the Fitzroy River estuary, Queensland, Australia. Time-series observations revealed periodic cycles in nitrate loads due to the tidal influence in the estuarine study site. The water surface nitrate loads were predicted from hyperspectral reflectance and water salinity measurements, with hyperspectral reflectance indicating the concentrations of optically active variables and salinity indicating the mixing of river water and seawater proportions. The accuracy assessment of model-predicted nitrate against in-situ measured nitrate values showed that the predicted nitrate values correlated well with the ground-truth data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work demonstrates the feasibility of predicting water surface nitrate from hyperspectral reflectance and salinity measurements.",http://arxiv.org/abs/2505.17483v1,CS,Mixed
E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing,"This paper presents an intelligent work automation approach in the context of contemporary digital transformation by integrating generative AI and Intelligent Document Processing (IDP) technologies with an Automation Agent to realize End-to-End (E2E) automation of corporate financial expense processing tasks. While traditional Robotic Process Automation (RPA) has proven effective for repetitive, rule-based simple task automation, it faces limitations in handling unstructured data, exception management, and complex decision-making. This study designs and implements a four-stage integrated process comprising automatic recognition of supporting documents such as receipts via OCR/IDP, item classification based on a policy-driven database, intelligent exception handling supported by generative AI (large language models, LLMs), and human-in-the-loop final decision-making with continuous system learning through an Automation Agent. Applied to a major Korean enterprise (Company S), the system demonstrated quantitative benefits including over 80% reduction in processing time for paper receipt expense tasks, decreased error rates, and improved compliance, as well as qualitative benefits such as enhanced accuracy and consistency, increased employee satisfaction, and data-driven decision support. Furthermore, the system embodies a virtuous cycle by learning from human judgments to progressively improve automatic exception handling capabilities. Empirically, this research confirms that the organic integration of generative AI, IDP, and Automation Agents effectively overcomes the limitations of conventional automation and enables E2E automation of complex corporate processes. The study also discusses potential extensions to other domains such as accounting, human resources, and procurement, and proposes future directions for AI-driven hyper-automation development.",http://arxiv.org/abs/2505.20733v1,IS,Mixed
The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions,"Self-disclosure, the sharing of one's thoughts and feelings, is affected by the perceived relationship between individuals. While chatbots are increasingly used for self-disclosure, the impact of a chatbot's framing on users' self-disclosure remains under-explored. We investigated how a chatbot's description of its relationship with users, particularly in terms of ephemerality, affects self-disclosure. Specifically, we compared a Familiar chatbot, presenting itself as a companion remembering past interactions, with a Stranger chatbot, presenting itself as a new, unacquainted entity in each conversation. In a mixed factorial design, participants engaged with either the Familiar or Stranger chatbot in two sessions across two days, with one conversation focusing on Emotional- and another Factual-disclosure. When Emotional-disclosure was sought in the first chatting session, Stranger-condition participants felt more comfortable self-disclosing. However, when Factual-disclosure was sought first, these differences were replaced by more enjoyment among Familiar-condition participants. Qualitative findings showed Stranger afforded anonymity and reduced judgement, whereas Familiar sometimes felt intrusive unless rapport was built via low-risk Factual-disclosure.",http://arxiv.org/abs/2505.20464v1,IS,Qualitative
UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space,"Diffusion models have shown great potential in generating realistic image detail. However, adapting these models to video super-resolution (VSR) remains challenging due to their inherent stochasticity and lack of temporal modeling. In this paper, we propose UltraVSR, a novel framework that enables ultra-realistic and temporal-coherent VSR through an efficient one-step diffusion space. A central component of UltraVSR is the Degradation-aware Restoration Schedule (DRS), which estimates a degradation factor from the low-resolution input and transforms iterative denoising process into a single-step reconstruction from from low-resolution to high-resolution videos. This design eliminates randomness from diffusion noise and significantly speeds up inference. To ensure temporal consistency, we propose a lightweight yet effective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution unit and an RTS-attention unit. By partially shifting feature components along the temporal dimension, these two units collaboratively facilitate effective feature propagation, fusion, and alignment across neighboring frames, without relying on explicit temporal layers. The RTS module is integrated into a pretrained text-to-image diffusion model and is further enhanced through Spatio-temporal Joint Distillation (SJD), which improves temporal coherence while preserving realistic details. Additionally, we introduce a Temporally Asynchronous Inference (TAI) strategy to capture long-range temporal dependencies under limited memory constraints. Extensive experiments show that UltraVSR achieves state-of-the-art performance, both qualitatively and quantitatively, in a single sampling step.",http://arxiv.org/abs/2505.19958v1,IS,Mixed
AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection,"Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness.",http://arxiv.org/abs/2505.19528v2,IS,Mixed
The Role of Video Generation in Enhancing Data-Limited Action Understanding,"Video action understanding tasks in real-world scenarios always suffer data limitations. In this paper, we address the data-limited action understanding problem by bridging data scarcity. We propose a novel method that employs a text-to-video diffusion transformer to generate annotated data for model training. This paradigm enables the generation of realistic annotated data on an infinite scale without human intervention. We proposed the information enhancement strategy and the uncertainty-based label smoothing tailored to generate sample training. Through quantitative and qualitative analysis, we observed that real samples generally contain a richer level of information than generated samples. Based on this observation, the information enhancement strategy is proposed to enhance the informative content of the generated samples from two aspects: the environments and the characters. Furthermore, we observed that some low-quality generated samples might negatively affect model training. To address this, we devised the uncertainty-based label smoothing strategy to increase the smoothing of these samples, thus reducing their impact. We demonstrate the effectiveness of the proposed method on four datasets across five tasks and achieve state-of-the-art performance for zero-shot action recognition.",http://arxiv.org/abs/2505.19495v1,IS,Mixed
Training-free Stylized Text-to-Image Generation with Fast Inference,"Although diffusion models exhibit impressive generative capabilities, existing methods for stylized image generation based on these models often require textual inversion or fine-tuning with style images, which is time-consuming and limits the practical applicability of large-scale diffusion models. To address these challenges, we propose a novel stylized image generation method leveraging a pre-trained large-scale diffusion model without requiring fine-tuning or any additional optimization, termed as OmniPainter. Specifically, we exploit the self-consistency property of latent consistency models to extract the representative style statistics from reference style images to guide the stylization process. Additionally, we then introduce the norm mixture of self-attention, which enables the model to query the most relevant style patterns from these statistics for the intermediate output content features. This mechanism also ensures that the stylized results align closely with the distribution of the reference style images. Our qualitative and quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art approaches.",http://arxiv.org/abs/2505.19063v2,IS,Mixed
Do LLMs have a Gender (Entropy) Bias?,"We investigate the existence and persistence of a specific type of gender bias in some of the popular LLMs and contribute a new benchmark dataset, RealWorldQuestioning (released on HuggingFace ), developed from real-world questions across four key domains in business and health contexts: education, jobs, personal financial management, and general health. We define and study entropy bias, which we define as a discrepancy in the amount of information generated by an LLM in response to real questions users have asked. We tested this using four different LLMs and evaluated the generated responses both qualitatively and quantitatively by using ChatGPT-4o (as ""LLM-as-judge""). Our analyses (metric-based comparisons and ""LLM-as-judge"" evaluation) suggest that there is no significant bias in LLM responses for men and women at a category level. However, at a finer granularity (the individual question level), there are substantial differences in LLM responses for men and women in the majority of cases, which ""cancel"" each other out often due to some responses being better for males and vice versa. This is still a concern since typical users of these tools often ask a specific question (only) as opposed to several varied ones in each of these common yet important areas of life. We suggest a simple debiasing approach that iteratively merges the responses for the two genders to produce a final result. Our approach demonstrates that a simple, prompt-based debiasing strategy can effectively debias LLM outputs, thus producing responses with higher information content than both gendered variants in 78% of the cases, and consistently achieving a balanced integration in the remaining cases.",http://arxiv.org/abs/2505.20343v1,IS,Mixed
AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa,"Climate policy scenario generation and evaluation have traditionally relied on integrated assessment models (IAMs) and expert-driven qualitative analysis. These methods enable stakeholders, such as policymakers and researchers, to anticipate impacts, plan governance strategies, and develop mitigation measures. However, traditional methods are often time-intensive, reliant on simple extrapolations of past trends, and limited in capturing the complex and interconnected nature of energy and climate issues. With the advent of artificial intelligence (AI), particularly generative AI models trained on vast datasets, these limitations can be addressed, ensuring robustness even under limited data conditions. In this work, we explore the novel method that employs generative AI, specifically large language models (LLMs), to simulate climate policy scenarios for Sub-Saharan Africa. These scenarios focus on energy transition themes derived from the historical United Nations Climate Change Conference (COP) documents. By leveraging generative models, the project aims to create plausible and diverse policy scenarios that align with regional climate goals and energy challenges. Given limited access to human evaluators, automated techniques were employed for scenario evaluation. We generated policy scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%) passed expert validation, accurately reflecting the intended impacts provided in the corresponding prompts. We compared these validated responses against assessments from a human climate expert and two additional LLMs (gemma2-2B and mistral-7B). Our structured, embedding-based evaluation framework shows that generative AI effectively generate scenarios that are coherent, relevant, plausible, and diverse. This approach offers a transformative tool for climate policy planning in data-constrained regions.",http://arxiv.org/abs/2505.18694v1,IS,Mixed
What Needs Attention? Prioritizing Drivers of Developers' Trust and Adoption of Generative AI,"Generative AI (genAI) tools are advertised as productivity aids. Yet, issues related to miscalibrated trust and usage friction continue to hinder their adoption. Additionally, AI can be exclusionary, failing to support diverse users adequately, further exacerbating these concerns. One such aspect of diversity is cognitive diversity -- variations in users' cognitive styles -- that leads to divergence in interaction styles. When an individual's cognitive styles are unsupported, it creates additional barriers to technology adoption. Thus, to design tools that developers trust, we must first understand what factors affect their trust and intentions to use these tools in practice? We developed a theoretical model of factors influencing trust and adoption intentions towards genAI through a large-scale survey with developers (N=238) at GitHub and Microsoft. Using Partial Least Squares-Structural Equation Modeling (PLS-SEM), we found that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust, which along with their cognitive styles, affects their intentions to use these tools in work. An Importance-Performance Matrix Analysis (IPMA) identified factors that, despite their strong influence, underperform, revealing specific genAI aspects that need design prioritization. We bolster these findings by qualitatively analyzing developers' perceived challenges and risks of genAI usage to uncover why these gaps persist in development contexts. For genAI to indeed be a true productivity aid rather than a disguised productivity sink, it must align with developers' goals, maintain contextual transparency, reduce cognitive burden, and provide equitable interaction support. We provide practical suggestions to guide future genAI tool design for effective, trustworthy, and inclusive human-genAI interactions.",http://arxiv.org/abs/2505.17418v1,IS,Mixed
OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning,"Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its capacity to generalize segmentation beyond predefined categories. However, existing methods typically predict segmentation masks with simple forward inference, lacking explicit reasoning and interpretability. This makes it challenging for OVS model to distinguish similar categories in open-world settings due to the lack of contextual understanding and discriminative visual cues. To address this limitation, we propose a step-by-step visual reasoning framework for open-vocabulary segmentation, named OpenSeg-R. The proposed OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical visual reasoning before segmentation. Specifically, we generate both generic and image-specific reasoning for each image, forming structured triplets that explain the visual reason for objects in a coarse-to-fine manner. Based on these reasoning steps, we can compose detailed description prompts, and feed them to the segmentor to produce more accurate segmentation masks. To the best of our knowledge, OpenSeg-R is the first framework to introduce explicit step-by-step visual reasoning into OVS. Experimental results demonstrate that OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary semantic segmentation across five benchmark datasets. Moreover, it achieves consistent gains across all metrics on open-vocabulary panoptic segmentation. Qualitative results further highlight the effectiveness of our reasoning-guided framework in improving both segmentation precision and interpretability. Our code is publicly available at https://github.com/Hanzy1996/OpenSeg-R.",http://arxiv.org/abs/2505.16974v1,IS,Mixed
PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG,"The introduction of Large Language Models (LLMs) has significantly transformed Natural Language Processing (NLP) applications by enabling more advanced analysis of customer personas. At Volvo Construction Equipment (VCE), customer personas have traditionally been developed through qualitative methods, which are time-consuming and lack scalability. The main objective of this paper is to generate synthetic customer personas and integrate them into a Retrieval-Augmented Generation (RAG) chatbot to support decision-making in business processes. To this end, we first focus on developing a persona-based RAG chatbot integrated with verified personas. Next, synthetic personas are generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and evaluated based on completeness, relevance, and consistency using McNemar's test. In the final step, the chatbot's knowledge base is augmented with synthetic personas and additional segment information to assess improvements in response accuracy and practical utility. Key findings indicate that Few-Shot prompting outperformed CoT in generating more complete personas, while CoT demonstrated greater efficiency in terms of response time and token usage. After augmenting the knowledge base, the average accuracy rating of the chatbot increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants found the updated system useful in business contexts.",http://arxiv.org/abs/2505.17156v1,IS,Qualitative
Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions,"During job recruitment, traditional applicant selection methods often lack transparency. Candidates are rarely given sufficient justifications for recruiting decisions, whether they are made manually by human recruiters or through the use of black-box Applicant Tracking Systems (ATS). To address this problem, our work introduces a multi-agent AI system that uses Large Language Models (LLMs) to guide job seekers during the recruitment process. Using an iterative user-centric design approach, we first conducted a two-phased exploratory study with four active job seekers to inform the design and development of the system. Subsequently, we conducted an in-depth, qualitative user study with 20 active job seekers through individual one-to-one interviews to evaluate the developed prototype. The results of our evaluation demonstrate that participants perceived our multi-agent recruitment system as significantly more actionable, trustworthy, and fair compared to traditional methods. Our study further helped us uncover in-depth insights into factors contributing to these perceived user experiences. Drawing from these insights, we offer broader design implications for building user-aligned, multi-agent explainable AI systems across diverse domains.",http://arxiv.org/abs/2505.20312v1,IS,Qualitative
Erased or Dormant? Rethinking Concept Erasure Through Reversibility,"To what extent does concept erasure eliminate generative capacity in diffusion models? While prior evaluations have primarily focused on measuring concept suppression under specific textual prompts, we explore a complementary and fundamental question: do current concept erasure techniques genuinely remove the ability to generate targeted concepts, or do they merely achieve superficial, prompt-specific suppression? We systematically evaluate the robustness and reversibility of two representative concept erasure methods, Unified Concept Editing and Erased Stable Diffusion, by probing their ability to eliminate targeted generative behaviors in text-to-image models. These methods attempt to suppress undesired semantic concepts by modifying internal model parameters, either through targeted attention edits or model-level fine-tuning strategies. To rigorously assess whether these techniques truly erase generative capacity, we propose an instance-level evaluation strategy that employs lightweight fine-tuning to explicitly test the reactivation potential of erased concepts. Through quantitative metrics and qualitative analyses, we show that erased concepts often reemerge with substantial visual fidelity after minimal adaptation, indicating that current methods suppress latent generative representations without fully eliminating them. Our findings reveal critical limitations in existing concept erasure approaches and highlight the need for deeper, representation-level interventions and more rigorous evaluation standards to ensure genuine, irreversible removal of concepts from generative models.",http://arxiv.org/abs/2505.16174v1,IS,Mixed
Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space,"Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like ""soft"" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.",http://arxiv.org/abs/2505.15778v1,IS,Mixed
"FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion","Human facial images encode a rich spectrum of information, encompassing both stable identity-related traits and mutable attributes such as pose, expression, and emotion. While recent advances in image generation have enabled high-quality identity-conditional face synthesis, precise control over non-identity attributes remains challenging, and disentangling identity from these mutable factors is particularly difficult. To address these limitations, we propose a novel identity-conditional diffusion model that introduces two lightweight control modules designed to independently manipulate facial pose, expression, and emotion without compromising identity preservation. These modules are embedded within the cross-attention layers of the base diffusion model, enabling precise attribute control with minimal parameter overhead. Furthermore, our tailored training strategy, which leverages cross-attention between the identity feature and each non-identity control feature, encourages identity features to remain orthogonal to control signals, enhancing controllability and diversity. Quantitative and qualitative evaluations, along with perceptual user studies, demonstrate that our method surpasses existing approaches in terms of control accuracy over pose, expression, and emotion, while also improving generative diversity under identity-only conditioning.",http://arxiv.org/abs/2505.15313v1,IS,Mixed
Towards a Working Definition of Designing Generative User Interfaces,"Generative UI is transforming interface design by facilitating AI-driven collaborative workflows between designers and computational systems. This study establishes a working definition of Generative UI through a multi-method qualitative approach, integrating insights from a systematic literature review of 127 publications, expert interviews with 18 participants, and analyses of 12 case studies. Our findings identify five core themes that position Generative UI as an iterative and co-creative process. We highlight emerging design models, including hybrid creation, curation-based workflows, and AI-assisted refinement strategies. Additionally, we examine ethical challenges, evaluation criteria, and interaction models that shape the field. By proposing a conceptual foundation, this study advances both theoretical discourse and practical implementation, guiding future HCI research toward responsible and effective generative UI design practices.",http://arxiv.org/abs/2505.15049v1,IS,Mixed
FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance,"Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as ""switch leap with 0.5 turn"" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.",http://arxiv.org/abs/2505.13437v1,IS,Qualitative
RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers,"We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.",http://arxiv.org/abs/2505.13344v1,IS,Mixed
"topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation","Topic models are statistical tools that allow their users to gain qualitative and quantitative insights into the contents of textual corpora without the need for close reading. They can be applied in a wide range of settings from discourse analysis, through pretraining data curation, to text filtering. Topic models are typically parameter-rich, complex models, and interpreting these parameters can be challenging for their users. It is typical practice for users to interpret topics based on the top 10 highest ranking terms on a given topic. This list-of-words approach, however, gives users a limited and biased picture of the content of topics. Thoughtful user interface design and visualizations can help users gain a more complete and accurate understanding of topic models' output. While some visualization utilities do exist for topic models, these are typically limited to a certain type of topic model. We introduce topicwizard, a framework for model-agnostic topic model interpretation, that provides intuitive and interactive tools that help users examine the complex semantic relations between documents, words and topics learned by topic models.",http://arxiv.org/abs/2505.13034v1,IS,Mixed
SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds,"Segmentation evaluation metrics traditionally rely on binary decision logic: predictions are either correct or incorrect, based on rigid IoU thresholds. Detection--based metrics such as F1 and mAP determine correctness at the object level using fixed overlap cutoffs, while overlap--based metrics like Intersection over Union (IoU) and Dice operate at the pixel level, often overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify detection and segmentation assessment, but it remains dependent on hard-threshold matching--treating predictions below the threshold as entirely incorrect. This binary framing obscures important distinctions between qualitatively different errors and fails to reward gradual model improvements. We propose SoftPQ, a flexible and interpretable instance segmentation metric that redefines evaluation as a graded continuum rather than a binary classification. SoftPQ introduces tunable upper and lower IoU thresholds to define a partial matching region and applies a sublinear penalty function to ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit smoother score behavior, greater robustness to structural segmentation errors, and more informative feedback for model development and evaluation. Through controlled perturbation experiments, we show that SoftPQ captures meaningful differences in segmentation quality that existing metrics overlook, making it a practical and principled alternative for both benchmarking and iterative model refinement.",http://arxiv.org/abs/2505.12155v2,IS,Mixed
Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making,"Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.",http://arxiv.org/abs/2505.21503v1,IS,Quantitative
Are Language Models Consequentialist or Deontological Moral Reasoners?,"As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .",http://arxiv.org/abs/2505.21479v1,IS,Quantitative
VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin,"The performance of speaker verification systems is adversely affected by speaker aging. However, due to challenges in data collection, particularly the lack of sustained and large-scale longitudinal data for individuals, research on speaker aging remains difficult. In this paper, we present VoxAging, a large-scale longitudinal dataset collected from 293 speakers (226 English speakers and 67 Mandarin speakers) over several years, with the longest time span reaching 17 years (approximately 900 weeks). For each speaker, the data were recorded at weekly intervals. We studied the phenomenon of speaker aging and its effects on advanced speaker verification systems, analyzed individual speaker aging processes, and explored the impact of factors such as age group and gender on speaker aging research.",http://arxiv.org/abs/2505.21445v1,IS,Quantitative
A Structured Unplugged Approach for Foundational AI Literacy in Primary Education,"Younger generations are growing up in a world increasingly shaped by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them. However, education in this field often emphasizes tool-based learning, prioritizing usage over understanding the underlying concepts. This lack of knowledge leaves non-experts, especially children, prone to misconceptions, unrealistic expectations, and difficulties in recognizing biases and stereotypes. In this paper, we propose a structured and replicable teaching approach that fosters foundational AI literacy in primary students, by building upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI. To assess the effectiveness of our approach, we conducted an empirical study with thirty-one fifth-grade students across two classes, evaluating their progress through a post-test and a satisfaction survey. Our results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations. Moreover, the approach proved engaging, with students particularly enjoying activities that linked AI concepts to real-world reasoning. Materials: https://github.com/tail-unica/ai-literacy-primary-ed.",http://arxiv.org/abs/2505.21398v1,IS,Quantitative
3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin,"Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at \href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR",http://arxiv.org/abs/2505.21238v1,IS,Quantitative
A Representation Level Analysis of NMT Model Robustness to Grammatical Errors,"Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term Robustness Heads. We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on Robustness Heads for updating the ungrammatical word representation.",http://arxiv.org/abs/2505.21224v1,IS,Quantitative
Interpretable DNFs,"A classifier is considered interpretable if each of its decisions has an explanation which is small enough to be easily understood by a human user. A DNF formula can be seen as a binary classifier $\kappa$ over boolean domains. The size of an explanation of a positive decision taken by a DNF $\kappa$ is bounded by the size of the terms in $\kappa$, since we can explain a positive decision by giving a term of $\kappa$ that evaluates to true. Since both positive and negative decisions must be explained, we consider that interpretable DNFs are those $\kappa$ for which both $\kappa$ and $\overline{\kappa}$ can be expressed as DNFs composed of terms of bounded size. In this paper, we study the family of $k$-DNFs whose complements can also be expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision trees and nested $k$-DNFs, a novel family of models. Experiments indicate that nested $k$-DNFs are an interesting alternative to decision trees in terms of interpretability and accuracy.",http://arxiv.org/abs/2505.21212v1,IS,Quantitative
Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation,"Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: https://github.com/SuperSupermoon/Lunguage",http://arxiv.org/abs/2505.21190v1,IS,Mixed
IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model,"Existing human motion generation methods with trajectory and pose inputs operate global processing on both modalities, leading to suboptimal outputs. In this paper, we propose IKMo, an image-keyframed motion generation method based on the diffusion model with trajectory and pose being decoupled. The trajectory and pose inputs go through a two-stage conditioning framework. In the first stage, the dedicated optimization module is applied to refine inputs. In the second stage, trajectory and pose are encoded via a Trajectory Encoder and a Pose Encoder in parallel. Then, motion with high spatial and semantic fidelity is guided by a motion ControlNet, which processes the fused trajectory and pose data. Experiment results based on HumanML3D and KIT-ML datasets demonstrate that the proposed method outperforms state-of-the-art on all metrics under trajectory-keyframe constraints. In addition, MLLM-based agents are implemented to pre-process model inputs. Given texts and keyframe images from users, the agents extract motion descriptions, keyframe poses, and trajectories as the optimized inputs into the motion generation model. We conducts a user study with 10 participants. The experiment results prove that the MLLM-based agents pre-processing makes generated motion more in line with users' expectation. We believe that the proposed method improves both the fidelity and controllability of motion generation by the diffusion model.",http://arxiv.org/abs/2505.21146v1,IS,Quantitative
Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation,"Large Vision Language Models (LVLMs) have achieved remarkable progress in multimodal tasks, yet they also exhibit notable social biases. These biases often manifest as unintended associations between neutral concepts and sensitive human attributes, leading to disparate model behaviors across demographic groups. While existing studies primarily focus on detecting and quantifying such biases, they offer limited insight into the underlying mechanisms within the models. To address this gap, we propose an explanatory framework that combines information flow analysis with multi-round dialogue evaluation, aiming to understand the origin of social bias from the perspective of imbalanced internal information utilization. Specifically, we first identify high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, we design a multi-turn dialogue mechanism to evaluate the extent to which these key tokens encode sensitive information. Extensive experiments reveal that LVLMs exhibit systematic disparities in information usage when processing images of different demographic groups, suggesting that social bias is deeply rooted in the model's internal reasoning dynamics. Furthermore, we complement our findings from a textual modality perspective, showing that the model's semantic representations already display biased proximity patterns, thereby offering a cross-modal explanation of bias formation.",http://arxiv.org/abs/2505.21106v1,IS,Quantitative
CXXCrafter: An LLM-Based Agent for Automated C/C++ Open Source Software Building,"Project building is pivotal to support various program analysis tasks, such as generating intermediate rep- resentation code for static analysis and preparing binary code for vulnerability reproduction. However, automating the building process for C/C++ projects is a highly complex endeavor, involving tremendous technical challenges, such as intricate dependency management, diverse build systems, varied toolchains, and multifaceted error handling mechanisms. Consequently, building C/C++ projects often proves to be difficult in practice, hindering the progress of downstream applications. Unfortunately, research on facilitating the building of C/C++ projects remains to be inadequate. The emergence of Large Language Models (LLMs) offers promising solutions to automated software building. Trained on extensive corpora, LLMs can help unify diverse build systems through their comprehension capabilities and address complex errors by leveraging tacit knowledge storage. Moreover, LLM-based agents can be systematically designed to dynamically interact with the environment, effectively managing dynamic building issues. Motivated by these opportunities, we first conduct an empirical study to systematically analyze the current challenges in the C/C++ project building process. Particularly, we observe that most popular C/C++ projects encounter an average of five errors when relying solely on the default build systems. Based on our study, we develop an automated build system called CXXCrafter to specifically address the above-mentioned challenges, such as dependency resolution. Our evaluation on open-source software demonstrates that CXXCrafter achieves a success rate of 78% in project building. Specifically, among the Top100 dataset, 72 projects are built successfully by both CXXCrafter and manual efforts, 3 by CXXCrafter only, and 14 manually only. ...",http://arxiv.org/abs/2505.21069v1,IS,Quantitative
Predicting Implicit Arguments in Procedural Video Instructions,"Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like {verb,what,where/with}. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step's where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.",http://arxiv.org/abs/2505.21068v1,IS,Quantitative
Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning,"In the era of Industry 4.0 and smart manufacturing, process systems engineering must adapt to digital transformation. While reinforcement learning offers a model-free approach to process control, its applications are limited by the dependence on accurate digital twins and well-designed reward functions. To address these limitations, this paper introduces a novel framework that integrates inverse reinforcement learning (IRL) with multi-task learning for data-driven, multi-mode control design. Using historical closed-loop data as expert demonstrations, IRL extracts optimal reward functions and control policies. A latent-context variable is incorporated to distinguish modes, enabling the training of mode-specific controllers. Case studies on a continuous stirred tank reactor and a fed-batch bioreactor validate the effectiveness of this framework in handling multi-mode data and training adaptable controllers.",http://arxiv.org/abs/2505.21026v1,IS,Quantitative
Context-Aware Content Moderation for German Newspaper Comments,"The increasing volume of online discussions requires advanced automatic content moderation to maintain responsible discourse. While hate speech detection on social media is well-studied, research on German-language newspaper forums remains limited. Existing studies often neglect platform-specific context, such as user history and article themes. This paper addresses this gap by developing and evaluating binary classification models for automatic content moderation in German newspaper forums, incorporating contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging the One Million Posts Corpus from the Austrian newspaper Der Standard, we assess the impact of context-aware models. Results show that CNN and LSTM models benefit from contextual information and perform competitively with state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification does not improve with added context and underperforms.",http://arxiv.org/abs/2505.20963v1,IS,Quantitative
"What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma","Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma.",http://arxiv.org/abs/2505.12727v1,IS,Mixed
Privacy and Confidentiality Requirements Engineering for Process Data,"The application and development of process mining techniques face significant challenges due to the lack of publicly available real-life event logs. One reason for companies to abstain from sharing their data are privacy and confidentiality concerns. Privacy concerns refer to personal data as specified in the GDPR and have been addressed in existing work by providing privacy-preserving techniques for event logs. However, the concept of confidentiality in event logs not pertaining to individuals remains unclear, although they might contain a multitude of sensitive business data. This work addresses confidentiality of process data based on the privacy and confidentiality engineering method (PCRE). PCRE interactively explores privacy and confidentiality requirements regarding process data with different stakeholders and defines privacy-preserving actions to address possible concerns. We co-construct and evaluate PCRE based on structured interviews with process analysts in two manufacturing companies. PCRE is generic, hence applicable in different application domains. The goal is to systematically scrutinize process data and balance the trade-off between privacy and utility loss.",http://arxiv.org/abs/2505.10965v1,IS,Qualitative
Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles,"Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.",http://arxiv.org/abs/2505.07409v1,IS,Mixed
How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations,"With the ever-growing adoption of artificial intelligence (AI), AI-based software and its negative impact on the environment are no longer negligible, and studying and mitigating this impact has become a critical area of research. However, it is currently unclear which role environmental sustainability plays during AI adoption in industry and how AI regulations influence Green AI practices and decision-making in industry. We therefore aim to investigate the Green AI perception and management of industry practitioners. To this end, we conducted a total of 11 interviews with participants from 10 different organizations that adopted AI-based software. The interviews explored three main themes: AI adoption, current efforts in mitigating the negative environmental impact of AI, and the influence of the EU AI Act and the Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that 9 of 11 participants prioritized business efficiency during AI adoption, with minimal consideration of environmental sustainability. Monitoring and mitigation of AI's environmental impact were very limited. Only one participant monitored negative environmental effects. Regarding applied mitigation practices, six participants reported no actions, with the others sporadically mentioning techniques like prompt engineering, relying on smaller models, or not overusing AI. Awareness and compliance with the EU AI Act are low, with only one participant reporting on its influence, while the CSRD drove sustainability reporting efforts primarily in larger companies. All in all, our findings reflect a lack of urgency and priority for sustainable AI among these companies. We suggest that current regulations are not very effective, which has implications for policymakers. Additionally, there is a need to raise industry awareness, but also to provide user-friendly techniques and tools for Green AI practices.",http://arxiv.org/abs/2505.07317v1,IS,Qualitative
AI in Software Engineering: Perceived Roles and Their Impact on Adoption,"This paper investigates how developers conceptualize AI-powered Development Tools and how these role attributions influence technology acceptance. Through qualitative analysis of 38 interviews and a quantitative survey with 102 participants, we identify two primary Mental Models: AI as an inanimate tool and AI as a human-like teammate. Factor analysis further groups AI roles into Support Roles (e.g., assistant, reference guide) and Expert Roles (e.g., advisor, problem solver). We find that assigning multiple roles to AI correlates positively with Perceived Usefulness and Perceived Ease of Use, indicating that diverse conceptualizations enhance AI adoption. These insights suggest that AI4SE tools should accommodate varying user expectations through adaptive design strategies that align with different Mental Models.",http://arxiv.org/abs/2504.20329v1,IS,Mixed
Are We on the Same Page? Examining Developer Perception Alignment in Open Source Code Reviews,"Code reviews are a critical aspect of open-source software (OSS) development, ensuring quality and fostering collaboration. This study examines perceptions, challenges, and biases in OSS code review processes, focusing on the perspectives of Contributors and Maintainers. Through surveys (n=289), interviews (n=23), and repository analysis (n=81), we identify key areas of alignment and disparity. While both groups share common objectives, differences emerge in priorities, e.g, with Maintainers emphasizing alignment with project goals while Contributors overestimated the value of novelty. Bias, particularly familiarity bias, disproportionately affects underrepresented groups, discouraging participation and limiting community growth. Misinterpretation of approach differences as bias further complicates reviews. Our findings underscore the need for improved documentation, better tools, and automated solutions to address delays and enhance inclusivity. This work provides actionable strategies to promote fairness and sustain the long-term innovation of OSS ecosystems.",http://arxiv.org/abs/2504.18407v1,IS,Mixed
Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work,"While generative artificial intelligence (GenAI) is finding increased adoption in workplaces, current tools are primarily designed for individual use. Prior work established the potential for these tools to enhance personal creativity and productivity towards shared goals; however, we don't know yet how to best take into account the nuances of group work and team dynamics when deploying GenAI in work settings. In this paper, we investigate the potential of collaborative GenAI agents to augment teamwork in synchronous group settings through an exploratory study that engaged 25 professionals across 6 teams in speculative design workshops and individual follow-up interviews. Our workshops included a mixed reality provotype to simulate embodied collaborative GenAI agents capable of actively participating in group discussions. Our findings suggest that, if designed well, collaborative GenAI agents offer valuable opportunities to enhance team problem-solving by challenging groupthink, bridging communication gaps, and reducing social friction. However, teams' willingness to integrate GenAI agents depended on its perceived fit across a number of individual, team, and organizational factors. We outline the key design tensions around agent representation, social prominence, and engagement and highlight the opportunities spatial and immersive technologies could offer to modulate GenAI influence on team outcomes and strike a balance between augmentation and agency.",http://arxiv.org/abs/2504.14779v1,IS,Qualitative
"Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework","This case study presents our user-centered design model for Socially Intelligent Agent (SIA) development frameworks through our experience developing Estuary, an open source multimodal framework for building low-latency real-time socially interactive agents. We leverage the Rapid Assessment Process (RAP) to collect the thoughts of leading researchers in the field of SIAs regarding the current state of the art for SIA development as well as their evaluation of how well Estuary may potentially address current research gaps. We achieve this through a series of end-user interviews conducted by a fellow researcher in the community. We hope that the findings of our work will not only assist the continued development of Estuary but also guide the development of other future frameworks and technologies for SIAs.",http://arxiv.org/abs/2504.14427v1,IS,Qualitative
The Role of Empathy in Software Engineering -- A Socio-Technical Grounded Theory,"Empathy, defined as the ability to understand and share others' perspectives and emotions, is essential in software engineering (SE), where developers often collaborate with diverse stakeholders. It is also considered as a vital competency in many professional fields such as medicine, healthcare, nursing, animal science, education, marketing, and project management. Despite its importance, empathy remains under-researched in SE. To further explore this, we conducted a socio-technical grounded theory (STGT) study through in-depth semi-structured interviews with 22 software developers and stakeholders. Our study explored the role of empathy in SE and how SE activities and processes can be improved by considering empathy. Through applying the systematic steps of STGT data analysis and theory development, we developed a theory that explains the role of empathy in SE. Our theory details the contexts in which empathy arises, the conditions that shape it, the causes and consequences of its presence and absence. We also identified contingencies for enhancing empathy or overcoming barriers to its expression. Our findings provide practical implications for SE practitioners and researchers, offering a deeper understanding of how to effectively integrate empathy into SE processes.",http://arxiv.org/abs/2504.13002v1,IS,Qualitative
Who cares about testing?: Co-creations of Socio-technical Software Testing Experiences,"Software testing is crucial for ensuring software quality, yet developers' engagement with it varies widely. Identifying the technical, organizational and social factors that lead to differences in engagement is required to remove barriers and utilize enablers for testing. Much research emphasizes the usefulness of testing strategies and technical solutions, less is known about why developers do (not) test. This study investigates the lived experience of software developers to illuminate how their opinions about testing change. Learning about personal evolutions of practice, we explore when and why testing is used. Employing socio-technical grounded theory (STGT), we construct a theory by systematically analyzing data from 19 in-depth, semi-structured interviews with software developers. Allowing interviewees to reflect on how and why they approach software testing, we explore perspectives that are rooted in their contextual experiences. We develop eleven categories of circumstances that act as conditions for the application and adaptation of testing practices and introduce three concepts that we then use to present a theory that explains why developers do (not) use testing practices. This study reveals a new perspective on the connection between testing artifacts and collective reflection of practitioners. It has direct implications for practice and contributes to the groundwork of socio-technical research which embraces testing as an experience in which human- and social aspects are entangled with organizational and technical circumstances.",http://arxiv.org/abs/2504.07208v1,IS,Qualitative
TALLMesh: a simple application for performing Thematic Analysis with Large Language Models,"Thematic analysis (TA) is a widely used qualitative research method for identifying and interpreting patterns within textual data, such as qualitative interviews. Recent research has shown that it is possible to satisfactorily perform TA using Large Language Models (LLMs). This paper presents a novel application using LLMs to assist researchers in conducting TA. The application enables users to upload textual data, generate initial codes and themes. All of this is possible through a simple Graphical User Interface, (GUI) based on the streamlit framework, working with python scripts for the analysis, and using Application Program Interfaces of LLMs. Having a GUI is particularly important for researchers in fields where coding skills may not be prevalent, such as social sciences or humanities. With the app, users can iteratively refine codes and themes adopting a human-in-the-loop process, without the need to work with programming and scripting. The paper describes the application key features, highlighting its potential for qualitative research while preserving methodological rigor. The paper discusses the design and interface of the app and outlines future directions for this work.",http://arxiv.org/abs/2504.13892v1,IS,Qualitative
Integrating DAST in Kanban and CI/CD: A Real World Security Case Study,"Modern development methodologies, such as Kanban and continuous integration and continuous deployment (CI/CD), are critical for web application development -- as software products must adapt to changing requirements and deploy products to users quickly. As web application attacks and exploited vulnerabilities are rising, it is increasingly crucial to integrate security into modern development practices. Yet, the iterative and incremental nature of these processes can clash with the sequential nature of security engineering. Thus, it is challenging to adopt security practices and activities in modern development practices. Dynamic Application Security Testing (DAST) is a security practice within software development frameworks that bolsters system security. This study delves into the intersection of Agile development and DAST, exploring how a software organization attempted to integrate DAST into their Kanban workflows and CI/CD pipelines to identify and mitigate security vulnerabilities within the development process. Through an action research case study incorporating interviews among team members, this research elucidates the challenges, mitigation techniques, and best practices associated with incorporating DAST into Agile methodologies from developers' perspectives. We provide insights into integrating security practices with modern development, ensuring both speed and security in software delivery.",http://arxiv.org/abs/2503.21947v1,IS,Qualitative
Code Review Comprehension: Reviewing Strategies Seen Through Code Comprehension Theories,"Despite the popularity and importance of modern code review, the understanding of the cognitive processes that enable reviewers to analyze code and provide meaningful feedback is lacking. To address this gap, we observed and interviewed ten experienced reviewers while they performed 25 code reviews from their review queue. Since comprehending code changes is essential to perform code review and the primary challenge for reviewers, we focused our analysis on this cognitive process. Using Letovsky's model of code comprehension, we performed a theory-driven thematic analysis to investigate how reviewers apply code comprehension to navigate changes and provide feedback. Our findings confirm that code comprehension is fundamental to code review. We extend Letovsky's model to propose the Code Review Comprehension Model and demonstrate that code review, like code comprehension, relies on opportunistic strategies. These strategies typically begin with a context-building phase, followed by code inspection involving code reading, testing, and discussion management. To interpret and evaluate the proposed change, reviewers construct a mental model of the change as an extension of their understanding of the overall software system and contrast mental representations of expected and ideal solutions against the actual implementation. Based on our findings, we discuss how review tools and practices can better support reviewers in employing their strategies and in forming understanding. Data and material: https://doi.org/10.5281/zenodo.14748996",http://arxiv.org/abs/2503.21455v1,IS,Qualitative
Agentic Business Process Management: The Past 30 Years And Practitioners' Future Perspectives,"With the advent of generative Artificial Intelligence (genAI), the notion of an agent has seen a resurgence in popularity. This has also led to speculation about the extent to which business process management, as a discipline and research field, may impact and be impacted by the deployment of genAI-based agents. To better ground such speculations into the state-of-the-art, we draw from the past 30 years of research on agents and business process management to establish the concept of Agentic Business Process Management (agentic BPM) that is only loosely coupled to the genAI hype. We conduct a series of interviews with BPM practitioners to explore their understanding, expectations, and concerns related to agent autonomy, adaptability, human collaboration, and governance in processes. The findings reflect both challenges with respect to data inconsistencies, manual interventions, identification of process bottlenecks, actionability of process improvements, as well as the opportunities of enhanced efficiency, predictive process insights and proactive decision-making support. While the technology offers potential benefits, practitioners also anticipate risks such as biases, over-reliance, lack of transparency, and job displacement within organizations. These concerns underscore the need for a robust methodological framework for managing agents in organizations.",http://arxiv.org/abs/2504.03693v1,IS,Qualitative
A Qualitative Study of User Perception of M365 AI Copilot,"Adopting AI copilots in professional workflows presents opportunities for enhanced productivity, efficiency, and decision making. In this paper, we present results from a six month trial of M365 Copilot conducted at our organisation in 2024. A qualitative interview study was carried out with 27 participants. The study explored user perceptions of M365 Copilot's effectiveness, productivity impact, evolving expectations, ethical concerns, and overall satisfaction. Initial enthusiasm for the tool was met with mixed post trial experiences. While some users found M365 Copilot beneficial for tasks such as email coaching, meeting summaries, and content retrieval, others reported unmet expectations in areas requiring deeper contextual understanding, reasoning, and integration with existing workflows. Ethical concerns were a recurring theme, with users highlighting issues related to data privacy, transparency, and AI bias. While M365 Copilot demonstrated value in specific operational areas, its broader impact remained constrained by usability limitations and the need for human oversight to validate AI generated outputs.",http://arxiv.org/abs/2503.17661v2,IS,Qualitative
Invisible Labor: The Backbone of Open Source Software,"Invisible labor is an intrinsic part of the modern workplace, and includes labor that is undervalued or unrecognized such as creating collaborative atmospheres. Open source software (OSS) is software that is viewable, editable and shareable by anyone with internet access. Contributors are mostly volunteers, who participate for personal edification and because they believe in the spirit of OSS rather than for employment. Volunteerism often leads to high personnel turnover, poor maintenance and inconsistent project management. This in turn, leads to a difficulty with sustainability long term. We believe that the key to sustainable management is the invisible labor that occurs behind the scenes. It is unclear how OSS contributors think about the invisible labor they perform or how that affects OSS sustainability. We interviewed OSS contributors and asked them about their invisible labor contributions, leadership departure, membership turnover and sustainability. We found that invisible labor is responsible for good leadership, reducing contributor turnover, and creating legitimacy for the project as an organization.",http://arxiv.org/abs/2503.13405v1,IS,Qualitative
Hybrid Work in Agile Software Development: Recurring Meetings,"The Covid-19 pandemic established hybrid work as the new norm in software development companies. In large-scale agile, meetings of different types are pivotal for collaboration, and decisions need to be taken on how they are organized and carried out in hybrid work. This study investigates how recurring meetings are organized and carried out in hybrid work in a large-scale agile environment. We performed a single case study by conducting 27 semi-structured interviews with members of 15 agile teams, product owners, managers, and specialists from two units of Ericsson, a multinational telecommunications company with a ""2 days per week at the office"" policy. A key insight from this study is that different types of meetings in agile software development should be primarily organized onsite or remotely based on the meeting intent, i.e., meetings requiring active discussion or brainstorming, such as retrospectives or technical discussions, benefit from onsite attendance, whereas large information sharing meetings work well remotely. In hybrid work, community meetings can contribute to knowledge sharing within organizations, help strengthen social ties, and prevent siloed collaboration. Additionally, the use of cameras is recommended for small discussion-oriented remote and hybrid meetings.",http://arxiv.org/abs/2503.13002v1,IS,Qualitative
An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction,"Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting a single, most likely suffix. However, if the future course of a process is exposed to uncertainty or has high variability, the expressiveness of a single suffix prediction can be limited. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report provides a detailed evaluation of the U-ED-LSTM's predictive performance and assesses its calibration on four real-life event logs with three different hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable predictive performance across various datasets, ii) aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes, and iii) the approach effectively captures uncertainties present in event logs.",http://arxiv.org/abs/2505.21339v1,IS,Quantitative
Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs),"System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.",http://arxiv.org/abs/2505.21091v1,IS,Quantitative
BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism,"Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.",http://arxiv.org/abs/2505.20660v1,IS,Quantitative
IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios,"Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available.",http://arxiv.org/abs/2505.20640v1,IS,Quantitative
TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone,"Understanding how facial affect analysis (FAA) systems perform across different demographic groups requires reliable measurement of sensitive attributes such as ancestry, often approximated by skin tone, which itself is highly influenced by lighting conditions. This study compares two objective skin tone classification methods: the widely used Individual Typology Angle (ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness across skin tone groups defined by each method. Results reveal a severe underrepresentation of dark skin tones ($ im 2 \%$), alongside fairness disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$ method yields more consistent subgrouping and enables clearer diagnostics through metrics such as Equal Opportunity. Grad-CAM analysis further highlights differences in model attention patterns by skin tone, suggesting variation in feature encoding. To support future mitigation efforts, we also propose a modular fairness-aware pipeline that integrates perceptual skin tone estimation, model interpretability, and fairness evaluation. These findings emphasize the relevance of skin tone measurement choices in fairness assessment and suggest that ITA-based evaluations may overlook disparities affecting darker-skinned individuals.",http://arxiv.org/abs/2505.20637v1,IS,Quantitative
BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics,"Decoding speech directly from neural activity is a central goal in brain-computer interface (BCI) research. In recent years, exciting advances have been made through the growing use of intracranial field potential recordings, such as stereo-ElectroEncephaloGraphy (sEEG) and ElectroCorticoGraphy (ECoG). These neural signals capture rich population-level activity but present key challenges: (i) task-relevant neural signals are sparsely distributed across sEEG electrodes, and (ii) they are often entangled with task-irrelevant neural signals in both sEEG and ECoG. To address these challenges, we introduce a unified Coarse-to-Fine neural disentanglement framework, BrainStratify, which includes (i) identifying functional groups through spatial-context-guided temporal-spatial modeling, and (ii) disentangling distinct neural dynamics within the target functional group using Decoupled Product Quantization (DPQ). We evaluate BrainStratify on two open-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like vocal production and speech perception. Extensive experiments show that BrainStratify, as a unified framework for decoding speech from intracranial neural signals, significantly outperforms previous decoding methods. Overall, by combining data-driven stratification with neuroscience-inspired modularity, BrainStratify offers a robust and interpretable solution for speech decoding from intracranial recordings.",http://arxiv.org/abs/2505.20480v1,IS,Quantitative
ARM: Adaptive Reasoning Model,"While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the ""overthinking"" problem -- excessive and unnecessary reasoning -- which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones -- Direct Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a 2x speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens -- ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage.",http://arxiv.org/abs/2505.20258v1,IS,Quantitative
From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance,"Transformer-based CLIP models are widely used for text-image probing and feature extraction, making it relevant to understand the internal mechanisms behind their predictions. While recent works show that Sparse Autoencoders (SAEs) yield interpretable latent components, they focus on what these encode and miss how they drive predictions. We introduce a scalable framework that reveals what latent components activate for, how they align with expected semantics, and how important they are to predictions. To achieve this, we adapt attribution patching for instance-wise component attributions in CLIP and highlight key faithfulness limitations of the widely used Logit Lens technique. By combining attributions with semantic alignment scores, we can automatically uncover reliance on components that encode semantically unexpected or spurious concepts. Applied across multiple CLIP variants, our method uncovers hundreds of surprising components linked to polysemous words, compound nouns, visual typography and dataset artifacts. While text embeddings remain prone to semantic ambiguity, they are more robust to spurious correlations compared to linear classifiers trained on image embeddings. A case study on skin lesion detection highlights how such classifiers can amplify hidden shortcuts, underscoring the need for holistic, mechanistic interpretability. We provide code at https://github.com/maxdreyer/attributing-clip.",http://arxiv.org/abs/2505.20229v1,IS,Mixed
Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits,"Regret minimization in stochastic non-stationary bandits gained popularity over the last decade, as it can model a broad class of real-world problems, from advertising to recommendation systems. Existing literature relies on various assumptions about the reward-generating process, such as Bernoulli or subgaussian rewards. However, in settings such as finance and telecommunications, heavy-tailed distributions naturally arise. In this work, we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption that the finite absolute centered moments of maximum order $1+\epsilon$ are uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We focus on the most popular non-stationary bandit setting, i.e., the piecewise-stationary setting, in which the mean of reward-generating distributions may change at unknown time steps. We provide a novel Catoni-style change-point detection strategy tailored for heavy-tailed distributions that relies on recent advancements in the theory of sequential estimation, which is of independent interest. We introduce Robust-CPD-UCB, which combines this change-point detection strategy with optimistic algorithms for bandits, providing its regret upper bound and an impossibility result on the minimum attainable regret for any policy. Finally, we validate our approach through numerical experiments on synthetic and real-world datasets.",http://arxiv.org/abs/2505.20051v1,IS,Quantitative
Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction,"Recent advancements in generalizable novel view synthesis have achieved impressive quality through interpolation between nearby views. However, rendering high-resolution images remains computationally intensive due to the need for dense sampling of all rays. Recognizing that natural scenes are typically piecewise smooth and sampling all rays is often redundant, we propose a novel depth-guided bundle sampling strategy to accelerate rendering. By grouping adjacent rays into a bundle and sampling them collectively, a shared representation is generated for decoding all rays within the bundle. To further optimize efficiency, our adaptive sampling strategy dynamically allocates samples based on depth confidence, concentrating more samples in complex regions while reducing them in smoother areas. When applied to ENeRF, our method achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on the DTU dataset. Extensive experiments on synthetic and real-world datasets demonstrate state-of-the-art rendering quality and up to 2x faster rendering compared to existing generalizable methods. Code is available at https://github.com/KLMAV-CUC/GDB-NeRF.",http://arxiv.org/abs/2505.19793v1,IS,Quantitative
HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance,"Human-centered images often suffer from severe generic degradation during transmission and are prone to human motion blur (HMB), making restoration challenging. Existing research lacks sufficient focus on these issues, as both problems often coexist in practice. To address this, we design a degradation pipeline that simulates the coexistence of HMB and generic noise, generating synthetic degraded data to train our proposed HAODiff, a human-aware one-step diffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG), which leverages high-quality images, residual noise (LQ minus HQ), and HMB segmentation masks as training targets. It produces a positive-negative prompt pair for classifier-free guidance (CFG) in a single diffusion step. The resulting adaptive dual prompts let HAODiff exploit CFG more effectively, boosting robustness against diverse degradations. For fair evaluation, we introduce MPII-Test, a benchmark rich in combined noise and HMB cases. Extensive experiments show that our HAODiff surpasses existing state-of-the-art (SOTA) methods in terms of both quantitative metrics and visual quality on synthetic and real-world datasets, including our introduced MPII-Test. Code is available at: https://github.com/gobunu/HAODiff.",http://arxiv.org/abs/2505.19742v1,IS,Quantitative
Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression,"Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.",http://arxiv.org/abs/2505.19602v1,IS,Qualitative
Leveraging GANs for citation intent classification and its impact on citation network analysis,"Citations play a fundamental role in the scientific ecosystem, serving as a foundation for tracking the flow of knowledge, acknowledging prior work, and assessing scholarly influence. In scientometrics, they are also central to the construction of quantitative indicators. Not all citations, however, serve the same function: some provide background, others introduce methods, or compare results. Therefore, understanding citation intent allows for a more nuanced interpretation of scientific impact. In this paper, we adopted a GAN-based method to classify citation intents. Our results revealed that the proposed method achieves competitive classification performance, closely matching state-of-the-art results with substantially fewer parameters. This demonstrates the effectiveness and efficiency of leveraging GAN architectures combined with contextual embeddings in intent classification task. We also investigated whether filtering citation intents affects the centrality of papers in citation networks. Analyzing the network constructed from the unArXiv dataset, we found that paper rankings can be significantly influenced by citation intent. All four centrality metrics examined- degree, PageRank, closeness, and betweenness - were sensitive to the filtering of citation types. The betweenness centrality displayed the greatest sensitivity, showing substantial changes in ranking when specific citation intents were removed.",http://arxiv.org/abs/2505.21162v1,IS,Quantitative
HuMoCon: Concept Discovery for Human Motion Understanding,"We present HuMoCon, a novel motion-video understanding framework designed for advanced human behavior analysis. The core of our method is a human motion concept discovery framework that efficiently trains multi-modal encoders to extract semantically meaningful and generalizable features. HuMoCon addresses key challenges in motion concept discovery for understanding and reasoning, including the lack of explicit multi-modality feature alignment and the loss of high-frequency information in masked autoencoding frameworks. Our approach integrates a feature alignment strategy that leverages video for contextual understanding and motion for fine-grained interaction modeling, further with a velocity reconstruction mechanism to enhance high-frequency feature expression and mitigate temporal over-smoothing. Comprehensive experiments on standard benchmarks demonstrate that HuMoCon enables effective motion concept discovery and significantly outperforms state-of-the-art methods in training large models for human motion understanding. We will open-source the associated code with our paper.",http://arxiv.org/abs/2505.20920v1,IS,Quantitative
Stationary MMD Points for Cubature,"Approximation of a target probability distribution using a finite set of points is a problem of fundamental importance, arising in cubature, data compression, and optimisation. Several authors have proposed to select points by minimising a maximum mean discrepancy (MMD), but the non-convexity of this objective precludes global minimisation in general. Instead, we consider \emph{stationary} points of the MMD which, in contrast to points globally minimising the MMD, can be accurately computed. Our main theoretical contribution is the (perhaps surprising) result that, for integrands in the associated reproducing kernel Hilbert space, the cubature error of stationary MMD points vanishes \emph{faster} than the MMD. Motivated by this \emph{super-convergence} property, we consider discretised gradient flows as a practical strategy for computing stationary points of the MMD, presenting a refined convergence analysis that establishes a novel non-asymptotic finite-particle error bound, which may be of independent interest.",http://arxiv.org/abs/2505.20754v1,IS,Quantitative
Recurrent Neural Operators: Stable Long-Term PDE Prediction,"Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.",http://arxiv.org/abs/2505.20721v1,IS,Quantitative
Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation,"Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to unseen domains under extreme data scarcity and multiple shifts. Previous approaches mainly focused on sample selection strategies, while overlooking the fundamental problem on underlying optimization. Initially, we critically analyze the widely-adopted entropy minimization framework in WTTA and uncover its significant limitations in noisy optimization dynamics that substantially hinder adaptation efficiency. Through our analysis, we identify region confidence as a superior alternative to traditional entropy, however, its direct optimization remains computationally prohibitive for real-time applications. In this paper, we introduce a novel region-integrated method ReCAP that bypasses the lengthy process. Specifically, we propose a probabilistic region modeling scheme that flexibly captures semantic changes in embedding space. Subsequently, we develop a finite-to-infinite asymptotic approximation that transforms the intractable region confidence into a tractable and upper-bounded proxy. These innovations significantly unlock the overlooked potential dynamics in local region in a concise solution. Our extensive experiments demonstrate the consistent superiority of ReCAP over existing methods across various datasets and wild scenarios.",http://arxiv.org/abs/2505.20704v1,IS,Quantitative
How Do Experts Make Sense of Integrated Process Models?,"A range of integrated modeling approaches have been developed to enable a holistic representation of business process logic together with all relevant business rules. These approaches address inherent problems with separate documentation of business process models and business rules. In this study, we explore how expert process workers make sense of the information provided through such integrated modeling approaches. To do so, we complement verbal protocol analysis with eye-tracking metrics to reveal nuanced user behaviours involved in the main phases of sensemaking, namely information foraging and information processing. By studying expert process workers engaged in tasks based on integrated modeling of business processes and rules, we provide insights that pave the way for a better understanding of sensemaking practices and improved development of business process and business rule integration approaches. Our research underscores the importance of offering personalized support mechanisms that increase the efficacy and efficiency of sensemaking practices for process knowledge workers.",http://arxiv.org/abs/2505.20667v1,IS,Quantitative
HCQA-1.5 @ Ego4D EgoSchema Challenge 2025,"In this report, we present the method that achieves third place for Ego4D EgoSchema Challenge in CVPR 2025. To improve the reliability of answer prediction in egocentric video question answering, we propose an effective extension to the previously proposed HCQA framework. Our approach introduces a multi-source aggregation strategy to generate diverse predictions, followed by a confidence-based filtering mechanism that selects high-confidence answers directly. For low-confidence cases, we incorporate a fine-grained reasoning module that performs additional visual and contextual analysis to refine the predictions. Evaluated on the EgoSchema blind test set, our method achieves 77% accuracy on over 5,000 human-curated multiple-choice questions, outperforming last year's winning solution and the majority of participating teams. Our code will be added at https://github.com/Hyu-Zhang/HCQA.",http://arxiv.org/abs/2505.20644v1,IS,Quantitative
"Causality and ""In-the-Wild"" Video-Based Person Re-ID: A Survey","Video-based person re-identification (Re-ID) remains brittle in real-world deployments despite impressive benchmark performance. Most existing models rely on superficial correlations such as clothing, background, or lighting that fail to generalize across domains, viewpoints, and temporal variations. This survey examines the emerging role of causal reasoning as a principled alternative to traditional correlation-based approaches in video-based Re-ID. We provide a structured and critical analysis of methods that leverage structural causal models, interventions, and counterfactual reasoning to isolate identity-specific features from confounding factors. The survey is organized around a novel taxonomy of causal Re-ID methods that spans generative disentanglement, domain-invariant modeling, and causal transformers. We review current evaluation metrics and introduce causal-specific robustness measures. In addition, we assess practical challenges of scalability, fairness, interpretability, and privacy that must be addressed for real-world adoption. Finally, we identify open problems and outline future research directions that integrate causal modeling with efficient architectures and self-supervised learning. This survey aims to establish a coherent foundation for causal video-based person Re-ID and to catalyze the next phase of research in this rapidly evolving domain.",http://arxiv.org/abs/2505.20540v1,IS,Quantitative
CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic,"Recent advances in computational pathology have led to the emergence of numerous foundation models. However, these approaches fail to replicate the diagnostic process of pathologists, as they either simply rely on general-purpose encoders with multi-instance learning for classification or directly apply multimodal models to generate reports from images. A significant limitation is their inability to emulate the diagnostic logic employed by pathologists, who systematically examine slides at low magnification for overview before progressively zooming in on suspicious regions to formulate comprehensive diagnoses. To address this gap, we introduce CPathAgent, an innovative agent-based model that mimics pathologists' reasoning processes by autonomously executing zoom-in/out and navigation operations across pathology images based on observed visual features. To achieve this, we develop a multi-stage training strategy unifying patch-level, region-level, and whole-slide capabilities within a single model, which is essential for mimicking pathologists, who require understanding and reasoning capabilities across all three scales. This approach generates substantially more detailed and interpretable diagnostic reports compared to existing methods, particularly for huge region understanding. Additionally, we construct an expert-validated PathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical intermediate scale between patches and whole slides, as diagnosticians typically examine several key regions rather than entire slides at once. Extensive experiments demonstrate that CPathAgent consistently outperforms existing approaches across three scales of benchmarks, validating the effectiveness of our agent-based diagnostic approach and highlighting a promising direction for the future development of computational pathology.",http://arxiv.org/abs/2505.20510v1,IS,Quantitative
Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data,"The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.",http://arxiv.org/abs/2505.20485v1,IS,Mixed
Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis,"We propose GRGS, a generalizable and relightable 3D Gaussian framework for high-fidelity human novel view synthesis under diverse lighting conditions. Unlike existing methods that rely on per-character optimization or ignore physical constraints, GRGS adopts a feed-forward, fully supervised strategy that projects geometry, material, and illumination cues from multi-view 2D observations into 3D Gaussian representations. Specifically, to reconstruct lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement (LGR) module trained on synthetically relit data to predict accurate depth and surface normals. Based on the high-quality geometry, a Physically Grounded Neural Rendering (PGNR) module is further proposed to integrate neural prediction with physics-based shading, supporting editable relighting with shadows and indirect illumination. Besides, we design a 2D-to-3D projection training scheme that leverages differentiable supervision from ambient occlusion, direct, and indirect lighting maps, which alleviates the computational cost of explicit ray tracing. Extensive experiments demonstrate that GRGS achieves superior visual quality, geometric consistency, and generalization across characters and lighting conditions.",http://arxiv.org/abs/2505.21502v1,IS,Mixed
DecisionFlow: Advancing Large Language Model as Principled Decision Maker,"In high-stakes domains such as healthcare and finance, effective decision-making demands not just accurate outcomes but transparent and explainable reasoning. However, current language models often lack the structured deliberation needed for such tasks, instead generating decisions and justifications in a disconnected, post-hoc manner. To address this, we propose DecisionFlow, a novel decision modeling framework that guides models to reason over structured representations of actions, attributes, and constraints. Rather than predicting answers directly from prompts, DecisionFlow builds a semantically grounded decision space and infers a latent utility function to evaluate trade-offs in a transparent, utility-driven manner. This process produces decisions tightly coupled with interpretable rationales reflecting the model's reasoning. Empirical results on two high-stakes benchmarks show that DecisionFlow not only achieves up to 30% accuracy gains over strong prompting baselines but also enhances alignment in outcomes. Our work is a critical step toward integrating symbolic reasoning with LLMs, enabling more accountable, explainable, and reliable LLM decision support systems. We release the data and code at https://github.com/xiusic/DecisionFlow.",http://arxiv.org/abs/2505.21397v1,IS,Quantitative
Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?,"Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a ""Holmes-test"" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.",http://arxiv.org/abs/2505.21374v1,IS,Quantitative
Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA,"LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference.",http://arxiv.org/abs/2505.20971v1,IS,Quantitative
"Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior","Post-hoc interpretability methods typically attribute a model's behavior to its components, data, or training trajectory in isolation. This leads to explanations that lack a unified view and may miss key interactions. While combining existing methods or applying them at different training stages offers broader insights, these approaches usually lack theoretical support. In this work, we present ExPLAIND, a unified framework that integrates all three perspectives. First, we generalize recent work on gradient path kernels, which reformulate models trained by gradient descent as a kernel machine, to more realistic training settings. Empirically, we find that both a CNN and a Transformer model are replicated accurately by this reformulation. Second, we derive novel parameter- and step-wise influence scores from the kernel feature maps. We show their effectiveness in parameter pruning that is comparable to existing methods, reinforcing their value for model component attribution. Finally, jointly interpreting model components and data over the training process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking. Among other things, our findings support previously proposed stages of Grokking, while refining the final phase as one of alignment of input embeddings and final layers around a representation pipeline learned after the memorization phase. Overall, ExPLAIND provides a theoretically grounded, unified framework to interpret model behavior and training dynamics.",http://arxiv.org/abs/2505.20076v1,IS,Quantitative
ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving,"Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases. Code and dataset will be found in https://github.com/Liuxueyi/ReasonPlan.",http://arxiv.org/abs/2505.20024v1,IS,Quantitative
AI Cat Narrator: Designing an AI Tool for Exploring the Shared World and Social Connection with a Cat,"As technology continues to advance, the interaction between humans and cats is becoming more diverse. Our research introduces a new tool called the AI Cat Narrator, which offers a unique perspective on the shared lives of humans and cats. We combined the method of ethnography with fictional storytelling, using a defamiliarization strategy to merge real-world data seen through the eyes of cats with excerpts from cat literature. This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative. Our findings indicate that using defamiliarized data for training purposes significantly contributes to the development of characters that are both more empathetic and individualized. The contributions of our study are twofold: 1) proposing an innovative approach to prompting a reevaluation of living alongside cats; 2) establishing a collaborative, exploratory tool developed by humans, cats, and AI together.",http://arxiv.org/abs/2406.06192v1,IS,Qualitative
"New contexts, old heuristics: How young people in India and the US trust online content in the age of generative AI","We conducted in-person ethnography in India and the US to investigate how young people (18-24) trusted online content, just as generative AI (genAI) became mainstream. We found that when online, how participants determined what content to trust was shaped by emotional states, which we term ""information modes."" Our participants reflexively shifted between modes to maintain ""emotional equilibrium,"" and eschewed engaging literacy skills in the more passive modes in which they spent the most time. We found participants imported trust heuristics from established online contexts into emerging ones (i.e., genAI). This led them to use ill-fitting trust heuristics, and exposed them to the risk of trusting false and misleading information. While many had reservations about AI, prioritizing efficiency, they used genAI and habitual heuristics to quickly achieve goals at the expense of accuracy. We conclude that literacy interventions designed to match users' distinct information modes will be most effective.",http://arxiv.org/abs/2405.02522v2,IS,Qualitative
Revealing Networks: Understanding Effective Teacher Practices in AI-Supported Classrooms using Transmodal Ordered Network Analysis,"Learning analytics research increasingly studies classroom learning with AI-based systems through rich contextual data from outside these systems, especially student-teacher interactions. One key challenge in leveraging such data is generating meaningful insights into effective teacher practices. Quantitative ethnography bears the potential to close this gap by combining multimodal data streams into networks of co-occurring behavior that drive insight into favorable learning conditions. The present study uses transmodal ordered network analysis to understand effective teacher practices in relationship to traditional metrics of in-system learning in a mathematics classroom working with AI tutors. Incorporating teacher practices captured by position tracking and human observation codes into modeling significantly improved the inference of how efficiently students improved in the AI tutor beyond a model with tutor log data features only. Comparing teacher practices by student learning rates, we find that students with low learning rates exhibited more hint use after monitoring. However, after an extended visit, students with low learning rates showed learning behavior similar to their high learning rate peers, achieving repeated correct attempts in the tutor. Observation notes suggest conceptual and procedural support differences can help explain visit effectiveness. Taken together, offering early conceptual support to students with low learning rates could make classroom practice with AI tutors more effective. This study advances the scientific understanding of effective teacher practice in classrooms learning with AI tutors and methodologies to make such practices visible.",http://arxiv.org/abs/2312.10826v1,IS,Mixed
Knowns and Unknowns: An Experience Report on Discovering Tacit Knowledge of Maritime Surveyors,"Context: Requirements elicitation is an essential activity to ensure that systems provide the necessary functionality to users, and that they are fit for purpose. In addition to traditional `reductionist' techniques, the use of observations and ethnography-style techniques have been proposed to identify requirements. Research Problem: One frequently heard issue with observational techniques is that they are costly to use, as developers would lose considerable time to partake, and also depend on luck in identifying requirements. Very few experience reports exist to evaluate observational techniques in practice. Results: In this experience report, we draw on several data sources, covering insights from both developers and users. The data were collected through 9 interviews with users and developers, and over 80 hours of observation of prospective users in the maritime domain. We capture `knowns' and `unknowns' from both developers and users, and highlight the importance of observational studies. Contribution: While observational techniques are costly to use, we conclude that essential information is uncovered, which is key for developers to understand system users and their concerns.",http://arxiv.org/abs/2301.10211v2,IS,Qualitative
"The Types, Roles, and Practices of Documentation in Data Analytics Open Source Software Libraries: A Collaborative Ethnography of Documentation Work","Computational research and data analytics increasingly relies on complex ecosystems of open source software (OSS) ""libraries"" -- curated collections of reusable code that programmers import to perform a specific task. Software documentation for these libraries is crucial in helping programmers/analysts know what libraries are available and how to use them. Yet documentation for open source software libraries is widely considered low-quality. This article is a collaboration between CSCW researchers and contributors to data analytics OSS libraries, based on ethnographic fieldwork and qualitative interviews. We examine several issues around the formats, practices, and challenges around documentation in these largely volunteer-based projects. There are many different kinds and formats of documentation that exist around such libraries, which play a variety of educational, promotional, and organizational roles. The work behind documentation is similarly multifaceted, including writing, reviewing, maintaining, and organizing documentation. Different aspects of documentation work require contributors to have different sets of skills and overcome various social and technical barriers. Finally, most of our interviewees do not report high levels of intrinsic enjoyment for doing documentation work (compared to writing code). Their motivation is affected by personal and project-specific factors, such as the perceived level of credit for doing documentation work versus more ""technical"" tasks like adding new features or fixing bugs. In studying documentation work for data analytics OSS libraries, we gain a new window into the changing practices of data-intensive research, as well as help practitioners better understand how to support this often invisible and infrastructural work in their projects.",http://arxiv.org/abs/1805.12398v1,IS,Qualitative
Beyond opening up the black box: Investigating the role of algorithmic systems in Wikipedian organizational culture,"Scholars and practitioners across domains are increasingly concerned with algorithmic transparency and opacity, interrogating the values and assumptions embedded in automated, black-boxed systems, particularly in user-generated content platforms. I report from an ethnography of infrastructure in Wikipedia to discuss an often understudied aspect of this topic: the local, contextual, learned expertise involved in participating in a highly automated social-technical environment. Today, the organizational culture of Wikipedia is deeply intertwined with various data-driven algorithmic systems, which Wikipedians rely on to help manage and govern the ""anyone can edit"" encyclopedia at a massive scale. These bots, scripts, tools, plugins, and dashboards make Wikipedia more efficient for those who know how to work with them, but like all organizational culture, newcomers must learn them if they want to fully participate. I illustrate how cultural and organizational expertise is enacted around algorithmic agents by discussing two autoethnographic vignettes, which relate my personal experience as a veteran in Wikipedia. I present thick descriptions of how governance and gatekeeping practices are articulated through and in alignment with these automated infrastructures. Over the past 15 years, Wikipedian veterans and administrators have made specific decisions to support administrative and editorial workflows with automation in particular ways and not others. I use these cases of Wikipedia's bot-supported bureaucracy to discuss several issues in the fields of critical algorithms studies, critical data studies, and fairness, accountability, and transparency in machine learning -- most principally arguing that scholarship and practice must go beyond trying to ""open up the black box"" of such systems and also examine sociocultural processes like newcomer socialization.",http://arxiv.org/abs/1709.09093v2,IS,Qualitative
LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling,"Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to -41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.",http://arxiv.org/abs/2505.19187v1,IS,Quantitative
MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection,"Recent advances in AI-generated content (AIGC) have significantly accelerated image editing techniques, driving increasing demand for diverse and fine-grained edits. Despite these advances, existing image editing methods still face challenges in achieving high precision and semantic accuracy in complex scenarios. Recent studies address this issue by incorporating multimodal large language models (MLLMs) into image editing pipelines. However, current MLLM-based methods mainly rely on interpreting textual instructions, leaving the intrinsic visual understanding of large models largely unexplored, thus resulting in insufficient alignment between textual semantics and visual outcomes. To overcome these limitations, we propose MIND-Edit, an end-to-end image-editing framework integrating pretrained diffusion model with MLLM. MIND-Edit introduces two complementary strategies: (1) a text instruction optimization strategy that clarifies ambiguous user instructions based on semantic reasoning from the MLLM, and (2) an MLLM insight-driven editing strategy that explicitly leverages the intrinsic visual understanding capability of the MLLM to infer editing intent and guide the diffusion process via generated visual embeddings. Furthermore, we propose a joint training approach to effectively integrate both strategies, allowing them to reinforce each other for more accurate instruction interpretation and visually coherent edits aligned with user intent. Extensive experiments demonstrate that MIND-Edit outperforms state-of-the-art image editing methods in both quantitative metrics and visual quality, particularly under complex and challenging scenarios.",http://arxiv.org/abs/2505.19149v1,IS,Quantitative
Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra,"This paper proposes a tropical geometry-based edge detection framework that reformulates convolution and gradient computations using min-plus and max-plus algebra. The tropical formulation emphasizes dominant intensity variations, contributing to sharper and more continuous edge representations. Three variants are explored: an adaptive threshold-based method, a multi-kernel min-plus method, and a max-plus method emphasizing structural continuity. The framework integrates multi-scale processing, Hessian filtering, and wavelet shrinkage to enhance edge transitions while maintaining computational efficiency. Experiments on MATLAB built-in grayscale and color images suggest that tropical formulations integrated with classical operators, such as Canny and LoG, can improve boundary detection in low-contrast and textured regions. Quantitative evaluation using standard edge metrics indicates favorable edge clarity and structural coherence. These results highlight the potential of tropical algebra as a scalable and noise-aware formulation for edge detection in practical image analysis tasks.",http://arxiv.org/abs/2505.18625v1,IS,Quantitative
Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?,"Probing techniques have shown promise in revealing how LLMs encode human-interpretable concepts, particularly when applied to curated datasets. However, the factors governing a dataset's suitability for effective probe training are not well-understood. This study hypothesizes that probe performance on such datasets reflects characteristics of both the LLM's generated responses and its internal feature space. Through quantitative analysis of probe performance and LLM response uncertainty across a series of tasks, we find a strong correlation: improved probe performance consistently corresponds to a reduction in response uncertainty, and vice versa. Subsequently, we delve deeper into this correlation through the lens of feature importance analysis. Our findings indicate that high LLM response variance is associated with a larger set of important features, which poses a greater challenge for probe models and often results in diminished performance. Moreover, leveraging the insights from response uncertainty analysis, we are able to identify concrete examples where LLM representations align with human knowledge across diverse domains, offering additional evidence of interpretable reasoning in LLMs.",http://arxiv.org/abs/2505.18575v1,IS,Quantitative
"Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support","Various industries have produced a large number of documents such as industrial plans, technical guidelines, and regulations that are structurally complex and content-wise fragmented. This poses significant challenges for experts and decision-makers in terms of retrieval and understanding. Although existing LLM-based Retrieval-Augmented Generation methods can provide context-related suggestions, they lack quantitative weighting and traceable reasoning paths, making it difficult to offer multi-level and transparent decision support. To address this issue, this paper proposes the RAD method, which integrates Multi-Criteria Decision Making with the semantic understanding capabilities of LLMs. The method automatically extracts key criteria from industry documents, builds a weighted hierarchical decision model, and generates structured reports under model guidance. The RAD framework introduces explicit weight assignment and reasoning chains in decision generation to ensure accuracy, completeness, and traceability. Experiments show that in various decision-making tasks, the decision reports generated by RAD significantly outperform existing methods in terms of detail, rationality, and structure, demonstrating its application value and potential in complex decision support scenarios.",http://arxiv.org/abs/2505.18483v1,IS,Quantitative
LA-RCS: LLM-Agent-Based Robot Control System,"LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: https://la-rcs.github.io",http://arxiv.org/abs/2505.18214v1,IS,Mixed
Revenue Optimization with Price-Sensitive and Interdependent Demand,"As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3], Revenue Management aims to maximize an organization's revenue by considering three types of decision categories: structural, pricing, and quantity. In this document, our primary focus will be on decisions related to pricing and quantity for the sale of airline tickets on a direct flight over a certain number of time periods. More specifically, we will only focus on the optimization aspect of this problem. We will assume the demand data to be given, since Air France estimates it beforehand using real data. Similarly, we assume all price options to be predetermined by Air France's algorithms and verified by their analysts. Our objective will be to maximize the revenue of a direct flight by choosing the prices for each product from the predefined set of options. -- Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un organisme \`a partir de trois types de cat\'egories de d\'ecision : structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons principalement aux d\'ecisions de type prix et quantit\'e pour la vente de billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps. Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct en choisissant les prix de chaque produit parmi ceux impos\'es.",http://arxiv.org/abs/2505.16748v1,IS,Quantitative
"Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences","Artificial intelligence (AI) has recently seen transformative breakthroughs in the life sciences, expanding possibilities for researchers to interpret biological information at an unprecedented capacity, with novel applications and advances being made almost daily. In order to maximise return on the growing investments in AI-based life science research and accelerate this progress, it has become urgent to address the exacerbation of long-standing research challenges arising from the rapid adoption of AI methods. We review the increased erosion of trust in AI research outputs, driven by the issues of poor reusability and reproducibility, and highlight their consequent impact on environmental sustainability. Furthermore, we discuss the fragmented components of the AI ecosystem and lack of guiding pathways to best support Open and Sustainable AI (OSAI) model development. In response, this perspective introduces a practical set of OSAI recommendations directly mapped to over 300 components of the AI ecosystem. Our work connects researchers with relevant AI resources, facilitating the implementation of sustainable, reusable and transparent AI. Built upon life science community consensus and aligned to existing efforts, the outputs of this perspective are designed to aid the future development of policy and structured pathways for guiding AI implementation.",http://arxiv.org/abs/2505.16619v1,IS,Quantitative
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training","We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to introspect and explain their own functioning. Here, we show that i) contemporary LLMs are capable of providing accurate, quantitative descriptions of their own internal processes during certain kinds of decision-making, ii) that it is possible to improve these capabilities through training, and iii) that this training generalizes to at least some degree. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes during decision-making (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain what they are doing as they make other complex decisions, not just decisions they have learned to make via fine-tuning. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",http://arxiv.org/abs/2505.17120v1,IS,Quantitative
BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design,"Protein binder design has been transformed by hallucination-based methods that optimize structure prediction confidence metrics, such as the interface predicted TM-score (ipTM), via backpropagation. However, these metrics do not reflect the statistical likelihood of a binder-target complex under the learned distribution and yield sparse gradients for optimization. In this work, we propose a method to extract such likelihoods from structure predictors by reinterpreting their confidence outputs as an energy-based model (EBM). By leveraging the Joint Energy-based Modeling (JEM) framework, we introduce pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a design pipeline that maintains the same optimization framework as BindCraft but replaces ipTM with our energy-based objective. BECraft outperforms BindCraft, RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in silico binder success rates while reducing structural clashes. Furthermore, pTMEnergy establishes a new state-of-the-art in structure-based virtual screening tasks for miniprotein and RNA aptamer binders.",http://arxiv.org/abs/2505.21241v1,IS,Quantitative
RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images,"Robust unsupervised anomaly detection (AD) in real-world scenarios is an important task. Current methods exhibit severe performance degradation on the MVTec AD 2 benchmark due to its complex real-world challenges. To solve this problem, we propose a robust framework RoBiS, which consists of three core modules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to preserve the information of small anomalies through overlapping window cropping. (2) The data augmentation of noise addition and lighting simulation is carried out on the training data to improve the robustness of AD model. We use INP-Former as our baseline, which could generate better results on the various sub-images. (3) The traditional statistical-based binarization strategy (mean+3std) is combined with our previous work, MEBin (published in CVPR2025), for joint adaptive binarization. Then, SAM is further employed to refine the segmentation results. Compared with some methods reported by the MVTec AD 2, our RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on Test_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on Test_private_mixed. Code is available at https://github.com/xrli-U/RoBiS.",http://arxiv.org/abs/2505.21152v1,IS,Quantitative
Explanation User Interfaces: A Systematic Literature Review,"Artificial Intelligence (AI) is one of the major technological advancements of this century, bearing incredible potential for users through AI-powered applications and tools in numerous domains. Being often black-box (i.e., its decision-making process is unintelligible), developers typically resort to eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour of AI models to produce systems that are transparent, fair, reliable, and trustworthy. However, presenting explanations to the user is not trivial and is often left as a secondary aspect of the system's design process, leading to AI systems that are not useful to end-users. This paper presents a Systematic Literature Review on Explanation User Interfaces (XUIs) to gain a deeper understanding of the solutions and design guidelines employed in the academic literature to effectively present explanations to users. To improve the contribution and real-world impact of this survey, we also present a framework for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide practitioners and academics in the design and evaluation of XUIs.",http://arxiv.org/abs/2505.20085v1,IS,Quantitative
Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4,"LLMs have been demonstrated to align with the values of Western or North American cultures. Prior work predominantly showed this effect through leveraging surveys that directly ask (originally people and now also LLMs) about their values. However, it is hard to believe that LLMs would consistently apply those values in real-world scenarios. To address that, we take a bottom-up approach, asking LLMs to reason about cultural norms in narratives from different cultures. We find that GPT-4 tends to generate norms that, while not necessarily incorrect, are significantly less culture-specific. In addition, while it avoids overtly generating stereotypes, the stereotypical representations of certain cultures are merely hidden rather than suppressed in the model, and such stereotypes can be easily recovered. Addressing these challenges is a crucial step towards developing LLMs that fairly serve their diverse user base.",http://arxiv.org/abs/2505.18322v1,IS,Mixed
Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics,"Social media's rise establishes user-generated content (UGC) as pivotal for travel decisions, yet analytical methods lack scalability. This study introduces a dual-method LLM framework: unsupervised expectation extraction from UGC paired with survey-informed supervised fine-tuning. Findings reveal leisure/social expectations drive engagement more than foundational natural/emotional factors. By establishing LLMs as precision tools for expectation quantification, we advance tourism analytics methodology and propose targeted strategies for experience personalization and social travel promotion. The framework's adaptability extends to consumer behavior research, demonstrating computational social science's transformative potential in marketing optimization.",http://arxiv.org/abs/2505.16118v1,IS,Quantitative
DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior,"Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.",http://arxiv.org/abs/2505.11701v1,IS,Quantitative
Federated Low-Rank Adaptation for Foundation Models: A Survey,"Effectively leveraging private datasets remains a significant challenge in developing foundation models. Federated Learning (FL) has recently emerged as a collaborative framework that enables multiple users to fine-tune these models while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA) offers a resource-efficient alternative for fine-tuning foundation models by dramatically reducing the number of trainable parameters. This survey examines how LoRA has been integrated into federated fine-tuning for foundation models, an area we term FedLoRA, by focusing on three key challenges: distributed learning, heterogeneity, and efficiency. We further categorize existing work based on the specific methods used to address each challenge. Finally, we discuss open research questions and highlight promising directions for future investigation, outlining the next steps for advancing FedLoRA.",http://arxiv.org/abs/2505.13502v1,IS,Quantitative
Structure from Collision,"Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.",http://arxiv.org/abs/2505.21335v1,IS,Quantitative
Sci-Fi: Symmetric Constraint for Frame Inbetweening,"Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.",http://arxiv.org/abs/2505.21205v1,IS,Quantitative
STEB: In Search of the Best Evaluation Approach for Synthetic Time Series,"The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.",http://arxiv.org/abs/2505.21160v1,IS,Quantitative
Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score,"Conformal prediction (CP) is a powerful framework for uncertainty quantification, providing prediction sets with coverage guarantees when calibrated on sufficient labeled data. However, in real-world applications where labeled data is often limited, standard CP can lead to coverage deviation and output overly large prediction sets. In this paper, we extend CP to the semi-supervised setting and propose SemiCP, leveraging both labeled data and unlabeled data for calibration. Specifically, we introduce a novel nonconformity score function, NNM, designed for unlabeled data. This function selects labeled data with similar pseudo-label scores to estimate nonconformity scores, integrating them into the calibration process to overcome sample size limitations. We theoretically demonstrate that, under mild assumptions, SemiCP provide asymptotically coverage guarantee for prediction sets. Extensive experiments further validate that our approach effectively reduces instability and inefficiency under limited calibration data, can be adapted to conditional coverage settings, and integrates seamlessly with existing CP methods.",http://arxiv.org/abs/2505.21147v1,IS,Quantitative
Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation,"Query auto-completion (QAC) plays a crucial role in modern search systems. However, in real-world applications, there are two pressing challenges that still need to be addressed. First, there is a need for hierarchical personalized representations for users. Previous approaches have typically used users' search behavior as a single, overall representation, which proves inadequate in more nuanced generative scenarios. Additionally, query prefixes are typically short and may contain typos or sensitive information, increasing the likelihood of generating toxic content compared to traditional text generation tasks. Such toxic content can degrade user experience and lead to public relations issues. Therefore, the second critical challenge is detoxifying QAC systems. To address these two limitations, we propose a novel model (LaD) that captures personalized information from both long-term and short-term interests, incorporating adaptive detoxification. In LaD, personalized information is captured hierarchically at both coarse-grained and fine-grained levels. This approach preserves as much personalized information as possible while enabling online generation within time constraints. To move a futher step, we propose an online training method based on Reject Preference Optimization (RPO). By incorporating a special token [Reject] during both the training and inference processes, the model achieves adaptive detoxification. Consequently, the generated text presented to users is both non-toxic and relevant to the given prefix. We conduct comprehensive experiments on industrial-scale datasets and perform online A/B tests, delivering the largest single-experiment metric improvement in nearly two years of our product. Our model has been deployed on Kuaishou search, driving the primary traffic for hundreds of millions of active users. The code is available at https://github.com/JXZe/LaD.",http://arxiv.org/abs/2505.20966v1,IS,Quantitative
Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models,"Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.",http://arxiv.org/abs/2505.20955v1,IS,Quantitative
Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects,"Diffusion models have significantly advanced text-to-image generation, laying the foundation for the development of personalized generative frameworks. However, existing methods lack precise layout controllability and overlook the potential of dynamic features of reference subjects in improving fidelity. In this work, we propose Layout-Controllable Personalized Diffusion (LCP-Diffusion) model, a novel framework that integrates subject identity preservation with flexible layout guidance in a tuning-free approach. Our model employs a Dynamic-Static Complementary Visual Refining module to comprehensively capture the intricate details of reference subjects, and introduces a Dual Layout Control mechanism to enforce robust spatial control across both training and inference stages. Extensive experiments validate that LCP-Diffusion excels in both identity preservation and layout controllability. To the best of our knowledge, this is a pioneering work enabling users to ""create anything anywhere"".",http://arxiv.org/abs/2505.20909v1,IS,Quantitative
Robust and Computation-Aware Gaussian Processes,"Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.",http://arxiv.org/abs/2505.21133v1,IS,Quantitative
EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification,"Speech emotion recognition predicts a speaker's emotional state from speech signals using discrete labels or continuous dimensions such as arousal, valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that integrates spherical VAD region classification to guide VAD regression for improved emotion prediction. In our framework, VAD values are transformed into spherical coordinates that are divided into multiple spherical regions, and an auxiliary classification task predicts which spherical region each point belongs to, guiding the regression process. Additionally, we incorporate a dynamic weighting scheme and a style pooling layer with multi-head self-attention to capture spectral and temporal dynamics, further boosting performance. This combined training strategy reinforces structured learning and improves prediction consistency. Experimental results show that our approach exceeds baseline methods, confirming the validity of the proposed framework.",http://arxiv.org/abs/2505.19693v1,IS,Quantitative
Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach,"In this paper, we focus on the often-overlooked issue of embedding collapse in existing diffusion-based sequential recommendation models and propose ADRec, an innovative framework designed to mitigate this problem. Diverging from previous diffusion-based methods, ADRec applies an independent noise process to each token and performs diffusion across the entire target sequence during training. ADRec captures token interdependency through auto-regression while modeling per-token distributions through token-level diffusion. This dual approach enables the model to effectively capture both sequence dynamics and item representations, overcoming the limitations of existing methods. To further mitigate embedding collapse, we propose a three-stage training strategy: (1) pre-training the embedding weights, (2) aligning these weights with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec applies the denoising process only to the last token, ensuring that the meaningful patterns in historical interactions are preserved. Our comprehensive empirical evaluation across six datasets underscores the effectiveness of ADRec in enhancing both the accuracy and efficiency of diffusion-based sequential recommendation systems.",http://arxiv.org/abs/2505.19544v1,IS,Quantitative
On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective,"Weak-to-strong generalization, where a student model trained on imperfect labels generated by a weaker teacher nonetheless surpasses that teacher, has been widely observed but the mechanisms that enable it have remained poorly understood. In this paper, through a theoretical analysis of simple models, we uncover three core mechanisms that can drive this phenomenon. First, by analyzing ridge regression, we study the interplay between the teacher and student regularization and prove that a student can compensate for a teacher's under-regularization and achieve lower test error. We also analyze the role of the parameterization regime of the models. Second, by analyzing weighted ridge regression, we show that a student model with a regularization structure more aligned to the target, can outperform its teacher. Third, in a nonlinear multi-index setting, we demonstrate that a student can learn easy, task-specific features from the teacher while leveraging its own broader pre-training to learn hard-to-learn features that the teacher cannot capture.",http://arxiv.org/abs/2505.18346v1,IS,Quantitative
A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction,"Unattended scheduled appointments, defined as patient no-shows, adversely affect both healthcare providers and patients' health, disrupting the continuity of care, operational efficiency, and the efficient allocation of medical resources. Accurate predictive modelling is needed to reduce the impact of no-shows. Although machine learning methods, such as logistic regression, random forest models, and decision trees, are widely used in predicting patient no-shows, they often rely on hard decision splits and static feature importance, limiting their adaptability to specific or complex patient behaviors. To address this limitation, we propose a new hybrid Multi-Head Attention Soft Random Forest (MHASRF) model that integrates attention mechanisms into a random forest model using probabilistic soft splitting instead of hard splitting. The MHASRF model assigns attention weights differently across the trees, enabling attention on specific patient behaviors. The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a 93.59% F1 score, surpassing the performance of decision tree, logistic regression, random forest, and naive Bayes models. Furthermore, MHASRF was able to identify key predictors of patient no-shows using two levels of feature importance (tree level and attention mechanism level), offering deeper insights into patient no-show predictors. The proposed model is a robust, adaptable, and interpretable method for predicting patient no-shows that will help healthcare providers in optimizing resources.",http://arxiv.org/abs/2505.17344v1,IS,Quantitative
Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype,"This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB) methods and introduces an experimental framework for scalable, interpretable offer selection, addressing the challenge of fast-changing offers. The approach models context at the product category level, allowing offers to span multiple categories and enabling knowledge transfer across similar offers. This improves learning efficiency and generalization in dynamic environments. The framework extends standard CMAB methodology to support multi-category contexts, and achieves scalability through efficient feature engineering and modular design. Advanced features such as MPG (Member Purchase Gap) and MF (Matrix Factorization) capture nuanced user-offer interactions, with implementation in Python for practical deployment. A key contribution is interpretability at scale: logistic regression models yield transparent weight vectors, accessible via a large language model (LLM) interface for real-time, user-level tracking and explanation of evolving preferences. This enables the generation of detailed member profiles and identification of behavioral patterns, supporting personalized offer optimization and enhancing trust in automated decisions. By situating our prototype alongside established paradigms like Generalized Linear Models and Thompson Sampling, we demonstrate its value for both research and real-world CMAB applications.",http://arxiv.org/abs/2505.16918v1,IS,Quantitative
SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding,"Capturing nonlinear relationships without sacrificing interpretability remains a persistent challenge in regression modeling. We introduce SplitWise, a novel framework that enhances stepwise regression. It adaptively transforms numeric predictors into threshold-based binary features using shallow decision trees, but only when such transformations improve model fit, as assessed by the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). This approach preserves the transparency of linear models while flexibly capturing nonlinear effects. Implemented as a user-friendly R package, SplitWise is evaluated on both synthetic and real-world datasets. The results show that it consistently produces more parsimonious and generalizable models than traditional stepwise and penalized regression techniques.",http://arxiv.org/abs/2505.15423v1,IS,Quantitative
Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable,"Existing detectors are often trained on biased datasets, leading to the possibility of overfitting on non-causal image attributes that are spuriously correlated with real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when applied to unbiased datasets. One common solution is to perform dataset alignment through generative reconstruction, matching the semantic content between real and synthetic images. However, we revisit this approach and show that pixel-level alignment alone is insufficient. The reconstructed images still suffer from frequency-level misalignment, which can perpetuate spurious correlations. To illustrate, we observe that reconstruction models tend to restore the high-frequency details lost in real images (possibly due to JPEG compression), inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images for testing detector performance on the most aligned dataset, and EvalGEN, featuring the latest generative models for assessing detectors under new generative architectures such as visual auto-regressive generators. Finally, our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on in-the-wild benchmarks, highlighting the improved generalizability of unbiased detectors.",http://arxiv.org/abs/2505.14359v2,IS,Quantitative
Nonparametric Teaching for Graph Property Learners,"Inferring properties of graph-structured data, e.g., the solubility of molecules, essentially involves learning the implicit mapping from graphs to their properties. This learning process is often costly for graph property learners like Graph Convolutional Networks (GCNs). To address this, we propose a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning process through a novel nonparametric teaching perspective. Specifically, the latter offers a theoretical framework for teaching implicitly defined (i.e., nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of graph-property pairs, with the GraNT teacher selecting a subset of them to promote faster convergence in GCN training. By analytically examining the impact of graph structure on parameter-based gradient descent during training, and recasting the evolution of GCNs--shaped by parameter updates--through functional gradient descent in nonparametric teaching, we show for the first time that teaching graph property learners (i.e., GCNs) is consistent with teaching structure-aware nonparametric learners. These new findings readily commit GraNT to enhancing learning efficiency of the graph property learner, showing significant reductions in training time for graph-level regression (-36.62%), graph-level classification (-38.19%), node-level regression (-30.97%) and node-level classification (-47.30%), all while maintaining its generalization performance.",http://arxiv.org/abs/2505.14170v2,IS,Quantitative
TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems,"Regression models are crucial in recommender systems. However, retransformation bias problem has been conspicuously neglected within the community. While many works in other fields have devised effective bias correction methods, all of them are post-hoc cures externally to the model, facing practical challenges when applied to real-world recommender systems. Hence, we propose a preemptive paradigm to eradicate the bias intrinsically from the models via minor model refinement. Specifically, a novel TranSUN method is proposed with a joint bias learning manner to offer theoretically guaranteed unbiasedness under empirical superior convergence. It is further generalized into a novel generic regression model family, termed Generalized TranSUN (GTS), which not only offers more theoretical insights but also serves as a generic framework for flexibly developing various bias-free models. Comprehensive experimental results demonstrate the superiority of our methods across data from various domains, which have been successfully deployed in two real-world industrial recommendation scenarios, i.e. product and short video recommendation scenarios in Guess What You Like business domain in the homepage of Taobao App (a leading e-commerce platform), to serve the major online traffic. Codes will be released after this paper is published.",http://arxiv.org/abs/2505.13881v1,IS,Quantitative
Synthetic-Powered Predictive Inference,"Conformal prediction is a framework for predictive inference with a distribution-free, finite-sample guarantee. However, it tends to provide uninformative prediction sets when calibration data are scarce. This paper introduces Synthetic-powered predictive inference (SPPI), a novel framework that incorporates synthetic data -- e.g., from a generative model -- to improve sample efficiency. At the core of our method is a score transporter: an empirical quantile mapping that aligns nonconformity scores from trusted, real data with those from synthetic data. By carefully integrating the score transporter into the calibration process, SPPI provably achieves finite-sample coverage guarantees without making any assumptions about the real and synthetic data distributions. When the score distributions are well aligned, SPPI yields substantially tighter and more informative prediction sets than standard conformal prediction. Experiments on image classification and tabular regression demonstrate notable improvements in predictive efficiency in data-scarce settings.",http://arxiv.org/abs/2505.13432v1,IS,Quantitative
Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness,"Cognitive decline often surfaces in language years before diagnosis. It is frequently non-experts, such as those closest to the patient, who first sense a change and raise concern. As LLMs become integrated into daily communication and used over prolonged periods, it may even be an LLM that notices something is off. But what exactly do they notice--and should be noticing--when making that judgment? This paper investigates how dementia is perceived through language by non-experts. We presented transcribed picture descriptions to non-expert humans and LLMs, asking them to intuitively judge whether each text was produced by someone healthy or with dementia. We introduce an explainable method that uses LLMs to extract high-level, expert-guided features representing these picture descriptions, and use logistic regression to model human and LLM perceptions and compare with clinical diagnoses. Our analysis reveals that human perception of dementia is inconsistent and relies on a narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a richer, more nuanced feature set that aligns more closely with clinical patterns. Still, both groups show a tendency toward false negatives, frequently overlooking dementia cases. Through our interpretable framework and the insights it provides, we hope to help non-experts better recognize the linguistic signs that matter.",http://arxiv.org/abs/2505.13418v1,IS,Quantitative
No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference,"Prediction-Powered Inference (PPI) is a popular strategy for combining gold-standard and possibly noisy pseudo-labels to perform statistical estimation. Prior work has shown an asymptotic ""free lunch"" for PPI++, an adaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always less than or equal to the variance obtained from using gold-standard labels alone. Notably, this result holds *regardless of the quality of the pseudo-labels*. In this work, we demystify this result by conducting an exact finite-sample analysis of the estimation error of PPI++ on the mean estimation problem. We give a ""no free lunch"" result, characterizing the settings (and sample sizes) where PPI++ has provably worse estimation error than using gold-standard labels alone. Specifically, PPI++ will outperform if and only if the correlation between pseudo- and gold-standard is above a certain level that depends on the number of labeled samples ($n$). In some cases our results simplify considerably: For Gaussian data, the correlation must be at least $1/ qrt{n - 2}$ in order to see improvement, and a similar result holds for binary labels. In experiments, we illustrate that our theoretical findings hold on real-world datasets, and give insights into trade-offs between single-sample and sample-splitting variants of PPI++.",http://arxiv.org/abs/2505.20178v1,IS,Quantitative
MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration,"The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains like healthcare and finance demands reliable explanations of their decision-making processes. While inherently interpretable GNN architectures like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to generating explanations based on spurious correlations, potentially undermining trust in critical applications. We present MetaGMT, a meta-learning framework that enhances explanation fidelity through a novel bi-level optimization approach. We demonstrate that MetaGMT significantly improves both explanation quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive classification accuracy while producing more faithful explanations (with an increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline methods. These advancements in interpretability could enable safer deployment of GNNs in sensitive domains by (1) facilitating model debugging through more reliable explanations, (2) supporting targeted retraining when biases are identified, and (3) enabling meaningful human oversight. By addressing the critical challenge of explanation reliability, our work contributes to building more trustworthy and actionable GNN systems for real-world applications.",http://arxiv.org/abs/2505.19445v1,IS,Quantitative
Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement,"Understanding true influence in social media requires distinguishing correlation from causation--particularly when analyzing misinformation spread. While existing approaches focus on exposure metrics and network structures, they often fail to capture the causal mechanisms by which external temporal signals trigger engagement. We introduce a novel joint treatment-outcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects. Our approach adapts causal inference techniques from healthcare to estimate Average Treatment Effects (ATE) within the sequential nature of social media interactions, tackling challenges from external confounding signals. Through our experiments on real-world misinformation and disinformation datasets, we show that our models outperform existing benchmarks by 15--22% in predicting engagement across diverse counterfactual scenarios, including exposure adjustment, timing shifts, and varied intervention durations. Case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation, the expert-based empirical influence.",http://arxiv.org/abs/2505.19355v1,IS,Quantitative
PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation,"Patent similarity evaluation plays a critical role in intellectual property analysis. However, existing methods often overlook the intricate structure of patent documents, which integrate technical specifications, legal boundaries, and application contexts. We introduce PatentMind, a novel framework for patent similarity assessment based on a Multi-Aspect Reasoning Graph (MARG). PatentMind decomposes patents into three core dimensions: technical feature, application domain, and claim scope, to compute dimension-specific similarity scores. These scores are dynamically weighted through a four-stage reasoning process which integrates contextual signals to emulate expert-level judgment. To support evaluation, we construct PatentSimBench, a human-annotated benchmark comprising 500 patent pairs. Experimental results demonstrate that PatentMind achieves a strong correlation ($r=0.938$) with expert annotations, significantly outperforming embedding-based models and advanced prompt engineering methods.These results highlight the effectiveness of modular reasoning frameworks in overcoming key limitations of embedding-based methods for analyzing patent similarity.",http://arxiv.org/abs/2505.19347v1,IS,Quantitative
CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations,"Recent advances in lightweight time series forecasting models suggest the inherent simplicity of time series forecasting tasks. In this paper, we present CMoS, a super-lightweight time series forecasting model. Instead of learning the embedding of the shapes, CMoS directly models the spatial correlations between different time series chunks. Additionally, we introduce a Correlation Mixing technique that enables the model to capture diverse spatial correlations with minimal parameters, and an optional Periodicity Injection technique to ensure faster convergence. Despite utilizing as low as 1% of the lightweight model DLinear's parameters count, experimental results demonstrate that CMoS outperforms existing state-of-the-art models across multiple datasets. Furthermore, the learned weights of CMoS exhibit great interpretability, providing practitioners with valuable insights into temporal structures within specific application scenarios.",http://arxiv.org/abs/2505.19090v1,IS,Quantitative
Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation,"Recent text-to-image (T2I) generation models have advanced significantly, enabling the creation of high-fidelity images from textual prompts. However, existing evaluation benchmarks primarily focus on the explicit alignment between generated images and prompts, neglecting the alignment with real-world knowledge beyond prompts. To address this gap, we introduce Align Beyond Prompts (ABP), a comprehensive benchmark designed to measure the alignment of generated images with real-world knowledge that extends beyond the explicit user prompts. ABP comprises over 2,000 meticulously crafted prompts, covering real-world knowledge across six distinct scenarios. We further introduce ABPScore, a metric that utilizes existing Multimodal Large Language Models (MLLMs) to assess the alignment between generated images and world knowledge beyond prompts, which demonstrates strong correlations with human judgments. Through a comprehensive evaluation of 8 popular T2I models using ABP, we find that even state-of-the-art models, such as GPT-4o, face limitations in integrating simple real-world knowledge into generated images. To mitigate this issue, we introduce a training-free strategy within ABP, named Inference-Time Knowledge Injection (ITKI). By applying this strategy to optimize 200 challenging samples, we achieved an improvement of approximately 43% in ABPScore. The dataset and code are available in https://github.com/smile365317/ABP.",http://arxiv.org/abs/2505.18730v1,IS,Quantitative
Accelerating Learned Image Compression Through Modeling Neural Training Dynamics,"As learned image compression (LIC) methods become increasingly computationally demanding, enhancing their training efficiency is crucial. This paper takes a step forward in accelerating the training of LIC methods by modeling the neural training dynamics. We first propose a Sensitivity-aware True and Dummy Embedding Training mechanism (STDET) that clusters LIC model parameters into few separate modes where parameters are expressed as affine transformations of reference parameters within the same mode. By further utilizing the stable intra-mode correlations throughout training and parameter sensitivities, we gradually embed non-reference parameters, reducing the number of trainable parameters. Additionally, we incorporate a Sampling-then-Moving Average (SMA) technique, interpolating sampled weights from stochastic gradient descent (SGD) training to obtain the moving average weights, ensuring smooth temporal behavior and minimizing training state variances. Overall, our method significantly reduces training space dimensions and the number of trainable parameters without sacrificing model performance, thus accelerating model convergence. We also provide a theoretical analysis on the Noisy quadratic model, showing that the proposed method achieves a lower training variance than standard SGD. Our approach offers valuable insights for further developing efficient training methods for LICs.",http://arxiv.org/abs/2505.18107v1,IS,Quantitative
DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification,"Neighborhood-aware tokenized graph Transformers have recently shown great potential for node classification tasks. Despite their effectiveness, our in-depth analysis of neighborhood tokens reveals two critical limitations in the existing paradigm. First, current neighborhood token generation methods fail to adequately capture attribute correlations within a neighborhood. Second, the conventional self-attention mechanism suffers from attention diversion when processing neighborhood tokens, where high-hop neighborhoods receive disproportionate focus, severely disrupting information interactions between the target node and its neighborhood tokens. To address these challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking graph Transformer. DAM-GT introduces a novel dual positional encoding scheme that incorporates attribute-aware encoding via an attribute clustering strategy, effectively preserving node correlations in both topological and attribute spaces. In addition, DAM-GT formulates a new attention mechanism with a simple yet effective masking strategy to guide interactions between target nodes and their neighborhood tokens, overcoming the issue of attention diversion. Extensive experiments on various graphs with different homophily levels as well as different scales demonstrate that DAM-GT consistently outperforms state-of-the-art methods in node classification tasks.",http://arxiv.org/abs/2505.17660v1,IS,Quantitative
FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation,"Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.",http://arxiv.org/abs/2505.20353v1,IS,Quantitative
Do Large Language Models (Really) Need Statistical Foundations?,"Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.",http://arxiv.org/abs/2505.19145v1,IS,Mixed
Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models,"Large Transformer based language models achieve remarkable performance but remain opaque in how they plan, structure, and realize text. We introduce Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework that factorizes generation into three semantic scales_global context, intermediate structure, and local word choices and aligns each scale with specific layer ranges in Transformer architectures. To identify scale boundaries, we propose two complementary metrics: attention span thresholds and inter layer mutual information peaks. Across four representative models (GPT-2, BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global partitions, corroborated by probing tasks and causal interventions. We find that decoder_only models allocate more layers to intermediate and global processing while encoder_only models emphasize local feature extraction. Through targeted interventions, we demonstrate that local scale manipulations primarily influence lexical diversity, intermediate-scale modifications affect sentence structure and length, and global_scale perturbations impact discourse coherence all with statistically significant effects. MSPGT thus offers a unified, architecture-agnostic method for interpreting, diagnosing, and controlling large language models, bridging the gap between mechanistic interpretability and emergent capabilities.",http://arxiv.org/abs/2505.18244v1,IS,Quantitative
Linear Mixture Distributionally Robust Markov Decision Processes,"Many real-world decision-making problems face the off-dynamics challenge: the agent learns a policy in a source domain and deploys it in a target domain with different state transitions. The distributionally robust Markov decision process (DRMDP) addresses this challenge by finding a robust policy that performs well under the worst-case environment within a pre-specified uncertainty set of transition dynamics. Its effectiveness heavily hinges on the proper design of these uncertainty sets, based on prior knowledge of the dynamics. In this work, we propose a novel linear mixture DRMDP framework, where the nominal dynamics is assumed to be a linear mixture model. In contrast with existing uncertainty sets directly defined as a ball centered around the nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a ball around the mixture weighting parameter. We show that this new framework provides a more refined representation of uncertainties compared to conventional models based on $(s,a)$-rectangularity and $d$-rectangularity, when prior knowledge about the mixture model is present. We propose a meta algorithm for robust policy learning in linear mixture DRMDPs with general $f$-divergence defined uncertainty sets, and analyze its sample complexities under three divergence metrics instantiations: total variation, Kullback-Leibler, and $\chi^2$ divergences. These results establish the statistical learnability of linear mixture DRMDPs, laying the theoretical foundation for future research on this new setting.",http://arxiv.org/abs/2505.18044v1,IS,Quantitative
Spacetime Geometry of Denoising in Diffusion Models,"We present a novel perspective on diffusion models using the framework of information geometry. We show that the set of noisy samples, taken across all noise levels simultaneously, forms a statistical manifold -- a family of denoising probability distributions. Interpreting the noise level as a temporal parameter, we refer to this manifold as spacetime. This manifold naturally carries a Fisher-Rao metric, which defines geodesics -- shortest paths between noisy points. Notably, this family of distributions is exponential, enabling efficient geodesic computation even in high-dimensional settings without retraining or fine-tuning. We demonstrate the practical value of this geometric viewpoint in transition path sampling, where spacetime geodesics define smooth sequences of Boltzmann distributions, enabling the generation of continuous trajectories between low-energy metastable states. Code is available at: https://github.com/Aalto-QuML/diffusion-spacetime-geometry.",http://arxiv.org/abs/2505.17517v1,IS,Quantitative
Optimal Transport with Heterogeneously Missing Data,"We consider the problem of solving the optimal transport problem between two empirical distributions with missing values. Our main assumption is that the data is missing completely at random (MCAR), but we allow for heterogeneous missingness probabilities across features and across the two distributions. As a first contribution, we show that the Wasserstein distance between empirical Gaussian distributions and linear Monge maps between arbitrary distributions can be debiased without significantly affecting the sample complexity. Secondly, we show that entropic regularized optimal transport can be estimated efficiently and consistently using iterative singular value thresholding (ISVT). We propose a validation set-free hyperparameter selection strategy for ISVT that leverages our estimator of the Bures-Wasserstein distance, which could be of independent interest in general matrix completion problems. Finally, we validate our findings on a wide range of numerical applications.",http://arxiv.org/abs/2505.17291v1,IS,Quantitative
Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference,"Graph Neural Networks (GNNs) have gained prominence for their ability to process graph-structured data across various domains. However, interpreting GNN decisions remains a significant challenge, leading to the adoption of saliency maps for identifying influential nodes and edges. Despite their utility, the reliability of GNN saliency maps has been questioned, particularly in terms of their robustness to noise. In this study, we propose a statistical testing framework to rigorously evaluate the significance of saliency maps. Our main contribution lies in addressing the inflation of the Type I error rate caused by double-dipping of data, leveraging the framework of Selective Inference. Our method provides statistically valid $p$-values while controlling the Type I error rate, ensuring that identified salient subgraphs contain meaningful information rather than random artifacts. To demonstrate the effectiveness of our method, we conduct experiments on both synthetic and real-world datasets, showing its effectiveness in assessing the reliability of GNN interpretations.",http://arxiv.org/abs/2505.16893v1,IS,Quantitative
Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning,"Text-to-multiview (T2MV) generation, which produces coherent multiview images from a single text prompt, remains computationally intensive, while accelerated T2MV methods using few-step diffusion models often sacrifice image fidelity and view consistency. To address this, we propose a novel reinforcement learning (RL) finetuning framework tailored for few-step T2MV diffusion models to jointly optimize per-view fidelity and cross-view consistency. Specifically, we first reformulate T2MV denoising across all views as a single unified Markov decision process, enabling multiview-aware policy optimization driven by a joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV sampling technique that adds an inversion-denoising pass to reinforce both viewpoint and text conditioning, resulting in improved T2MV generation at the cost of inference time. To internalize its performance gains into the base sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that uses reward advantages of ZMV-Sampling over standard sampling as learning signals for policy updates. Finally, noting that the joint-view reward objective under-optimizes per-view fidelity but naively optimizing single-view metrics neglects cross-view alignment, we reframe RL finetuning for T2MV diffusion models as a constrained optimization problem that maximizes per-view fidelity subject to an explicit joint-view constraint, thereby enabling more efficient and balanced policy updates. By integrating this constrained optimization paradigm with MV-ZigAL, we establish our complete RL finetuning framework, referred to as MVC-ZigAL, which effectively refines the few-step T2MV diffusion baseline in both fidelity and consistency while preserving its few-step efficiency.",http://arxiv.org/abs/2505.20107v1,IS,Quantitative
MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support,"Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses vary significantly and evolve over time. Clinical data used to support these decisions are often irregularly sampled, sparse, and noisy. Existing decision support systems commonly rely on discretization and imputation, which can distort critical temporal dynamics and degrade decision quality. Moreover, they often overlook the clinical significance of irregular recording frequencies, filtering out patterns in how and when data is collected. Reinforcement Learning (RL) is a natural fit for clinical decision-making, enabling sequential, long-term optimization in dynamic, uncertain environments. However, most existing treatment recommendation systems are model-free and trained solely on offline data, making them sample-inefficient, sensitive to data quality, and poorly generalizable across tasks or cohorts. To address these limitations, we propose MedDreamer, a two-phase model-based RL framework for personalized treatment recommendation. MedDreamer uses a world model with an Adaptive Feature Integration (AFI) module to effectively model irregular, sparse clinical data. Through latent imagination, it simulates plausible patient trajectories to enhance learning, refining its policy using a mix of real and imagined experiences. This enables learning policies that go beyond suboptimal historical decisions while remaining grounded in clinical data. To our knowledge, this is the first application of latent imagination to irregular healthcare data. Evaluations on sepsis and mechanical ventilation (MV) treatment using two large-scale EHR datasets show that MedDreamer outperforms both model-free and model-based baselines in clinical outcomes and off-policy metrics.",http://arxiv.org/abs/2505.19785v1,IS,Quantitative
"Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality","This position paper argues that Mean Opinion Score (MOS), while historically foundational, is no longer sufficient as the sole supervisory signal for multimedia quality assessment models. MOS reduces rich, context-sensitive human judgments to a single scalar, obscuring semantic failures, user intent, and the rationale behind quality decisions. We contend that modern quality assessment models must integrate three interdependent capabilities: (1) context-awareness, to adapt evaluations to task-specific goals and viewing conditions; (2) reasoning, to produce interpretable, evidence-grounded justifications for quality judgments; and (3) multimodality, to align perceptual and semantic cues using vision-language models. We critique the limitations of current MOS-centric benchmarks and propose a roadmap for reform: richer datasets with contextual metadata and expert rationales, and new evaluation metrics that assess semantic alignment, reasoning fidelity, and contextual sensitivity. By reframing quality assessment as a contextual, explainable, and multimodal modeling task, we aim to catalyze a shift toward more robust, human-aligned, and trustworthy evaluation systems.",http://arxiv.org/abs/2505.19696v1,IS,Quantitative
Training-Free Multi-Step Audio Source Separation,"Audio source separation aims to separate a mixture into target sources. Previous audio source separation systems usually conduct one-step inference, which does not fully explore the separation ability of models. In this work, we reveal that pretrained one-step audio source separation models can be leveraged for multi-step separation without additional training. We propose a simple yet effective inference method that iteratively applies separation by optimally blending the input mixture with the previous step's separation result. At each step, we determine the optimal blending ratio by maximizing a metric. We prove that our method always yield improvement over one-step inference, provide error bounds based on model smoothness and metric robustness, and provide theoretical analysis connecting our method to denoising along linear interpolation paths between noise and clean distributions, a property we link to denoising diffusion bridge models. Our approach effectively delivers improved separation performance as a ""free lunch"" from existing models. Our empirical results demonstrate that our multi-step separation approach consistently outperforms one-step inference across both speech enhancement and music source separation tasks, and can achieve scaling performance similar to training a larger model, using more data, or in some cases employing a multi-step training objective. These improvements appear not only on the optimization metric during multi-step inference, but also extend to nearly all non-optimized metrics (with one exception). We also discuss limitations of our approach and directions for future research.",http://arxiv.org/abs/2505.19534v1,IS,Quantitative
DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction,"This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.",http://arxiv.org/abs/2505.21473v1,IS,Quantitative
LazyVLM: Neuro-Symbolic Approach to Video Analytics,"Current video analytics approaches face a fundamental trade-off between flexibility and efficiency. End-to-end Vision Language Models (VLMs) often struggle with long-context processing and incur high computational costs, while neural-symbolic methods depend heavily on manual labeling and rigid rule design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics system that provides a user-friendly query interface similar to VLMs, while addressing their scalability limitation. LazyVLM enables users to effortlessly drop in video data and specify complex multi-frame video queries using a semi-structured text interface for video analytics. To address the scalability limitations of VLMs, LazyVLM decomposes multi-frame video queries into fine-grained operations and offloads the bulk of the processing to efficient relational query execution and vector similarity search. We demonstrate that LazyVLM provides a robust, efficient, and user-friendly solution for querying open-domain video data at scale.",http://arxiv.org/abs/2505.21459v1,IS,Quantitative
A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features,"Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.",http://arxiv.org/abs/2505.21317v1,IS,Quantitative
LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning,"Large pre-trained models are commonly adapted to downstream tasks using parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA), which injects small trainable low-rank matrices instead of updating all weights. While LoRA dramatically reduces trainable parameters with little overhead, it can still underperform full fine-tuning in accuracy and often converges more slowly. We introduce LoFT, a novel low-rank adaptation method that behaves like full fine-tuning by aligning the optimizer's internal dynamics with those of updating all model weights. LoFT not only learns weight updates in a low-rank subspace (like LoRA) but also properly projects the optimizer's first and second moments (Adam's momentum and variance) into the same subspace, mirroring full-model updates. By aligning the low-rank update itself with the full update, LoFT eliminates the need for tuning extra hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach substantially narrows the performance gap between adapter-based tuning and full fine-tuning and consistently outperforms standard LoRA-style methods, all without increasing inference cost.",http://arxiv.org/abs/2505.21289v1,IS,Quantitative
OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models,"Text-to-Image (T2I) models have achieved remarkable success in generating visual content from text inputs. Although multiple safety alignment strategies have been proposed to prevent harmful outputs, they often lead to overly cautious behavior -- rejecting even benign prompts -- a phenomenon known as $\textit{over-refusal}$ that reduces the practical utility of T2I models. Despite over-refusal having been observed in practice, there is no large-scale benchmark that systematically evaluates this phenomenon for T2I models. In this paper, we present an automatic workflow to construct synthetic evaluation data, resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on $\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful but benign prompts across nine safety-related categories, along with 1,785 genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility trade-off. Using OVERT, we evaluate several leading T2I models and find that over-refusal is a widespread issue across various categories (Figure 1), underscoring the need for further research to enhance the safety alignment of T2I models without compromising their functionality.As a preliminary attempt to reduce over-refusal, we explore prompt rewriting; however, we find it often compromises faithfulness to the meaning of the original prompts. Finally, we demonstrate the flexibility of our generation framework in accommodating diverse safety requirements by generating customized evaluation data adapting to user-defined policies.",http://arxiv.org/abs/2505.21347v1,IS,Quantitative
Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting,"Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.",http://arxiv.org/abs/2505.20729v1,IS,Mixed
Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights,"Reasoning with tabular data holds increasing importance in modern applications, yet comprehensive evaluation methodologies for reasoning-intensive Table Question Answering (QA) tasks remain nascent. Existing research is constrained by two primary bottlenecks: 1) Reliance on costly manually annotated real-world data, which is difficult to cover complex reasoning scenarios; 2) The heterogeneity of table structures hinders systematic analysis of the intrinsic mechanisms behind the underperformance of LLMs, especially in reasoning-intensive tasks. To address these issues, we propose an automated generation pipeline AutoT2T that transforms mathematical word problems into table-based reasoning tasks, eliminating the need for manual annotation. The pipeline can generate multiple variants of a table for the same reasoning problem, including noisy versions to support robustness evaluation. Based on this, we construct a new benchmark TabularGSM, which systematically spans a range of table complexities and trap problems. Experimental analyses through AutoT2T and TabularGSM reveal that the tight coupling between reasoning and retrieval or identification processes is a key factor underlying the failure of LLMs in complex Table QA tasks. This highlights the necessity for models to develop synergistic reasoning capabilities in order to perform effectively in complex Table QA tasks.",http://arxiv.org/abs/2505.19563v1,IS,Quantitative
Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning,"Controllable code generation, the ability to synthesize code that follows a specified style while maintaining functionality, remains a challenging task. We propose a two-stage training framework combining contrastive learning and conditional decoding to enable flexible style control. The first stage aligns code style representations with semantic and structural features. In the second stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned style vector to guide generation. Our method supports style interpolation and user personalization via lightweight mixing. Compared to prior work, our unified framework offers improved stylistic control without sacrificing code correctness. This is among the first approaches to combine contrastive alignment with conditional decoding for style-guided code generation.",http://arxiv.org/abs/2505.19442v1,IS,Quantitative
Alchemist: Turning Public Text-to-Image Data into Generative Gold,"Pre-training equips text-to-image (T2I) models with broad world knowledge, but this alone is often insufficient to achieve high aesthetic quality and alignment. Consequently, supervised fine-tuning (SFT) is crucial for further refinement. However, its effectiveness highly depends on the quality of the fine-tuning dataset. Existing public SFT datasets frequently target narrow domains (e.g., anime or specific art styles), and the creation of high-quality, general-purpose SFT datasets remains a significant challenge. Current curation methods are often costly and struggle to identify truly impactful samples. This challenge is further complicated by the scarcity of public general-purpose datasets, as leading models often rely on large, proprietary, and poorly documented internal data, hindering broader research progress. This paper introduces a novel methodology for creating general-purpose SFT datasets by leveraging a pre-trained generative model as an estimator of high-impact training samples. We apply this methodology to construct and release Alchemist, a compact (3,350 samples) yet highly effective SFT dataset. Experiments demonstrate that Alchemist substantially improves the generative quality of five public T2I models while preserving diversity and style. Additionally, we release the fine-tuned models' weights to the public.",http://arxiv.org/abs/2505.19297v1,IS,Quantitative
PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs,"Recently, significant progress has been made in developing reasoning-capable Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques. However, this long-CoT reasoning process imposes substantial memory overhead due to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache quantization has emerged as a promising compression technique and has been extensively studied in short-context scenarios. However, directly applying existing methods to long-CoT LLMs causes significant performance degradation due to the following two reasons: (1) Large cumulative error: Existing methods fail to adequately leverage available memory, and they directly quantize the KV Cache during each decoding step, leading to large cumulative quantization error. (2) Short-context calibration: Due to Rotary Positional Embedding (RoPE), the use of short-context data during calibration fails to account for the distribution of less frequent channels in the Key Cache, resulting in performance loss. We propose Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To reduce cumulative error, we design a progressive quantization strategy to gradually lower the bit-width of KV Cache in each block. Then, we propose block-wise memory allocation to assign a higher bit-width to more sensitive transformer blocks. (2) To increase the calibration length without additional overhead, we propose a new calibration strategy with positional interpolation that leverages short calibration data with positional interpolation to approximate the data distribution of long-context data. Extensive experiments on 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark performance by up to 8% over SOTA baselines under the same memory budget. Our code is available at https://github.com/thu-nics/PM-KVQ.",http://arxiv.org/abs/2505.18610v1,IS,Quantitative
SPIRAL integration of generative AI in an undergraduate creative media course: effects on self-efficacy and career outcome expectations,"Computing education and computing students are rapidly integrating generative AI, but we know relatively little about how different pedagogical strategies for intentionally integrating generative AI affect students' self-efficacy and career interests. This study investigates a SPIRAL integration of generative AI (Skills Practiced Independently, Revisited with AI Later), implemented in an introductory undergraduate creative media and technology course in Fall 2023 (n=31). Students first developed domain skills for half the semester, then revisited earlier material integrating using generative AI, with explicit instruction on how to use it critically and ethically. We contribute a mixed methods quantitative and qualitative analysis of changes in self-efficacy and career interests over time, including longitudinal qualitative interviews (n=9) and thematic analysis. We found positive changes in both students' creative media self-efficacy and generative AI use self-efficacy, and mixed changes for ethical generative AI use self-efficacy. We also found students experienced demystification, transitioning from initial fear about generative AI taking over their fields and jobs, to doubting AI capability to do so and/or that society will push back against AI, through personal use of AI and observing others' use of AI vicariously. For career interests, our SPIRAL integration of generative AI use appeared to have either a neutral or positive influence on students, including widening their perceived career options, depending on their view of how AI would influence the career itself. These findings suggest that careful pedagogical sequencing can mitigate some potential negative impacts of AI, while promoting ethical and critical AI use that supports or has a neutral effect on students' career formation. To our knowledge our SPIRAL integration strategy applied to generative AI integration is novel.",http://arxiv.org/abs/2505.18771v1,IS,Mixed
Lost in Models? Structuring Managerial Decision Support in Process Mining with Multi-criteria Decision Making,"Process mining is increasingly adopted in modern organizations, producing numerous process models that, while valuable, can lead to model overload and decision-making complexity. This paper explores a multi-criteria decision-making (MCDM) approach to evaluate and prioritize process models by incorporating both quantitative metrics (e.g., fitness, precision) and qualitative factors (e.g., cultural fit). An illustrative logistics example demonstrates how MCDM, specifically the Analytic Hierarchy Process (AHP), facilitates trade-off analysis and promotes alignment with managerial objectives. Initial insights suggest that the MCDM approach enhances context-sensitive decision-making, as selected models address both operational metrics and broader managerial needs. While this study is an early-stage exploration, it provides an initial foundation for deeper exploration of MCDM-driven strategies to enhance the role of process mining in complex organizational settings.",http://arxiv.org/abs/2505.10236v1,IS,Mixed
Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation,"Trajectory prediction is a key element of autonomous vehicle systems, enabling them to anticipate and react to the movements of other road users. Evaluating the robustness of prediction models against adversarial attacks is essential to ensure their reliability in real-world traffic. However, current approaches tend to focus on perturbing the past positions of surrounding agents, which can generate unrealistic scenarios and overlook critical vulnerabilities. This limitation may result in overly optimistic assessments of model performance in real-world conditions. In this work, we demonstrate that perturbing not just past but also future states of adversarial agents can uncover previously undetected weaknesses and thereby provide a more rigorous evaluation of model robustness. Our novel approach incorporates dynamic constraints and preserves tactical behaviors, enabling more effective and realistic adversarial attacks. We introduce new performance measures to assess the realism and impact of these adversarial trajectories. Testing our method on a state-of-the-art prediction model revealed significant increases in prediction errors and collision rates under adversarial conditions. Qualitative analysis further showed that our attacks can expose critical weaknesses, such as the inability of the model to detect potential collisions in what appear to be safe predictions. These results underscore the need for more comprehensive adversarial testing to better evaluate and improve the reliability of trajectory prediction models for autonomous vehicles.",http://arxiv.org/abs/2505.06134v1,IS,Qualitative
AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity,"The paper considers the use of GPT models with retrieval-augmented generation (RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity and NATO Article 5 trust opinion scores in different web sources: news sites found via Google Search API, Youtube videos with comments, and Reddit discussions. A RAG approach using GPT-4.1 model was applied to analyse news where NATO related topics were discussed. Two levels of RAG analytics were used: on the first level, the GPT model generates qualitative news summaries and quantitative opinion scores using zero-shot prompts; on the second level, the GPT model generates the summary of news summaries. Quantitative news opinion scores generated by the GPT model were analysed using Bayesian regression to get trend lines. The distributions found for the regression parameters make it possible to analyse an uncertainty in specified news opinion score trends. Obtained results show a downward trend for analysed scores of opinion related to NATO unity. This approach does not aim to conduct real political analysis; rather, it consider AI based approaches which can be used for further analytics as a part of a complex analytical approach. The obtained results demonstrate that the use of GPT models for news analysis can give informative qualitative and quantitative analytics, providing important insights. The dynamic model based on neural ordinary differential equations was considered for modelling public opinions. This approach makes it possible to analyse different scenarios for evolving public opinions.",http://arxiv.org/abs/2505.06313v1,IS,Mixed
From Flowers to Fascism? The Cottagecore to Tradwife Pipeline on Tumblr,"In this work we collected and analyzed social media posts to investigate aesthetic-based radicalization where users searching for Cottagecore content may find Tradwife content co-opted by white supremacists, white nationalists, or other far-right extremist groups. Through quantitative analysis of over 200,000 Tumblr posts and qualitative coding of about 2,500 Tumblr posts, we did not find evidence of a explicit radicalization. We found that problematic Tradwife posts found in the literature may be confined to Tradwife-only spaces, while content in the Cottagecore tag generally did not warrant extra moderation. However, we did find evidence of a mainstreaming effect in the overlap between the Tradwife and Cottagecore communities. In our qualitative analysis there was more interaction between queer and Tradwife identities than expected based on the literature, and some Tradwives even explicitly included queer people and disavowed racism in the Tradwife community on Tumblr. This could be genuine, but more likely it was an example of extremists re-branding their content and following platform norms to spread ideologies that would otherwise be rejected by Tumblr users. Additionally, through temporal analysis we observed a change in the central tags used by Tradwives in the Cottagecore tag pre- and post- 2021. Initially these posts focused on aesthetics and hobbies like baking and gardening, but post-2021 the central tags focused more on religion, traditional gender roles, and homesteading, all markers of reactionary ideals.",http://arxiv.org/abs/2505.04561v1,IS,Mixed
Doing Audits Right? The Role of Sampling and Legal Content Analysis in Systemic Risk Assessments and Independent Audits in the Digital Services Act,"A central requirement of the European Union's Digital Services Act (DSA) is that online platforms undergo internal and external audits. A key component of these audits is the assessment of systemic risks, including the dissemination of illegal content, threats to fundamental rights, impacts on democratic processes, and gender-based violence. The DSA Delegated Regulation outlines how such audits should be conducted, setting expectations for both platforms and auditors. This article evaluates the strengths and limitations of different qualitative and quantitative methods for auditing these systemic risks and proposes a mixed-method approach for DSA compliance. We argue that content sampling, combined with legal and empirical analysis, offers a viable method for risk-specific audits. First, we examine relevant legal provisions on sample selection for audit purposes. We then assess sampling techniques and methods suitable for detecting systemic risks, focusing on how representativeness can be understood across disciplines. Finally, we review initial systemic risk assessment reports submitted by platforms, analyzing their testing and sampling methodologies. By proposing a structured, mixed-method approach tailored to specific risk categories and platform characteristics, this article addresses the challenge of evidence-based audits under the DSA. Our contribution emphasizes the need for adaptable, context-sensitive auditing strategies and adds to the emerging field of DSA compliance research.",http://arxiv.org/abs/2505.03601v1,IS,Mixed
Modeling Musical Genre Trajectories through Pathlet Learning,"The increasing availability of user data on music streaming platforms opens up new possibilities for analyzing music consumption. However, understanding the evolution of user preferences remains a complex challenge, particularly as their musical tastes change over time. This paper uses the dictionary learning paradigm to model user trajectories across different musical genres. We define a new framework that captures recurring patterns in genre trajectories, called pathlets, enabling the creation of comprehensible trajectory embeddings. We show that pathlet learning reveals relevant listening patterns that can be analyzed both qualitatively and quantitatively. This work improves our understanding of users' interactions with music and opens up avenues of research into user behavior and fostering diversity in recommender systems. A dataset of 2000 user histories tagged by genre over 17 months, supplied by Deezer (a leading music streaming company), is also released with the code.",http://arxiv.org/abs/2505.03480v1,IS,Mixed
Generating HomeAssistant Automations Using an LLM-based Chatbot,"To combat climate change, individuals are encouraged to adopt sustainable habits, in particular, with their household, optimizing their electrical consumption. Conversational agents, such as Smart Home Assistants, hold promise as effective tools for promoting sustainable practices within households. Our research investigated the application of Large Language Models (LLM) in enhancing smart home automation and promoting sustainable household practices, specifically using the HomeAssistant framework. In particular, it highlights the potential of GPT models in generating accurate automation routines. While the LLMs showed proficiency in understanding complex commands and creating valid JSON outputs, challenges such as syntax errors and message malformations were noted, indicating areas for further improvement. Still, despite minimal quantitative differences between ""green"" and ""no green"" prompts, qualitative feedback highlighted a positive shift towards sustainability in the routines generated with environmentally focused prompts. Then, an empirical evaluation (N=56) demonstrated that the system was well-received and found engaging by users compared to its traditional rule-based counterpart. Our findings highlight the role of LLMs in advancing smart home technologies and suggest further research to refine these models for broader, real-world applications to support sustainable living.",http://arxiv.org/abs/2505.02802v1,IS,Mixed
"The GenAI Generation: Student Views of Awareness, Preparedness, and Concern","Generative AI (GenAI) is revolutionizing education and workforce development, profoundly shaping how students learn, engage, and prepare for their future. Outpacing the development of uniform policies and structures, GenAI has heralded a unique era and given rise to the GenAI Generation: a cohort of students whose education has been increasingly shaped by the opportunities and challenges GenAI presents during its widespread adoption within society. This study examines our students' perceptions of GenAI through a concise survey with optional open-ended questions, focusing on their awareness, preparedness, and concerns. Evaluation of more than 250 responses with more than 40% providing detailed qualitative feedback reveals a core dual sentiment: while most students express enthusiasm for GenAI, an even greater proportion voice a spectrum of concerns about ethics, job displacement, and the adequacy of educational structures given the highly transformative technology. These findings offer critical insights into how students view the potential and pitfalls of GenAI for future career impacts, with accompanying recommendations to guide educational institutions in navigating a future driven by GenAI.",http://arxiv.org/abs/2505.02230v1,IS,Mixed
Overcoming Obstacles: Challenges of Gender Inequality in Undergraduate ICT Programs,"Context: Gender inequality is a widely discussed issue across various sectors, including Information Technology and Communication (ICT). In Brazil, women represent less than 18% of ICT students in higher education. Prior studies highlight gender-related barriers that discourage women from staying in ICT. However, they provide limited insights into their perceptions as undergraduate students and the factors influencing their participation and confidence. Goal: This study explores the perceptions of women undergraduate students in ICT regarding gender inequality. Method: A survey of 402 women from 18 Brazilian states enrolled in ICT courses was conducted using a mixed-method approach, combining quantitative and qualitative analyses. Results: Women students reported experiencing discriminatory practices from peers and professors, both inside and outside the classroom. Gender stereotypes were found to undermine their self-confidence and self-esteem, occasionally leading to course discontinuation. Conclusions: Factors such as lack of representation, inappropriate jokes, isolation, mistrust, and difficulty being heard contribute to harmful outcomes, including reduced participation and reluctance to take leadership roles. Addressing these issues is essential to creating a safe and respectful learning environment for all students.",http://arxiv.org/abs/2505.02857v1,IS,Mixed
Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse,"Parental verbal abuse leaves lasting emotional impacts, yet current therapeutic approaches often lack immersive self-reflection opportunities. To address this, we developed a VR experience powered by LLMs to foster reflection on parental verbal abuse. Participants with relevant experiences engage in a dual-phase VR experience: first assuming the role of a verbally abusive parent, interacting with an LLM portraying a child, then observing the LLM reframing abusive dialogue into warm, supportive expressions as a nurturing parent. A qualitative study with 12 participants showed that the experience encourages reflection on their past experiences and fosters supportive emotions. However, these effects vary with participants' personal histories, emphasizing the need for greater personalization in AI-driven emotional support. This study explores the use of LLMs in immersive environment to promote emotional reflection, offering insights into the design of AI-driven emotional support systems.",http://arxiv.org/abs/2504.18410v1,IS,Qualitative
Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement,"Artificial Intelligence (AI) has become an important part of our everyday lives, yet user requirements for designing AI-assisted systems in law enforcement remain unclear. To address this gap, we conducted qualitative research on decision-making within a law enforcement agency. Our study aimed to identify limitations of existing practices, explore user requirements and understand the responsibilities that humans expect to undertake in these systems. Participants in our study highlighted the need for a system capable of processing and analysing large volumes of data efficiently to help in crime detection and prevention. Additionally, the system should satisfy requirements for scalability, accuracy, justification, trustworthiness and adaptability to be adopted in this domain. Participants also emphasised the importance of having end users review the input data that might be challenging for AI to interpret, and validate the generated output to ensure the system's accuracy. To keep up with the evolving nature of the law enforcement domain, end users need to help the system adapt to the changes in criminal behaviour and government guidance, and technical experts need to regularly oversee and monitor the system. Furthermore, user-friendly human interaction with the system is essential for its adoption and some of the participants confirmed they would be happy to be in the loop and provide necessary feedback that the system can learn from. Finally, we argue that it is very unlikely that the system will ever achieve full automation due to the dynamic and complex nature of the law enforcement domain.",http://arxiv.org/abs/2504.17393v2,IS,Qualitative
Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management,"This position paper critically surveys a broad spectrum of recent empirical developments on human-AI agents collaboration, highlighting both their technical achievements and persistent gaps. We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms. By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms. The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses. Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.",http://arxiv.org/abs/2505.00018v1,IS,Mixed
LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis,"The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This research examines the trustworthiness of LLM-driven design insights, using qualitative coding as a case study to explore the interpretive processes central to RfD. We introduce LLMCode, an open-source tool integrating two metrics, namely Intersection over Union (IoU) and Modified Hausdorff Distance, to assess the alignment between human and LLM-generated insights. Across two studies involving 26 designers, we find that while the model performs well with deductive coding, its ability to emulate a designer's deeper interpretive lens over the data is limited, emphasising the importance of human-AI collaboration. Our results highlight a reciprocal dynamic where users refine LLM outputs and adapt their own perspectives based on the model's suggestions. These findings underscore the importance of fostering appropriate reliance on LLMs by designing tools that preserve interpretive depth while facilitating intuitive collaboration between designers and AI.",http://arxiv.org/abs/2504.16671v1,IS,Qualitative
Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience,"There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.",http://arxiv.org/abs/2504.16548v1,IS,Mixed
EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework,"Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.",http://arxiv.org/abs/2504.14928v1,IS,Mixed
LACE: Exploring Turn-Taking and Parallel Interaction Modes in Human-AI Co-Creation for Iterative Image Generation,"This paper introduces LACE, a co-creative system enabling professional artists to leverage generative AI through controlled prompting and iterative refinement within Photoshop. Addressing challenges in precision, iterative coherence, and workflow compatibility, LACE allows flexible control via layer-based editing and dual-mode collaboration (turn-taking and parallel). A pilot study (N=21) demonstrates significant improvements in user satisfaction, ownership, usability, and artistic perception compared to standard AI workflows. We offer comprehensive findings, system details, nuanced user feedback, and implications for integrating generative AI in professional art practices.",http://arxiv.org/abs/2504.14827v1,IS,Mixed
ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking,"Synthesizing knowledge from large document collections is a critical yet increasingly complex aspect of qualitative research and knowledge work. While AI offers automation potential, effectively integrating it into human-centric sensemaking workflows remains challenging. We present ScholarMate, an interactive system designed to augment qualitative analysis by unifying AI assistance with human oversight. ScholarMate enables researchers to dynamically arrange and interact with text snippets on a non-linear canvas, leveraging AI for theme suggestions, multi-level summarization, and evidence-based theme naming, while ensuring transparency through traceability to source documents. Initial pilot studies indicated that users value this mixed-initiative approach, finding the balance between AI suggestions and direct manipulation crucial for maintaining interpretability and trust. We further demonstrate the system's capability through a case study analyzing 24 papers. By balancing automation with human control, ScholarMate enhances efficiency and supports interpretability, offering a valuable approach for productive human-AI collaboration in demanding sensemaking tasks common in knowledge work.",http://arxiv.org/abs/2504.14406v2,IS,Qualitative
From job titles to jawlines: Using context voids to study generative AI systems,"In this paper, we introduce a speculative design methodology for studying the behavior of generative AI systems, framing design as a mode of inquiry. We propose bridging seemingly unrelated domains to generate intentional context voids, using these tasks as probes to elicit AI model behavior. We demonstrate this through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to generate headshots from professional Curricula Vitae (CVs). In contrast to traditional ways, our approach assesses system behavior under conditions of radical uncertainty -- when forced to invent entire swaths of missing context -- revealing subtle stereotypes and value-laden assumptions. We qualitatively analyze how the system interprets identity and competence markers from CVs, translating them into visual portraits despite the missing context (i.e. physical descriptors). We show that within this context void, the AI system generates biased representations, potentially relying on stereotypical associations or blatant hallucinations.",http://arxiv.org/abs/2504.13947v1,IS,Qualitative
Turn-taking annotation for quantitative and qualitative analyses of conversation,"This paper has two goals. First, we present the turn-taking annotation layers created for 95 minutes of conversational speech of the Graz Corpus of Read and Spontaneous Speech (GRASS), available to the scientific community. Second, we describe the annotation system and the annotation process in more detail, so other researchers may use it for their own conversational data. The annotation system was developed with an interdisciplinary application in mind. It should be based on sequential criteria according to Conversation Analysis, suitable for subsequent phonetic analysis, thus time-aligned annotations were made Praat, and it should be suitable for automatic classification, which required the continuous annotation of speech and a label inventory that is not too large and results in a high inter-rater agreement. Turn-taking was annotated on two layers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP; similar to transition relevance places). We provide a detailed description of the annotation process and of segmentation and labelling criteria. A detailed analysis of inter-rater agreement and common confusions shows that agreement for IPU annotation is near-perfect, that agreement for PCOMP annotations is substantial, and that disagreements often are either partial or can be explained by a different analysis of a sequence which also has merit. The annotation system can be applied to a variety of conversational data for linguistic studies and technological applications, and we hope that the annotations, as well as the annotation system will contribute to a stronger cross-fertilization between these disciplines.",http://arxiv.org/abs/2504.09980v1,IS,Quantitative
TigerGPT: A New AI Chatbot for Adaptive Campus Climate Surveys,"Campus climate surveys play a pivotal role in capturing how students, faculty, and staff experience university life, yet traditional methods frequently suffer from low participation and minimal follow-up. We present TigerGPT, a new AI chatbot that generates adaptive, context-aware dialogues enriched with visual elements. Through real-time follow-up prompts, empathetic messaging, and flexible topic selection, TigerGPT elicits more in-depth feedback compared to traditional static survey forms. Based on established principles of conversational design, the chatbot employs empathetic cues, bolded questions, and user-driven topic selection. It retains some role-based efficiency (e.g., collecting user role through quick clicks) but goes beyond static scripts by employing GenAI adaptiveness. In a pilot study with undergraduate students, we collected both quantitative metrics (e.g., satisfaction ratings) and qualitative insights (e.g., written comments). Most participants described TigerGPT as engaging and user-friendly; about half preferred it over conventional surveys, attributing this preference to its personalized conversation flow and supportive tone. The findings indicate that an AI survey chatbot is promising in gaining deeper insight into campus climate.",http://arxiv.org/abs/2504.13925v1,IS,Mixed
"Artificial Intelligence and the Dual Paradoxes: Examining the Interplay of Efficiency, Resource Consumption, and Labor Dynamics","Artificial Intelligence's (AI) rapid development and growth not only transformed industries but also fired up important debates about its impacts on employment, resource allocation, and the ethics involved in decision-making. It serves to understand how changes within an industry will be able to influence society with that change. Advancing AI technologies will create a dual paradox of efficiency, greater resource consumption, and displacement of traditional labor. In this context, we explore the impact of AI on energy consumption, human labor roles, and hybrid roles widespread human labor replacement. We used mixed methods involving qualitative and quantitative analyses of data identified from various sources. Findings suggest that AI increases energy consumption and has impacted human labor roles to a minimal extent, considering that its applicability is limited to some tasks that require human judgment. In this context, the",http://arxiv.org/abs/2504.10503v1,IS,Mixed
Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms,"The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored. In this paper, we conduct a large-scale analysis of over 2.5M user posts from the r/ChatGPT Reddit community to understand users' security and privacy concerns and attitudes toward conversational AI platforms. Our qualitative analysis reveals that users are concerned about each stage of the data lifecycle (i.e., collection, usage, and retention). They seek mitigations for security vulnerabilities, compliance with privacy regulations, and greater transparency and control in data handling. We also find that users exhibit varied behaviors and preferences when interacting with these platforms. Some users proactively safeguard their data and adjust privacy settings, while others prioritize convenience over privacy risks, dismissing privacy concerns in favor of benefits, or feel resigned to inevitable data sharing. Through qualitative content and regression analysis, we discover that users' concerns evolve over time with the evolving AI landscape and are influenced by technological developments and major events. Based on our findings, we provide recommendations for users, platforms, enterprises, and policymakers to enhance transparency, improve data controls, and increase user trust and adoption.",http://arxiv.org/abs/2504.06552v2,IS,Mixed
User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment,"Navigational challenges significantly impact the independence and mobility of Individuals with Visual Impairment (IVI). While numerous assistive technologies exist, their adoption remains limited due to usability challenges, financial constraints, and a lack of alignment with user needs. This study employs a mixed-methods approach, combining structured surveys and virtual workshops with 19 IVI to investigate their experiences, needs, and preferences regarding assistive technologies for navigation and daily living. The survey results provide insights into participants technological competence, preferences for assistive devices, and willingness to adopt new solutions. In parallel, workshop discussions offer qualitative perspectives on key navigation challenges, including difficulties in detecting overhead obstacles, navigating environments with complex layout, and the limitations of existing technologies. Findings highlight the need for assistive devices that integrate both navigational guidance and high-level spatial awareness, allowing users to build mental maps of their surroundings. Additionally, multimodal feedback, combining audio, haptic, and tactile cues, emerges as a crucial feature to accommodate diverse user preferences and environmental conditions. The study also underscores financial and training barriers that limit access to advanced assistive technologies. Based on these insights, we recommend the development of customizable, user-friendly, and most importantly affordable navigation aids that align with the daily needs of IVI. The findings from this study provide guidance for technology developers, researchers, and policymakers working toward more inclusive and effective assistive solutions.",http://arxiv.org/abs/2504.06379v1,IS,Mixed
VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and Information-Seeking,"In the biomedical domain, visualizing the document embeddings of an extensive corpus has been widely used in information-seeking tasks. However, three key challenges with existing visualizations make it difficult for clinicians to find information efficiently. First, the document embeddings used in these visualizations are generated statically by pretrained language models, which cannot adapt to the user's evolving interest. Second, existing document visualization techniques cannot effectively display how the documents are relevant to users' interest, making it difficult for users to identify the most pertinent information. Third, existing embedding generation and visualization processes suffer from a lack of interpretability, making it difficult to understand, trust and use the result for decision-making. In this paper, we present a novel visual analytics pipeline for user driven document representation and iterative information seeking (VADIS). VADIS introduces a prompt-based attention model (PAM) that generates dynamic document embedding and document relevance adjusted to the user's query. To effectively visualize these two pieces of information, we design a new document map that leverages a circular grid layout to display documents based on both their relevance to the query and the semantic similarity. Additionally, to improve the interpretability, we introduce a corpus-level attention visualization method to improve the user's understanding of the model focus and to enable the users to identify potential oversight. This visualization, in turn, empowers users to refine, update and introduce new queries, thereby facilitating a dynamic and iterative information-seeking experience. We evaluated VADIS quantitatively and qualitatively on a real-world dataset of biomedical research papers to demonstrate its effectiveness.",http://arxiv.org/abs/2504.05697v1,IS,Mixed
"Blending Queries and Conversations: Understanding Tactics, Trust, Verification, and System Choice in Web Search and Chat Interactions","This paper presents a user study (N=22) where participants used an interface combining Web Search and a Generative AI-Chat feature to solve health-related information tasks. We study how people behaved with the interface, why they behaved in certain ways, and what the outcomes of these behaviours were. A think-aloud protocol captured their thought processes during searches. Our findings suggest that GenAI is neither a search panacea nor a major regression compared to standard Web Search interfaces. Qualitative and quantitative analyses identified 78 tactics across five categories and provided insight into how and why different interface features were used. We find evidence that pre-task confidence and trust both influenced which interface feature was used. In both systems, but particularly when using the chat feature, trust was often misplaced in favour of ease-of-use and seemingly perfect answers, leading to increased confidence post-search despite having incorrect results. We discuss what our findings mean in the context of our defined research questions and outline several open questions for future research.",http://arxiv.org/abs/2504.05156v1,IS,Mixed
Usability Testing of an Explainable AI-enhanced Tool for Clinical Decision Support: Insights from the Reflexive Thematic Analysis,"Artificial intelligence-augmented technology represents a considerable opportunity for improving healthcare delivery. Significant progress has been made to demonstrate the value of complex models to enhance clinicians` efficiency in decision-making. However, the clinical adoption of such models is scarce due to multifaceted implementation issues, with the explainability of AI models being among them. One of the substantially documented areas of concern is the unclear AI explainability that negatively influences clinicians` considerations for accepting the complex model. With a usability study engaging 20 U.S.-based clinicians and following the qualitative reflexive thematic analysis, this study develops and presents a concrete framework and an operational definition of explainability. The framework can inform the required customizations and feature developments in AI tools to support clinicians` preferences and enhance their acceptance.",http://arxiv.org/abs/2504.04703v1,IS,Qualitative
Learning Sparse Disentangled Representations for Multimodal Exclusion Retrieval,"Multimodal representations are essential for cross-modal retrieval, but they often lack interpretability, making it difficult to understand the reasoning behind retrieved results. Sparse disentangled representations offer a promising solution; however, existing methods rely heavily on text tokens, resulting in high-dimensional embeddings. In this work, we propose a novel approach that generates compact, fixed-size embeddings that maintain disentanglement while providing greater control over retrieval tasks. We evaluate our method on challenging exclusion queries using the MSCOCO and Conceptual Captions benchmarks, demonstrating notable improvements over dense models like CLIP, BLIP, and VISTA (with gains of up to 11% in AP@10), as well as over sparse disentangled models like VDR (achieving up to 21% gains in AP@10). Furthermore, we present qualitative results that emphasize the enhanced interpretability of our disentangled representations.",http://arxiv.org/abs/2504.03184v1,IS,Qualitative
Dynamic Vision from EEG Brain Recordings: How much does EEG know?,"Reconstructing and understanding dynamic visual information (video) from brain EEG recordings is challenging due to the non-stationary nature of EEG signals, their low signal-to-noise ratio (SNR), and the limited availability of EEG-Video stimulus datasets. Most recent studies have focused on reconstructing static images from EEG recordings. In this work, we propose a framework to reconstruct dynamic visual stimuli from EEG data and conduct an in-depth study of the information encoded in EEG signals. Our approach first trains a feature extraction network using a triplet-based contrastive learning strategy within an EEG-video generation framework. The extracted EEG features are then used for video synthesis with a modified StyleGAN-ADA, which incorporates temporal information as conditioning. Additionally, we analyze how different brain regions contribute to processing dynamic visual stimuli. Through several empirical studies, we evaluate the effectiveness of our framework and investigate how much dynamic visual information can be inferred from EEG signals. The inferences we derive through our extensive studies would be of immense value to future research on extracting visual dynamics from EEG.",http://arxiv.org/abs/2505.21385v1,IS,Quantitative
Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and Mitigation of Risks,"Users often struggle to navigate the privacy / publicity boundary in sharing images online: they may lack awareness of image privacy risks and/or the ability to apply effective mitigation strategies. To address this challenge, we introduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that enables users to identify and mitigate privacy risks with images they intend to share. Driven by design requirements from a formative user study with 7 image-editing experts, Imago Obscura enables users to articulate their image-sharing intent and privacy concerns. The system uses these inputs to surface contextually pertinent privacy risks, and then recommends and facilitates application of a suite of obfuscation techniques found to be effective in prior literature -- e.g., inpainting, blurring, and generative content replacement. We evaluated Imago Obscura with 15 end-users in a lab study and found that it greatly improved users' awareness of image privacy risks and their ability to address those risks, allowing them to make more informed sharing decisions.",http://arxiv.org/abs/2505.20916v1,IS,Mixed
Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset,"Wearable human activity recognition has been shown to benefit from the inclusion of acoustic data, as the sounds around a person often contain valuable context. However, due to privacy concerns, it is usually not ethically feasible to record and save microphone data from the device, since the audio could, for instance, also contain private conversations. Rather, the data should be processed locally, which in turn requires processing power and consumes energy on the wearable device. One special use case of contextual information that can be utilized to augment special tasks in human activity recognition is water flow detection, which can, e.g., be used to aid wearable hand washing detection. We created a new label called tap water for the recently released HD-Epic data set, creating 717 hand-labeled annotations of tap water flow, based on existing annotations of the water class. We analyzed the relation of tap water and water in the dataset and additionally trained and evaluated two lightweight classifiers to evaluate the newly added label class, showing that the new class can be learned more easily.",http://arxiv.org/abs/2505.20788v1,IS,Quantitative
System-driven Cloud Architecture Design Support with Structured State Management and Guided Decision Assistance,"Cloud architecture design is a complex process requiring both technical expertise and architectural knowledge to develop solutions from frequently ambiguous requirements. We present CloudArchitectBuddy, a system-driven cloud architecture design support application with two key mechanisms: (1) structured state management that enhances design understanding through explicit representation of requirements and architectural decisions, and (2) guided decision assistance that facilitates design progress through proactive verification and requirement refinement. Our study with 16 industry practitioners showed that while our approach achieved comparable design quality to a chat interface, participants rated our system higher for usability and appreciated its ability to help understand architectural relationships and identify missing requirements. However, participants also expressed a need for user-initiated interactions where they could freely provide design instructions and engage in detailed discussions with LLMs. These results suggest that integrating a chat interface into our structured and guided workflow approach would create a more practical solution, balancing systematic design support with conversational flexibility for comprehensive cloud architecture development.",http://arxiv.org/abs/2505.20701v1,IS,Mixed
Adaptive Indexing for Approximate Query Processing in Exploratory Data Analysis,"Minimizing data-to-analysis time while enabling real-time interaction and efficient analytical computations on large datasets are fundamental objectives of contemporary exploratory systems. Although some of the recent adaptive indexing and on-the-fly processing approaches address most of these needs, there are cases, where they do not always guarantee reliable performance. Some examples of such cases include: exploring areas with a high density of objects; executing the first exploratory queries or exploring previously unseen areas (where the index has not yet adapted sufficiently); and working with very large data files on commodity hardware, such as low-specification laptops. In such demanding cases, approximate and incremental techniques can be exploited to ensure efficiency and scalability by allowing users to prioritize response time over result accuracy, acknowledging that exact results are not always necessary. Therefore, approximation mechanisms that enable smooth user interaction by defining the trade-off between accuracy and performance based on vital factors (e.g., task, preferences, available resources) are of great importance. Considering the aforementioned, in this work, we present an adaptive approximate query processing framework for interactive on-the-fly analysis (with out a preprocessing phase) over large raw data. The core component of the framework is a main-memory adaptive indexing scheme (VALINOR-A) that interoperates with user-driven sampling and incremental aggregation computations. Additionally, an effective error-bounded approximation strategy is designed and integrated in the query processing process. We conduct extensive experiments using both real and synthetic datasets, demonstrating the efficiency and effectiveness of the proposed framework.",http://arxiv.org/abs/2505.19872v1,IS,Quantitative
AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems,"The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scenarios. However, the field currently lacks standardized evaluation protocols to systematically assess these methods. To address this critical gap, we propose: (1) an interactive textual recommendation simulator incorporating rich user and item metadata and three typical evaluation scenarios (classic, evolving-interest, and cold-start recommendation tasks); (2) a unified modular framework for developing and studying agentic recommender systems; and (3) the first comprehensive benchmark comparing 10 classical and agentic recommendation methods. Our findings demonstrate the superiority of agentic systems and establish actionable design guidelines for their core components. The benchmark environment has been rigorously validated through an open challenge and remains publicly available with a continuously maintained leaderboard~\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html}, fostering ongoing community engagement and reproducible research. The benchmark is available at: \hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.",http://arxiv.org/abs/2505.19623v1,IS,Quantitative
Curation and Analysis of MIMICEL -- An Event Log for MIMIC-IV Emergency Department,"The global issue of overcrowding in emergency departments (ED) necessitates the analysis of patient flow through ED to enhance efficiency and alleviate overcrowding. However, traditional analytical methods are time-consuming and costly. The healthcare industry is embracing process mining tools to analyse healthcare processes and patient flows. Process mining aims to discover, monitor, and enhance processes by obtaining knowledge from event log data. However, the availability of event logs is a prerequisite for applying process mining techniques. Hence, this paper aims to generate an event log for analysing processes in ED. In this study, we extract an event log from the MIMIC-IV-ED dataset and name it MIMICEL. MIMICEL captures the process of patient journey in ED, allowing for analysis of patient flows and improving ED efficiency. We present analyses conducted using MIMICEL to demonstrate the utility of the dataset. The curation of MIMICEL facilitates extensive use of MIMIC-IV-ED data for ED analysis using process mining techniques, while also providing the process mining research communities with a valuable dataset for study.",http://arxiv.org/abs/2505.19389v1,IS,Quantitative
DARTH: Declarative Recall Through Early Termination for Approximate Nearest Neighbor Search,"Approximate Nearest Neighbor Search (ANNS) presents an inherent tradeoff between performance and recall (i.e., result quality). Each ANNS algorithm provides its own algorithm-dependent parameters to allow applications to influence the recall/performance tradeoff of their searches. This situation is doubly problematic. First, the application developers have to experiment with these algorithm-dependent parameters to fine-tune the parameters that produce the desired recall for each use case. This process usually takes a lot of effort. Even worse, the chosen parameters may produce good recall for some queries, but bad recall for hard queries. To solve these problems, we present DARTH, a method that uses target declarative recall. DARTH uses a novel method for providing target declarative recall on top of an ANNS index by employing an adaptive early termination strategy integrated into the search algorithm. Through a wide range of experiments, we demonstrate that DARTH effectively meets user-defined recall targets while achieving significant speedups, up to 14.6x (average: 6.8x; median: 5.7x) faster than the search without early termination for HNSW and up to 41.8x (average: 13.6x; median: 8.1x) for IVF. This paper appeared in ACM SIGMOD 2026.",http://arxiv.org/abs/2505.19001v1,IS,Quantitative
Beyond Replacement or Augmentation: How Creative Workers Reconfigure Division of Labor with Generative AI,"The introduction of generative AI tools such as ChatGPT into creative workplaces has sparked highly visible, but binary worker replacement and augmentation debates. This study reframes this argument by examining how creative professionals re-specify a division of labor with these tools. Through 17 ethnomethodologically informed interviews with international creative agency workers we demonstrate how roles are assigned to generative AI tools, how their contributions are modified and remediated, and how workers practically manage their outputs to reflect assumptions of internal and external stakeholders. This paper makes 3 unique contributions to CSCW: (1) we conceptualize generative AI prompting as a type of workplace situated, reflexive delegation, (2) we demonstrate that workers must continuously configure and repair AI role boundaries to maintain workplace intelligibility and accountability; and (3) we introduce the notion of interpretive templatized trust, where workers devise strategies to adapt automated generative templates for their setting, and reinforce stakeholder trust. This contribution has implications for organizing productive human-AI work in creative and stakeholder centric environments.",http://arxiv.org/abs/2505.18938v1,IS,Qualitative
Marginal Fairness: Fair Decision-Making under Risk Measures,"This paper introduces marginal fairness, a new individual fairness notion for equitable decision-making in the presence of protected attributes such as gender, race, and religion. This criterion ensures that decisions based on generalized distortion risk measures are insensitive to distributional perturbations in protected attributes, regardless of whether these attributes are continuous, discrete, categorical, univariate, or multivariate. To operationalize this notion and reflect real-world regulatory environments (such as the EU gender-neutral pricing regulation), we model business decision-making in highly regulated industries (such as insurance and finance) as a two-step process: (i) a predictive modeling stage, in which a prediction function for the target variable (e.g., insurance losses) is estimated based on both protected and non-protected covariates; and (ii) a decision-making stage, in which a generalized distortion risk measure is applied to the target variable, conditional only on non-protected covariates, to determine the decision. In this second step, we modify the risk measure such that the decision becomes insensitive to the protected attribute, thus enforcing fairness to ensure equitable outcomes under risk-sensitive, regulatory constraints. Furthermore, by utilizing the concept of cascade sensitivity, we extend the marginal fairness framework to capture how dependencies between covariates propagate the influence of protected attributes through the modeling pipeline. A numerical study and an empirical implementation using an auto insurance dataset demonstrate how the framework can be applied in practice.",http://arxiv.org/abs/2505.18895v1,IS,Quantitative
Anonymity-washing,"Anonymization is a foundational principle of data privacy regulation, yet its practical application remains riddled with ambiguity and inconsistency. This paper introduces the concept of anonymity-washing -- the misrepresentation of the anonymity level of ``sanitized'' personal data -- as a critical privacy concern. While both legal and technical critiques of anonymization exist, they tend to address isolated aspects of the problem. In contrast, this paper offers a comprehensive overview of the conditions that enable anonymity-washing. It synthesizes fragmented legal interpretations, technical misunderstandings, and outdated regulatory guidance and complements them with a systematic review of national and international resources, including legal cases, data protection authority guidelines, and technical documentation. Our findings reveal a lack of coherent support for practitioners, contributing to the persistent misuse of pseudonymization and obsolete anonymization techniques. We conclude by recommending targeted education, clearer technical guidance, and closer cooperation between regulators, researchers, and industry to bridge the gap between legal norms and technical reality.",http://arxiv.org/abs/2505.18627v1,IS,Quantitative
A vision-intelligent framework for mapping the genealogy of vernacular architecture,"The study of vernacular architecture involves recording, ordering, and analysing buildings to probe their physical, social, and cultural explanations. Traditionally, this process is conducted manually and intuitively by researchers. Because human perception is selective and often partial, the resulting interpretations of architecture are invariably broad and loose, often lingering on form descriptions that adhere to a preset linear historical progression or crude regional demarcations. This study proposes a research framework by which intelligent technologies can be systematically assembled to augment researchers' intuition in mapping or uncovering the genealogy of vernacular architecture and its connotative socio-cultural system. We employ this framework to examine the stylistic classification of 1,277 historical shophouses in Singapore's Chinatown. Findings extend beyond the chronological classification established by the Urban Redevelopment Authority of Singapore in the 1980s and 1990s, presenting instead a phylogenetic network to capture the formal evolution of shophouses across time and space. The network organises the shophouse types into nine distinct clusters, revealing concurrent evidences of cultural evolution and diffusion. Moreover, it provides a critical perspective on the multi-ethnic character of Singapore shophouses by suggesting that the distinct cultural influences of different ethnic groups led to a pattern of parallel evolution rather than direct convergence. Our work advances a quantitative genealogy of vernacular architecture, which not only assists in formal description but also reveals the underlying forces of development and change. It also exemplified the potential of collaboration between studies in vernacular architecture and computer science, demonstrating how leveraging the strengths of both fields can yield remarkable insights.",http://arxiv.org/abs/2505.18552v1,IS,Quantitative
Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment,"Driving is a key component of independence and quality of life for older adults. However, cognitive decline associated with conditions such as mild cognitive impairment and dementia can compromise driving safety and often lead to premature driving cessation. Conditionally automated vehicles, which require drivers to take over control when automation reaches its operational limits, offer a potential assistive solution. However, their effectiveness depends on the driver's ability to respond to takeover requests (TORs) in a timely and appropriate manner. Understanding emotional responses during TORs can provide insight into drivers' engagement, stress levels, and readiness to resume control, particularly in cognitively vulnerable populations. This study investigated affective responses, measured via facial expression analysis of valence and arousal, during TORs among cognitively healthy older adults and those with cognitive impairment. Facial affect data were analyzed across different road geometries and speeds to evaluate within- and between-group differences in affective states. Within-group comparisons using the Wilcoxon signed-rank test revealed significant changes in valence and arousal during TORs for both groups. Cognitively healthy individuals showed adaptive increases in arousal under higher-demand conditions, while those with cognitive impairment exhibited reduced arousal and more positive valence in several scenarios. Between-group comparisons using the Mann-Whitney U test indicated that cognitively impaired individuals displayed lower arousal and higher valence than controls across different TOR conditions. These findings suggest reduced emotional response and awareness in cognitively impaired drivers, highlighting the need for adaptive vehicle systems that detect affective states and support safe handovers for vulnerable users.",http://arxiv.org/abs/2505.18416v1,IS,Quantitative
Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights,"Effective communication between AI and humans is essential for successful human-AI co-creation. However, many current co-creative AI systems lack effective communication, which limits their potential for collaboration. This paper presents the initial design of the Framework for AI Communication (FAICO) for co-creative AI, developed through a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impact on user experience, offering preliminary guidelines for designing human-centered AI communication. To improve the framework, we conducted a preliminary study with two focus groups involving skilled individuals in AI, HCI, and design. These sessions sought to understand participants' preferences for AI communication, gather their perceptions of the framework, collect feedback for refinement, and explore its use in co-creative domains like collaborative writing and design. Our findings reveal a preference for a human-AI feedback loop over linear communication and emphasize the importance of context in fostering mutual understanding. Based on these insights, we propose actionable strategies for applying FAICO in practice and future directions, marking the first step toward developing comprehensive guidelines for designing effective human-centered AI communication in co-creation.",http://arxiv.org/abs/2505.18385v1,IS,Qualitative
The Relational Origins of Rules in Online Communities,"Where do rules come from in online communities? While prior studies of online community governance in social computing have sought to characterize rules by their functions within communities and documented practices of rule enforcement, they have largely overlooked rule adoption and change. This study investigates how and why online communities adopt and change their rules. We conducted a grounded theory-based analysis of 40 in-depth interviews with community leaders from subreddits, Fandom wikis, and Fediverse servers, and identified seven processes involved in the adoption of online community rules. Our findings reveal that, beyond regulating behavior and solving functional intra-community problems, rules are also adopted and changed for relational reasons, such as signaling or reinforcing community legitimacy and identity to other communities. While rule change was often prompted by challenges during community growth or decline, change also depended on volunteer leaders' work capacity, the presence of member feedback mechanisms, and relational dynamics between leaders and members. The findings extend prior theories from social computing and organizational research, illustrating how institutionalist and ecological explanations of the relational origins of rules complement more functional accounts. The results also support design recommendations that integrate the relational aspects of rules and rulemaking to facilitate successful governance across communities' lifecycles.",http://arxiv.org/abs/2505.18318v1,IS,Qualitative
Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of Vulnerabilities in Privacy Protection,"Traditional methods for raising awareness of privacy protection often fail to engage users or provide hands-on insights into how privacy vulnerabilities are exploited. To address this, we incorporate an adversarial mechanic in the design of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to simulate natural interactions, the game challenges players to impersonate characters and extract sensitive information from an AI agent, Aegis. A user study (n=22) revealed that players employed diverse deceptive linguistic strategies, including storytelling and emotional rapport, to manipulate Aegis. After playing, players reported connecting in-game scenarios with real-world privacy vulnerabilities, such as phishing and impersonation, and expressed intentions to strengthen privacy control, such as avoiding oversharing personal information with AI systems. This work highlights the potential of LLMs to simulate complex relational interactions in serious games, while demonstrating how an adversarial game strategy provides unique insights for designs for social good, particularly privacy protection.",http://arxiv.org/abs/2505.16954v1,IS,Mixed
Optimising the decision threshold in a weighted voting system: The case of the IMF's Board of Governors,"In a weighted majority voting game, the players' weights are determined based on the decision-maker's intentions. The weights are challenging to change in numerous cases, as they represent some desired disparity. However, the voting weights and the actual voting power do not necessarily coincide. Changing a decision threshold would offer some remedy. The International Monetary Fund (IMF) is one of the most important international organisations that uses a weighted voting system to make decisions. The voting weights in its Board of Governors depend on the quotas of the 191 member countries, which reflect their economic strengths to some extent. We analyse the connection between the decision threshold and the a priori voting power of the countries by calculating the Banzhaf indices for each threshold between 50% and 87\%. The difference between the quotas and voting powers is minimised if the decision threshold is 58% or 60%.",http://arxiv.org/abs/2505.16654v1,IS,Mixed
Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events,"During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque ""data-driven fitting"" to transparent ""role-based simulation with mechanistic interpretation"" for panic emotion prediction during emergencies. Our implementation is publicly available at: https://anonymous.4open.science/r/PsychoAgent-19DD.",http://arxiv.org/abs/2505.16455v1,IS,Quantitative
"The EU AI Act, Stakeholder Needs, and Explainable AI: Aligning Regulatory Compliance in a Clinical Decision Support System","Explainable AI (XAI) is a promising solution to ensure compliance with the EU AI Act, the first multi-national regulation for AI. XAI aims to enhance transparency and human oversight of AI systems, particularly ``black-box models'', which are criticized as incomprehensible. However, the discourse around the main stakeholders in the AI Act and XAI appears disconnected. While XAI prioritizes the end user's needs as the primary goal, the AI Act focuses on the obligations of the provider and deployer of the AI system. We aim to bridge this divide and provide guidance on how these two worlds are related. By fostering an interdisciplinary discussion in a cross-functional team with XAI, AI Act, legal, and requirements engineering experts, we walk through the steps necessary to analyze an AI-based clinical decision support system to clarify the end-user needs and assess AI Act applicability. By analyzing our justified understanding using an AI system under development as a case, we show that XAI techniques can fill a gap between stakeholder needs and the requirements of the AI Act. We look at the similarities and contrasts between the legal requirements and the needs of stakeholders. In doing so, we encourage researchers and practitioners from the XAI community to reflect on their role towards the AI Act by achieving a mutual understanding of the implications of XAI and the AI Act within different disciplines.",http://arxiv.org/abs/2505.20311v1,IS,Quantitative
Flow Matching based Sequential Recommender Model,"Generative models, particularly diffusion model, have emerged as powerful tools for sequential recommendation. However, accurately modeling user preferences remains challenging due to the noise perturbations inherent in the forward and reverse processes of diffusion-based methods. Towards this end, this study introduces FMRec, a Flow Matching based model that employs a straight flow trajectory and a modified loss tailored for the recommendation task. Additionally, from the diffusion-model perspective, we integrate a reconstruction loss to improve robustness against noise perturbations, thereby retaining user preferences during the forward process. In the reverse process, we employ a deterministic reverse sampler, specifically an ODE-based updating function, to eliminate unnecessary randomness, thereby ensuring that the generated recommendations closely align with user needs. Extensive evaluations on four benchmark datasets reveal that FMRec achieves an average improvement of 6.53% over state-of-the-art methods. The replication code is available at https://github.com/FengLiu-1/FMRec.",http://arxiv.org/abs/2505.16298v1,IS,Quantitative
Classifying and Tracking International Aid Contribution Towards SDGs,"International aid is a critical mechanism for promoting economic growth and well-being in developing nations, supporting progress toward the Sustainable Development Goals (SDGs). However, tracking aid contributions remains challenging due to labor-intensive data management, incomplete records, and the heterogeneous nature of aid data. Recognizing the urgency of this challenge, we partnered with government agencies to develop an AI model that complements manual classification and mitigates human bias in subjective interpretation. By integrating SDG-specific semantics and leveraging prior knowledge from language models, our approach enhances classification accuracy and accommodates the diversity of aid projects. When applied to a comprehensive dataset spanning multiple years, our model can reveal hidden trends in the temporal evolution of international development cooperation. Expert interviews further suggest how these insights can empower policymakers with data-driven decision-making tools, ultimately improving aid effectiveness and supporting progress toward SDGs.",http://arxiv.org/abs/2505.15223v1,IS,Mixed
Post-Post-API Age: Studying Digital Platforms in Scant Data Access Times,"Over the past decade, data provided by digital platforms has informed substantial research in HCI to understand online human interaction and communication. Following the closure of major social media APIs that previously provided free access to large-scale data (the ""post-API age""), emerging data access programs required by the European Union's Digital Services Act (DSA) have sparked optimism about increased platform transparency and renewed opportunities for comprehensive research on digital platforms, leading to the ""post-post-API age."" However, it remains unclear whether platforms provide adequate data access in practice. To assess how platforms make data available under the DSA, we conducted a comprehensive survey followed by in-depth interviews with 19 researchers to understand their experiences with data access in this new era. Our findings reveal significant challenges in accessing social media data, with researchers facing multiple barriers including complex API application processes, difficulties obtaining credentials, and limited API usability. These challenges have exacerbated existing institutional, regional, and financial inequities in data access. Based on these insights, we provide actionable recommendations for platforms, researchers, and policymakers to foster more equitable and effective data access, while encouraging broader dialogue within the CSCW community around interdisciplinary and multi-stakeholder solutions.",http://arxiv.org/abs/2505.09877v1,IS,Mixed
"""You Cannot Sound Like GPT"": Signs of language discrimination and resistance in computer science publishing","LLMs have been celebrated for their potential to help multilingual scientists publish their research. Rather than interpret LLMs as a solution, we hypothesize their adoption can be an indicator of existing linguistic exclusion in scientific writing. Using the case study of ICLR, an influential, international computer science conference, we examine how peer reviewers critique writing clarity. Analyzing almost 80,000 peer reviews, we find significant bias against authors associated with institutions in countries where English is less widely spoken. We see only a muted shift in the expression of this bias after the introduction of ChatGPT in late 2022. To investigate this unexpectedly minor change, we conduct interviews with 14 conference participants from across five continents. Peer reviewers describe associating certain features of writing with people of certain language backgrounds, and such groups in turn with the quality of scientific work. While ChatGPT masks some signs of language background, reviewers explain that they now use ChatGPT ""style"" and non-linguistic features as indicators of author demographics. Authors, aware of this development, described the ongoing need to remove features which could expose their ""non-native"" status to reviewers. Our findings offer insight into the role of ChatGPT in the reproduction of scholarly language ideologies which conflate producers of ""good English"" with producers of ""good science.""",http://arxiv.org/abs/2505.08127v1,IS,Qualitative
Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?,"This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.",http://arxiv.org/abs/2505.08797v1,IS,Mixed
Theatrical Language Processing: Exploring AI-Augmented Improvisational Acting and Scriptwriting with LLMs,"The increasing convergence of artificial intelligence has opened new avenues, including its emerging role in enhancing creativity. It is reshaping traditional creative practices such as actor improvisation, which often struggles with predictable patterns, limited interaction, and a lack of engaging stimuli. In this paper, we introduce a new concept, Theatrical Language Processing (TLP), and an AI-driven creativity support tool, Scribble.ai, designed to augment actors' creative expression and spontaneity through interactive practice. We conducted a user study involving tests and interviews with fourteen participants. Our findings indicate that: (1) Actors expanded their creativity when faced with AI-produced irregular scenarios; (2) The AI's unpredictability heightened their problem-solving skills, specifically in interpreting unfamiliar situations; (3) However, AI often generated excessively detailed scripts, which limited interpretive freedom and hindered subtext exploration. Based on these findings, we discuss the new potential in enhancing creative expressions in film and theater studies through an AI-driven tool.",http://arxiv.org/abs/2505.04890v2,IS,Qualitative
A User-Centered Teleoperation GUI for Automated Vehicles: Identifying and Evaluating Information Requirements for Remote Driving and Assistance,"Teleoperation emerged as a promising fallback for situations beyond the capabilities of automated vehicles. Nevertheless, teleoperation still faces challenges, such as reduced situational awareness. Since situational awareness is primarily built through the remote operator's visual perception, the Graphical User Interface (GUI) design is critical. In addition to video feeds, supplemental informational elements are crucial - not only for the predominantly studied Remote Driving but also for the arising desk-based Remote Assistance concepts. This work develops a GUI for different teleoperation concepts by identifying key informational elements during the teleoperation process through expert interviews (N = 9). Following this, a static and dynamic GUI prototype is developed and evaluated in a click-dummy study (N = 36). Thereby, the dynamic GUI adapts the number of displayed elements according to the teleoperation phase. Results show that both GUIs achieve good System Usability Scale (SUS) ratings, with the dynamic GUI significantly outperforming the static version in both usability and task completion time. The User Experience Questionnaire (UEQ) score shows potential for improvement. To enhance the user experience, the GUI should be evaluated in a follow-up study that includes interaction with a real vehicle.",http://arxiv.org/abs/2504.21563v1,IS,Qualitative
Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design,"In child-centered design, directly engaging children is crucial for deeply understanding their experiences. However, current research often prioritizes adult perspectives, as interviewing children involves unique challenges such as environmental sensitivities and the need for trust-building. AI-powered virtual humans (VHs) offer a promising approach to facilitate engaging and multimodal interactions with children. This study establishes key design guidelines for LLM-powered virtual humans tailored to child interviews, standardizing multimodal elements including color schemes, voice characteristics, facial features, expressions, head movements, and gestures. Using ChatGPT-based prompt engineering, we developed three distinct Human-AI workflows (LLM-Auto, LLM-Interview, and LLM-Analyze) and conducted a user study involving 15 children aged 6 to 12. The results indicated that the LLM-Analyze workflow outperformed the others by eliciting longer responses, achieving higher user experience ratings, and promoting more effective child engagement.",http://arxiv.org/abs/2504.20016v1,IS,Qualitative
"""Two Means to an End Goal"": Connecting Explainability and Contestability in the Regulation of Public Sector AI","Explainability and its emerging counterpart contestability have become important normative and design principles for the trustworthy use of AI as they enable users and subjects to understand and challenge AI decisions. However, the regulation of AI systems spans technical, legal, and organizational dimensions, producing a multiplicity in meaning that complicates the implementation of explainability and contestability. Resolving this conceptual ambiguity requires specifying and comparing the meaning of both principles across regulation dimensions, disciplines, and actors. This process, here defined as translation, is essential to provide guidance on the principles' realization. We present the findings of a semi-structured interview study with 14 interdisciplinary AI regulation experts. We report on the experts' understanding of the intersection between explainability and contestability in public AI regulation, their advice for a decision subject and a public agency in a welfare allocation AI use case, and their perspectives on the connections and gaps within the research landscape. We provide differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further outline three translation processes in the alignment of top-down and bottom-up regulation, the assignment of responsibility for interpreting regulations, and the establishment of interdisciplinary collaboration. Our contributions include an empirically grounded conceptualization of the intersection between explainability and contestability and recommendations on implementing these principles in public institutions. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.",http://arxiv.org/abs/2504.18236v1,IS,Mixed
Beyond Platforms -- Growing Distributed Transaction Networks for Digital Commerce,"Many of today's IT infrastructures are proprietary platforms, like WhatsApp or Amazon. In some domains, like healthcare or finance, governments often take a strong regulatory role or even own the infrastructure. However, the biggest IT Infrastructure, the Internet itself, is run, evolved and governed in a cooperative manner. Decentralised architectures provide a number of advantages: They are potentially more inclusive for small players; more resilient in case of adversarial events, and seem to generate more innovation. However, we do not have much knowledge on how to evolve, adapt and govern decentralised infrastructures. This article reports empirical research on the development and governance of the Beckn Protocol, a protocol for decentralised transactions, and the successful development of domain-specific adaptations, their implementation and scaling. It explores how the architecture and governance support local innovation for specific business domains and how the domain-specific innovations and need feedback into the evolution of the protocol itself. The research applied a case study approach, combining interviews, document and code analysis. The article shows the possibility of such a decentralised approach to IT Infrastructures. It identifies a number of generativity mechanisms, socio-technical arrangements of the architecture, community support and governance that support adoption, innovation, and scaling it. It emphasises the governance of both the evolution of the open source specifications and software and how this relates to the governance of the conduct of network participants in operational networks. Finally, it emphasises the importance of feedback loops to both provide input for technical evolution and to recognise misconduct and develop means to address it.",http://arxiv.org/abs/2504.18602v1,IS,Mixed
DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent,"A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.",http://arxiv.org/abs/2504.17334v1,IS,Qualitative
Bridging Data Gaps and Building Knowledge Networks in Indian Football Analytics,"The global rise of football analytics has rapidly transformed how clubs make strategic decisions. However, in India, the adoption of analytics remains constrained by institutional resistance, infrastructural limitations, and cultural barriers -- challenges that grassroots innovation and low-cost data solutions have the potential to overcome. Despite the growing popularity of the Indian Super League, resource scarcity and fragmented governance continue to hinder the widespread adoption and impact of analytics. This mixed-methods study explores how informal, decentralised analytics communities -- comprising amateur analysts and Twitter-based ""data sleuths"" -- navigate these constraints through peer mentorship and grassroots innovation. Drawing on extensive digital ethnography, participant observation, and interviews, the study illustrates how these informal networks mitigate data scarcity, limited digital infrastructure, and institutional indifference while fostering skill development and professional growth. Building on these insights, the paper proposes HCI interventions such as decentralised knowledge platforms to facilitate structured, cross-border peer mentorship and low-cost data solutions -- including AI-assisted player tracking and mobile analytics dashboards -- rooted in principles of frugal innovation. These interventions aim to bridge the data divide, support inclusive technical engagement in sport, and enhance analytics-driven decision-making in resource-constrained environments. This paper contributes to HCIxB's focus on cross-border collaboration by highlighting how community-driven technological adaptation in the Global South can foster meaningful participation, skill-building, and long-term sustainability through informal learning networks and scalable, context-sensitive tools.",http://arxiv.org/abs/2504.16572v1,IS,Qualitative
Beyond Attention: Investigating the Threshold Where Objective Robot Exclusion Becomes Subjective,"As robots become increasingly involved in decision-making processes (e.g., personnel selection), concerns about fairness and social inclusion arise. This study examines social exclusion in robot-led group interviews by robot Ameca, exploring the relationship between objective exclusion (robot's attention allocation), subjective exclusion (perceived exclusion), mood change, and need fulfillment. In a controlled lab study (N = 35), higher objective exclusion significantly predicted subjective exclusion. In turn, subjective exclusion negatively impacted mood and need fulfillment but only mediated the relationship between objective exclusion and need fulfillment. A piecewise regression analysis identified a critical threshold at which objective exclusion begins to be perceived as subjective exclusion. Additionally, the standing position was the primary predictor of exclusion, whereas demographic factors (e.g., gender, height) had no significant effect. These findings underscore the need to consider both objective and subjective exclusion in human-robot interactions and have implications for fairness in robot-assisted hiring processes.",http://arxiv.org/abs/2504.15886v1,IS,Mixed
Under Pressure: Contextualizing Workplace Stress Towards User-Centered Interventions,"Stress is a pervasive challenge that significantly impacts worker health and well-being. Workplace stress is driven by various factors, ranging from organizational changes to poor workplace design. Although individual stress management strategies have been shown to be effective, current interventions often overlook personal and contextual factors shaping stress experiences. In this study, we conducted semi-structured interviews with eight office workers to gain a deeper understanding of their personal experiences with workplace stress. Our analysis reveals key stress triggers, coping mechanisms, and reflections on past stressful events. We highlight the multifaceted and individualized nature of workplace stress, emphasizing the importance of intervention timing, modality, and recognizing that stress is not solely a negative experience but can also have positive effects. Our findings provide actionable insights for the design of user-centered stress management solutions more attuned to the needs of office workers.",http://arxiv.org/abs/2504.15480v2,IS,Qualitative
Understanding Adolescents' Perceptions of Benefits and Risks in Health AI Technologies through Design Fiction,"Despite the growing research on users' perceptions of health AI, adolescents' perspectives remain underexplored. This study explores adolescents' perceived benefits and risks of health AI technologies in clinical and personal health settings. Employing Design Fiction, we conducted interviews with 16 adolescents (aged 13-17) using four fictional design scenarios that represent current and future health AI technologies as probes. Our findings reveal that with a positive yet cautious attitude, adolescents envision unique benefits and risks specific to their age group. While health AI technologies were seen as valuable learning resources, they also raised concerns about confidentiality with their parents. Additionally, we identified several factors, such as severity of health conditions and previous experience with AI, influencing their perceptions of trust and privacy in health AI. We explore how these insights can inform the future of design of health AI technologies to support learning, engagement, and trust as adolescents navigate their healthcare journey.",http://arxiv.org/abs/2504.13389v1,IS,Qualitative
Neurodiversity in Computing Education Research: A Systematic Literature Review,"Ensuring equitable access to computing education for all students-including those with autism, dyslexia, or ADHD-is essential to developing a diverse and inclusive workforce. To understand the state of disability research in computing education, we conducted a systematic literature review of research on neurodiversity in computing education. Our search resulted in 1,943 total papers, which we filtered to 14 papers based on our inclusion criteria. Our mixed-methods approach analyzed research methods, participants, contribution types, and findings. The three main contribution types included empirical contributions based on user studies (57.1%), opinion contributions and position papers (50%), and survey contributions (21.4%). Interviews were the most common methodology (75% of empirical contributions). There were often inconsistencies in how research methods were described (e.g., number of participants and interview and survey materials). Our work shows that research on neurodivergence in computing education is still very preliminary. Most papers provided curricular recommendations that lacked empirical evidence to support those recommendations. Three areas of future work include investigating the impacts of active learning, increasing awareness and knowledge about neurodiverse students' experiences, and engaging neurodivergent students in the design of pedagogical materials and computing education research.",http://arxiv.org/abs/2504.13058v1,IS,Mixed
The Jade Gateway to Trust: Exploring How Socio-Cultural Perspectives Shape Trust Within Chinese NFT Communities,"Today's world is witnessing an unparalleled rate of technological transformation. The emergence of non-fungible tokens (NFTs) has transformed how we handle digital assets and value. Despite their initial popularity, NFTs face declining adoption influenced not only by cryptocurrency volatility but also by trust dynamics within communities. From a social computing perspective, understanding these trust dynamics offers valuable insights for the development of both the NFT ecosystem and the broader digital economy. China presents a compelling context for examining these dynamics, offering a unique intersection of technological innovation and traditional cultural values. Through a content analysis of eight Chinese NFT-focused WeChat groups and 21 semi-structured interviews, we examine how socio-cultural factors influence trust formation and development. We found that trust in Chinese NFT communities is significantly molded by local cultural values. To be precise, Confucian virtues, such as benevolence, propriety, and integrity, play a crucial role in shaping these trust relationships. Our research identifies three critical trust dimensions in China's NFT market: (1) technological, (2) institutional, and (3) social. We examined the challenges in cultivating each dimension. Based on these insights, we developed tailored trust-building guidelines for Chinese NFT stakeholders. These guidelines address trust issues that factor into NFT's declining popularity and could offer valuable strategies for CSCW researchers, developers, and designers aiming to enhance trust in global NFT communities. Our research urges CSCW scholars to take into account the unique socio-cultural contexts when developing trust-enhancing strategies for digital innovations and online interactions.",http://arxiv.org/abs/2504.11928v1,IS,Qualitative
When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices,"Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work practices, perceptions, and relationships with customers and employers. We interviewed 30 domestic workers residing in the United States, who provided examples that highlight the insufficient transformative role of current online technologies in their work. By conducting a thematic analysis, we characterize how they approach and avoid these digital tools at different stages of their work. Through these findings, we investigate the limitations of technology and identify challenges and opportunities that could inform the design of more suitable tools to improve the conditions of this marginalized group.",http://arxiv.org/abs/2504.10265v1,IS,Qualitative
Cartographers in Cubicles: How Training and Preferences of Mapmakers Interplay with Structures and Norms in Not-for-Profit Organizations,"Choropleth maps are a common and effective way to visualize geographic thematic data. Although cartographers have established many principles about map design, data binning and color usage, less is known about how mapmakers make individual decisions in practice. We interview 16 cartographers and geographic information systems (GIS) experts from 13 government organizations, NGOs, and federal agencies about their choropleth mapmaking decisions and workflows. We categorize our findings and report on how mapmakers follow cartographic guidelines and personal rules of thumb, collaborate with other stakeholders within and outside their organization, and how organizational structures and norms are tied to decision-making during data preparation, data analysis, data binning, map styling, and map post-processing. We find several points of variation as well as regularity across mapmakers and organizations and present takeaways to inform cartographic education and practice, including broader implications and opportunities for CSCW, HCI, and information visualization researchers and practitioners.",http://arxiv.org/abs/2504.09438v3,IS,Qualitative
Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective,"Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to protect children under 16 years of age and not offering parental control features. This study explores families' use and co-use of GenAI, the perceived risks and opportunities of ChatGPT, and how parents mediate their children's use of GenAI. Through semi-structured interviews with 12 families, we identified ways families used and mediated GenAI and factors that influenced parents' GenAI mediation strategies. We contextualize our findings with a modified model of family mediation strategies, drawing from previous family media and mediation frameworks. We provide insights for future research on family-GenAI interactions and highlight the need for more robust protective measures on GenAI platforms for families.",http://arxiv.org/abs/2504.09004v2,IS,Qualitative
Disentangling Locality and Entropy in Ranking Distillation,"The training process of ranking models involves two key data selection decisions: a sampling strategy, and a labeling strategy. Modern ranking systems, especially those for performing semantic search, typically use a ``hard negative'' sampling strategy to identify challenging items using heuristics and a distillation labeling strategy to transfer ranking ""knowledge"" from a more capable model. In practice, these approaches have grown increasingly expensive and complex, for instance, popular pretrained rankers from SentenceTransformers involve 12 models in an ensemble with data provenance hampering reproducibility. Despite their complexity, modern sampling and labeling strategies have not been fully ablated, leaving the underlying source of effectiveness gains unclear. Thus, to better understand why models improve and potentially reduce the expense of training effective models, we conduct a broad ablation of sampling and distillation processes in neural ranking. We frame and theoretically derive the orthogonal nature of model geometry affected by example selection and the effect of teacher ranking entropy on ranking model optimization, establishing conditions in which data augmentation can effectively improve bias in a ranking model. Empirically, our investigation on established benchmarks and common architectures shows that sampling processes that were once highly effective in contrastive objectives may be spurious or harmful under distillation. We further investigate how data augmentation, in terms of inputs and targets, can affect effectiveness and the intrinsic behavior of models in ranking. Through this work, we aim to encourage more computationally efficient approaches that reduce focus on contrastive pairs and instead directly understand training dynamics under rankings, which better represent real-world settings.",http://arxiv.org/abs/2505.21058v1,IS,Quantitative
BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations,"In the domain of sponsored search advertising, the focus of Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.3 billion items.",http://arxiv.org/abs/2505.19164v1,IS,Quantitative
Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning,"In search advertising, keyword matching connects user queries with relevant ads. While token-based matching increases ad coverage, it can reduce relevance due to overly permissive semantic expansion. This work extends keyword reach through document-side semantic keyword expansion, using a language model to broaden token-level matching without altering queries. We propose a solution using a pre-trained siamese model to generate dense vector representations of ad keywords and identify semantically related variants through nearest neighbor search. To maintain precision, we introduce a cluster-based thresholding mechanism that adjusts similarity cutoffs based on local semantic density. Each expanded keyword maps to a group of seller-listed items, which may only partially align with the original intent. To ensure relevance, we enhance the downstream relevance model by adapting it to the expanded keyword space using an incremental learning strategy with a lightweight decision tree ensemble. This system improves both relevance and click-through rate (CTR), offering a scalable, low-latency solution adaptable to evolving query behavior and advertising inventory.",http://arxiv.org/abs/2505.18897v1,IS,Quantitative
TAPAS: A Pattern-Based Approach to Assessing Government Transparency,"Government transparency, widely recognized as a cornerstone of open government, depends on robust information management practices. Yet effective assessment of information management remains challenging, as existing methods fail to consider the actual working behavior of civil servants and are resource-intensive. Using a design science research approach, we present the Transparency Anti-Pattern Assessment System (TAPAS) -- a novel, data-driven methodology designed to evaluate government transparency through the identification of behavioral patterns that impede transparency. We demonstrate TAPAS's real-world applicability at a Dutch ministry, analyzing their electronic document management system data from the past two decades. We identify eight transparency anti-patterns grouped into four categories: Incomplete Documentation, Limited Accessibility, Unclear Information, and Delayed Documentation. We show that TAPAS enables continuous monitoring and provides actionable insights without requiring significant resource investments.",http://arxiv.org/abs/2505.16413v1,IS,Quantitative
MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding,"Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.",http://arxiv.org/abs/2505.15946v2,IS,Quantitative
A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach,"AI-based technologies have significant potential to enhance inclusive education and clinical-rehabilitative contexts for children with Special Educational Needs and Disabilities. AI can enhance learning experiences, empower students, and support both teachers and rehabilitators. However, their usage presents challenges that require a systemic-ecological vision, ethical considerations, and participatory research. Therefore, research and technological development must be rooted in a strong ethical-theoretical framework. The Capability Approach - a theoretical model of disability, human vulnerability, and inclusion - offers a more relevant perspective on functionality, effectiveness, and technological adequacy in inclusive learning environments. In this paper, we propose a participatory research strategy with different stakeholders through a case study on the ARTIS Project, which develops an AI-enriched interface to support children with text comprehension difficulties. Our research strategy integrates ethical, educational, clinical, and technological expertise in designing and implementing AI-based technologies for children's learning environments through focus groups and collaborative design sessions. We believe that this holistic approach to AI adoption in education can help bridge the gap between technological innovation and ethical responsibility.",http://arxiv.org/abs/2505.15466v1,IS,Qualitative
The Limits of Graph Samplers for Training Inductive Recommender Systems: Extended results,"Inductive Recommender Systems are capable of recommending for new users and with new items thus avoiding the need to retrain after new data reaches the system. However, these methods are still trained on all the data available, requiring multiple days to train a single model, without counting hyperparameter tuning. In this work we focus on graph-based recommender systems, i.e., systems that model the data as a heterogeneous network. In other applications, graph sampling allows to study a subgraph and generalize the findings to the original graph. Thus, we investigate the applicability of sampling techniques for this task. We test on three real world datasets, with three state-of-the-art inductive methods, and using six different sampling methods. We find that its possible to maintain performance using only 50% of the training data with up to 86% percent decrease in training time; however, using less training data leads to far worse performance. Further, we find that when it comes to data for recommendations, graph sampling should also account for the temporal dimension. Therefore, we find that if higher data reduction is needed, new graph based sampling techniques should be studied and new inductive methods should be designed.",http://arxiv.org/abs/2505.14241v1,IS,Quantitative
"""I will never pay for this"" Perception of fairness and factors affecting behaviour on 'pay-or-ok' models","The rise of cookie paywalls ('pay-or-ok' models) has prompted growing debates around the right to privacy and data protection, monetisation, and the legitimacy of user consent. Despite their increasing use across sectors, limited research has explored how users perceive these models or what shapes their decisions to either consent to tracking or pay. To address this gap, we conducted four focus groups (n= 14) to examine users' perceptions of cookie paywalls, their judgments of fairness, and the conditions under which they might consider paying, alongside a legal analysis within the EU data protection legal framework. Participants primarily viewed cookie paywalls as profit-driven, with fairness perceptions varying depending on factors such as the presence of a third option beyond consent or payment, transparency of data practices, and the authenticity or exclusivity of the paid content. Participants voiced expectations for greater transparency, meaningful control over data collection, and less coercive alternatives, such as contextual advertising or ""reject all"" buttons. Although some conditions, including trusted providers, exclusive content, and reasonable pricing, could make participants consider paying, most expressed reluctance or unwillingness to do so. Crucially, our findings raise concerns about economic exclusion, where privacy and data protection might end up becoming a privilege rather than fundamental rights. Consent given under financial pressure may not meet the standard of being freely given, as required by GDPR. To address these concerns, we recommend user-centred approaches that enhance transparency, reduce coercion, ensure the value of paid content, and explore inclusive alternatives. These measures are essential for supporting fairness, meaningful choice, and user autonomy in consent-driven digital environments.",http://arxiv.org/abs/2505.12892v3,IS,Qualitative
Beyond Individual UX: Defining Group Experience(GX) as a New Paradigm for Group-centered AI,"Recent advancements in HCI and AI have predominantly centered on individual user experiences, often neglecting the emergent dynamics of group interactions. This provocation introduces Group Experience(GX) to capture the collective perceptual, emotional, and cognitive dimensions that arise when individuals interact in cohesive groups. We challenge the conventional Human-centered AI paradigm and propose Group-centered AI(GCAI) as a framework that actively mediates group dynamics, amplifies diverse voices, and fosters ethical collective decision-making. Drawing on social psychology, organizational behavior, and group dynamics, we outline a group-centered design approach that balances individual autonomy with collective interests while developing novel evaluative metrics. Our analysis emphasizes rethinking traditional methodologies that focus solely on individual outcomes and advocates for innovative strategies to capture group collaboration. We call on researchers to bridge the gap between micro-level experiences and macro-level impacts, ultimately enriching and transforming collaborative human interactions.",http://arxiv.org/abs/2505.12780v1,IS,Quantitative
FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation,"Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/youlei202/FairSHAP.",http://arxiv.org/abs/2505.11111v1,IS,Quantitative
Pedestrian mobility citizen science complements expert mapping for enhancing inclusive neighborhood placemaking,"Cities are complex systems that demand integrated approaches, with increasing attention focused on the neighborhood level. This study examines the interplay between expert-based mapping and citizen science in the Primer de Maig neighborhood of Granollers, Catalonia, Spain--an area marked by poor-quality public spaces and long-standing socio-economic challenges. Seventy-two residents were organized into 19 groups to record their pedestrian mobility while engaging in protocolized playful social actions. Their GPS identified opportunity units for meaningful public space activation. Although 56% of observed actions occurred within expert-defined units, the remaining 44% took place elsewhere. Clustering analysis of geo-located action stops revealed seven distinct clusters, highlighting overlooked areas with significant social potential. These findings underscore the complementarity of top-down and bottom-up approaches, demonstrating how citizen science and community science approaches enriches urban diagnostics by integrating subjective, community-based perspectives in public space placemaking and informing inclusive, adaptive sustainable urban transformation strategies.",http://arxiv.org/abs/2505.11098v2,IS,Mixed
Explain What You Mean: Intent Augmented Knowledge Graph Recommender Built With An LLM,"Interaction sparsity is a long-standing challenge in recommendation systems. Sparsity manifests in environments with disproportional cardinality of groupings of entities, such as users and products in an online marketplace. It is also found for newly introduced entities, described as the cold-start problem. Recent efforts to mitigate this issue either enrich the connectivity data by incorporating social networks or external knowledge graphs, or fine-tune LLMs into interaction augmenters or next-item recommenders. However, these techniques tend to be resource demanding, requiring high computational power. They also have several limitations, including data availability, low quality, or synthetic noise issues. In this work, we propose LLM-based Intent Knowledge Graph Recommender (IKGR), a novel framework that leverages retrieval-augmented generation and an encoding approach to construct and densify a knowledge graph. IKGR leverages latent user-item affinities from an interaction knowledge graph and further densifies it through mutual intent connectivity. This addresses sparsity issues and allows the model to make intent-grounded recommendations with an interpretable embedding translation layer. Through extensive experiments on real-world datasets, we demonstrate that IKGR overcomes knowledge gaps and achieves substantial gains over state-of-the-art baselines on both publicly available and our internal recommendation datasets.",http://arxiv.org/abs/2505.10900v2,IS,Quantitative
Enhancing Collaboration Through Google Workspace: Assessing and Strengthening Current Practices,"This study investigates the effectiveness of Google Workspace in fostering collaboration within academic settings, specifically at the University of Makati. The aim is to evaluate its role in enhancing blended learning practices and identify areas for improvement among faculty, staff, and students. A survey was conducted with 50 participants, including academic staff, faculty, and students at the University of Makati who regularly use Google Workspace for academic and collaborative activities. Participants were selected through purposive sampling to ensure familiarity with the platform. The study employed a quantitative research design using structured surveys to assess user experiences with key features such as real-time document editing, communication tools, etc. The study found that Google Workspace and rated as ""Very Effective"" (mean score of 4.61) in promoting teamwork. Key advantages included improved collaboration, enhanced communication, and efficient management of group projects. However, several challenges were also noted, including low user adoption rates, limited Google Drive storage capacity, the need for better technical support, and limited offline functionality. Google Workspace significantly supports academic collaboration in the normal practices within the University of Makati, however, it faces challenges that impact its overall effectiveness. Addressing these issues could improve user experience and platform efficiency in educational contexts. It is recommended to enhance user adoption through targeted training and improve offline capabilities. Additionally, providing more advanced technical support could mitigate existing challenges.",http://arxiv.org/abs/2505.10598v1,IS,Quantitative
Evaluation Metrics for Misinformation Warning Interventions: Challenges and Prospects,"Misinformation has become a widespread issue in the 21st century, impacting numerous areas of society and underscoring the need for effective intervention strategies. Among these strategies, user-centered interventions, such as warning systems, have shown promise in reducing the spread of misinformation. Many studies have used various metrics to evaluate the effectiveness of these warning interventions. However, no systematic review has thoroughly examined these metrics in all studies. This paper provides a comprehensive review of existing metrics for assessing the effectiveness of misinformation warnings, categorizing them into four main groups: behavioral impact, trust and credulity, usability, and cognitive and psychological effects. Through this review, we identify critical challenges in measuring the effectiveness of misinformation warnings, including inconsistent use of cognitive and attitudinal metrics, the lack of standardized metrics for affective and emotional impact, variations in user trust, and the need for more inclusive warning designs. We present an overview of these metrics and propose areas for future research.",http://arxiv.org/abs/2505.09526v1,IS,Mixed
Simulating Ethics: Using LLM Debate Panels to Model Deliberation on Medical Dilemmas,"This paper introduces ADEPT, a system using Large Language Model (LLM) personas to simulate multi-perspective ethical debates. ADEPT assembles panels of 'AI personas', each embodying a distinct ethical framework or stakeholder perspective (like a deontologist, consequentialist, or disability rights advocate), to deliberate on complex moral issues. Its application is demonstrated through a scenario about prioritizing patients for a limited number of ventilators inspired by real-world challenges in allocating scarce medical resources. Two debates, each with six LLM personas, were conducted; they only differed in the moral viewpoints represented: one included a Catholic bioethicist and a care theorist, the other substituted a rule-based Kantian philosopher and a legal adviser. Both panels ultimately favoured the same policy -- a lottery system weighted for clinical need and fairness, crucially avoiding the withdrawal of ventilators for reallocation. However, each panel reached that conclusion through different lines of argument, and their voting coalitions shifted once duty- and rights-based voices were present. Examination of the debate transcripts shows that the altered membership redirected attention toward moral injury, legal risk and public trust, which in turn changed four continuing personas' final positions. The work offers three contributions: (i) a transparent, replicable workflow for running and analysing multi-agent AI debates in bioethics; (ii) evidence that the moral perspectives included in such panels can materially change the outcome even when the factual inputs remain constant; and (iii) an analysis of the implications and future directions for such AI-mediated approaches to ethical deliberation and policy.",http://arxiv.org/abs/2505.21112v1,IS,Quantitative
TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation,"Dataset transparency is a key enabler of responsible AI, but insights into multimodal dataset attributes that impact trustworthy and ethical aspects of AI applications remain scarce and are difficult to compare across datasets. To address this challenge, we introduce Trustworthy and Ethical Dataset Indicators (TEDI) that facilitate the systematic, empirical analysis of dataset documentation. TEDI encompasses 143 fine-grained indicators that characterize trustworthy and ethical attributes of multimodal datasets and their collection processes. The indicators are framed to extract verifiable information from dataset documentation. Using TEDI, we manually annotated and analyzed over 100 multimodal datasets that include human voices. We further annotated data sourcing, size, and modality details to gain insights into the factors that shape trustworthy and ethical dimensions across datasets. We find that only a select few datasets have documented attributes and practices pertaining to consent, privacy, and harmful content indicators. The extent to which these and other ethical indicators are addressed varies based on the data collection method, with documentation of datasets collected via crowdsourced and direct collection approaches being more likely to mention them. Scraping dominates scale at the cost of ethical indicators, but is not the only viable collection method. Our approach and empirical insights contribute to increasing dataset transparency along trustworthy and ethical dimensions and pave the way for automating the tedious task of extracting information from dataset documentation in future.",http://arxiv.org/abs/2505.17841v1,IS,Quantitative
Multimodal AI-based visualization of strategic leaders' emotional dynamics: a deep behavioral analysis of Trump's trade war discourse,"This study investigates the emotional rhythms and behavioral mechanisms of dominant political leaders in strategic decision-making. Using the Trump administration's 125 percent tariff hike on China as a case, it adopts a Multimodal Cognitive Behavioral Modeling framework. This includes micro-expression tracking, acoustic intonation analysis, semantic flow modeling, cognitive load simulation, and strategic behavior mapping to construct a full-cycle simulation of emotion, motivation, and output. Results reveal that Trump's decisions are not driven by rational deduction, but emerge from dominance-coherence rhythms. A six-axis National Strategic Tempo Intervention Framework is proposed to support anticipatory policy modeling.",http://arxiv.org/abs/2505.16274v1,IS,Quantitative
Stress Bytes: Decoding the Associations between Internet Use and Perceived Stress,"In today's digital era, internet plays a pervasive role in our lives, influencing everyday activities such as communication, work, and leisure. This online engagement intertwines with offline experiences, shaping individuals' overall well-being. Despite its significance, existing research often falls short in capturing the relationship between internet use and well-being, relying primarily on isolated studies and self-reported data. One of the major contributors to deteriorated well-being - both physical and mental - is stress. While some research has examined the relationship between internet use and stress, both positive and negative associations have been reported. Our primary goal in this work is to identify the associations between an individual's internet use and their stress. For achieving our goal, we conducted a longitudinal multimodal study that spanned seven months. We combined fine-grained URL-level web browsing traces of 1490 German internet users with their sociodemographics and monthly measures of stress. Further, we developed a conceptual framework that allows us to simultaneously explore different contextual dimensions, including how, where, when, and by whom the internet is used. Our analysis revealed several associations between internet use and stress that vary by context. Social media, entertainment, online shopping, and gaming were positively associated with stress, while productivity, news, and adult content use were negatively associated. In the future, the behavioral markers we identified can pave the way for designing individualized tools for people to self-monitor and self-moderate their online behaviors to enhance their well-being, reducing the burden on already overburdened mental health services.",http://arxiv.org/abs/2505.15377v1,IS,Quantitative
Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems,"This position paper argues that the theoretical inconsistency often observed among Responsible AI (RAI) metrics, such as differing fairness definitions or tradeoffs between accuracy and privacy, should be embraced as a valuable feature rather than a flaw to be eliminated. We contend that navigating these inconsistencies, by treating metrics as divergent objectives, yields three key benefits: (1) Normative Pluralism: Maintaining a full suite of potentially contradictory metrics ensures that the diverse moral stances and stakeholder values inherent in RAI are adequately represented. (2) Epistemological Completeness: The use of multiple, sometimes conflicting, metrics allows for a more comprehensive capture of multifaceted ethical concepts, thereby preserving greater informational fidelity about these concepts than any single, simplified definition. (3) Implicit Regularization: Jointly optimizing for theoretically conflicting objectives discourages overfitting to one specific metric, steering models towards solutions with enhanced generalization and robustness under real-world complexities. In contrast, efforts to enforce theoretical consistency by simplifying or pruning metrics risk narrowing this value diversity, losing conceptual depth, and degrading model performance. We therefore advocate for a shift in RAI theory and practice: from getting trapped in inconsistency to characterizing acceptable inconsistency thresholds and elucidating the mechanisms that permit robust, approximated consistency in practice.",http://arxiv.org/abs/2505.18139v1,IS,Quantitative
"A Toolkit for Compliance, a Toolkit for Justice: Drawing on Cross-sectoral Expertise to Develop a Pro-justice EU AI Act Toolkit","The introduction of the AI Act in the European Union presents the AI research and practice community with a set of new challenges related to compliance. While it is certain that AI practitioners will require additional guidance and tools to meet these requirements, previous research on toolkits that aim to translate the theory of AI ethics into development and deployment practice suggests that such resources suffer from multiple limitations. These limitations stem, in part, from the fact that the toolkits are either produced by industry-based teams or by academics whose work tends to be abstract and divorced from the realities of industry. In this paper, we discuss the challenge of developing an AI ethics toolkit for practitioners that helps them comply with new AI-focused regulation, but that also moves beyond mere compliance to consider broader socio-ethical questions throughout development and deployment. The toolkit was created through a cross-sectoral collaboration between an academic team based in the UK and an industry team in Italy. We outline the background and rationale for creating a pro-justice AI Act compliance toolkit, detail the process undertaken to develop it, and describe the collaboration and negotiation efforts that shaped its creation. We aim for the described process to serve as a blueprint for other teams navigating the challenges of academia-industry partnerships and aspiring to produce usable and meaningful AI ethics resources.",http://arxiv.org/abs/2505.17165v1,IS,Mixed
CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation,"Existing metrics often lack the granularity and interpretability to capture nuanced clinical differences between candidate and ground-truth radiology reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded tabular framework with Expert-curated labels and Attribute-level comparison for Radiology report evaluation (CLEAR). CLEAR not only examines whether a report can accurately identify the presence or absence of medical conditions, but also assesses whether it can precisely describe each positively identified condition across five key attributes: first occurrence, change, severity, descriptive location, and recommendation. Compared to prior works, CLEAR's multi-dimensional, attribute-level outputs enable a more comprehensive and clinically interpretable evaluation of report quality. Additionally, to measure the clinical alignment of CLEAR, we collaborate with five board-certified radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions. Our experiments show that CLEAR achieves high accuracy in extracting clinical attributes and provides automated metrics that are strongly aligned with clinical judgment.",http://arxiv.org/abs/2505.16325v1,IS,Quantitative
"Conceptual Modeling: Topics, Themes, and Technology Trends","Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed.",http://arxiv.org/abs/2505.13648v1,IS,Quantitative
Interpretable Risk Mitigation in LLM Agent Systems,"Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms.",http://arxiv.org/abs/2505.10670v1,IS,Quantitative
Emotion-sensitive Explanation Model,"Explainable AI (XAI) research has traditionally focused on rational users, aiming to improve understanding and reduce cognitive biases. However, emotional factors play a critical role in how explanations are perceived and processed. Prior work shows that prior and task-generated emotions can negatively impact the understanding of explanation. Building on these insights, we propose a three-stage model for emotion-sensitive explanation grounding: (1) emotional or epistemic arousal, (2) understanding, and (3) agreement. This model provides a conceptual basis for developing XAI systems that dynamically adapt explanation strategies to users emotional states, ultimately supporting more effective and user-centered decision-making.",http://arxiv.org/abs/2505.10454v2,IS,Quantitative
Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights,"Charts are crucial for data analysis and decision-making.Text-to-chart retrieval systems have become increasingly important for Business Intelligence (BI), where users need to find relevant charts that match their analytical needs. These needs can be categorized into precise queries that are well-specified and fuzzy queries that are more exploratory -- both require understanding the semantics and context of the charts. However, existing text-to-chart retrieval solutions often fail to capture the semantic content and contextual information of charts, primarily due to the lack of comprehensive metadata (or semantic insights). To address this limitation, we propose a training data development pipeline that automatically synthesizes hierarchical semantic insights for charts, covering visual patterns (visual-oriented), statistical properties (statistics-oriented), and practical applications (task-oriented), which produces 207,498 semantic insights for 69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to learn better representations of charts for text-to-chart retrieval. Our method leverages rich semantic insights during the training phase to develop a model that understands both visual and semantic aspects of charts.To evaluate text-to-chart retrieval performance, we curate the first benchmark, CRBench, for this task with 21,862 charts and 326 text queries from real-world BI applications, with ground-truth labels verified by the crowd workers.Experiments show that ChartFinder significantly outperforms existing methods in text-to-chart retrieval tasks across various settings. For precise queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than state-of-the-art models. In fuzzy query tasks, our method also demonstrates consistent improvements, with an average increase of 5% across nearly all metrics.",http://arxiv.org/abs/2505.10043v2,IS,Quantitative
Determining Absence of Unreasonable Risk: Approval Guidelines for an Automated Driving System Release,"This paper provides an overview of how the determination of absence of unreasonable risk can be operationalized. It complements previous theoretical work published by existing developers of Automated Driving Systems (ADS) on the overall engineering practices and methodologies for readiness determination. Readiness determination is, at its core, a risk assessment process. It is aimed at evaluating the residual risk associated with the deployment of a new software release candidate. The paper proposes methodological criteria to ground the readiness review process for an ADS release. While informed by Waymo's experience in this domain, the criteria presented are agnostic of any specific ADS technological solution and/or architectural choice, to support broad implementation by others in the industry. The paper continues with a discussion on governance and decision-making toward approval of a new software release candidate for the ADS. The implementation of the presented criteria requires the existence of appropriate safety management practices in addition to many other cultural, procedural, and operational considerations. As such, the paper is concluded by a statement of limitations for those wishing to replicate part or all of its content.",http://arxiv.org/abs/2505.09880v1,IS,Mixed
Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems,"A major bottleneck in multi-agent AI is the lack of simulateable models for the bottom-up emergence of social structure under realistic behavioral constraints. Similarly, many foundational theories in economics and sociology including the concepts of ""institutions"" and ""norms"" tend to describe social structures post hoc, often relying on implicit assumptions of shared culture, morality, or symbolic agreement. These concepts are often treated as primitives rather than reconstructed from agent-level behavior, leaving both their origins and operational definitions under-specified. To address this, we propose a three-stage bottom-up framework: Reciprocal Dynamics, capturing individual-level reciprocal exchanges; Norm Stabilization, the consolidation of shared expectations; and Institutional Construction, the externalization of stable patterns into scalable structures. By grounding social emergence in agent-level reciprocity, our framework enables the systematic exploration of how moral, cultural, and institutional structures emerge from cognitively minimal interactions.",http://arxiv.org/abs/2505.08319v1,IS,Quantitative
"Open AI-Romance with ChatGPT, Ready for Your Cyborg Lover?","Since late March 2024, a Chinese college student has shared her AI Romance with ChatGPT on Red, a popular Chinese social media platform, attracting millions of followers and sparking numerous imitations. This phenomenon has created an iconic figure among Chinese youth, particularly females. This study employs a case study and digital ethnography approach seeking to understand how technology (social media, generative AI) shapes Chinese female students' engagement with AI Romance and how AI Romance impacts the reshaping of gender power relations of Chinese female college students. There are three main findings. First, Open AI Romance is performative, mutually shaping, and creates flexible gender power dynamics and potential new configurations. Second, the cyborg lover identity is fluid, shared, and partially private due to technology and social platforms. Third, the rise of ChatGPT's DAN mode on Red introduces a simulated ""male"" app into a ""female"" platform, pushing the limits of policy guidelines, and social norms, making the platform even ""wilder."" This research provides a deeper understanding of the intersection between technology and social behavior, highlighting the role of AI and social media in evolving gender dynamics among Chinese youth. It sheds light on the performative nature of digital interactions and the potential for technology to redefine traditional gender power structures.",http://arxiv.org/abs/2410.03710v1,IS,Qualitative
Sphere Window: Challenges and Opportunities of 360° Video in Collaborative Design Workshops,"The increased ubiquity of 360{\deg} video presents a unique opportunity for designers to deeply engage with the world of users by capturing the complete visual context. However, the opportunities and challenges 360{\deg} video introduces for video design ethnography is unclear. This study investigates this gap through 16 workshops in which experienced designers engaged with 360{\deg} video. Our analysis shows that while 360{\deg} video enhances designers' ability to explore and understand user contexts, it also complicates the process of sharing insights. To address this challenge, we present two opportunities to support the use of 360{\deg} video by designers - the creation of designerly 360{\deg} video annotation tools, and 360{\deg} ``screenshots'' - in order to enable designers to leverage the complete context of 360{\deg} video for user research.",http://arxiv.org/abs/2407.12407v1,IS,Qualitative
"Like-minded, like-bodied: How users (18-26) trust online eating and health information","This paper investigates the relationship between social media and eating practices amongst 42 internet users aged 18-26. We conducted an ethnography in the US and India to observe how they navigated eating and health information online. We found that participants portrayed themselves online through a vocabulary we have labeled ""the good life"": performing holistic health by displaying a socially-ideal body. In doing so, participants unconsciously engaged in behaviors of disordered eating while actively eschewing them. They also valued personal testimonies, and readily tested tips from content creators who shared similar beliefs and bodies to them. In doing so, they discarded probabilistic thinking and opened themselves to harm. Our study found that their social media feeds did not unidirectionally influence participants - they also reflected participants' internalized views of health, in an intertwined, non-linear journey. Reducing the online spread of disordered eating practices requires addressing it within young people's social context.",http://arxiv.org/abs/2402.18753v1,IS,Qualitative
Proxy Design: A Method for Involving Proxy Users to Speak on Behalf of Vulnerable or Unreachable Users in Co-Design,"Designing digital artifacts is not a linear, straightforward process. This is particularly true when applying a user-centered design approach, or co-design, with users who are unable to participate in the design process. Although the reduced participation of a particular user group may harm the end result, the literature on solving this issue is sparse. In this article, proxy design is outlined as a method for involving a user group as proxy users to speak on behalf of a group that is difficult to reach. We present a design ethnography spanning three years at a cancer rehabilitation clinic, where digital artifacts were designed to be used collaboratively by nurses and patients. The empirical data were analyzed using content analysis and consisted of 20 observation days at the clinic, six proxy design workshops, 21 telephone consultations between patients and nurses, and log data from the digital artifact. We show that simulated consultations, with nurses roleplaying as proxies for patients ignited and initiated the design process and enabled an efficient in-depth understanding of patients. Moreover, we reveal how proxy design as a method further expanded the design. We illustrate: (1) proxy design as a method for initiating design, (2) proxy design as an embedded element in co-design and (3) six design guidelines that should be considered when engaging in proxy design. The main contribution is the conceptualization of proxy design as a method that can ignite and initiate the co-design process when important users are unreachable, vulnerable or unable to represent themselves in the co-design process. Based on the empirical findings from a design ethnography that involved nurses as proxy users speaking on behalf of patients, the article shows that roleplaying in proxy design is a fitting way of initiating the design process, outlining proxy design as an embedded element of co-design.",http://arxiv.org/abs/2310.18240v1,IS,Mixed
"Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-level Decision-Making","Algorithms in public services such as child welfare, criminal justice, and education are increasingly being used to make high-stakes decisions about human lives. Drawing upon findings from a two-year ethnography conducted at a child welfare agency, we highlight how algorithmic systems are embedded within a complex decision-making ecosystem at critical points of the child welfare process. Caseworkers interact with algorithms in their daily lives where they must collect information about families and feed it to algorithms to make critical decisions. We show how the interplay between systemic mechanics and algorithmic decision-making can adversely impact the fairness of the decision-making process itself. We show how functionality issues in algorithmic systems can lead to process-oriented harms where they adversely affect the nature of professional practice, and administration at the agency, and lead to inconsistent and unreliable decisions at the street level. In addition, caseworkers are compelled to undertake additional labor in the form of repair work to restore disrupted administrative processes and decision-making, all while facing organizational pressures and time and resource constraints. Finally, we share the case study of a simple algorithmic tool that centers caseworkers' decision-making within a trauma-informed framework and leads to better outcomes, however, required a significant amount of investments on the agency's part in creating the ecosystem for its proper use.",http://arxiv.org/abs/2308.05224v1,IS,Qualitative
Technology in Association With Mental Health: Meta-ethnography,"This research paper presents a meta-analysis of the multifaceted role of technology in mental health. The pervasive influence of technology on daily lives necessitates a deep understanding of its impact on mental health services. This study synthesizes literature covering Behavioral Intervention Technologies (BITs), digital mental health interventions during COVID-19, young men's attitudes toward mental health technologies, technology-based interventions for university students, and the applicability of mobile health technologies for individuals with serious mental illnesses. BITs are recognized for their potential to provide evidence-based interventions for mental health conditions, especially anxiety disorders. The COVID-19 pandemic acted as a catalyst for the adoption of digital mental health services, underscoring their crucial role in providing accessible and quality care; however, their efficacy needs to be reinforced by workforce training, high-quality evidence, and digital equity. A nuanced understanding of young men's attitudes toward mental health is imperative for devising effective online services. Technology-based interventions for university students are promising, although variable in effectiveness; their deployment must be evidence-based and tailored to individual needs. Mobile health technologies, particularly activity tracking, hold promise for individuals with serious mental illnesses. Collectively, technology has immense potential to revolutionize mental health care. However, the implementation must be evidence-based, ethical, and equitable, with continued research focusing on experiences across diverse populations, ensuring accessibility and efficacy for all.",http://arxiv.org/abs/2307.10513v2,IS,Quantitative
"A ""Distance Matters"" Paradox: Facilitating Intra-Team Collaboration Can Harm Inter-Team Collaboration","By identifying the socio-technical conditions required for teams to work effectively remotely, the Distance Matters framework has been influential in CSCW since its introduction in 2000. Advances in collaboration technology and practices have since brought teams increasingly closer to achieving these conditions. This paper presents a ten-month ethnography in a remote organization, where we observed that despite exhibiting excellent remote collaboration, teams paradoxically struggled to collaborate across team boundaries. We extend the Distance Matters framework to account for inter-team collaboration, arguing that challenges analogous to those in the original intra-team framework -- common ground, collaboration readiness, collaboration technology readiness, and coupling of work -- persist but are actualized differently at the inter-team scale. Finally, we identify a fundamental tension between the intra- and inter-team layers: the collaboration technology and practices that help individual teams thrive (e.g., adopting customized collaboration software) can also prompt collaboration challenges in the inter-team layer, and conversely the technology and practices that facilitate inter-team collaboration (e.g., strong centralized IT organizations) can harm practices at the intra-team layer. The addition of the inter-team layer to the Distance Matters framework opens new opportunities for CSCW, where balancing the tension between team and organizational collaboration needs will be a critical technological, operational, and organizational challenge for remote work in the coming decades.",http://arxiv.org/abs/2202.02484v1,IS,Qualitative
Private Power and Public Interests: An Ethnographic Examination of the Power Outages in Texas in February 2021,"In 21st century America, to many observers, the idea that 10's of millions of Americans could lose power and heat for multiple days in the middle of a record cold snap, was unthinkable. It came as an even greater surprise that it would be Texas - arguably one of the world's energy capitals - that failed to provide sufficient power to its residents. This paper explores the events that led to the outage, the experiences of those who lived through it, and the situation in Texas one to two months after the event. We have taken an ethnographic approach to capture both the empirical aspects of the situation, and the more interpretive descriptions of the accounts and thoughts of the participants. We believe this ethnography of events in Texas can serve as foundational evidence and therefore can be generalized to a wide variety of situations and methodologies.",http://arxiv.org/abs/2108.02224v2,IS,Mixed
Operationalizing Conflict and Cooperation between Automated Software Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight',"This paper replicates, extends, and refutes conclusions made in a study published in PLoS ONE (""Even Good Bots Fight""), which claimed to identify substantial levels of conflict between automated software agents (or bots) in Wikipedia using purely quantitative methods. By applying an integrative mixed-methods approach drawing on trace ethnography, we place these alleged cases of bot-bot conflict into context and arrive at a better understanding of these interactions. We found that overwhelmingly, the interactions previously characterized as problematic instances of conflict are typically better characterized as routine, productive, even collaborative work. These results challenge past work and show the importance of qualitative/quantitative collaboration. In our paper, we present quantitative metrics and qualitative heuristics for operationalizing bot-bot conflict. We give thick descriptions of kinds of events that present as bot-bot reverts, helping distinguish conflict from non-conflict. We computationally classify these kinds of events through patterns in edit summaries. By interpreting found/trace data in the socio-technical contexts in which people give that data meaning, we gain more from quantitative measurements, drawing deeper understandings about the governance of algorithmic systems in Wikipedia. We have also released our data collection, processing, and analysis pipeline, to facilitate computational reproducibility of our findings and to help other researchers interested in conducting similar mixed-method scholarship in other platforms and contexts.",http://arxiv.org/abs/1810.07273v1,IS,Mixed
Open-coopetition in the PC and mobile industries: the WebKit case,"In an era of software crisis, the move of firms towards geographically-distributed software development teams is being challenged by collaboration issues. On this matter, the open-source phenomenon may shed some light, as successful cases on distributed collaboration in the open-source community have been recurrently reported. While practitioners move with difficulties towards globally distributed software development, there is a lack of research addressing the collaboration dynamics of large-scale distributed software projects. More particularly, even if there are empirical manifestations of collaboration among software-houses that market rival-software products within the same market, there is a clear lack of research addressing the development of information systems by coopetitive manners. While addressing a previous call for the advancement of methods and techniques to support the visualization of temporal aspects to represent change and evolution in ecosystems, we combined and virtual-ethnography (VA) with a Social Network Analysis (SNA) over publicly-available and naturally-occurring data that allowed us to re-construct and visualize the evolution of the WebKit collaboration in a sequence of networks. We started by screening, publicly available data such as company announcements, financial reports and specialized-press that allowed us to gain insights of the industrial context. After attaining a better understanding of the the competitive dynamics of the mobile-devices and PC industries, we started extracting and analyzing the social network of the WebKit community leveraging SNA, which is an emergent method widely established across disciplines of social sciences in general.Our case confirmed much of the established literature on coopetition, but more detailed observations revealed that not all coopetition theoretical propositions can be generalized to the open-source arena.",http://arxiv.org/abs/1409.2255v1,IS,Mixed
Development of Digital Twin Environment through Integration of Commercial Metaverse Platform and IoT Sensors of Smart Building,"The digital transformation of smart cities and workplaces requires effective integration of physical and cyber spaces, yet existing digital twin solutions remain limited in supporting real-time, multi-user collaboration. While metaverse platforms enable shared virtual experiences, they have not supported comprehensive integration of IoT sensors on physical spaces, especially for large-scale smart architectural environments. This paper presents a digital twin environment that integrates Kajima Corp.'s smart building facility ""The GEAR"" in Singapore with a commercial metaverse platform Cluster. Our system consists of three key components: a standardized IoT sensor platform, a real-time data relay system, and an environmental data visualization framework. Quantitative end-to-end latency measurements confirm the feasibility of our approach for real-world applications in large architectural spaces. The proposed framework enables new forms of collaboration that transcend spatial constraints, advancing the development of next-generation interactive environments.",http://arxiv.org/abs/2505.15089v1,IS,Quantitative
Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training,"Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.",http://arxiv.org/abs/2505.14681v2,IS,Quantitative
Exploring Large Quantities of Secondary Data from High-Resolution Synchrotron X-ray Computed Tomography Scans Using AccuStripes,"The analysis of secondary quantitative data extracted from high-resolution synchrotron X-ray computed tomography scans represents a significant challenge for users. While a number of methods have been introduced for processing large three-dimensional images in order to generate secondary data, there are only a few techniques available for simple and intuitive visualization of such data in their entirety. This work employs the AccuStripes visualization technique for that purpose, which enables the visual analysis of secondary data represented by an ensemble of univariate distributions. It supports different schemes for adaptive histogram binnings in combination with several ways of rendering aggregated data and it allows the interactive selection of optimal visual representations depending on the data and the use case. We demonstrate the usability of AccuStripes on a high-resolution synchrotron scan of a particle-reinforced metal matrix composite sample, containing more than 20 million particles. Through AccuStripes, detailed insights are facilitated into distributions of derived particle characteristics of the entire sample. Furthermore, research questions such as how the overall shape of the particles is or how homogeneously they are distributed across the sample can be answered.",http://arxiv.org/abs/2505.10098v1,IS,Quantitative
Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations,"In the context of the new mandatory labor compliance in the European Union (EU), which will be implemented in 2027, supply chain enterprises face stringent working hour management requirements and compliance risks. In order to scientifically predict the enterprises' coping behaviors and performance outcomes under the policy impact, this paper constructs a methodological framework that integrates the AI synthetic data generation mechanism and structural path regression modeling to simulate the enterprises' strategic transition paths under the new regulations. In terms of research methodology, this paper adopts high-quality simulation data generated based on Monte Carlo mechanism and NIST synthetic data standards to construct a structural path analysis model that includes multiple linear regression, logistic regression, mediation effect and moderating effect. The variable system covers 14 indicators such as enterprise working hours, compliance investment, response speed, automation level, policy dependence, etc. The variable set with explanatory power is screened out through exploratory data analysis (EDA) and VIF multicollinearity elimination. The findings show that compliance investment has a significant positive impact on firm survival and its effect is transmitted through the mediating path of the level of intelligence; meanwhile, firms' dependence on the EU market significantly moderates the strength of this mediating effect. It is concluded that AI synthetic data combined with structural path modeling provides an effective tool for high-intensity regulatory simulation, which can provide a quantitative basis for corporate strategic response, policy design and AI-assisted decision-making in the pre-prediction stage lacking real scenario data. Keywords: AI synthetic data, structural path regression modeling, compliance response strategy, EU 2027 mandatory labor regulation",http://arxiv.org/abs/2505.06261v1,IS,Quantitative
Jekyll-and-Hyde Tipping Point in an AI's Behavior,"Trust in AI is undermined by the fact that there is no science that predicts -- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is likely to tip mid-response to become wrong, misleading, irrelevant or dangerous. With deaths and trauma already being blamed on LLMs, this uncertainty is even pushing people to treat their 'pet' LLM more politely to 'dissuade' it (or its future Artificial General Intelligence offspring) from suddenly turning on them. Here we address this acute need by deriving from first principles an exact formula for when a Jekyll-and-Hyde tipping point occurs at LLMs' most basic level. Requiring only secondary school mathematics, it shows the cause to be the AI's attention spreading so thin it suddenly snaps. This exact formula provides quantitative predictions for how the tipping-point can be delayed or prevented by changing the prompt and the AI's training. Tailored generalizations will provide policymakers and the public with a firm platform for discussing any of AI's broader uses and risks, e.g. as a personal counselor, medical advisor, decision-maker for when to use force in a conflict situation. It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''",http://arxiv.org/abs/2504.20980v1,IS,Quantitative
"Bitcoin, a DAO?","This paper investigates whether Bitcoin can be regarded as a decentralized autonomous organization (DAO), what insights it may offer for the broader DAO ecosystem, and how Bitcoin governance can be improved. First, a quantitative literature analysis reveals that Bitcoin is increasingly overlooked in DAO research, even though early works often classified it as a DAO. Next, the paper applies a DAO viability framework - centering on collective intelligence, digital democracy, and adaptation - to examine Bitcoin's organizational and governance mechanisms. Findings suggest that Bitcoin instantitates key DAO principles by enabling open participation, and employing decentralized decision-making through Bitcoin Improvement Proposals (BIPs), miner signaling, and user-activated soft forks. However, this governance carries potential risks, including reduced clarity on who truly 'votes' due to the concentration of economic power among large stakeholders. The paper concludes by highlighting opportunities to refine Bitcoin's deliberation process and reflecting on broader implications for DAO design, such as the absence of a legal entity. In doing so, it underscores Bitcoin's continued relevance as an archetype for decentralized governance, offering important findings for future DAO implementations.",http://arxiv.org/abs/2504.20838v1,IS,Quantitative
Information Retrieval in the Age of Generative AI: The RGB Model,"The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.",http://arxiv.org/abs/2504.20610v1,IS,Quantitative
A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks,"With AI-generated content becoming ubiquitous across the web, social media, and other digital platforms, it is vital to examine how such content are inspired and generated. The creation of AI-generated images often involves refining the input prompt iteratively to achieve desired visual outcomes. This study focuses on the relatively underexplored concept of image regeneration using AI, in which a human operator attempts to closely recreate a specific target image by iteratively refining their prompt. Image regeneration is distinct from normal image generation, which lacks any predefined visual reference. A separate challenge lies in determining whether existing image similarity metrics (ISMs) can provide reliable, objective feedback in iterative workflows, given that we do not fully understand if subjective human judgments of similarity align with these metrics. Consequently, we must first validate their alignment with human perception before assessing their potential as a feedback mechanism in the iterative prompt refinement process. To address these research gaps, we present a structured user study evaluating how iterative prompt refinement affects the similarity of regenerated images relative to their targets, while also examining whether ISMs capture the same improvements perceived by human observers. Our findings suggest that incremental prompt adjustments substantially improve alignment, verified through both subjective evaluations and quantitative measures, underscoring the broader potential of iterative workflows to enhance generative AI content creation across various application domains.",http://arxiv.org/abs/2504.20340v1,IS,Quantitative
Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South,"Youth online safety research in HCI has historically centered on perspectives from the Global North, often overlooking the unique particularities and cultural contexts of regions in the Global South. This paper presents a systematic review of 66 youth online safety studies published between 2014 and 2024, specifically focusing on regions in the Global South. Our findings reveal a concentrated research focus in Asian countries and predominance of quantitative methods. We also found limited research on marginalized youth populations and a primary focus on risks related to cyberbullying. Our analysis underscores the critical role of cultural factors in shaping online safety, highlighting the need for educational approaches that integrate social dynamics and awareness. We propose methodological recommendations and a future research agenda that encourages the adoption of situated, culturally sensitive methodologies and youth-centered approaches to researching youth online safety regions in the Global South. This paper advocates for greater inclusivity in youth online safety research, emphasizing the importance of addressing varied sociocultural contexts to better understand and meet the online safety needs of youth in the Global South.",http://arxiv.org/abs/2504.20308v1,IS,Quantitative
Generative AI in Education: Student Skills and Lecturer Roles,"Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging as a revolutionary tool in education that brings both positive aspects and challenges for educators and students, reshaping how learning and teaching are approached. This study aims to identify and evaluate the key competencies students need to effectively engage with GenAI in education and to provide strategies for lecturers to integrate GenAI into teaching practices. The study applied a mixed method approach with a combination of a literature review and a quantitative survey involving 130 students from South Asia and Europe to obtain its findings. The literature review identified 14 essential student skills for GenAI engagement, with AI literacy, critical thinking, and ethical AI practices emerging as the most critical. The student survey revealed gaps in prompt engineering, bias awareness, and AI output management. In our study of lecturer strategies, we identified six key areas, with GenAI Integration and Curriculum Design being the most emphasised. Our findings highlight the importance of incorporating GenAI into education. While literature prioritized ethics and policy development, students favour hands-on, project-based learning and practical AI applications. To foster inclusive and responsible GenAI adoption, institutions should ensure equitable access to GenAI tools, establish clear academic integrity policies, and advocate for global GenAI research initiatives.",http://arxiv.org/abs/2504.19673v1,IS,Mixed
What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns,"Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompt and model changes efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs, and we are able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.",http://arxiv.org/abs/2504.15815v1,IS,Quantitative
Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents,"There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more ""pessimistic"" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the ""social pact"". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.",http://arxiv.org/abs/2504.08640v1,IS,Quantitative
DataMap: A Portable Application for Visualizing High-Dimensional Data,"Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https://gexijin.github.io/datamap/. The source code can be found at https://github.com/gexijin/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed",http://arxiv.org/abs/2504.08875v1,IS,Quantitative
Dynamics of collective minds in online communities,"How communities respond to diverse societal challenges, from economic crises to political upheavals, is shaped by their collective minds - shared representations of ongoing events and current topics. In turn, collective minds are shaped by a continuous stream of influences, amplified by the rapid rise of online platforms. Online communities must understand these influences to maintain healthy discourse and avoid being manipulated, but understanding is hindered by limited observations and the inability to conduct counterfactual experiments. Here, we show how collective minds in online news communities can be influenced by different editorial agenda-setting practices and aspects of community dynamics, and how these influences can be reversed. We develop a computational model of collective minds, calibrated and validated with data from 400 million comments across five U.S. online news platforms and a large-scale survey. The model enables us to describe and experiment with a variety of influences and derive quantitative insights into their magnitude and persistence in different communities. We find that some editorial influences can be reversed relatively rapidly, but others, such as amplification and reframing of certain topics, as well as community influences such as trolling and counterspeech, tend to persist and durably change the collective mind. These findings illuminate ways collective minds can be manipulated and pathways for communities to maintain healthy and authentic collective discourse amid ongoing societal challenges.",http://arxiv.org/abs/2504.08152v1,IS,Mixed
Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support,"As AI-based clinical decision support (AI-CDS) is introduced in more and more aspects of healthcare services, HCI research plays an increasingly important role in designing for complementarity between AI and clinicians. However, current evaluations of AI-CDS often fail to capture when AI is and is not useful to clinicians. This position paper reflects on our work and influential AI-CDS literature to advocate for moving beyond evaluation metrics like Trust, Reliance, Acceptance, and Performance on the AI's task (what we term the ""trap"" of human-AI collaboration). Although these metrics can be meaningful in some simple scenarios, we argue that optimizing for them ignores important ways that AI falls short of clinical benefit, as well as ways that clinicians successfully use AI. As the fields of HCI and AI in healthcare develop new ways to design and evaluate CDS tools, we call on the community to prioritize ecologically valid, domain-appropriate study setups that measure the emergent forms of value that AI can bring to healthcare professionals.",http://arxiv.org/abs/2504.07423v1,IS,Mixed
Are Users More Willing to Use Formally Verified Password Managers?,"Formal verification has recently been increasingly used to prove the correctness and security of many applications. It is attractive because it can prove the absence of errors with the same certainty as mathematicians proving theorems. However, while most security experts recognize the value of formal verification, the views of non-technical users on this topic are unknown. To address this issue, we designed and implemented two experiments to understand how formal verification impacts users. Our approach started with a formative study involving 15 participants, followed by the main quantitative study with 200 individuals. We focus on the application domain of password managers since it has been documented that the lack of trust in password managers might lead to lower adoption. Moreover, recent efforts have focused on formally verifying (parts of) password managers. We conclude that formal verification is seen as desirable by users and identify three actional recommendations to improve formal verification communication efforts.",http://arxiv.org/abs/2504.02124v1,IS,Quantitative
Generative AI and the transformation of Work in Latin America -- Brazil,"This survey explores the impact perceived by employers and employees of GenAI in their work activities in Brazil. Generative AI (GenAI) is gradually transforming Brazil workforce, particularly in micro and small businesses, though its adoption remains uneven. This survey examines the perceptions of employers and employees across five sectors: Sales, Customer Service, Graphic Design or Photography, Journalism or Content Production, and Software Development or Coding. The results are analyzed in light of six key dimensions of workforce impact. The findings reveal a mix of optimism, apprehension, and untapped potential in the integration of AI tools. This study serves as a foundation for developing inclusive strategies that maximize AI's benefits while safeguarding workers' rights. The IIA-LNCC supports open research and remains committed to shaping a future where technology and human potential progress together.",http://arxiv.org/abs/2505.13490v1,IS,Quantitative
An Initial Exploration of Default Images in Text-to-Image Generation,"In the creative practice of text-to-image generation (TTI), images are generated from text prompts. However, TTI models are trained to always yield an output, even if the prompt contains unknown terms. In this case, the model may generate what we call ""default images"": images that closely resemble each other across many unrelated prompts. We argue studying default images is valuable for designing better solutions for TTI and prompt engineering. In this paper, we provide the first investigation into default images on Midjourney, a popular image generator. We describe our systematic approach to create input prompts triggering default images, and present the results of our initial experiments and several small-scale ablation studies. We also report on a survey study investigating how default images affect user satisfaction. Our work lays the foundation for understanding default images in TTI and highlights challenges and future research directions.",http://arxiv.org/abs/2505.09166v1,IS,Quantitative
The promise and perils of AI in medicine,"What does Artificial Intelligence (AI) have to contribute to health care? And what should we be looking out for if we are worried about its risks? In this paper we offer a survey, and initial evaluation, of hopes and fears about the applications of artificial intelligence in medicine. AI clearly has enormous potential as a research tool, in genomics and public health especially, as well as a diagnostic aid. It's also highly likely to impact on the organisational and business practices of healthcare systems in ways that are perhaps under-appreciated. Enthusiasts for AI have held out the prospect that it will free physicians up to spend more time attending to what really matters to them and their patients. We will argue that this claim depends upon implausible assumptions about the institutional and economic imperatives operating in contemporary healthcare settings. We will also highlight important concerns about privacy, surveillance, and bias in big data, as well as the risks of over trust in machines, the challenges of transparency, the deskilling of healthcare practitioners, the way AI reframes healthcare, and the implications of AI for the distribution of power in healthcare institutions. We will suggest that two questions, in particular, are deserving of further attention from philosophers and bioethicists. What does care look like when one is dealing with data as much as people? And, what weight should we give to the advice of machines in our own deliberations about medical decisions?",http://arxiv.org/abs/2505.06971v1,IS,Quantitative
Assessing the Dynamics of the Coffee Value Chain in Davao del Sur: An Agent-Based Modeling Approach,"The study investigates the coffee value chain dynamics in Davao del Sur using an agent-based model. Three main factors driving interactions among key players were identified: trust, risk, and transaction costs. The model was constructed using NetLogo 6.3.0, and data from a survey questionnaire collected three data points from BACOFA members. Five cases were explored, with each scenario simulated 1000 times. Findings suggest that producers often sell to the market rather than the cooperative due to higher prices. However, producers tend to prioritize trust in buyers and their risk attitude, leading to increased sales to the cooperative. The producer's risk attitude significantly influences their decision-making, affecting performance outcomes such as loans, demand, and price changes. All three factors play a role and exert varying impacts on the value chain. So, the stakeholders' decisions on prioritizing factors in improving relationships depend on their priorities. Nonetheless, simulations show that establishing a harmonious system benefiting all parties is possible. However, achieving this requires adjustments to demand, pricing, trust, and risk attitudes of key players, which may not align with the preferences of some parties in reality.",http://arxiv.org/abs/2505.05797v1,IS,Quantitative
Familiarizing with Music: Discovery Patterns for Different Music Discovery Needs,"Humans have the tendency to discover and explore. This natural tendency is reflected in data from streaming platforms as the amount of previously unknown content accessed by users. Additionally, in domains such as that of music streaming there is evidence that recommending novel content improves users' experience with the platform. Therefore, understanding users' discovery patterns, such as the amount to which and the way users access previously unknown content, is a topic of relevance for both the scientific community and the streaming industry, particularly the music one. Previous works studied how music consumption differs for users of different traits and looked at diversity, novelty, and consistency over time of users' music preferences. However, very little is known about how users discover and explore previously unknown music, and how this behavior differs for users of varying discovery needs. In this paper we bridge this gap by analyzing data from a survey answered by users of the major music streaming platform Deezer in combination with their streaming data. We first address questions regarding whether users who declare a higher interest in unfamiliar music listen to more diverse music, have more stable music preferences over time, and explore more music within a same time window, compared to those who declare a lower interest. We then investigate which type of music tracks users choose to listen to when they explore unfamiliar music, identifying clear patterns of popularity and genre representativeness that vary for users of different discovery needs. Our findings open up possibilities to infer users' interest in unfamiliar music from streaming data as well as possibilities to develop recommender systems that guide users in exploring music in a more natural way.",http://arxiv.org/abs/2505.03568v1,IS,Quantitative
"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications","As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.",http://arxiv.org/abs/2505.00049v1,IS,Quantitative
A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,"In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the ""receive"" and ""accept"" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",http://arxiv.org/abs/2505.00036v1,IS,Quantitative
RecGaze: The First Eye Tracking and User Interaction Dataset for Carousel Interfaces,"Carousel interfaces are widely used in e-commerce and streaming services, but little research has been devoted to them. Previous studies of interfaces for presenting search and recommendation results have focused on single ranked lists, but it appears their results cannot be extrapolated to carousels due to the added complexity. Eye tracking is a highly informative approach to understanding how users click, yet there are no eye tracking studies concerning carousels. There are very few interaction datasets on recommenders with carousel interfaces and none that contain gaze data. We introduce the RecGaze dataset: the first comprehensive feedback dataset on carousels that includes eye tracking results, clicks, cursor movements, and selection explanations. The dataset comprises of interactions from 3 movie selection tasks with 40 different carousel interfaces per user. In total, 87 users and 3,477 interactions are logged. In addition to the dataset, its description and possible use cases, we provide results of a survey on carousel design and the first analysis of gaze data on carousels, which reveals a golden triangle or F-pattern browsing behavior. Our work seeks to advance the field of carousel interfaces by providing the first dataset with eye tracking results on carousels. In this manner, we provide and encourage an empirical understanding of interactions with carousel interfaces, for building better recommender systems through gaze information, and also encourage the development of gaze-based recommenders.",http://arxiv.org/abs/2504.20792v1,IS,Quantitative
Selecting the Right LLM for eGov Explanations,"The perceived quality of the explanations accompanying e-government services is key to gaining trust in these institutions, consequently amplifying further usage of these services. Recent advances in generative AI, and concretely in Large Language Models (LLMs) allow the automation of such content articulations, eliciting explanations' interpretability and fidelity, and more generally, adapting content to various audiences. However, selecting the right LLM type for this has become a non-trivial task for e-government service providers. In this work, we adapted a previously developed scale to assist with this selection, providing a systematic approach for the comparative analysis of the perceived quality of explanations generated by various LLMs. We further demonstrated its applicability through the tax-return process, using it as an exemplar use case that could benefit from employing an LLM to generate explanations about tax refund decisions. This was attained through a user study with 128 survey respondents who were asked to rate different versions of LLM-generated explanations about tax refund decisions, providing a methodological basis for selecting the most appropriate LLM. Recognizing the practical challenges of conducting such a survey, we also began exploring the automation of this process by attempting to replicate human feedback using a selection of cutting-edge predictive techniques.",http://arxiv.org/abs/2504.21032v1,IS,Quantitative
"Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review","Comprehensive literature review requires synthesizing vast amounts of research -- a labor intensive and cognitively demanding process. Most prior work focuses either on helping researchers deeply understand a few papers (e.g., for triaging or reading), or retrieving from and visualizing a vast corpus. Deep analysis and synthesis of large paper collections (e.g., to produce a survey paper) is largely conducted manually with little support. We present DimInd, an interactive system that scaffolds literature review across large paper collections through LLM-generated structured representations. DimInd scaffolds literature understanding with multiple levels of compression, from papers, to faceted literature comparison tables with information extracted from individual papers, to taxonomies of concepts, to narrative syntheses. Users are guided through these successive information transformations while maintaining provenance to source text. In an evaluation with 23 researchers, DimInd supported participants in extracting information and conceptually organizing papers with less effort compared to a ChatGPT-assisted baseline workflow.",http://arxiv.org/abs/2504.18496v1,IS,Mixed
Reimagining Assistive Walkers: An Exploration of Challenges and Preferences in Older Adults,"The well-being of older adults relies significantly on maintaining balance and mobility. As physical ability declines, older adults often accept the need for assistive devices. However, existing walkers frequently fail to consider user preferences, leading to perceptions of imposition and reduced acceptance. This research explores the challenges faced by older adults, caregivers, and healthcare professionals when using walkers, assesses their perceptions, and identifies their needs and preferences. A holistic approach was employed, using tailored perception questionnaires for older adults (24 participants), caregivers (30 participants), and healthcare professionals (27 participants), all of whom completed the survey. Over 50% of caregivers and healthcare professionals displayed good knowledge, positive attitudes, and effective practices regarding walkers. However, over 30% of participants perceived current designs as fall risks, citing the need for significant upper body strength, potentially affecting safety and movement. More than 50% highlighted the importance of incorporating fall detection, ergonomic designs, noise reduction, and walker ramps to better meet user needs and preferences.",http://arxiv.org/abs/2504.18169v1,IS,Quantitative
"SoK: Timeline based event reconstruction for digital forensics: Terminology, methodology, and current challenges","Event reconstruction is a technique that examiners can use to attempt to infer past activities by analyzing digital artifacts. Despite its significance, the field suffers from fragmented research, with studies often focusing narrowly on aspects like timeline creation or tampering detection. This paper addresses the lack of a unified perspective by proposing a comprehensive framework for timeline-based event reconstruction, adapted from traditional forensic science models. We begin by harmonizing existing terminology and presenting a cohesive diagram that clarifies the relationships between key elements of the reconstruction process. Through a comprehensive literature survey, we classify and organize the main challenges, extending the discussion beyond common issues like data volume. Lastly, we highlight recent advancements and propose directions for future research, including specific research gaps. By providing a structured approach, key findings, and a clearer understanding of the underlying challenges, this work aims to strengthen the foundation of digital forensics.",http://arxiv.org/abs/2504.18131v1,IS,Quantitative
VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics,"Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation. VIGMA is available at https://github.com/komar41/VIGMA.",http://arxiv.org/abs/2504.17960v2,IS,Quantitative
From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs,"Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.",http://arxiv.org/abs/2504.15965v2,IS,Quantitative
Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts,"As artificial intelligence (AI) reshapes industries and societies, ensuring its trustworthiness-through mitigating ethical risks like bias, opacity, and accountability deficits-remains a global challenge. International Organization for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to foster responsible development by embedding fairness, transparency, and risk management into AI systems. However, their effectiveness varies across diverse regulatory landscapes, from the EU's risk-based AI Act to China's stability-focused measures and the U.S.'s fragmented state-led initiatives. This paper introduces a novel Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards address ethical risks within these contexts, proposing enhancements to strengthen their global applicability. By mapping ISO standards to the EU AI Act and surveying regulatory frameworks in ten regions-including the UK, Canada, India, Japan, Singapore, South Korea, and Brazil-we establish a baseline for ethical alignment. The framework, applied to case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO standards falter in enforcement (e.g., Colorado) and undervalue region-specific risks like privacy (China). We recommend mandatory risk audits, region-specific annexes, and a privacy-focused module to enhance ISO's adaptability. This approach not only synthesizes global trends but also offers a replicable tool for aligning standardization with ethical imperatives, fostering interoperability and trust in AI worldwide. Policymakers and standards bodies can leverage these insights to evolve AI governance, ensuring it meets diverse societal needs as the technology advances.",http://arxiv.org/abs/2504.16139v1,IS,Quantitative
Counterfactual Multi-player Bandits for Explainable Recommendation Diversification,"Existing recommender systems tend to prioritize items closely aligned with users' historical interactions, inevitably trapping users in the dilemma of ``filter bubble''. Recent efforts are dedicated to improving the diversity of recommendations. However, they mainly suffer from two major issues: 1) a lack of explainability, making it difficult for the system designers to understand how diverse recommendations are generated, and 2) limitations to specific metrics, with difficulty in enhancing non-differentiable diversity metrics. To this end, we propose a \textbf{C}ounterfactual \textbf{M}ulti-player \textbf{B}andits (CMB) method to deliver explainable recommendation diversification across a wide range of diversity metrics. Leveraging a counterfactual framework, our method identifies the factors influencing diversity outcomes. Meanwhile, we adopt the multi-player bandits to optimize the counterfactual optimization objective, making it adaptable to both differentiable and non-differentiable diversity metrics. Extensive experiments conducted on three real-world datasets demonstrate the applicability, effectiveness, and explainability of the proposed CMB.",http://arxiv.org/abs/2505.21165v1,IS,Quantitative
THE WASTIVE: An Interactive Ebb and Flow of Digital Fabrication Waste,"What if digital fabrication waste could observe the world? What would they see? What would they say? ""THE WASTIVE"" reimagines digital fabrication waste as sentient observers, giving them a poetic voice through interactive art. As viewers approach, the installation awakens, mimicking the rhythmic ebb and flow of ocean waves - a silent dialogue where discarded materials ""observe"" and respond to human presence. These interactions echo the gentle murmurs of the sea, transforming technological residue into a reflective, sensory experience. Through this artistic contemplation, ""THE WASTIVE"" invites audiences to reconsider their creative processes and consumption habits. It serves as a poetic call for more mindful, sustainable practices, provoking deeper reflections on our interconnectedness with the environment.",http://arxiv.org/abs/2505.21153v1,IS,Mixed
DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models,"In human-AI collaboration, a central challenge is deciding whether the AI should handle a task, be deferred to a human expert, or be addressed through collaborative effort. Existing Learning to Defer approaches typically make binary choices between AI and humans, neglecting their complementary strengths. They also lack interpretability, a critical property in high-stakes scenarios where users must understand and, if necessary, correct the model's reasoning. To overcome these limitations, we propose Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework for human-AI collaboration. DeCoDe makes strategy decisions based on human-interpretable concept representations, enhancing transparency throughout the decision process. It supports three flexible modes: autonomous AI prediction, deferral to humans, and human-AI collaborative complementarity, selected via a gating network that takes concept-level inputs and is trained using a novel surrogate loss that balances accuracy and human effort. This approach enables instance-specific, interpretable, and adaptive human-AI collaboration. Experiments on real-world datasets demonstrate that DeCoDe significantly outperforms AI-only, human-only, and traditional deferral baselines, while maintaining strong robustness and interpretability even under noisy expert annotations.",http://arxiv.org/abs/2505.19220v1,IS,Quantitative
HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation,"Graph Contrastive Learning (GCL), which fuses graph neural networks with contrastive learning, has evolved as a pivotal tool in user-item recommendations. While promising, existing GCL methods often lack explicit modeling of hierarchical item structures, which represent item similarities across varying resolutions. Such hierarchical item structures are ubiquitous in various items (e.g., online products and local businesses), and reflect their inherent organizational properties that serve as critical signals for enhancing recommendation accuracy. In this paper, we propose Hierarchical Graph Contrastive Learning (HGCL), a novel GCL method that incorporates hierarchical item structures for user-item recommendations. First, HGCL pre-trains a GCL module using cross-layer contrastive learning to obtain user and item representations. Second, HGCL employs a representation compression and clustering method to construct a two-hierarchy user-item bipartite graph. Ultimately, HGCL fine-tunes user and item representations by learning on the hierarchical graph, and then provides recommendations based on user-item interaction scores. Experiments on three widely adopted benchmark datasets ranging from 70K to 382K nodes confirm the superior performance of HGCL over existing baseline models, highlighting the contribution of hierarchical item structures in enhancing GCL methods for recommendation tasks.",http://arxiv.org/abs/2505.19020v1,IS,Quantitative
Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models,"Large language models (LLMs) have demonstrated exceptional performance in understanding and generating semantic patterns, making them promising candidates for sequential recommendation tasks. However, when combined with conventional recommendation models (CRMs), LLMs often face challenges related to high inference costs and static knowledge transfer methods. In this paper, we propose a novel mutual distillation framework, LLMD4Rec, that fosters dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based recommendation systems. Unlike traditional unidirectional distillation methods, LLMD4Rec enables iterative optimization by alternately refining both models, enhancing the semantic understanding of CRMs and enriching LLMs with collaborative signals from user-item interactions. By leveraging sample-wise adaptive weighting and aligning output distributions, our approach eliminates the need for additional parameters while ensuring effective knowledge transfer. Extensive experiments on real-world datasets demonstrate that LLMD4Rec significantly improves recommendation accuracy across multiple benchmarks without increasing inference costs. This method provides a scalable and efficient solution for combining the strengths of both LLMs and CRMs in sequential recommendation systems.",http://arxiv.org/abs/2505.18120v1,IS,Quantitative
Beyond Cascaded Architectures: An End-to-end Generative Framework for Industrial Advertising,"Traditional online industrial advertising systems suffer from the limitations of multi-stage cascaded architectures, which often discard high-potential candidates prematurely and distribute decision logic across disconnected modules. While recent generative recommendation approaches provide end-to-end solutions, they fail to address critical advertising requirements of key components for real-world deployment, such as explicit bidding, creative selection, ad allocation, and payment computation. To bridge this gap, we introduce End-to-End Generative Advertising (EGA), the first unified framework that holistically models user interests, point-of-interest (POI) and creative generation, ad allocation, and payment optimization within a single generative model. Our approach employs hierarchical tokenization and multi-token prediction to jointly generate POI recommendations and ad creatives, while a permutation-aware reward model and token-level bidding strategy ensure alignment with both user experiences and advertiser objectives. Additionally, we decouple allocation from payment using a differentiable ex-post regret minimization mechanism, guaranteeing approximate incentive compatibility at the POI level. Through extensive offline evaluations and large-scale online experiments on real-world advertising platforms, we demonstrate that EGA significantly outperforms traditional cascaded systems in both performance and practicality. Our results highlight its potential as a pioneering fully generative advertising solution, paving the way for next-generation industrial ad systems.",http://arxiv.org/abs/2505.17549v2,IS,Quantitative
MDVT: Enhancing Multimodal Recommendation with Model-Agnostic Multimodal-Driven Virtual Triplets,"The data sparsity problem significantly hinders the performance of recommender systems, as traditional models rely on limited historical interactions to learn user preferences and item properties. While incorporating multimodal information can explicitly represent these preferences and properties, existing works often use it only as side information, failing to fully leverage its potential. In this paper, we propose MDVT, a model-agnostic approach that constructs multimodal-driven virtual triplets to provide valuable supervision signals, effectively mitigating the data sparsity problem in multimodal recommendation systems. To ensure high-quality virtual triplets, we introduce three tailored warm-up threshold strategies: static, dynamic, and hybrid. The static warm-up threshold strategy exhaustively searches for the optimal number of warm-up epochs but is time-consuming and computationally intensive. The dynamic warm-up threshold strategy adjusts the warm-up period based on loss trends, improving efficiency but potentially missing optimal performance. The hybrid strategy combines both, using the dynamic strategy to find the approximate optimal number of warm-up epochs and then refining it with the static strategy in a narrow hyper-parameter space. Once the warm-up threshold is satisfied, the virtual triplets are used for joint model optimization by our enhanced pair-wise loss function without causing significant gradient skew. Extensive experiments on multiple real-world datasets demonstrate that integrating MDVT into advanced multimodal recommendation models effectively alleviates the data sparsity problem and improves recommendation performance, particularly in sparse data scenarios.",http://arxiv.org/abs/2505.16665v1,IS,Quantitative
LSM-VEC: A Large-Scale Disk-Based System for Dynamic Vector Search,"Vector search underpins modern AI applications by supporting approximate nearest neighbor (ANN) queries over high-dimensional embeddings in tasks like retrieval-augmented generation (RAG), recommendation systems, and multimodal search. Traditional ANN search indices (e.g., HNSW) are limited by memory constraints at large data scale. Disk-based indices such as DiskANN reduce memory overhead but rely on offline graph construction, resulting in costly and inefficient vector updates. The state-of-the-art clustering-based approach SPFresh offers better scalability but suffers from reduced recall due to coarse partitioning. Moreover, SPFresh employs in-place updates to maintain its index structure, limiting its efficiency in handling high-throughput insertions and deletions under dynamic workloads. This paper presents LSM-VEC, a disk-based dynamic vector index that integrates hierarchical graph indexing with LSM-tree storage. By distributing the proximity graph across multiple LSM-tree levels, LSM-VEC supports out-of-place vector updates. It enhances search efficiency via a sampling-based probabilistic search strategy with adaptive neighbor selection, and connectivity-aware graph reordering further reduces I/O without requiring global reconstruction. Experiments on billion-scale datasets demonstrate that LSM-VEC consistently outperforms existing disk-based ANN systems. It achieves higher recall, lower query and update latency, and reduces memory footprint by over 66.2%, making it well-suited for real-world large-scale vector search with dynamic updates.",http://arxiv.org/abs/2505.17152v1,IS,Quantitative
Tweedie Regression for Video Recommendation System,"Modern recommendation systems aim to increase click-through rates (CTR) for better user experience, through commonly treating ranking as a classification task focused on predicting CTR. However, there is a gap between this method and the actual objectives of businesses across different sectors. In video recommendation services, the objective of video on demand (VOD) extends beyond merely encouraging clicks, but also guiding users to discover their true interests, leading to increased watch time. And longer users watch time will leads to more revenue through increased chances of presenting online display advertisements. This research addresses the issue by redefining the problem from classification to regression, with a focus on maximizing revenue through user viewing time. Due to the lack of positive labels on recommendation, the study introduces Tweedie Loss Function, which is better suited in this scenario than the traditional mean square error loss. The paper also provides insights on how Tweedie process capture users diverse interests. Our offline simulation and online A/B test revealed that we can substantially enhance our core business objectives: user engagement in terms of viewing time and, consequently, revenue. Additionally, we provide a theoretical comparison between the Tweedie Loss and the commonly employed viewing time weighted Logloss, highlighting why Tweedie Regression stands out as an efficient solution. We further outline a framework for designing a loss function that focuses on a singular objective.",http://arxiv.org/abs/2505.06445v1,IS,Quantitative
Fairness Perceptions in Regression-based Predictive Models,"Regression-based predictive analytics used in modern kidney transplantation is known to inherit biases from training data. This leads to social discrimination and inefficient organ utilization, particularly in the context of a few social groups. Despite this concern, there is limited research on fairness in regression and its impact on organ utilization and placement. This paper introduces three novel divergence-based group fairness notions: (i) independence, (ii) separation, and (iii) sufficiency to assess the fairness of regression-based analytics tools. In addition, fairness preferences are investigated from crowd feedback, in order to identify a socially accepted group fairness criterion for evaluating these tools. A total of 85 participants were recruited from the Prolific crowdsourcing platform, and a Mixed-Logit discrete choice model was used to model fairness feedback and estimate social fairness preferences. The findings clearly depict a strong preference towards the separation and sufficiency fairness notions, and that the predictive analytics is deemed fair with respect to gender and race groups, but unfair in terms of age groups.",http://arxiv.org/abs/2505.04886v2,IS,Quantitative
DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization,"Traditional Partial Least Squares Regression (PLSR) models frequently underperform when handling data characterized by uneven categories. To address the issue, this paper proposes a Data Augmentation Partial Least Squares Regression (DAPLSR) model via manifold optimization. The DAPLSR model introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase the number of samples and utilizes the Value Difference Metric (VDM) to select the nearest neighbor samples that closely resemble the original samples for generating synthetic samples. In solving the model, in order to obtain a more accurate numerical solution for PLSR, this paper proposes a manifold optimization method that uses the geometric properties of the constraint space to improve model degradation and optimization. Comprehensive experiments show that the proposed DAPLSR model achieves superior classification performance and outstanding evaluation metrics on various datasets, significantly outperforming existing methods.",http://arxiv.org/abs/2504.16639v1,IS,Quantitative
Niche Dynamics in Complex Online Community Ecosystems,"Online communities are important organizational forms where members socialize and share information. Curiously, different online communities often overlap considerably in topic and membership. Recent research has investigated competition and mutualism among overlapping online communities through the lens of organizational ecology; however, it has not accounted for how the nonlinear dynamics of online attention may lead to episodic competition and mutualism. Neither has it explored the origins of competition and mutualism in the processes by which online communities select or adapt to their niches. This paper presents a large-scale study of 8,806 Reddit communities belonging to 1,919 clusters of high user overlap over a 5-year period. The method uses nonlinear time series methods to infer bursty, often short-lived ecological dynamics. Results reveal that mutualism episodes are longer lived and slightly more frequent than competition episodes. Next, it tests whether online communities find their niches by specializing to avoid competition using panel regression models. It finds that competitive ecological interactions lead to decreasing topic and user overlaps; however, changes that decrease such niche overlaps do not lead to mutualism. The discussion proposes that future designs may enable online community ecosystem management by informing online community leaders to organize ""spin-off"" communities or via feeds and recommendations.",http://arxiv.org/abs/2504.02153v2,IS,Quantitative
Automated Explanation of Machine Learning Models of Footballing Actions in Words,"While football analytics has changed the way teams and analysts assess performance, there remains a communication gap between machine learning practice and how coaching staff talk about football. Coaches and practitioners require actionable insights, which are not always provided by models. To bridge this gap, we show how to build wordalizations (a novel approach that leverages large language models) for shots in football. Specifically, we first build an expected goals model using logistic regression. We then use the co-efficients of this regression model to write sentences describing how factors (such as distance, angle and defensive pressure) contribute to the model's prediction. Finally, we use large language models to give an entertaining description of the shot. We describe our approach in a model card and provide an interactive open-source application describing shots in recent tournaments. We discuss how shot wordalisations might aid communication in coaching and football commentary, and give a further example of how the same approach can be applied to other actions in football.",http://arxiv.org/abs/2504.00767v1,IS,Quantitative
A two-stage model leveraging friendship network for community evolution prediction in interactive networks,"Interactive networks representing user participation and interactions in specific ""events"" are highly dynamic, with communities reflecting collective behaviors that evolve over time. Predicting these community evolutions is crucial for forecasting the trajectory of the related ""event"". Some models for community evolution prediction have been witnessed, but they primarily focused on coarse-grained evolution types (e.g., expand, dissolve, merge, split), often neglecting fine-grained evolution extents (e.g., the extent of community expansion). Furthermore, these models typically utilize only one network data (here is interactive network data) for dynamic community featurization, overlooking the more stable friendship network that represents the friendships between people to enrich community representations. To address these limitations, we propose a two-stage model that predicts both the type and extent of community evolution. Our model unifies multi-class classification for evolution type and regression for evolution extent within a single framework and fuses data from both interactive and friendship networks for a comprehensive community featurization. We also introduce a hybrid strategy to differentiate between evolution types that are difficult to distinguish. Experimental results on three datasets show the significant superiority of the proposed model over other models, confirming its efficacy in predicting community evolution in interactive networks.",http://arxiv.org/abs/2503.15788v1,IS,Quantitative
Investigating Cultural Dimensions and Technological Acceptance: The Adoption of Electronic Performance and Tracking Systems in Qatar's Football Sector,"Qatar's football sector has undergone a substantial technological transformation with the implementation of Electronic Performance and Tracking Systems (EPTS). This study examines the impact of cultural and technological factors on EPTS adoption, using Hofstede's Cultural Dimensions Theory and the Technology Acceptance Model (TAM) as theoretical frameworks. An initial exploratory study involved ten participants, followed by an expanded dataset comprising thirty stakeholders, including players, coaches, and staff from Qatari football organizations. Multiple regression analysis was conducted to evaluate the relationships between perceived usefulness, perceived ease of use, power distance, innovation receptiveness, integration complexity, and overall adoption. The results indicate that perceived usefulness, innovation receptiveness, and lower power distance significantly drive EPTS adoption, while ease of use is marginally significant and integration complexity is non-significant in this sample. These findings provide practical insights for sports technology stakeholders in Qatar and emphasize the importance of aligning cultural considerations with technological readiness for successful EPTS integration.",http://arxiv.org/abs/2503.16557v1,IS,Quantitative
NeckCheck: Predicting Neck Strain using Head Tracker Sensors,"Tech neck, a growing musculoskeletal concern caused by prolonged poor posture during device use, has significant health implications. This study investigates the relationship between head posture and muscular activity in the upper trapezius muscle to predict muscle strain by leveraging data from EMG sensors and head trackers. We train a regression model to predict EMG envelope readings using head movement data. We conduct preliminary experiments involving various postures to explore the correlation between these modalities and assess the feasibility of predicting muscle strain using head worn sensors. We discuss the key research challenges in sensing and predicting muscle fatigue. The results highlight the potential of this approach in real-time ergonomic feedback systems, contributing to the prevention and management of tech neck.",http://arxiv.org/abs/2503.12762v1,IS,Quantitative
Desirable Unfamiliarity: Insights from Eye Movements on Engagement and Readability of Dictation Interfaces,"Dictation interfaces support efficient text input, but the transcribed text can be hard to read. To understand how users read and review dictated text, we conducted a controlled eye-tracking experiment with 20 participants to compare five dictation interfaces: PLAIN (real-time transcription), AOC (periodic corrections), RAKE (keyword highlights), GP-TSM (grammar-preserving highlights), and SUMMARY (LLM-generated abstraction summary). The study analyzed participants' gaze patterns during their speech composition and reviewing processes. The findings show that during composition, participants spent only 7--11% of their time actively reading, and they favored real-time feedback and avoided distracting interface changes. During reviewing, although SUMMARY introduced unfamiliar words (requiring longer and more frequent fixation), they were easier to read (requiring fewer regressions). Participants preferred SUMMARY for the polished text that preserved fidelity to original meanings. RAKE guided the reading of self-produced text better than GP-TSM. RAKE guides the reading of self-produced text better than GP-TSM. These surprising findings suggest that dictation interfaces could consider showing summaries or key information to support recall instead of raw transcripts.",http://arxiv.org/abs/2503.08539v2,IS,Quantitative
A Multifacet Hierarchical Sentiment-Topic Model with Application to Multi-Brand Online Review Analysis,"Multi-brand analysis based on review comments and ratings is a commonly used strategy to compare different brands in marketing. It can help consumers make more informed decisions and help marketers understand their brand's position in the market. In this work, we propose a multifacet hierarchical sentiment-topic model (MH-STM) to detect brand-associated sentiment polarities towards multiple comparative aspects from online customer reviews. The proposed method is built on a unified generative framework that explains review words with a hierarchical brand-associated topic model and the overall polarity score with a regression model on the empirical topic distribution. Moreover, a novel hierarchical Polya urn (HPU) scheme is proposed to enhance the topic-word association among topic hierarchy, such that the general topics shared by all brands are separated effectively from the unique topics specific to individual brands. The performance of the proposed method is evaluated on both synthetic data and two real-world review corpora. Experimental studies demonstrate that the proposed method can be effective in detecting reasonable topic hierarchy and deriving accurate brand-associated rankings on multi-aspects.",http://arxiv.org/abs/2502.18927v1,IS,Quantitative
"Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and Future Potential","The integration of generative AI (GenAI) tools, particularly large language models (LLMs), is transforming professional coaching workflows. This study explores how coaches use GenAI, the perceived benefits and limitations of these tools, and broader attitudes toward AI-assisted coaching. A survey of 205 coaching professionals reveals widespread adoption of GenAI for research, content creation, and administrative support, while its role in relational and interpretative coaching remains limited. Findings indicate that AI literacy and perceived AI impact strongly predict GenAI adoption, with positive attitudes fostering greater use. Ethical considerations, particularly transparency and data privacy, are a key concern, with frequent AI users demonstrating greater ethical awareness. Regression analyses show that while perceived effectiveness drives GenAI adoption, concerns about AI replacing human coaches do not significantly influence usage. Coaches express interest in future AI capabilities that enhance personalization, real-time feedback, and administrative automation while maintaining human oversight. The study highlights that GenAI functions best as an augmentation tool rather than a replacement, emphasizing the need for AI literacy training, ethical guidelines, and human-centered AI integration. These findings contribute to the ongoing discourse on human-AI collaboration, advocating for responsible and effective AI adoption in professional coaching.",http://arxiv.org/abs/2502.14632v1,IS,Quantitative
Beyond surveys: A High-Precision Wealth Inequality Mapping of China's Rural Households Derived from Satellite and Street View Imageries,"Wide coverage and high-precision rural household wealth data is an important support for the effective connection between the national macro rural revitalization policy and micro rural entities, which helps to achieve precise allocation of national resources. However, due to the large number and wide distribution of rural areas, wealth data is difficult to collect and scarce in quantity. Therefore, this article attempts to integrate ""sky"" remote sensing images with ""ground"" village street view imageries to construct a fine-grained ""computable"" technical route for rural household wealth. With the intelligent interpretation of rural houses as the core, the relevant wealth elements of image data were extracted and identified, and regressed with the household wealth indicators of the benchmark questionnaire to form a high-precision township scale wealth prediction model (r=0.85); Furthermore, a national and township scale map of rural household wealth in China was promoted and drawn. Based on this, this article finds that there is a ""bimodal"" pattern in the distribution of wealth among rural households in China, which is reflected in a polarization feature of ""high in the south and low in the north, and high in the east and low in the west"" in space. This technological route may provide alternative solutions with wider spatial coverage and higher accuracy for high-cost manual surveys, promote the identification of shortcomings in rural construction, and promote the precise implementation of rural policies.",http://arxiv.org/abs/2502.12163v1,IS,Quantitative
A Simple but Effective Closed-form Solution for Extreme Multi-label Learning,"Extreme multi-label learning (XML) is a task of assigning multiple labels from an extremely large set of labels to each data instance. Many current high-performance XML models are composed of a lot of hyperparameters, which complicates the tuning process. Additionally, the models themselves are adapted specifically to XML, which complicates their reimplementation. To remedy this problem, we propose a simple method based on ridge regression for XML. The proposed method not only has a closed-form solution but also is composed of a single hyperparameter. Since there are no precedents on applying ridge regression to XML, this paper verified the performance of the method by using various XML benchmark datasets. Furthermore, we enhanced the prediction of low-frequency labels in XML, which hold informative content. This prediction is essential yet challenging because of the limited amount of data. Here, we employed a simple frequency-based weighting. This approach greatly simplifies the process compared with existing techniques. Experimental results revealed that it can achieve levels of performance comparable to, or even exceeding, those of models with numerous hyperparameters. Additionally, we found that the frequency-based weighting significantly improved the predictive performance for low-frequency labels, while requiring almost no changes in implementation. The source code for the proposed method is available on github at https://github.com/cars1015/XML-ridge.",http://arxiv.org/abs/2501.10179v1,IS,Quantitative
Determinants of Human Development Index (HDI): A Regression Analysis of Economic and Social Indicators,"This study aims to investigate the factors influencing the Human Development Index (HDI). Five variables-GDP per capita, health expenditure, education expenditure, infant mortality rate (per 1,000 live births), and average years of schooling-were analyzed to develop a regression model assessing their impact on HDI. The results indicate that GDP per capita, infant mortality rate, and average years of schooling are significant predictors of HDI. Specifically, the study finds a positive relationship between GDP per capita and average years of schooling with HDI, while infant mortality rate is negatively associated with HDI.",http://arxiv.org/abs/2502.00006v1,IS,Quantitative
Generative Regression Based Watch Time Prediction for Short-Video Recommendation,"Watch time prediction (WTP) has emerged as a pivotal task in short video recommendation systems, designed to quantify user engagement through continuous interaction modeling. Predicting users' watch times on videos often encounters fundamental challenges, including wide value ranges and imbalanced data distributions, which can lead to significant estimation bias when directly applying regression techniques. Recent studies have attempted to address these issues by converting the continuous watch time estimation into an ordinal regression task. While these methods demonstrate partial effectiveness, they exhibit notable limitations: (1) the discretization process frequently relies on bucket partitioning, inherently reducing prediction flexibility and accuracy and (2) the interdependencies among different partition intervals remain underutilized, missing opportunities for effective error correction. Inspired by language modeling paradigms, we propose a novel Generative Regression (GR) framework that reformulates WTP as a sequence generation task. Our approach employs \textit{structural discretization} to enable nearly lossless value reconstruction while maintaining prediction fidelity. Through carefully designed vocabulary construction and label encoding schemes, each watch time is bijectively mapped to a token sequence. To mitigate the training-inference discrepancy caused by teacher-forcing, we introduce a \textit{curriculum learning with embedding mixup} strategy that gradually transitions from guided to free-generation modes. We evaluate our method against state-of-the-art approaches on two public datasets and one industrial dataset. We also perform online A/B testing on the Kuaishou App to confirm the real-world effectiveness. The results conclusively show that GR outperforms existing techniques significantly.",http://arxiv.org/abs/2412.20211v3,IS,Quantitative
Modeling Battery Electric Vehicle Users' Charging Decisions in Scenarios with Both Time-Related and Distance-Related Anxiety,"As one of the most promising alternatives to internal combustion engine vehicles, battery electric vehicles (BEVs) have become increasingly prevalent in recent years. However, range anxiety is still a major concern among BEV users or potential users in recent years. The social-psychological factors were found to be associated with range anxiety, but how the charging decisions are affected by range anxiety is still unclear. Thus, in our study, through an online questionnaire issued in mainland China, we collected 230 participants' charging decisions in 60 range-anxiety-inducing scenarios in which both distance-related, and time-related anxiety co-existed. Then, an interpretable machine learning (ML) approach with the Shapley Additive Explanations method was used to model BEV users' charging decisions in these scenarios. To further explore users' decision-making mechanisms, a Bayesian-Network-regression mixed approach was used to model the inner topological structure among the factors influencing users' decisions. We find that both time-related and distance-related factors can affect users' charging decisions, but the influence of waiting time is softer compared to the BEV range. Users' charging decisions can also be moderated by users' psychological states (i.e., range anxiety level and trust in range estimation system), individual differences (i.e., age and personality), and BEV using experience (i.e., driving mileage, display mileage and range estimation cycle of range estimation system), of which, the range anxiety level is more directly related with users' charging decisions. Findings from this study can provide insights into the optimization of charge station distribution and customization of the charging recommendation system.",http://arxiv.org/abs/2412.16240v1,IS,Mixed
CRM: Retrieval Model with Controllable Condition,"Recommendation systems (RecSys) are designed to connect users with relevant items from a vast pool of candidates while aligning with the business goals of the platform. A typical industrial RecSys is composed of two main stages, retrieval and ranking: (1) the retrieval stage aims at searching hundreds of item candidates satisfied user interests; (2) based on the retrieved items, the ranking stage aims at selecting the best dozen items by multiple targets estimation for each item candidate, including classification and regression targets. Compared with ranking model, the retrieval model absence of item candidate information during inference, therefore retrieval models are often trained by classification target only (e.g., click-through rate), but failed to incorporate regression target (e.g., the expected watch-time), which limit the effectiveness of retrieval. In this paper, we propose the Controllable Retrieval Model (CRM), which integrates regression information as conditional features into the two-tower retrieval paradigm. This modification enables the retrieval stage could fulfill the target gap with ranking model, enhancing the retrieval model ability to search item candidates satisfied the user interests and condition effectively. We validate the effectiveness of CRM through real-world A/B testing and demonstrate its successful deployment in Kuaishou short-video recommendation system, which serves over 400 million users.",http://arxiv.org/abs/2412.13844v1,IS,Quantitative
Uncovering Student Engagement Patterns in Moodle with Interpretable Machine Learning,"Understanding and enhancing student engagement through digital platforms is critical in higher education. This study introduces a methodology for quantifying engagement across an entire module using virtual learning environment (VLE) activity log data. Using study session frequency, immediacy, and diversity, we create a cumulative engagement metric and model it against weekly VLE interactions with resources to identify critical periods and resources predictive of student engagement. In a case study of a computing module at University College London's Department of Statistical Science, we further examine how delivery methods (online, hybrid, in-person) impact student behaviour. Across nine regression models, we validate the consistency of the random forest model and highlight the interpretive strengths of generalised additive models for analysing engagement patterns. Results show weekly VLE clicks as reliable engagement predictors, with early weeks and the first assessment period being key. However, the impact of delivery methods on engagement is inconclusive due to inconsistencies across models. These findings support early intervention strategies to assist students at risk of disengagement. This work contributes to learning analytics research by proposing a refined VLE-based engagement metric and advancing data-driven teaching strategies in higher education.",http://arxiv.org/abs/2412.11826v1,IS,Mixed
Detecting Dark Patterns in User Interfaces Using Logistic Regression and Bag-of-Words Representation,"Dark patterns in user interfaces represent deceptive design practices intended to manipulate users' behavior, often leading to unintended consequences such as coerced purchases, involuntary data disclosures, or user frustration. Detecting and mitigating these dark patterns is crucial for promoting transparency, trust, and ethical design practices in digital environments. This paper proposes a novel approach for detecting dark patterns in user interfaces using logistic regression and bag-of-words representation. Our methodology involves collecting a diverse dataset of user interface text samples, preprocessing the data, extracting text features using the bag-of-words representation, training a logistic regression model, and evaluating its performance using various metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC). Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying instances of dark patterns, with high predictive performance and robustness to variations in dataset composition and model parameters. The insights gained from this study contribute to the growing body of knowledge on dark patterns detection and classification, offering practical implications for designers, developers, and policymakers in promoting ethical design practices and protecting user rights in digital environments.",http://arxiv.org/abs/2412.14187v1,IS,Quantitative
Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval with Semantic Guidance,"Modern music retrieval systems often rely on fixed representations of user preferences, limiting their ability to capture users' diverse and uncertain retrieval needs. To address this limitation, we introduce Diff4Steer, a novel generative retrieval framework that employs lightweight diffusion models to synthesize diverse seed embeddings from user queries that represent potential directions for music exploration. Unlike deterministic methods that map user query to a single point in embedding space, Diff4Steer provides a statistical prior on the target modality (audio) for retrieval, effectively capturing the uncertainty and multi-faceted nature of user preferences. Furthermore, Diff4Steer can be steered by image or text inputs, enabling more flexible and controllable music discovery combined with nearest neighbor search. Our framework outperforms deterministic regression methods and LLM-based generative retrieval baseline in terms of retrieval and ranking metrics, demonstrating its effectiveness in capturing user preferences, leading to more diverse and relevant recommendations. Listening examples are available at tinyurl.com/diff4steer.",http://arxiv.org/abs/2412.04746v1,IS,Quantitative
Contextual Bandits in Payment Processing: Non-uniform Exploration and Supervised Learning at Adyen,"Uniform random exploration in decision-making systems supports off-policy learning via supervision but incurs high regret, making it impractical for many applications. Conversely, non-uniform exploration offers better immediate performance but lacks support for off-policy learning. Recent research suggests that regression oracles can bridge this gap by combining non-uniform exploration with supervised learning. In this paper, we analyze these approaches within a real-world industrial context at Adyen, a large global payments processor characterized by batch logged delayed feedback, short-term memory, and dynamic action spaces under the Empirical Risk Minimization (ERM) framework. Our analysis reveals that while regression oracles significantly improve performance, they introduce challenges due to rigid algorithmic assumptions. Specifically, we observe that as a policy improves, subsequent generations may perform worse due to shifts in the reward distribution and increased class imbalance in the training data. This degradation occurs de spite improvements in other aspects of the training data, leading to decreased performance in successive policy iterations. We further explore the long-term impact of regression oracles, identifying a potential ""oscillation effect."" This effect arises when regression oracles influence probability estimates and the realizability of subsequent policy models, leading to fluctuations in performance across iterations. Our findings highlight the need for more adaptable algorithms that can leverage the benefits of regression oracles without introducing instability in policy performance over time.",http://arxiv.org/abs/2412.00569v1,IS,Quantitative
Detecting Visual Triggers in Cannabis Imagery: A CLIP-Based Multi-Labeling Framework with Local-Global Aggregation,"This study investigates the interplay of visual and textual features in online discussions about cannabis edibles and their impact on user engagement. Leveraging the CLIP model, we analyzed 42,743 images from Facebook (March 1 to August 31, 2021), with a focus on detecting food-related visuals and examining the influence of image attributes such as colorfulness and brightness on user interaction. For textual analysis, we utilized the BART model as a denoising autoencoder to classify ten topics derived from structural topic modeling, exploring their relationship with user engagement. Linear regression analysis identified significant positive correlations between food-related visuals (e.g., fruit, candy, and bakery) and user engagement scores, as well as between engagement and text topics such as cannabis legalization. In contrast, negative associations were observed with image colorfulness and certain textual themes. These findings offer actionable insights for policymakers and regulatory bodies in designing warning labels and marketing regulations to address potential risks associated with recreational cannabis edibles.",http://arxiv.org/abs/2412.08648v1,IS,Quantitative
Motion Analysis of Upper Limb and Hand in a Haptic Rotation Task,"Humans seem to have a bias to overshoot when rotating a rotary knob blindfolded around a specified target angle (i.e. during haptic rotation). Whereas some influence factors that strengthen or weaken such an effect are already known, the underlying reasons for the overshoot are still unknown. This work approaches the topic of haptic rotations by analyzing a detailed recording of the movement. We propose an experimental framework and an approach to investigate which upper limb and hand joint movements contribute significantly to a haptic rotation task and to the angle overshoot based on the acquired data. With stepwise regression with backward elimination, we analyze a rotation around 90 degrees counterclockwise with two fingers under different grasping orientations. Our results showed that the wrist joint, the sideways finger movement in the proximal joints, and the distal finger joints contributed significantly to overshooting. This suggests that two phenomena are behind the overshooting: 1) The significant contribution of the wrist joint indicates a bias of a hand-centered egocentric reference frame. 2) Significant contribution of the finger joints indicates a rolling of the fingertips over the rotary knob surface and, thus, a change of contact point for which probably the human does not compensate.",http://arxiv.org/abs/2411.12765v1,IS,Quantitative
"When to Commute During the COVID-19 Pandemic and Beyond: Analysis of Traffic Crashes in Washington, D.C","Many workers in cities across the world, who have been teleworking because of the COVID-19 pandemic, are expected to be back to their commutes. As this process is believed to be gradual and telecommuting is likely to remain an option for many workers, hybrid model and flexible schedules might become the norm in the future. This variable work schedules allows employees to commute outside of traditional rush hours. Moreover, many studies showed that commuters might be skeptical of using trains, buses, and carpools and could turn to personal vehicles to get to work, which might increase congestion and crashes in the roads. This study attempts to provide information on the safest time to commute to Washington, DC area analyzing historical traffic crash data before the COVID-19 pandemic. It also aims to advance our understanding of traffic crashes and other relating factors such as weather in the Washington, DC area. We created a model to predict crashes by time of the day, using a negative binomial regression after rejecting a Poisson regression, and additionally explored the validity of a Random Forest regression. Our main consideration for an eventual application of this study is to reduce crashes in Washington DC, using this tool that provides people with better options on when to commute and when to telework, if available. The study also provides policymakers and researchers with real-world insights that decrease the number of traffic crashes to help achieve the goals of The Vision Zero Initiative adopted by the district.",http://arxiv.org/abs/2411.05957v1,IS,Quantitative
Identifying Economic Factors Affecting Unemployment Rates in the United States,"In this study, we seek to understand how macroeconomic factors such as GDP, inflation, Unemployment Insurance, and S&P 500 index; as well as microeconomic factors such as health, race, and educational attainment impacted the unemployment rate for about 20 years in the United States. Our research question is to identify which factor(s) contributed the most to the unemployment rate surge using linear regression. Results from our studies showed that GDP (negative), inflation (positive), Unemployment Insurance (contrary to popular opinion; negative), and S&P 500 index (negative) were all significant factors, with inflation being the most important one. As for health issue factors, our model produced resultant correlation scores for occurrences of Cardiovascular Disease, Neurological Disease, and Interpersonal Violence with unemployment. Race as a factor showed a huge discrepancies in the unemployment rate between Black Americans compared to their counterparts. Asians had the lowest unemployment rate throughout the years. As for education attainment, results showed that having a higher education attainment significantly reduced one chance of unemployment. People with higher degrees had the lowest unemployment rate. Results of this study will be beneficial for policymakers and researchers in understanding the unemployment rate during the pandemic.",http://arxiv.org/abs/2411.02374v1,IS,Quantitative
DIFF: Dual Side-Information Filtering and Fusion for Sequential Recommendation,"Side-information Integrated Sequential Recommendation (SISR) benefits from auxiliary item information to infer hidden user preferences, which is particularly effective for sparse interactions and cold-start scenarios. However, existing studies face two main challenges. (i) They fail to remove noisy signals in item sequence and (ii) they underutilize the potential of side-information integration. To tackle these issues, we propose a novel SISR model, Dual Side-Information Filtering and Fusion (DIFF), which employs frequency-based noise filtering and dual multi-sequence fusion. Specifically, we convert the item sequence to the frequency domain to filter out noisy short-term fluctuations in user interests. We then combine early and intermediate fusion to capture diverse relationships across item IDs and attributes. Thanks to our innovative filtering and fusion strategy, DIFF is more robust in learning subtle and complex item correlations in the sequence. DIFF outperforms state-of-the-art SISR models, achieving improvements of up to 14.1% and 12.5% in Recall@20 and NDCG@20 across four benchmark datasets.",http://arxiv.org/abs/2505.13974v1,IS,Quantitative
Sliding Speed Influences Electrovibration-Induced Finger Friction Dynamics on Touchscreens,"Electrovibration technology can render tactile textures on capacitive touchscreens by modulating friction between the finger and the screen through electrostatic attraction force generated by applying an alternating voltage signal to the screen. This signal should be carefully calibrated for realistic and robust texture rendering. However, this process is challenging due to variations in sliding speed, applied force, and individual skin mechanics, which affect friction in complex and unpredictable ways. Here, we investigate how exploration conditions affect electrovibration-induced finger friction on touchscreens and the role of skin mechanics in this process. Ten participants slid their index fingers across an electrovibration-enabled touchscreen at five sliding speeds ($20 im100$ mm/s) and applied force levels ($0.2 im0.6$ N) while we measured contact forces and skin accelerations. The touchscreen was excited with amplitude-modulated voltage signals across frequencies relevant to touch. We modeled the finger-touchscreen friction response as a first-order system and the skin mechanics as a mass-spring-damper system. Our results showed that the sliding speed influenced the cutoff frequency of the friction response as well as the moving mass and stiffness of the finger for the tested exploration ranges. Specifically, for every 1 mm/s increase in speed, the cutoff frequency, the finger moving mass, and stiffness increased by $13.8$ Hz, $3.23\times 10^{-5}$ kg, and $4.04$ N/m, respectively. Further correlation analysis revealed that finger stiffness affected the cutoff frequency more than the moving mass. Finally, we developed a practical model for electrovibration-induced finger friction on touchscreens that accounts for sliding speed variations, paving the way for delivering consistent haptic feedback through electrovibration.",http://arxiv.org/abs/2505.11162v1,IS,Quantitative
Utilization of Skin Color Change for Image-based Tactile Sensing,"Measurement of pressure distribution applied to a fingertip is crucial for the teleoperation of robots and human computer interface. Previous studies have acquired pressure distribution by affixing a sensor array to the fingertip or by optically recording the deformation of an object. However, these existing methods inhibit the fingertip from directly contacting the texture, and the pressure applied to the fingertip is measured indirectly. In this study, we propose a method to measure pressure distribution by directly touching a transparent object, focusing on the change in skin color induced by the applied pressure, caused by blood flow. We evaluated the relationship between pressure and skin color change when local pressure is applied, and found a correlation between the pressure and the color change. However, the contact area and the color change area did not align perfectly. We further explored the factor causing the spatial non-uniformity of the color change, by accounting for the stress distribution using finite element analysis. These results suggest that the proposed measurement method can be utilized to measure the internal stress distribution, and it is anticipated to serve as a simple sensor in the field of human computer interface.",http://arxiv.org/abs/2505.09402v1,IS,Quantitative
Interest Changes: Considering User Interest Life Cycle in Recommendation System,"In recommendation systems, user interests are always in a state of constant flux. Typically, a user interest experiences a emergent phase, a stable phase, and a declining phase, which are referred to as the ""user interest life-cycle"". Recent papers on user interest modeling have primarily focused on how to compute the correlation between the target item and user's historical behaviors, without thoroughly considering the life-cycle features of user interest. In this paper, we propose an effective method called Deep Interest Life-cycle Network (DILN), which not only captures the interest life-cycle features efficiently, but can also be easily integrated to existing ranking models. DILN contains two key components: Interest Life-cycle Encoder Module constructs historical activity histograms of the user interest and then encodes them into dense representation. Interest Life-cycle Fusion Module injects the encoded dense representation into multiple expert networks, with the aim of enabling the specific phase of interest life-cycle to activate distinct experts. Online A/B testing reveals that DILN achieves significant improvements of +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which demonstrates its effectiveness. In addition, DILN inherently increase the exposure of users' emergent and stable interests while decreasing the exposure of declining interests. DILN has been deployed on the Lofter App.",http://arxiv.org/abs/2505.08471v1,IS,Quantitative
Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation,"Recommendation systems in AI-based medical diagnostics and treatment constitute a critical component of AI in healthcare. Although some studies have explored this area and made notable progress, healthcare recommendation systems remain in their nascent stage. And these researches mainly target the treatment process such as drug or disease recommendations. In addition to the treatment process, the diagnostic process, particularly determining which medical examinations are necessary to evaluate the condition, also urgently requires intelligent decision support. To bridge this gap, we first formalize the task of medical examination recommendations. Compared to traditional recommendations, the medical examination recommendation involves more complex interactions. This complexity arises from two folds: 1) The historical medical records for examination recommendations are heterogeneous and redundant, which makes the recommendation results susceptible to noise. 2) The correlation between the medical history of patients is often irregular, making it challenging to model spatiotemporal dependencies. Motivated by the above observation, we propose a novel Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation (DST-GKAN) with a two-stage learning paradigm to solve the above challenges. In the first stage, we exploit a task-adaptive diffusion model to distill recommendation-oriented information by reducing the noises in heterogeneous medical data. In the second stage, a spatiotemporal graph KANsformer is proposed to simultaneously model the complex spatial and temporal relationships. Moreover, to facilitate the medical examination recommendation research, we introduce a comprehensive dataset. The experimental results demonstrate the state-of-the-art performance of the proposed method compared to various competitive baselines.",http://arxiv.org/abs/2505.07431v1,IS,Mixed
Characterising Topic Familiarity and Query Specificity Using Eye-Tracking Data,"Eye-tracking data has been shown to correlate with a user's knowledge level and query formulation behaviour. While previous work has focused primarily on eye gaze fixations for attention analysis, often requiring additional contextual information, our study investigates the memory-related cognitive dimension by relying solely on pupil dilation and gaze velocity to infer users' topic familiarity and query specificity without needing any contextual information. Using eye-tracking data collected via a lab user study (N=18), we achieved a Macro F1 score of 71.25% for predicting topic familiarity with a Gradient Boosting classifier, and a Macro F1 score of 60.54% with a k-nearest neighbours (KNN) classifier for query specificity. Furthermore, we developed a novel annotation guideline -- specifically tailored for question answering -- to manually classify queries as Specific or Non-specific. This study demonstrates the feasibility of eye-tracking to better understand topic familiarity and query specificity in search.",http://arxiv.org/abs/2505.03136v1,IS,Quantitative
Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models,"Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.",http://arxiv.org/abs/2505.02847v3,IS,Quantitative
"Thoughtful, Confused, or Untrustworthy: How Text Presentation Influences Perceptions of AI Writing Tools","AI writing tools have been shown to dramatically change the way people write, yet the effects of AI text presentation are not well understood nor always intentionally designed. Although text presentation in existing large language model interfaces is linked to the speed of the underlying model, text presentation speed can impact perceptions of AI systems, potentially influencing whether AI suggestions are accepted or rejected. In this paper, we analyze the effects of varying text generation speed in creative and professional writing scenarios on an online platform (n=297). We find that speed is correlated with perceived humanness and trustworthiness of the AI tool, as well as the perceived quality of the generated text. We discuss its implications on creative and writing processes, along with future steps in the intentional design of AI writing tool interfaces.",http://arxiv.org/abs/2504.20365v1,IS,Quantitative
LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation,"Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.",http://arxiv.org/abs/2504.20013v2,IS,Quantitative
"What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality","Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.",http://arxiv.org/abs/2504.16373v1,IS,Quantitative
Measuring Interest Group Positions on Legislation: An AI-Driven Analysis of Lobbying Reports,"Special interest groups (SIGs) in the U.S. participate in a range of political activities, such as lobbying and making campaign donations, to influence policy decisions in the legislative and executive branches. The competing interests of these SIGs have profound implications for global issues such as international trade policies, immigration, climate change, and global health challenges. Despite the significance of understanding SIGs' policy positions, empirical challenges in observing them have often led researchers to rely on indirect measurements or focus on a select few SIGs that publicly support or oppose a limited range of legislation. This study introduces the first large-scale effort to directly measure and predict a wide range of bill positions-Support, Oppose, Engage (Amend and Monitor)- across all legislative bills introduced from the 111th to the 117th Congresses. We leverage an advanced AI framework, including large language models (LLMs) and graph neural networks (GNNs), to develop a scalable pipeline that automatically extracts these positions from lobbying activities, resulting in a dataset of 42k bills annotated with 279k bill positions of 12k SIGs. With this large-scale dataset, we reveal (i) a strong correlation between a bill's progression through legislative process stages and the positions taken by interest groups, (ii) a significant relationship between firm size and lobbying positions, (iii) notable distinctions in lobbying position distribution based on bill subject, and (iv) heterogeneity in the distribution of policy preferences across industries. We introduce a novel framework for examining lobbying strategies and offer opportunities to explore how interest groups shape the political landscape.",http://arxiv.org/abs/2504.15333v1,IS,Quantitative
Linear Item-Item Model with Neural Knowledge for Session-based Recommendation,"Session-based recommendation (SBR) aims to predict users' subsequent actions by modeling short-term interactions within sessions. Existing neural models primarily focus on capturing complex dependencies for sequential item transitions. As an alternative solution, linear item-item models mainly identify strong co-occurrence patterns across items and support faster inference speed. Although each paradigm has been actively studied in SBR, their fundamental differences in capturing item relationships and how to bridge these distinct modeling paradigms effectively remain unexplored. In this paper, we propose a novel SBR model, namely Linear Item-Item model with Neural Knowledge (LINK), which integrates both types of knowledge into a unified linear framework. Specifically, we design two specialized components of LINK: (i) Linear knowledge-enhanced Item-item Similarity model (LIS), which refines the item similarity correlation via self-distillation, and (ii) Neural knowledge-enhanced Item-item Transition model (NIT), which seamlessly incorporates complicated neural knowledge distilled from the off-the-shelf neural model. Extensive experiments demonstrate that LINK outperforms state-of-the-art linear SBR models across six real-world datasets, achieving improvements of up to 14.78% and 11.04% in Recall@20 and MRR@20 while showing up to 813x fewer inference FLOPs. Our code is available at https://github.com/jin530/LINK.",http://arxiv.org/abs/2504.15057v1,IS,Quantitative
Interpreting Network Differential Privacy,"How do we interpret the differential privacy (DP) guarantee for network data? We take a deep dive into a popular form of network DP ($\varepsilon$--edge DP) to find that many of its common interpretations are flawed. Drawing on prior work for privacy with correlated data, we interpret DP through the lens of adversarial hypothesis testing and demonstrate a gap between the pairs of hypotheses actually protected under DP (tests of complete networks) and the sorts of hypotheses implied to be protected by common claims (tests of individual edges). We demonstrate some conditions under which this gap can be bridged, while leaving some questions open. While some discussion is specific to edge DP, we offer selected results in terms of abstract DP definitions and provide discussion of the implications for other forms of network DP.",http://arxiv.org/abs/2504.12520v1,IS,Quantitative
Comprehensive Classification of Web Tracking Systems: Technological In-sights and Analysis,"Web tracking (WT) systems are advanced technologies used to monitor and analyze online user behavior. Initially focused on HTML and static webpages, these systems have evolved with the proliferation of IoT, edge computing, and Big Data, encompassing a broad array of interconnected devices with APIs, interfaces and computing nodes for interaction. WT systems are pivotal in technological innovation and business development, although trends like GDPR complicate data extraction and mandate transparency. Specifically, this study examines WT systems purely from a technological perspective, excluding organizational and privacy implications. A novel classification scheme based on technological architecture and principles is proposed, compared to two preexisting frameworks. The scheme categorizes WT systems into six classes, emphasizing technological mechanisms such as HTTP proto-cols, APIs, and user identification techniques. Additionally, a survey of over 1,000 internet users, conducted via Google Forms, explores user awareness of WT systems. Findings indicate that knowledge of WT technologies is largely unrelated to demographic factors such as age or gender but is strongly influenced by a user's background in computer science. Most users demonstrate only a basic understanding of WT tools, and this awareness does not correlate with heightened concerns about data misuse. As such, the research highlights gaps in user education about WT technologies and underscores the need for a deeper examination of their technical underpinnings. This study provides a foundation for further exploration of WT systems from multiple perspectives, contributing to advance-ments in classification, implementation, and user awareness.",http://arxiv.org/abs/2504.13922v2,IS,Quantitative
Mobile-Driven Incentive Based Exercise for Blood Glucose Control in Type 2 Diabetes,"We propose and create an incentive based recommendation algorithm aimed at improving the lifestyle of diabetic patients. This algorithm is integrated into a real world mobile application to provide personalized health recommendations. Initially, users enter data such as step count, calorie intake, gender, age, weight, height and blood glucose levels. When the data is preprocessed, the app identifies the personalized health and glucose management goals. The recommendation engine suggests exercise routines and dietary adjustments based on these goals. As users achieve their goals and follow these recommendations, they receive incentives, encouraging adherence and promoting positive health outcomes. Furthermore, the mobile application allows users to monitor their progress through descriptive analytics, which displays their daily activities and health metrics in graphical form. To evaluate the proposed methodology, the study was conducted with 10 participants, with type 2 diabetes for three weeks. The participants were recruited through advertisements and health expert references. The application was installed on the patient phone to use it for three weeks. The expert was also a part of this study by monitoring the patient health record. To assess the algorithm performance, we computed efficiency and proficiency. As a result, the algorithm showed proficiency and efficiency scores of 90% and 92%, respectively. Similarly, we computed user experience with application in terms of attractiveness, hedonic and pragmatic quality, involving 35 people in the study. As a result, it indicated an overall positive user response. The findings show a clear positive correlation between exercise and rewards, with noticeable improvements observed in user outcomes after exercise.",http://arxiv.org/abs/2504.13909v1,IS,Quantitative
Investigating Affective Use and Emotional Well-being on ChatGPT,"As AI chatbots see increased adoption and integration into everyday life, questions have been raised about the potential impact of human-like or anthropomorphic AI on users. In this work, we investigate the extent to which interactions with ChatGPT (with a focus on Advanced Voice Mode) may impact users' emotional well-being, behaviors and experiences through two parallel studies. To study the affective use of AI chatbots, we perform large-scale automated analysis of ChatGPT platform usage in a privacy-preserving manner, analyzing over 3 million conversations for affective cues and surveying over 4,000 users on their perceptions of ChatGPT. To investigate whether there is a relationship between model usage and emotional well-being, we conduct an Institutional Review Board (IRB)-approved randomized controlled trial (RCT) on close to 1,000 participants over 28 days, examining changes in their emotional well-being as they interact with ChatGPT under different experimental settings. In both on-platform data analysis and the RCT, we observe that very high usage correlates with increased self-reported indicators of dependence. From our RCT, we find that the impact of voice-based interactions on emotional well-being to be highly nuanced, and influenced by factors such as the user's initial emotional state and total usage duration. Overall, our analysis reveals that a small number of users are responsible for a disproportionate share of the most affective cues.",http://arxiv.org/abs/2504.03888v1,IS,Quantitative
Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles,"Bias in news reporting significantly impacts public perception, particularly regarding crime, politics, and societal issues. Traditional bias detection methods, predominantly reliant on human moderation, suffer from subjective interpretations and scalability constraints. Here, we introduce an AI-driven framework leveraging advanced large language models (LLMs), specifically GPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to systematically identify and mitigate biases in news articles. To this end, we collect an extensive dataset consisting of over 30,000 crime-related articles from five politically diverse news sources spanning a decade (2013-2023). Our approach employs a two-stage methodology: (1) bias detection, where each LLM scores and justifies biased content at the paragraph level, validated through human evaluation for ground truth establishment, and (2) iterative debiasing using GPT-4o Mini, verified by both automated reassessment and human reviewers. Empirical results indicate GPT-4o Mini's superior accuracy in bias detection and effectiveness in debiasing. Furthermore, our analysis reveals temporal and geographical variations in media bias correlating with socio-political dynamics and real-world events. This study contributes to scalable computational methodologies for bias mitigation, promoting fairness and accountability in news reporting.",http://arxiv.org/abs/2504.03520v1,IS,Quantitative
Scenario Discovery for Urban Planning: The Case of Green Urbanism and the Impact on Stress,"Urban environments significantly influence mental health outcomes, yet the role of an effective framework for decision-making under deep uncertainty (DMDU) for optimizing urban policies for stress reduction remains underexplored. While existing research has demonstrated the effects of urban design on mental health, there is a lack of systematic scenario-based analysis to guide urban planning decisions. This study addresses this gap by applying Scenario Discovery (SD) in urban planning to evaluate the effectiveness of urban vegetation interventions in stress reduction across different urban environments using a predictive model based on emotional responses collected from a neuroscience-based outdoor experiment in Lisbon. Combining these insights with detailed urban data from Copenhagen, we identify key intervention thresholds where vegetation-based solutions succeed or fail in mitigating stress responses. Our findings reveal that while increased vegetation generally correlates with lower stress levels, high-density urban environments, crowding, and individual psychological traits (e.g., extraversion) can reduce its effectiveness. This work showcases our Scenario Discovery framework as a systematic approach for identifying robust policy pathways in urban planning, opening the door for its exploration in other urban decision-making contexts where uncertainty and design resiliency are critical.",http://arxiv.org/abs/2504.02905v1,IS,Quantitative
Disinformation about autism in Latin America and the Caribbean: Mapping 150 false causes and 150 false cures of ASD in conspiracy theory communities on Telegram,"How do conspiracy theory communities in Latin America and the Caribbean structure, articulate, and sustain the dissemination of disinformation about autism? To answer this question, this research investigates the structuring, articulation, and promotion of autism-related disinformation in conspiracy theory communities in Latin America and the Caribbean. By analyzing publications from 1,659 Telegram communities over ten years (2015 - 2025) and examining more than 58 million pieces of shared content from approximately 5.3 million users, this study explores how false narratives about autism are promoted, including unfounded claims about its causes and promises of miraculous cures. The adopted methodology combines network analysis, time series analysis, thematic clustering, and content analysis, enabling the identification of dissemination patterns, key influencers, and interconnections with other conspiracy theories. Among the key findings, Brazilian communities stand out as the leading producers and distributors of these narratives in the region, accounting for 46% of the analyzed content. Additionally, there has been an exponential 15,000% (x151) increase in the volume of autism-related disinformation since the COVID-19 pandemic in Latin America and the Caribbean, highlighting the correlation between health crises and the rise of conspiracy beliefs. The research also reveals that false cures, such as chlorine dioxide (CDS), ozone therapy, and extreme diets, are widely promoted within these communities and commercially exploited, often preying on desperate families in exchange for money. By addressing the research question, this study aims to contribute to the understanding of the disinformation ecosystem and proposes critical reflections on how to confront these harmful narratives.",http://arxiv.org/abs/2504.01991v1,IS,Qualitative
Approximation-First Timeseries Monitoring Query At Scale,"Timeseries monitoring systems such as Prometheus play a crucial role in gaining observability of the underlying system components. These systems collect timeseries metrics from various system components and perform monitoring queries over periodic window-based aggregations (i.e., rule queries). However, despite wide adoption, the operational costs and query latency of rule queries remain high. In this paper, we identify major bottlenecks associated with repeated data scans and query computations concerning window overlaps in rule queries, and present PromSketch, an approximation-first query framework as intermediate caches for monitoring systems. It enables low operational costs and query latency, by combining approximate window-based query frameworks and sketch-based precomputation. PromSketch is implemented as a standalone module that can be integrated into Prometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over time queries. Our evaluation shows that PromSketch achieves up to a two orders of magnitude reduction in query latency over Prometheus and VictoriaMetrics, while lowering operational dollar costs of query processing by two orders of magnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics with at most 5% average errors across statistics. The source code has been made available at https://github.com/Froot-NetSys/promsketch.",http://arxiv.org/abs/2505.10560v1,IS,Quantitative
DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward,"Model-based offline reinforcement learning (RL) has emerged as a promising approach for recommender systems, enabling effective policy learning by interacting with frozen world models. However, the reward functions in these world models, trained on sparse offline logs, often suffer from inaccuracies. Specifically, existing methods face two major limitations in addressing this challenge: (1) deterministic use of reward functions as static look-up tables, which propagates inaccuracies during policy learning, and (2) static uncertainty designs that fail to effectively capture decision risks and mitigate the impact of these inaccuracies. In this work, a dual-agent framework, DARLR, is proposed to dynamically update world models to enhance recommendation policies. To achieve this, a \textbf{\textit{selector}} is introduced to identify reference users by balancing similarity and diversity so that the \textbf{\textit{recommender}} can aggregate information from these users and iteratively refine reward estimations for dynamic reward shaping. Further, the statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty to better align with evolving recommendation requirements. Extensive experiments on four benchmark datasets demonstrate the superior performance of DARLR, validating its effectiveness. The code is available at https://github.com/ArronDZhang/DARLR.",http://arxiv.org/abs/2505.07257v1,IS,Quantitative
Spatially Disaggregated Energy Consumption and Emissions in End-use Sectors for Germany and Spain,"High-resolution energy consumption and emissions datasets are essential for localized policy-making, resource optimization, and climate action planning. They enable municipalities to monitor mitigation strategies and foster engagement among governments, businesses, and communities. However, smaller municipalities often face data limitations that hinder tailored climate strategies. This study generates detailed final energy consumption and emissions data at the local administrative level for Germany and Spain. Using national datasets, we apply spatial disaggregation techniques with open data sources. A key innovation is the application of XGBoost for imputing missing data, combined with a stepwise spatial disaggregation process incorporating district- and province-level statistics. Prioritizing reproducibility, our open-data approach provides a scalable framework for municipalities to develop actionable climate plans. To ensure transparency, we assess the reliability of imputed values and assign confidence ratings to the disaggregated data.",http://arxiv.org/abs/2505.05139v1,IS,Quantitative
A Conversational Approach to Well-being Awareness Creation and Behavioural Intention,"The promotion of a healthy lifestyle is one of the main drivers of an individual's overall physical and psycho-emotional well-being. Digital technologies are more and more adopted as ''facilitators'' for this goal, to raise awareness and solicit healthy lifestyle habits. This study aims to experiment the effects of the adoption of a digital conversational tool to influence awareness creation and behavioural change in the context of a well-being lifestyle. Our aim is to collect evidence of the aspects that must be taken into account when designing and implementing such tools in well-being promotion campaigns. To this end, we created a conversational application for promoting well-being and healthy lifestyles, which presents relevant information and asks specific questions to its intended users within an interaction happening through a chat interface; the conversational tool presents itself as a well-being counsellor named Allegra and follows a coaching approach to structure the interaction with the user. In our user study, participants were asked to first interact with Allegra in one of three experimental conditions, corresponding to different conversational styles; then, they answered a questionnaire about their experience. The questionnaire items were related to intrinsic motivation factors as well as awareness creation and behavioural change. The collected data allowed us to assess the hypotheses of our model that put in connection those variables. Our results confirm the positive effect of intrinsic motivation factors on both awareness creation and behavioural intention in the context of well-being and healthy lifestyle; on the other hand, we did not record any statistically significant effect of different language and communication styles on the outcomes.",http://arxiv.org/abs/2504.21702v1,IS,Quantitative
Fairness Is More Than Algorithms: Racial Disparities in Time-to-Recidivism,"Racial disparities in recidivism remain a persistent challenge within the criminal justice system, increasingly exacerbated by the adoption of algorithmic risk assessment tools. Past works have primarily focused on bias induced by these tools, treating recidivism as a binary outcome. Limited attention has been given to non-algorithmic factors (including socioeconomic ones) in driving racial disparities from a systemic perspective. To that end, this work presents a multi-stage causal framework to investigate the advent and extent of disparities by considering time-to-recidivism rather than a simple binary outcome. The framework captures interactions among races, the algorithm, and contextual factors. This work introduces the notion of counterfactual racial disparity and offers a formal test using survival analysis that can be conducted with observational data to assess if differences in recidivism arise from algorithmic bias, contextual factors, or their interplay. In particular, it is formally established that if sufficient statistical evidence for differences across racial groups is observed, it would support rejecting the null hypothesis that non-algorithmic factors (including socioeconomic ones) do not affect recidivism. An empirical study applying this framework to the COMPAS dataset reveals that short-term recidivism patterns do not exhibit racial disparities when controlling for risk scores. However, statistically significant disparities emerge with longer follow-up periods, particularly for low-risk groups. This suggests that factors beyond algorithmic scores, possibly structural disparities in housing, employment, and social support, may accumulate and exacerbate recidivism risks over time. This underscores the need for policy interventions extending beyond algorithmic improvements to address broader influences on recidivism trajectories.",http://arxiv.org/abs/2504.18629v1,IS,Mixed
"Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving","Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to inefficient use of computational resources. For example, in cloud-based services, streaming content faster than users can read appears unnecessary, resulting in wasted computational resources and potential delays for other users, particularly during peak usage periods. To address this issue, we propose an adaptive streaming method that dynamically adjusts the pacing of LLM streaming output in real-time based on inferred cognitive load. Our approach estimates the cognitive load associated with streaming content and strategically slows down the stream during complex or information-rich segments, thereby freeing computational resources for other users. Our statistical analysis of computational savings, combined with crowdsourced user studies, provides insights into the trade-offs between service efficiency and user satisfaction, demonstrating that our method can significantly reduce computational consumption up to 16.8\%. This context-aware computational resource management strategy presents a practical framework for enhancing system efficiency in cloud-based conversational AI interfaces without compromising user experience.",http://arxiv.org/abs/2504.17999v1,IS,Quantitative
DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions,"While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on ""DeBiasMe,"" AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.",http://arxiv.org/abs/2504.16770v1,IS,Quantitative
Meltdown: Bridging the Perception Gap in Sustainable Food Behaviors Through Immersive VR,"Climate change education often struggles to bridge the perception gap between everyday actions and their long-term environmental consequences. In response, we developed Meltdown, an immersive virtual reality (VR) escape room that simulates a grocery shopping and food waste management experience to educate university students in Singapore about sustainable consumption. The game emphasizes sustainable food choices and disposal practices, combining interactive elements and narrative feedback to promote behavioral change. Through a user study with 36 university students, we observed statistically significant improvements in participants objective knowledge, perceived confidence, and intention to adopt sustainable behaviors. Our results suggest that experiential VR environments can enhance climate education by making abstract environmental concepts more immediate and personally relevant.",http://arxiv.org/abs/2504.14324v1,IS,Mixed
Using LLMs as prompt modifier to avoid biases in AI image generators,"This study examines how Large Language Models (LLMs) can reduce biases in text-to-image generation systems by modifying user prompts. We define bias as a model's unfair deviation from population statistics given neutral prompts. Our experiments with Stable Diffusion XL, 3.5 and Flux demonstrate that LLM-modified prompts significantly increase image diversity and reduce bias without the need to change the image generators themselves. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests rather than superficial variations. The method works particularly well for less advanced image generators, though limitations persist for certain contexts like disability representation. All prompts and generated images are available at https://iisys-hof.github.io/llm-prompt-img-gen/",http://arxiv.org/abs/2504.11104v1,IS,Quantitative
MURR: Model Updating with Regularized Replay for Searching a Document Stream,"The Internet produces a continuous stream of new documents and user-generated queries. These naturally change over time based on events in the world and the evolution of language. Neural retrieval models that were trained once on a fixed set of query-document pairs will quickly start misrepresenting newly-created content and queries, leading to less effective retrieval. Traditional statistical sparse retrieval can update collection statistics to reflect these changes in the use of language in documents and queries. In contrast, continued fine-tuning of the language model underlying neural retrieval approaches such as DPR and ColBERT creates incompatibility with previously-encoded documents. Re-encoding and re-indexing all previously-processed documents can be costly. In this work, we explore updating a neural dual encoder retrieval model without reprocessing past documents in the stream. We propose MURR, a model updating strategy with regularized replay, to ensure the model can still faithfully search existing documents without reprocessing, while continuing to update the model for the latest topics. In our simulated streaming environments, we show that fine-tuning models using MURR leads to more effective and more consistent retrieval results than other strategies as the stream of documents and queries progresses.",http://arxiv.org/abs/2504.10250v1,IS,Quantitative
Emergency Communication: OTFS-Based Semantic Transmission with Diffusion Noise Suppression,"Due to their flexibility and dynamic coverage capabilities, Unmanned Aerial Vehicles (UAVs) have emerged as vital platforms for emergency communication in disaster-stricken areas. However, the complex channel conditions in high-speed mobile scenarios significantly impact the reliability and efficiency of traditional communication systems. This paper presents an intelligent emergency communication framework that integrates Orthogonal Time Frequency Space (OTFS) modulation, semantic communication, and a diffusion-based denoising module to address these challenges. OTFS ensures robust communication under dynamic channel conditions due to its superior anti-fading characteristics and adaptability to rapidly changing environments. Semantic communication further enhances transmission efficiency by focusing on key information extraction and reducing data redundancy. Moreover, a diffusion-based channel denoising module is proposed to leverage the gradual noise reduction process and statistical noise modeling, optimizing the accuracy of semantic information recovery. Experimental results demonstrate that the proposed solution significantly improves link stability and transmission performance in high-mobility UAV scenarios, achieving at least a 3dB SNR gain over existing methods.",http://arxiv.org/abs/2504.07420v1,IS,Quantitative
Bridging Queries and Tables through Entities in Table Retrieval,"Table retrieval is essential for accessing information stored in structured tabular formats; however, it remains less explored than text retrieval. The content of the table primarily consists of phrases and words, which include a large number of entities, such as time, locations, persons, and organizations. Entities are well-studied in the context of text retrieval, but there is a noticeable lack of research on their applications in table retrieval. In this work, we explore how to leverage entities in tables to improve retrieval performance. First, we investigate the important role of entities in table retrieval from a statistical perspective and propose an entity-enhanced training framework. Subsequently, we use the type of entities to highlight entities instead of introducing an external knowledge base. Moreover, we design an interaction paradigm based on entity representations. Our proposed framework is plug-and-play and flexible, making it easy to integrate into existing table retriever training processes. Empirical results on two table retrieval benchmarks, NQ-TABLES and OTT-QA, show that our proposed framework is both simple and effective in enhancing existing retrievers. We also conduct extensive analyses to confirm the efficacy of different components. Overall, our work provides a promising direction for elevating table retrieval, enlightening future research in this area.",http://arxiv.org/abs/2504.06551v1,IS,Quantitative
Task load dependent decision referrals for joint binary classification in human-automation teams,"We consider the problem of optimal decision referrals in human-automation teams performing binary classification tasks. The automation, which includes a pre-trained classifier, observes data for a batch of independent tasks, analyzes them, and may refer a subset of tasks to a human operator for fresh and final analysis. Our key modeling assumption is that human performance degrades with task load. We model the problem of choosing which tasks to refer as a stochastic optimization problem and show that, for a given task load, it is optimal to myopically refer tasks that yield the largest reduction in expected cost, conditional on the observed data. This provides a ranking scheme and a policy to determine the optimal set of tasks for referral. We evaluate this policy against a baseline through an experimental study with human participants. Using a radar screen simulator, participants made binary target classification decisions under time constraint. They were guided by a decision rule provided to them, but were still prone to errors under time pressure. An initial experiment estimated human performance model parameters, while a second experiment compared two referral policies. Results show statistically significant gains for the proposed optimal referral policy over a blind policy that determines referrals using the automation and human-performance models but not based on the observed data.",http://arxiv.org/abs/2504.04248v1,IS,Quantitative
Leveraging Language Models for Analyzing Longitudinal Experiential Data in Education,"We propose a novel approach to leveraging pre-trained language models (LMs) for early forecasting of academic trajectories in STEM students using high-dimensional longitudinal experiential data. This data, which captures students' study-related activities, behaviors, and psychological states, offers valuable insights for forecasting-based interventions. Key challenges in handling such data include high rates of missing values, limited dataset size due to costly data collection, and complex temporal variability across modalities. Our approach addresses these issues through a comprehensive data enrichment process, integrating strategies for managing missing values, augmenting data, and embedding task-specific instructions and contextual cues to enhance the models' capacity for learning temporal patterns. Through extensive experiments on a curated student learning dataset, we evaluate both encoder-decoder and decoder-only LMs. While our findings show that LMs effectively integrate data across modalities and exhibit resilience to missing data, they primarily rely on high-level statistical patterns rather than demonstrating a deeper understanding of temporal dynamics. Furthermore, their ability to interpret explicit temporal information remains limited. This work advances educational data science by highlighting both the potential and limitations of LMs in modeling student trajectories for early intervention based on longitudinal experiential data.",http://arxiv.org/abs/2503.21617v1,IS,Quantitative
Feasible Action Space Reduction for Quantifying Causal Responsibility in Continuous Spatial Interactions,"Understanding the causal influence of one agent on another agent is crucial for safely deploying artificially intelligent systems such as automated vehicles and mobile robots into human-inhabited environments. Existing models of causal responsibility deal with simplified abstractions of scenarios with discrete actions, thus, limiting real-world use when understanding responsibility in spatial interactions. Based on the assumption that spatially interacting agents are embedded in a scene and must follow an action at each instant, Feasible Action-Space Reduction (FeAR) was proposed as a metric for causal responsibility in a grid-world setting with discrete actions. Since real-world interactions involve continuous action spaces, this paper proposes a formulation of the FeAR metric for measuring causal responsibility in space-continuous interactions. We illustrate the utility of the metric in prototypical space-sharing conflicts, and showcase its applications for analysing backward-looking responsibility and in estimating forward-looking responsibility to guide agent decision making. Our results highlight the potential of the FeAR metric for designing and engineering artificial agents, as well as for assessing the responsibility of agents around humans.",http://arxiv.org/abs/2505.17739v1,IS,Quantitative
Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity,"Popularity bias occurs when popular items are recommended far more frequently than they should be, negatively impacting both user experience and recommendation accuracy. Existing debiasing methods mitigate popularity bias often uniformly across all users and only partially consider the time evolution of users or items. However, users have different levels of preference for item popularity, and this preference is evolving over time. To address these issues, we propose a novel method called CausalEPP (Causal Intervention on Evolving Personal Popularity) for taming recommendation bias, which accounts for the evolving personal popularity of users. Specifically, we first introduce a metric called {Evolving Personal Popularity} to quantify each user's preference for popular items. Then, we design a causal graph that integrates evolving personal popularity into the conformity effect, and apply deconfounded training to mitigate the popularity bias of the causal graph. During inference, we consider the evolution consistency between users and items to achieve a better recommendation. Empirical studies demonstrate that CausalEPP outperforms baseline methods in reducing popularity bias while improving recommendation accuracy.",http://arxiv.org/abs/2505.14310v1,IS,Quantitative
Comparing Apples to Oranges: A Taxonomy for Navigating the Global Landscape of AI Regulation,"AI governance has transitioned from soft law-such as national AI strategies and voluntary guidelines-to binding regulation at an unprecedented pace. This evolution has produced a complex legislative landscape: blurred definitions of ""AI regulation"" mislead the public and create a false sense of safety; divergent regulatory frameworks risk fragmenting international cooperation; and uneven access to key information heightens the danger of regulatory capture. Clarifying the scope and substance of AI regulation is vital to uphold democratic rights and align international AI efforts. We present a taxonomy to map the global landscape of AI regulation. Our framework targets essential metrics-technology or application-focused rules, horizontal or sectoral regulatory coverage, ex ante or ex post interventions, maturity of the digital legal landscape, enforcement mechanisms, and level of stakeholder participation-to classify the breadth and depth of AI regulation. We apply this framework to five early movers: the European Union's AI Act, the United States' Executive Order 14110, Canada's AI and Data Act, China's Interim Measures for Generative AI Services, and Brazil's AI Bill 2338/2023. We further offer an interactive visualization that distills these dense legal texts into accessible insights, highlighting both commonalities and differences. By delineating what qualifies as AI regulation and clarifying each jurisdiction's approach, our taxonomy reduces legal uncertainty, supports evidence-based policymaking, and lays the groundwork for more inclusive, globally coordinated AI governance.",http://arxiv.org/abs/2505.13673v1,IS,Quantitative
Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications,"Voting advice applications (VAAs) help millions of voters understand which political parties or candidates best align with their views. This paper explores the potential risks these applications pose to the democratic process when targeted by adversarial entities. In particular, we expose 11 manipulation strategies and measure their impact using data from Switzerland's primary VAA, Smartvote, collected during the last two national elections. We find that altering application parameters, such as the matching method, can shift a party's recommendation frequency by up to 105%. Cherry-picking questionnaire items can increase party recommendation frequency by over 261%, while subtle changes to parties' or candidates' responses can lead to a 248% increase. To address these vulnerabilities, we propose adversarial robustness properties VAAs should satisfy, introduce empirical metrics for assessing the resilience of various matching methods, and suggest possible avenues for research toward mitigating the effect of manipulation. Our framework is key to ensuring secure and reliable AI-based VAAs poised to emerge in the near future.",http://arxiv.org/abs/2505.13329v1,IS,Quantitative
PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation,"Generating high-quality images without prompt engineering expertise remains a challenge for text-to-image (T2I) models, which often misinterpret poorly structured prompts, leading to distortions and misalignments. While humans easily recognize these flaws, metrics like CLIP fail to capture structural inconsistencies, exposing a key limitation in current evaluation methods. To address this, we introduce PromptIQ, an automated framework that refines prompts and assesses image quality using our novel Component-Aware Similarity (CAS) metric, which detects and penalizes structural errors. Unlike conventional methods, PromptIQ iteratively generates and evaluates images until the user is satisfied, eliminating trial-and-error prompt tuning. Our results show that PromptIQ significantly improves generation quality and evaluation accuracy, making T2I models more accessible for users with little to no prompt engineering expertise.",http://arxiv.org/abs/2505.06467v1,IS,Quantitative
Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects,"Conventional AI evaluation approaches concentrated within the AI stack exhibit systemic limitations for exploring, navigating and resolving the human and societal factors that play out in real world deployment such as in education, finance, healthcare, and employment sectors. AI capability evaluations can capture detail about first-order effects, such as whether immediate system outputs are accurate, or contain toxic, biased or stereotypical content, but AI's second-order effects, i.e. any long-term outcomes and consequences that may result from AI use in the real world, have become a significant area of interest as the technology becomes embedded in our daily lives. These secondary effects can include shifts in user behavior, societal, cultural and economic ramifications, workforce transformations, and long-term downstream impacts that may result from a broad and growing set of risks. This position paper argues that measuring the indirect and secondary effects of AI will require expansion beyond static, single-turn approaches conducted in silico to include testing paradigms that can capture what actually materializes when people use AI technology in context. Specifically, we describe the need for data and methods that can facilitate contextual awareness and enable downstream interpretation and decision making about AI's secondary effects, and recommend requirements for a new ecosystem.",http://arxiv.org/abs/2505.18893v2,IS,Quantitative
Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems,"Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models. Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\% in MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.",http://arxiv.org/abs/2505.18366v1,IS,Quantitative
Novobo: Supporting Teachers' Peer Learning of Instructional Gestures by Teaching a Mentee AI-Agent Together,"Instructional gestures are essential for teaching, as they enhance communication and support student comprehension. However, existing training methods for developing these embodied skills can be time-consuming, isolating, or overly prescriptive. Research suggests that developing these tacit, experiential skills requires teachers' peer learning, where they learn from each other and build shared knowledge. This paper introduces Novobo, an apprentice AI-agent stimulating teachers' peer learning of instructional gestures through verbal and bodily inputs. Positioning the AI as a mentee employs the learning-by-teaching paradigm, aiming to promote deliberate reflection and active learning. Novobo encourages teachers to evaluate its generated gestures and invite them to provide demonstrations. An evaluation with 30 teachers in 10 collaborative sessions showed Novobo prompted teachers to share tacit knowledge through conversation and movement. This process helped teachers externalize, exchange, and internalize their embodied knowledge, promoting collaborative learning and building a shared understanding of instructional gestures within the local teaching community. This work advances understanding of how teachable AI agents can enhance collaborative learning in teacher professional development, offering valuable design insights for leveraging AI to promote the sharing and construction of embodied and practical knowledge.",http://arxiv.org/abs/2505.17557v2,IS,Mixed
Community Moderation and the New Epistemology of Fact Checking on Social Media,"Social media platforms have traditionally relied on internal moderation teams and partnerships with independent fact-checking organizations to identify and flag misleading content. Recently, however, platforms including X (formerly Twitter) and Meta have shifted towards community-driven content moderation by launching their own versions of crowd-sourced fact-checking -- Community Notes. If effectively scaled and governed, such crowd-checking initiatives have the potential to combat misinformation with increased scale and speed as successfully as community-driven efforts once did with spam. Nevertheless, general content moderation, especially for misinformation, is inherently more complex. Public perceptions of truth are often shaped by personal biases, political leanings, and cultural contexts, complicating consensus on what constitutes misleading content. This suggests that community efforts, while valuable, cannot replace the indispensable role of professional fact-checkers. Here we systemically examine the current approaches to misinformation detection across major platforms, explore the emerging role of community-driven moderation, and critically evaluate both the promises and challenges of crowd-checking at scale.",http://arxiv.org/abs/2505.20067v1,IS,Mixed
Designing Semantically-Resonant Abstract Patterns for Data Visualization,"We present a structured design methodology for creating semantically-resonant abstract patterns, making the pattern design process accessible to the general public. Semantically-resonant patterns are those that intuitively evoke the concept they represent within a specific set (e.g., in a vegetable concept set, small dots for olives and large dots for tomatoes), analogous to the concept of semantically-resonant colors (e.g., using olive green for olives and red for tomatoes). Previous research has shown that semantically-resonant colors can improve chart reading speed, and designers have made attempts to integrate semantic cues into abstract pattern designs. However, a systematic framework for developing such patterns was lacking. To bridge this gap, we conducted a series of workshops with design experts, resulting in a design methodology that summarizes the methodology for designing semantically-resonant abstract patterns. We evaluated our design methodology through another series of workshops with non-design participants. The results indicate that our proposed design methodology effectively supports the general public in designing semantically-resonant abstract patterns for both abstract and concrete concepts.",http://arxiv.org/abs/2505.14816v2,IS,Mixed
What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection,"Despite decades of HCI and Meeting Science research, complaints about ineffective meetings are still pervasive. We argue that meeting technologies lack support for prospective reflection, that is, thinking about why a meeting is needed and what might happen. To explore this, we designed a Meeting Purpose Assistant (MPA) technology probe to coach users to articulate their meeting's purpose and challenges, and act accordingly. The MPA used Generative AI to support personalized and actionable prospective reflection across the diversity of meeting contexts. Using a participatory prompting methodology, 18 employees of a global technology company reflected with the MPA on upcoming meetings. Observed impacts were: clarifying meeting purposes, challenges, and success conditions; changing perspectives and flexibility; improving preparation and communication; and proposing changed plans. We also identify perceived social, temporal, and technological barriers to using the MPA. We present system and workflow design considerations for developing AI-assisted reflection support for meetings.",http://arxiv.org/abs/2505.14370v1,IS,Mixed
Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies,"This paper elaborates on the concept of moral exercises as a means to help AI actors cultivate virtues that enable effective human oversight of AI systems. We explore the conceptual framework and significance of moral exercises, situating them within the contexts of philosophical discourse, ancient practices, and contemporary AI ethics scholarship. We outline the core pillars of the moral exercises methodology - eliciting an engaged personal disposition, fostering relational understanding, and cultivating technomoral wisdom - and emphasize their relevance to key activities and competencies essential for human oversight of AI systems. Our argument is supported by findings from three pilot studies involving a company, a multidisciplinary team of AI researchers, and higher education students. These studies allow us to explore both the potential and the limitations of moral exercises. Based on the collected data, we offer insights into how moral exercises can foster a responsible AI culture within organizations, and suggest directions for future research.",http://arxiv.org/abs/2505.15851v1,IS,Quantitative
Know Or Not: a library for evaluating out-of-knowledge base robustness,"While the capabilities of large language models (LLMs) have progressed significantly, their use in high-stakes applications have been limited due to risks of hallucination. One key approach in reducing hallucination is retrieval-augmented generation (RAG), but even in such setups, LLMs may still hallucinate when presented with questions outside of the knowledge base. Such behavior is unacceptable in high-stake applications where LLMs are expected to abstain from answering queries it does not have sufficient context on. In this work, we present a novel methodology for systematically evaluating out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not know) in the RAG setting, without the need for manual annotation of gold standard answers. We implement our methodology in knowornot, an open-source library that enables users to develop their own customized evaluation data and pipelines for OOKB robustness. knowornot comprises four main features. Firstly, it provides a unified, high-level API that streamlines the process of setting up and running robustness benchmarks. Secondly, its modular architecture emphasizes extensibility and flexibility, allowing users to easily integrate their own LLM clients and RAG settings. Thirdly, its rigorous data modeling design ensures experiment reproducibility, reliability and traceability. Lastly, it implements a comprehensive suite of tools for users to customize their pipelines. We demonstrate the utility of knowornot by developing a challenging benchmark, PolicyBench, which spans four Question-Answer (QA) chatbots on government policies, and analyze its OOKB robustness. The source code of knowornot is available https://github.com/govtech-responsibleai/KnowOrNot.",http://arxiv.org/abs/2505.13545v1,IS,Quantitative
LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas,"Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",http://arxiv.org/abs/2505.12257v1,IS,Quantitative
LiDDA: Data Driven Attribution at LinkedIn,"Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.",http://arxiv.org/abs/2505.09861v2,IS,Quantitative
Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts,"Public procurement plays a critical role in government operations, ensuring the efficient allocation of resources and fostering economic growth. However, traditional procurement data is often stored in rigid, tabular formats, limiting its analytical potential and hindering transparency. This research presents a methodological framework for transforming structured procurement data into a semantic knowledge graph, leveraging ontological modeling and automated data transformation techniques. By integrating RDF and SPARQL-based querying, the system enhances the accessibility and interpretability of procurement records, enabling complex semantic queries and advanced analytics. Furthermore, by incorporating machine learning-driven predictive modeling, the system extends beyond conventional data analysis, offering insights into procurement trends and risk assessment. This work contributes to the broader field of public procurement intelligence by improving data transparency, supporting evidence-based decision-making, and enabling in-depth analysis of procurement activities in North Macedonia.",http://arxiv.org/abs/2505.09798v1,IS,Quantitative
Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence,"As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.",http://arxiv.org/abs/2505.08828v1,IS,Quantitative
Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies,"Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.",http://arxiv.org/abs/2505.07552v1,IS,Quantitative
Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction,"Trust is a fundamental component of human-agent interaction. With the increasing presence of artificial agents in daily life, it is essential to understand how people perceive and trust these agents. One of the key challenges affecting this perception is the Uncanny Valley Effect (UVE), where increasingly human-like artificial beings can be perceived as eerie or repelling. Despite growing interest in trust and the UVE, existing research varies widely in terms of how these concepts are defined and operationalized. This inconsistency raises important questions about how and under what conditions the UVE influences trust in agents. A systematic understanding of their relationship is currently lacking. This review aims to examine the impact of the UVE on human trust in agents and to identify methodological patterns, limitations, and gaps in the existing empirical literature. Following PRISMA guidelines, a systematic search identified 53 empirical studies that investigated both UVE-related constructs and trust or trust-related outcomes. Studies were analyzed based on a structured set of categories, including types of agents and interactions, methodological and measurement approaches, and key findings. The results of our systematic review reveal that most studies rely on static images or hypothetical scenarios with limited real-time interaction, and the majority use subjective trust measures. This review offers a novel framework for classifying trust measurement approaches with regard to the best-practice criteria for empirically investigating the UVE. As the first systematic attempt to map the intersection of UVE and trust, this review contributes to a deeper understanding of their interplay and offers a foundation for future research. Keywords: the uncanny valley effect, trust, human-likeness, affinity response, human-agent interaction",http://arxiv.org/abs/2505.05543v1,IS,Quantitative
CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation,"Despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. This is due to the content of existing large training datasets collected on the Internet, which are predominantly based on Western European or American popular culture. Meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. In an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. We propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the Russian one. We explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the Kandinsky 3.1 text-to-image model. Human evaluation results demonstrate an increase in the level of awareness of Russian culture in the model.",http://arxiv.org/abs/2505.04851v1,IS,Quantitative
Scalable Class-Centric Visual Interactive Labeling,"Large unlabeled datasets demand efficient and scalable data labeling solutions, in particular when the number of instances and classes is large. This leads to significant visual scalability challenges and imposes a high cognitive load on the users. Traditional instance-centric labeling methods, where (single) instances are labeled in each iteration struggle to scale effectively in these scenarios. To address these challenges, we introduce cVIL, a Class-Centric Visual Interactive Labeling methodology designed for interactive visual data labeling. By shifting the paradigm from assigning-classes-to-instances to assigning-instances-to-classes, cVIL reduces labeling effort and enhances efficiency for annotators working with large, complex and class-rich datasets. We propose a novel visual analytics labeling interface built on top of the conceptual cVIL workflow, enabling improved scalability over traditional visual labeling. In a user study, we demonstrate that cVIL can improve labeling efficiency and user satisfaction over instance-centric interfaces. The effectiveness of cVIL is further demonstrated through a usage scenario, showcasing its potential to alleviate cognitive load and support experts in managing extensive labeling tasks efficiently.",http://arxiv.org/abs/2505.03618v1,IS,Quantitative
Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow,"Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases like cancer, yet current digital pathology tools hinder diagnosis. The immense scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the limited views traditional monitors offer. This mismatch forces constant panning and zooming, increasing pathologist cognitive load, causing diagnostic fatigue, and slowing pathologists' adoption of digital methods. PathVis, our mixed-reality visualization platform for Apple Vision Pro, addresses these challenges. It transforms the pathologist's interaction with data, replacing cumbersome mouse-and-monitor navigation with intuitive exploration using natural hand gestures, eye gaze, and voice commands in an immersive workspace. PathVis integrates AI to enhance diagnosis. An AI-driven search function instantly retrieves and displays the top five similar patient cases side-by-side, improving diagnostic precision and efficiency through rapid comparison. Additionally, a multimodal conversational AI assistant offers real-time image interpretation support and aids collaboration among pathologists across multiple Apple devices. By merging the directness of traditional pathology with advanced mixed-reality visualization and AI, PathVis improves diagnostic workflows, reduces cognitive strain, and makes pathology practice more effective and engaging. The PathVis source code and a demo video are publicly available at: https://github.com/jaiprakash1824/Path_Vis",http://arxiv.org/abs/2505.02780v1,IS,Quantitative
Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations,"The rapid technological evolution has accelerated software development for various domains and use cases, contributing to a growing share of global carbon emissions. While recent large language models (LLMs) claim to assist developers in optimizing code for performance and energy efficiency, their efficacy in real-world scenarios remains under exploration. In this work, we explore the effectiveness of LLMs in reducing the environmental footprint of real-world projects, focusing on software written in Matlab-widely used in both academia and industry for scientific and engineering applications. We analyze energy-focused optimization on 400 scripts across 100 top GitHub repositories. We examine potential 2,176 optimizations recommended by leading LLMs, such as GPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy consumption, memory usage, execution time consumption, and code correctness. The developer serves as a real-world baseline for comparing typical human and LLM-generated optimizations. Mapping these optimizations to 13 high-level themes, we found that LLMs propose a broad spectrum of improvements--beyond energy efficiency--including improving code readability and maintainability, memory management, error handling while the developer overlooked some parallel processing, error handling etc. However, our statistical tests reveal that the energy-focused optimizations unexpectedly negatively impacted memory usage, with no clear benefits regarding execution time or energy consumption. Our qualitative analysis of energy-time trade-offs revealed that some themes, such as vectorization preallocation, were among the common themes shaping these trade-offs. With LLMs becoming ubiquitous in modern software development, our study serves as a call to action: prioritizing the evaluation of common coding practices to identify the green ones.",http://arxiv.org/abs/2503.20126v1,IS,Mixed
An Online Integrated Development Environment for Automated Programming Assessment Systems,"The increasing demand for programmers has led to a surge in participants in programming courses, making it increasingly challenging for instructors to assess student code manually. As a result, automated programming assessment systems (APASs) have been developed to streamline this process. These APASs support lecturers by managing and evaluating student programming exercises at scale. However, these tools often do not provide feature-rich online editors compared to their traditional integrated development environments (IDEs) counterparts. This absence of key features, such as syntax highlighting and autocompletion, can negatively impact the learning experience, as these tools are crucial for effective coding practice. To address this gap, this research contributes to the field of programming education by extracting and defining requirements for an online IDE in an educational context and presenting a prototypical implementation of an open-source solution for a scalable and secure online IDE. The usability of the new online IDE was assessed using the Technology Acceptance Model (TAM), gathering feedback from 27 first-year students through a structured survey. In addition to these qualitative insights, quantitative measures such as memory (RAM) usage were evaluated to determine the efficiency and scalability of the tool under varying usage conditions.",http://arxiv.org/abs/2503.13127v1,IS,Mixed
Using Large Language Models to Develop Requirements Elicitation Skills,"Requirements Elicitation (RE) is a crucial software engineering skill that involves interviewing a client and then devising a software design based on the interview results. Teaching this inherently experiential skill effectively has high cost, such as acquiring an industry partner to interview, or training course staff or other students to play the role of a client. As a result, a typical instructional approach is to provide students with transcripts of real or fictitious interviews to analyze, which exercises the skill of extracting technical requirements but fails to develop the equally important interview skill itself. As an alternative, we propose conditioning a large language model to play the role of the client during a chat-based interview. We perform a between-subjects study (n=120) in which students construct a high-level application design from either an interactive LLM-backed interview session or an existing interview transcript describing the same business processes. We evaluate our approach using both a qualitative survey and quantitative observations about participants' work. We find that both approaches provide sufficient information for participants to construct technically sound solutions and require comparable time on task, but the LLM-based approach is preferred by most participants. Importantly, we observe that LLM-backed interview is seen as both more realistic and more engaging, despite the LLM occasionally providing imprecise or contradictory information. These results, combined with the wide accessibility of LLMs, suggest a new way to practice critical RE skills in a scalable and realistic manner without the overhead of arranging live interviews.",http://arxiv.org/abs/2503.07800v2,IS,Mixed
Work in Progress: AI-Powered Engineering-Bridging Theory and Practice,"This paper explores how generative AI can help automate and improve key steps in systems engineering. It examines AI's ability to analyze system requirements based on INCOSE's ""good requirement"" criteria, identifying well-formed and poorly written requirements. The AI does not just classify requirements but also explains why some do not meet the standards. By comparing AI assessments with those of experienced engineers, the study evaluates the accuracy and reliability of AI in identifying quality issues. Additionally, it explores AI's ability to classify functional and non-functional requirements and generate test specifications based on these classifications. Through both quantitative and qualitative analysis, the research aims to assess AI's potential to streamline engineering processes and improve learning outcomes. It also highlights the challenges and limitations of AI, ensuring its safe and ethical use in professional and academic settings.",http://arxiv.org/abs/2502.04256v1,IS,Mixed
Experience with GitHub Copilot for Developer Productivity at Zoominfo,"This paper presents a comprehensive evaluation of GitHub Copilot's deployment and impact on developer productivity at Zoominfo, a leading Go-To-Market (GTM) Intelligence Platform. We describe our systematic four-phase approach to evaluating and deploying GitHub Copilot across our engineering organization, involving over 400 developers. Our analysis combines both quantitative metrics, focusing on acceptance rates of suggestions given by GitHub Copilot and qualitative feedback given by developers through developer satisfaction surveys. The results show an average acceptance rate of 33% for suggestions and 20% for lines of code, with high developer satisfaction scores of 72%. We also discuss language-specific performance variations, limitations, and lessons learned from this medium-scale enterprise deployment. Our findings contribute to the growing body of knowledge about AI-assisted software development in enterprise settings.",http://arxiv.org/abs/2501.13282v1,IS,Mixed
Poisson Hail on a Wireless Ground,"This paper defines a new model which incorporates three key ingredients of a large class of wireless communication systems: (1) spatial interactions through interference, (2) dynamics of the queueing type, with users joining and leaving, and (3) carrier sensing and collision avoidance as used in, e.g., WiFi. In systems using (3), rather than directly accessing the shared resources upon arrival, a customer is considerate and waits to access them until nearby users in service have left. This new model can be seen as a missing piece of a larger puzzle that contains such dynamics as spatial birth-and-death processes, the Poisson-Hail model, and wireless dynamics as key other pieces. It is shown that, under natural assumptions, this model can be represented as a Markov process on the space of counting measures. The main results are then two-fold. The first is on the shape of the stability region and, more precisely, on the characterization of the critical value of the arrival rate that separates stability from instability. The second is of a more qualitative or perhaps even ethical nature. There is evidence that for natural values of the system parameters, the implementation of sensing and collision avoidance stabilizes a system that would be unstable if immediate access to the shared resources would be granted. In other words, for these parameters, renouncing greedy access makes sharing sustainable, whereas indulging in greedy access kills the system.",http://arxiv.org/abs/2501.10712v1,IS,Qualitative
How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models,"Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average).",http://arxiv.org/abs/2501.01741v1,IS,Mixed
Barriers to Adopting Design for Assembly in Modular Product Architecture: Development of a Conceptual Model Through Content Analysis,"This study investigates the barriers to integrating Design for Assembly (DFA) principles within modular product architectures established using the Modular Function Deployment (MFD) method -- a critical stage for deploying mass customization production while reducing costs. Despite the potential benefits of DFA, its application in modular architectures development remains underutilized, due to a mix of challenges. Through content analysis of qualitative data gathered from a focus group and interviews with industry experts and practitioners, we identified four major categories of such challenges, or barriers to adoption of DFA: technological, economic, regulatory, and organizational (TERO). Key challenges include compliance with regulatory requirements for data usage, intellectual property concerns, and limited availability of quantitative data in the initial stages of MFD. The findings reveal that multidisciplinary collaboration is essential to addressing these barriers, as it enhances informed decision making and eases the practical integration of DFA. By analyzing insights from both academic literature and industrial practice, this research develops a conceptual model that describes the main issues of applying DFA in MFD, providing a valuable guide for companies aiming to improve their modular products assembly process. Ultimately, this study provides groundwork to support industry practitioners in overcoming existing barriers, promoting more cost effective, high quality modular design processes with the inclusion of efficient assembly considerations.",http://arxiv.org/abs/2411.17768v1,IS,Mixed
A Toolkit for Measuring the Impacts of Public Funding on Open Source Software Development,"Governments are increasingly employing funding for open source software (OSS) development as a policy lever to support the security of software supply chains, digital sovereignty, economic growth, and national competitiveness in science and innovation, among others. However, the impacts of public funding on OSS development remain poorly understood, with a lack of consensus on how to meaningfully measure them. This gap hampers assessments of the return on public investment and impedes the optimisation of public-interest funding strategies. We address this gap with a toolkit of methodological considerations that may inform such measurements, drawing on prior work on OSS valuations and community health metrics by the Community Health Analytics Open Source Software (CHAOSS) project as well as our first-hand learnings as practitioners tasked with evaluating funding programmes by the Next Generation Internet initiative and the Sovereign Tech Agency. We discuss salient considerations, including the importance of accounting for funding objectives, project life stage and social structure, and regional and organisational cost factors. Next, we present a taxonomy of potential social, economic, and technological impacts that can be both positive and negative, direct and indirect, internal (i.e. within a project) and external (i.e. among a project's ecosystem of dependents and users), and manifest over various time horizons. Furthermore, we discuss the merits and limitations of qualitative, quantitative, and mixed-methods approaches, as well as options for and hazards of estimating multiplier effects. With this toolkit, we contribute to the multi-stakeholder conversation about the value and impacts of funding on OSS developers and society at large.",http://arxiv.org/abs/2411.06027v1,IS,Mixed
How do practitioners gain confidence in assurance cases?,"CONTEXT: Assurance Cases (ACs) are prepared to argue that the system's desired quality attributes (e.g., safety or security) are satisfied. While there is strong adoption of ACs, practitioners are often left asking an important question: are we confident that the claims made by the case are true? While many confidence assessment methods (CAMs) exist, little is known about the use of these methods in practice OBJECTIVE: Develop an understanding of the current state of practice for AC confidence assessment: what methods are used in practice and what barriers exist for their use? METHOD: Structured interviews were performed with practitioners with experience contributing to real-world ACs. Open-coding was performed on transcripts. A description of the current state of AC practice and future considerations for researchers was synthesized from the results. RESULTS: A total of n = 19 practitioners were interviewed. The most common CAMs were (peer-)review of ACs, dialectic reasoning (""defeaters""), and comparing against checklists. Participants preferred qualitative methods and expressed concerns about quantitative CAMs. Barriers to using CAMs included additional work, inadequate guidance, subjectivity and interpretation of results, and trustworthiness of methods. CONCLUSION: While many CAMs are described in the literature there is a gap between the proposed methods and needs of practitioners. Researchers working in this area should consider the need to: connect CAMs to established practices, use CAMs to communicate with interest holders, crystallize the details of CAM application, curate accessible guidance, and confirm that methods are trustworthy.",http://arxiv.org/abs/2411.03657v1,IS,Mixed
Matrix Multiplication in the MPC Model,"In this paper, we study the matrix multiplication problem in the MPC model. We have two matrices, and the task is to compute their product. These matrices are evenly distributed over $P$ processors. Each processor has $M$ memory such that $P \cdot M \geq $ (size of the matrices). The computation proceeds in synchronous rounds. In a communication round, a processor can send and receive messages to(from) any other processor, with the total size of messages sent or received being $O(M)$. We give an almost complete characterisation of the problem in various settings. We prove tight upper bounds and lower bounds for the problems in three different settings--when the given input matrices are (i) general square matrices, (ii) rectangular matrices, and (iii) sparse square matrices (that is, each row and column contains a bounded number of nonzero elements). In particular, we prove the following results: 1. Multiplication of two $n \times n$ matrices in the MPC model with $n^\alpha$ processors each with $O(n^{2-\alpha})$ memory, requires $\Theta(n^{\frac{\alpha}{2}})$ rounds in semirings. 2. Multiplication of two rectangular matrices of size $n \times d$ and $d \times n$ (where $d \leq n$) respectively, with $n$ processors of $O(n)$ memory requires $\Theta(\frac{d}{ qrt{n}})$ rounds in semirings. 3. Multiplication of two rectangular matrices of size $d \times n$ and $n \times d$ ( where $d \leq n$) respectively requires i. $\Theta( qrt{d} + \log_d n)$ rounds with $n$ processors and $O(d)$ memory per processor in semirings ii. $\Theta (\frac{d}{ qrt{n}})$ rounds with $d$ processors and $O(n)$ memory per processor in semirings. 4. Multiplication of two $d$-sparse matrices (each row and column of the matrices contains at most $d$-nonzero elements) with $n$ processors and $O(d)$ memory per processor can be done in $O(d^{0.9})$ rounds in semirings.",http://arxiv.org/abs/2505.19137v1,IS,Quantitative
Multimodal Generative AI for Story Point Estimation in Software Development,"This research explores the application of Multimodal Generative AI to enhance story point estimation in Agile software development. By integrating text, image, and categorical data using advanced models like BERT, CNN, and XGBoost, our approach surpasses the limitations of traditional single-modal estimation methods. The results demonstrate strong accuracy for simpler story points, while also highlighting challenges in more complex categories due to data imbalance. This study further explores the impact of categorical data, particularly severity, on the estimation process, emphasizing its influence on model performance. Our findings emphasize the transformative potential of multimodal data integration in refining AI-driven project management, paving the way for more precise, adaptable, and domain-specific AI capabilities. Additionally, this work outlines future directions for addressing data variability and enhancing the robustness of AI in Agile methodologies.",http://arxiv.org/abs/2505.16290v1,IS,Quantitative
"Who ""Controls"" Where Work Shall be Done? State-of-Practice in Post-Pandemic Remote Work Regulation","The COVID-19 pandemic has permanently altered workplace structures, making remote work a widespread practice. While many employees advocate for flexibility, many employers reconsider their attitude toward remote work and opt for structured return-to-office mandates. Media headlines repeatedly emphasize that the corporate world is returning to full-time office work. This study examines how companies employing software engineers and supporting roles regulate work location, whether corporate policies have evolved in the last five years, and, if so, how, and why. We collected data on remote work regulation from corporate HR and/or management representatives from 68 corporate entities that vary in size, location, and orientation towards remote or office work. Our findings reveal that although many companies prioritize office-centred working (50%), most companies in our sample permit hybrid working to varying degrees (85%). Remote work regulation does not reveal any particular new ""best practice"" as policies differ greatly, but the single most popular arrangement was the three in-office days per week. More than half of the companies (51%) encourage or mandate office days, and more than quarter (28%) have changed regulations, gradually increasing the mandatory office presence or implementing differentiated conditions. Although no companies have increased flexibility, only four companies are returning to full-time office work. Our key recommendation for office-oriented companies is to consider a trust-based alternative to strict office presence mandates, while for companies oriented toward remote working, we warn about the points of no (or hard) return. Finally, the current state of policies is clearly not final, as companies continue to experiment and adjust their work regulation.",http://arxiv.org/abs/2505.15743v1,IS,Quantitative
Do you monitor CI Practices? I don't know. You tell me: A case study,"Background: In this paper we seek for understand the benefits and challenges of monitoring CI practices in day-to-day software development. Aims: We aim to evaluate the impact of monitoring seven CI practices in a real-world scenario on three public organizations in Brazil. Method: We first developed a CI practices monitoring suite tool and conducted a multiple-case study applying a mixed-methods strategy, combining surveys, interviews, log data, and mining data from CI services. Results: We verified organization' interest in monitoring CI practices. Monitoring provided an overview of the organization's CI status, not covered by other tools, motivated constant improvement in these practices, a perception of software quality, improve the communication and and it is easy to adopt. Conclusions: We recommend that companies adopt monitoring of CI practices and that CI services integrate this monitoring into their dashboards.",http://arxiv.org/abs/2503.02610v1,IS,Mixed
ML DevOps Adoption in Practice: A Mixed-Method Study of Implementation Patterns and Organizational Benefits,"Machine Learning (ML) DevOps, also known as MLOps, has emerged as a critical framework for efficiently operationalizing ML models in various industries. This study investigates the adoption trends, implementation efforts, and benefits of ML DevOps through a combination of literature review and empirical analysis. By surveying 150 professionals across industries and conducting in-depth interviews with 20 practitioners, the study provides insights into the growing adoption of ML DevOps, particularly in sectors like finance and healthcare. The research identifies key challenges, such as fragmented tooling, data management complexities, and skill gaps, which hinder widespread adoption. However, the findings highlight significant benefits, including improved deployment frequency, reduced error rates, enhanced collaboration between data science and DevOps teams, and lower operational costs. Organizations leveraging ML DevOps report accelerated model deployment, increased scalability, and better compliance with industry regulations. The study also explores the technical and cultural efforts required for successful implementation, such as investments in automation tools, real-time monitoring, and upskilling initiatives. The results indicate that while challenges remain, ML DevOps presents a viable path to optimizing ML lifecycle management, ensuring model reliability, and enhancing business value. Future research should focus on standardizing ML DevOps practices, assessing the return on investment across industries, and developing frameworks for seamless integration with traditional DevOps methodologies",http://arxiv.org/abs/2502.05634v1,IS,Mixed
From Bugs to Benefits: Improving User Stories by Leveraging Crowd Knowledge with CrUISE-AC,"Costs for resolving software defects increase exponentially in late stages. Incomplete or ambiguous requirements are one of the biggest sources for defects, since stakeholders might not be able to communicate their needs or fail to share their domain specific knowledge. Combined with insufficient developer experience, teams are prone to constructing incorrect or incomplete features. To prevent this, requirements engineering has to explore knowledge sources beyond stakeholder interviews. Publicly accessible issue trackers for systems within the same application domain hold essential information on identified weaknesses, edge cases, and potential error sources, all documented by actual users. Our research aims at (1) identifying, and (2) leveraging such issues to improve an agile requirements artifact known as a ""user story"". We present CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance Criteria) as a fully automated method that investigates issues and generates non-trivial additional acceptance criteria for a given user story by employing NLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five independent experts in two distinct business domains. Our findings suggest that issue trackers hold valuable information pertinent to requirements engineering. Our evaluation shows that 80-82% of the generated acceptance criteria add relevant requirements to the user stories. Limitations are the dependence on accessible input issues and the fact that we do not check generated criteria for being conflict-free or non-overlapping with criteria from other user stories.",http://arxiv.org/abs/2501.15181v2,IS,Qualitative
Automating Explanation Need Management in App Reviews: A Case Study from the Navigation App Industry,"Providing explanations in response to user reviews is a time-consuming and repetitive task for companies, as many reviews present similar issues requiring nearly identical responses. To improve efficiency, this paper proposes a semi-automated approach to managing explanation needs in user reviews. The approach leverages taxonomy categories to classify reviews and assign them to relevant internal teams or sources for responses. 2,366 app reviews from the Google Play Store and Apple App Store were scraped and analyzed using a word and phrase filtering system to detect explanation needs. The detected needs were categorized and assigned to specific internal teams at the company Graphmasters GmbH, using a hierarchical assignment strategy that prioritizes the most relevant teams. Additionally, external sources, such as existing support articles and past review responses, were integrated to provide comprehensive explanations. The system was evaluated through interviews and surveys with the Graphmasters support team, which consists of four employees. The results showed that the hierarchical assignment method improved the accuracy of team assignments, with correct teams being identified in 79.2% of cases. However, challenges in interrater agreement and the need for new responses in certain cases, particularly for Apple App Store reviews, were noted. Future work will focus on refining the taxonomy and enhancing the automation process to reduce manual intervention further.",http://arxiv.org/abs/2501.08087v1,IS,Mixed
Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development,"Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators engage with those questions, the views of users can provide relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers' views at a time when GenAI and perceptions of it are rapidly evolving; and (3) an analysis of developers' views, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and much more, providing organizations and policymakers with valuable insights into how the technology is being used and what concerns stakeholders would like to see addressed.",http://arxiv.org/abs/2411.10877v3,IS,Mixed
The Enhancement of Software Delivery Performance through Enterprise DevSecOps and Generative Artificial Intelligence in Chinese Technology Firms,"This study investigates the impact of integrating DevSecOps and Generative Artificial Intelligence (GAI) on software delivery performance within technology firms. Utilizing a qualitative research methodology, the research involved semi-structured interviews with industry practitioners and analysis of case studies from organizations that have successfully implemented these methodologies. The findings reveal significant enhancements in research and development (R&D) efficiency, improved source code management, and heightened software quality and security. The integration of GAI facilitated automation of coding tasks and predictive analytics, while DevSecOps ensured that security measures were embedded throughout the development lifecycle. Despite the promising results, the study identifies gaps related to the generalizability of the findings due to the limited sample size and the qualitative nature of the research. This paper contributes valuable insights into the practical implementation of DevSecOps and GAI, highlighting their potential to transform software delivery processes in technology firms. Future research directions include quantitative assessments of the impact on specific business outcomes and comparative studies across different industries.",http://arxiv.org/abs/2411.02255v1,IS,Mixed
User Feedback in Continuous Software Engineering: Revealing the State-of-Practice,"Context: Organizations opt for continuous delivery of incremental updates to deal with uncertainty and minimize waste. However, applying continuous engineering (CSE) practices requires a continuous feedback loop with input from customers and end-users. Challenges: It becomes increasingly challenging to apply traditional requirements elicitation and validation techniques with ever-shrinking software delivery cycles. At the same time, frequent deliveries generate an abundance of usage data and telemetry informing engineering teams of end-user behavior. The literature describing how practitioners work with user feedback in CSE, is limited. Objectives: We aim to explore the state of practice related to utilization of user feedback in CSE. Specifically, what practices are used, how, and the shortcomings of these practices. Method: We conduct a qualitative survey and report analysis from 21 interviews in 13 product development companies. We apply thematic and cross-case analysis to interpret the data. Results: Based on our earlier work we suggest a conceptual model of how user feedback is utilized in CSE. We further report the identified challenges with the continuous collection and analysis of user feedback and identify implications for practice. Conclusions: Companies use a combination of qualitative and quantitative methods to infer end-user preferences. At the same time, continuous collection, analysis, interpretation, and use of data in decisions are problematic. The challenges pertain to selecting the right metrics and analysis techniques, resource allocation, and difficulties in accessing vaguely defined user groups. Our advice to practitioners in CSE is to ensure sufficient resources and effort for interpretation of the feedback, which can be facilitated by telemetry dashboards.",http://arxiv.org/abs/2410.07459v1,IS,Mixed
"""I Don't Use AI for Everything"": Exploring Utility, Attitude, and Responsibility of AI-empowered Tools in Software Development","AI-empowered tools have emerged as a transformative force, fundamentally reshaping the software development industry and promising far-reaching impacts across diverse sectors. This study investigates the adoption, impact, and security considerations of AI-empowered tools in the software development process. Through semi-structured interviews with 19 software practitioners from diverse backgrounds, we explore three key aspects: the utility of AI tools, developers' attitudes towards them, and security and privacy responsibilities. Our findings reveal widespread adoption of AI tools across various stages of software development. Developers generally express positive attitudes towards AI, viewing it as an efficiency-enhancing assistant rather than a job replacement threat. However, they also recognized limitations in AI's ability to handle complex, unfamiliar, or highly specialized tasks in software development. Regarding security and privacy, we found varying levels of risk awareness among developers, with larger companies implementing more comprehensive risk management strategies. Our study provides insights into the current state of AI adoption in software development and offers recommendations for practitioners, organizations, AI providers, and regulatory bodies to effectively navigate the integration of AI in the software industry.",http://arxiv.org/abs/2409.13343v2,IS,Qualitative
Assessing Python Style Guides: An Eye-Tracking Study with Novice Developers,"The incorporation and adaptation of style guides play an essential role in software development, influencing code formatting, naming conventions, and structure to enhance readability and simplify maintenance. However, many of these guides often lack empirical studies to validate their recommendations. Previous studies have examined the impact of code styles on developer performance, concluding that some styles have a negative impact on code readability. However, there is a need for more studies that assess other perspectives and the combination of these perspectives on a common basis through experiments. This study aimed to investigate, through eye-tracking, the impact of guidelines in style guides, with a special focus on the PEP8 guide in Python, recognized for its best practices. We conducted a controlled experiment with 32 Python novices, measuring time, the number of attempts, and visual effort through eye-tracking, using fixation duration, fixation count, and regression count for four PEP8 recommendations. Additionally, we conducted interviews to explore the subjects' difficulties and preferences with the programs. The results highlighted that not following the PEP8 Line Break after an Operator guideline increased the eye regression count by 70% in the code snippet where the standard should have been applied. Most subjects preferred the version that adhered to the PEP8 guideline, and some found the left-aligned organization of operators easier to understand. The other evaluated guidelines revealed other interesting nuances, such as the True Comparison, which negatively impacted eye metrics for the PEP8 standard, although subjects preferred the PEP8 suggestion. We recommend practitioners selecting guidelines supported by experimental evaluations.",http://arxiv.org/abs/2408.14566v1,IS,Mixed
Who Introduces and Who Fixes? Analyzing Code Quality in Collaborative Student's Projects,"This paper investigates code quality education by analyzing how errors are introduced and corrected in group projects within an embedded systems course. We identify who introduces errors, who fixes them, and when these actions occur. Students learn code quality rules for C and embedded systems. We address three questions: RQ1: What is the impact of group formation on code quality? RQ2: How do students interact to fix code issues? RQ3: When are issues introduced and resolved? We analyzed data from eight individual labs and two group projects involving 34 students. The course provides continuous, automated feedback on code quality. Findings show that the most active contributors often introduce the most issues. Many issues are fixed late in the project. Individual labs tend to have fewer issues due to their structured nature. Most problems are fixed by the original author, while cross-student fixes take longer, especially in shared code. Critical issues are fixed quickly, but non-critical ones may be ignored, showing a focus on functionality over quality.",http://arxiv.org/abs/2505.14315v1,IS,Quantitative
Verification of Digital Twins using Classical and Statistical Model Checking,"With the increasing adoption of digital techniques, the concept of digital twin (DT) has received a widespread attention in both industry and academia. While several definitions exist for a DT, most definitions focus on the existence of a virtual entity (VE) of a real-world object or process, often comprising interconnected models which interact with each other, undergoing changes continuously owing to the synchronization with the real-world object. These interactions might lead to inconsistencies at execution time, due to their highly stochastic and/or time-critical nature, which may lead to undesirable behavior. In addition, the continuously varying nature of VE owing to its synchronization with the real-world object further contributes to the complexity arising from these interactions and corresponding model execution times, which could possibly affect its overall functioning at runtime. This creates a need to perform (continuous) verification of the VE, to ensure that it behaves consistently at runtime by adhering to desired properties such as deadlock freeness, functional correctness, liveness and timeliness. Some critical properties such as deadlock freeness can only be verified using classical model checking; on the other hand, statistical model checking provides the possibility to model actual stochastic temporal behavior. We therefore propose to use both these techniques to verify the correctness and the fulfillment of desirable properties of VE. We present our observations and findings from applying these techniques on the DT of an autonomously driving truck. Results from these verification techniques suggest that this DT adheres to properties of deadlock freeness and functional correctness, but not adhering to timeliness properties.",http://arxiv.org/abs/2505.04322v1,IS,Mixed
Moral Testing of Autonomous Driving Systems,"Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS.",http://arxiv.org/abs/2505.03683v1,IS,Quantitative
A Self-Healing and Fault-Tolerant Cloud-based Digital Twin Processing Management Model,"Digital twins, integral to cloud platforms, bridge physical and virtual worlds, fostering collaboration among stakeholders in manufacturing and processing. However, the cloud platforms face challenges like service outages, vulnerabilities, and resource contention, hindering critical digital twin application development. The existing research works have limited focus on reliability and fault tolerance in digital twin processing. In this context, this paper proposed a novel Self-healing and Faulttolerant cloud-based Digital Twin processing Management (SF-DTM) model. It employs collaborative digital twin tasks resource requirement estimation unit which utilizes newly devised Federated learning with cosine Similarity integration (SimiFed). Further, SF-DTM incorporates a self-healing fault-tolerance strategy employing a frequent sequence fault-prone pattern analytics unit for deciding the most admissible VM allocation. The implementation and evaluation of SF-DTM model using real traces demonstrates its effectiveness and resilience, revealing improved availability, higher Mean Time Between Failure (MTBF), and lower Mean Time To Repair (MTTR) compared with non-SF-DTM approaches, enhancing collaborative DT application management. SF-DTM improved the services availability up to 13.2% over non-SF-DTM-based DT processing.",http://arxiv.org/abs/2505.01215v1,IS,Quantitative
PatchFuzz: Patch Fuzzing for JavaScript Engines,"Patch fuzzing is a technique aimed at identifying vulnerabilities that arise from newly patched code. While researchers have made efforts to apply patch fuzzing to testing JavaScript engines with considerable success, these efforts have been limited to using ordinary test cases or publicly available vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of these approaches is hindered by the challenges associated with automating the PoC collection. To address these limitations, we propose an end-to-end sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It automates the collection of PoCs of a broader range of historical vulnerabilities and leverages both the PoCs and their corresponding patches to uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing git commits which intend to fix security bugs. Subsequently, it extracts and processes PoCs from these commits to form the seeds for fuzzing, while utilizing code revisions to focus limited fuzzing resources on the more vulnerable code areas through selective instrumentation. The mutation strategy of PatchFuzz is also optimized to maximize the potential of the PoCs. Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54 bugs across six popular JavaScript engines have been exposed and a total of $62,500 bounties has been received.",http://arxiv.org/abs/2505.00289v1,IS,Quantitative
Canonicalization for Unreproducible Builds in Java,"The increasing complexity of software supply chains and the rise of supply chain attacks have elevated concerns around software integrity. Users and stakeholders face significant challenges in validating that a given software artifact corresponds to its declared source. Reproducible Builds address this challenge by ensuring that independently performed builds from identical source code produce identical binaries. However, achieving reproducibility at scale remains difficult, especially in Java, due to a range of non-deterministic factors and caveats in the build process. In this work, we focus on reproducibility in Java-based software, archetypal of enterprise applications. We introduce a conceptual framework for reproducible builds, we analyze a large dataset from Reproducible Central, and we develop a novel taxonomy of six root causes of unreproducibility. We study actionable mitigations: artifact and bytecode canonicalization using OSS-Rebuild and jNorm respectively. Finally, we present Chains-Rebuild, a tool that raises reproducibility success from 9.48% to 26.89% on 12,283 unreproducible artifacts. To sum up, our contributions are the first large-scale taxonomy of build unreproducibility causes in Java, a publicly available dataset of unreproducible builds, and Chains-Rebuild, a canonicalization tool for mitigating unreproducible builds in Java.",http://arxiv.org/abs/2504.21679v1,IS,Quantitative
Understanding and Detecting Peer Dependency Resolving Loop in npm Ecosystem,"As the default package manager for Node.js, npm has become one of the largest package management systems in the world. To facilitate dependency management for developers, npm supports a special type of dependency, Peer Dependency, whose installation and usage differ from regular dependencies. However, conflicts between peer dependencies can trap the npm client into infinite loops, leading to resource exhaustion and system crashes. We name this problem PeerSpin. Although PeerSpin poses a severe risk to ecosystems, it was overlooked by previous studies, and its impacts have not been explored. To bridge this gap, this paper conducts the first in-depth study to understand and detect PeerSpin in the npm ecosystem. First, by systematically analyzing the npm dependency resolution, we identify the root cause of PeerSpin and characterize two peer dependency patterns to guide detection. Second, we propose a novel technique called Node-Replacement-Conflict based PeerSpin Detection, which leverages the state of the directory tree during dependency resolution to achieve accurate and efficient PeerSpin detection. Based on this technique, we developed a tool called PeerChecker to detect PeerSpin. Finally, we apply PeerChecker to the entire NPM ecosystem and find that 5,662 packages, totaling 72,968 versions, suffer from PeerSpin. Up until now, we confirmed 28 real PeerSpin problems by reporting them to the package maintainer. We also open source all PeerSpin analysis implementations, tools, and data sets to the public to help the community detect PeerSpin issues and enhance the reliability of the npm ecosystem.",http://arxiv.org/abs/2505.12676v2,IS,Quantitative
Understanding the Sneaky Patterns of Pop-up Windows in the Mobile Ecosystem,"In mobile applications, Pop-up window (PoW) plays a crucial role in improving user experience, guiding user actions, and delivering key information. Unfortunately, the excessive use of PoWs severely degrades the user experience. These PoWs often sneakily mislead users in their choices, employing tactics that subtly manipulate decision-making processes. In this paper, we provide the first in-depth study on the Sneaky patterns in the mobile ecosystem. Our research first highlights five distinct Sneaky patterns that compromise user experience, including text mislead, UI mislead, forced action, out of context and privacy-intrusive by default. To further evaluate the impact of such Sneaky patterns at large, we developed an automated analysis pipeline called Poker, to tackle the challenges of identifying, dismissing, and collecting diverse PoWs in real-world apps. Evaluation results showed that Poker achieves high precision and recall in detecting PoWs, efficiently dismissed over 88% of PoWs with minimal user interaction, with good robustness and reliability in comprehensive app exploration. Further, our systematic analysis over the top 100 popular apps in China and U.S. revealing that both regions displayed significant ratios of Sneaky patterns, particularly in promotional contexts, with high occurrences in categories such as shopping and video apps. The findings highlight the strategic deployment of Sneaky tactics that compromise user trust and ethical app design.",http://arxiv.org/abs/2505.12056v1,IS,Quantitative
LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps,"The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8\% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.",http://arxiv.org/abs/2505.10537v1,IS,Quantitative
AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons,"Vehicle platooning, with vehicles traveling in close formation coordinated through Vehicle-to-Everything (V2X) communications, offers significant benefits in fuel efficiency and road utilization. However, it is vulnerable to sophisticated falsification attacks by authenticated insiders that can destabilize the formation and potentially cause catastrophic collisions. This paper addresses this challenge: misbehavior detection in vehicle platooning systems. We present AttentionGuard, a transformer-based framework for misbehavior detection that leverages the self-attention mechanism to identify anomalous patterns in mobility data. Our proposal employs a multi-head transformer-encoder to process sequential kinematic information, enabling effective differentiation between normal mobility patterns and falsification attacks across diverse platooning scenarios, including steady-state (no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an extensive simulation dataset featuring various attack vectors (constant, gradual, and combined falsifications) and operational parameters (controller types, vehicle speeds, and attacker positions). Experimental results demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance maintained during complex maneuvers. Notably, our system performs effectively with minimal latency (100ms decision intervals), making it suitable for real-time transportation safety applications. Comparative analysis reveals superior detection capabilities and establishes the transformer-encoder as a promising approach for securing Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider threats.",http://arxiv.org/abs/2505.10273v1,IS,Quantitative
Wormhole Detection Based on Z-Score And Neighbor Table Comparison,"Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks. By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks. This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.) as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks. Finally, we perform a large number of simulations to evaluate the proposed method. Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief.",http://arxiv.org/abs/2505.09405v1,IS,Quantitative
EnvCDiff: Joint Refinement of Environmental Information and Channel Fingerprints via Conditional Generative Diffusion Model,"The paradigm shift from environment-unaware communication to intelligent environment-aware communication is expected to facilitate the acquisition of channel state information for future wireless communications. Channel Fingerprint (CF), as an emerging enabling technology for environment-aware communication, provides channel-related knowledge for potential locations within the target communication area. However, due to the limited availability of practical devices for sensing environmental information and measuring channel-related knowledge, most of the acquired environmental information and CF are coarse-grained, insufficient to guide the design of wireless transmissions. To address this, this paper proposes a deep conditional generative learning approach, namely a customized conditional generative diffusion model (CDiff). The proposed CDiff simultaneously refines environmental information and CF, reconstructing a fine-grained CF that incorporates environmental information, referred to as EnvCF, from its coarse-grained counterpart. Experimental results show that the proposed approach significantly improves the performance of EnvCF construction compared to the baselines.",http://arxiv.org/abs/2505.07894v1,IS,Quantitative
Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support,"Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms dispatched for emergency response, often require dynamic reconfiguration at runtime under the supervision of human operators. This introduces human-on-the-loop requirements, where evolving needs shape ongoing system functionality and behaviors. While traditional personas support upfront, static requirements elicitation, we propose a persona-based advocate framework for runtime requirements engineering to provide ethically informed, safety-driven, and regulatory-aware decision support. Our approach extends standard personas into event-driven personas. When triggered by events such as adverse environmental conditions, evolving mission state, or operational constraints, the framework updates the sUAS operator's view of the personas, ensuring relevance to current conditions. We create three key advocate personas, namely Safety Controller, Ethical Governor, and Regulatory Auditor, to manage trade-offs among risk, ethical considerations, and regulatory compliance. We perform a proof-of-concept validation in an emergency response scenario using sUAS, showing how our advocate personas provide context-aware guidance grounded in safety, regulatory, and ethical constraints. By evolving static, design-time personas into adaptive, event-driven advocates, the framework surfaces mission-critical runtime requirements in response to changing conditions. These requirements shape operator decisions in real time, aligning actions with the operational demands of the moment.",http://arxiv.org/abs/2505.04551v1,IS,Mixed
Evaluating the Impact of Data Cleaning on the Quality of Generated Pull Request Descriptions,"Pull Requests (PRs) are central to collaborative coding, summarizing code changes for reviewers. However, many PR descriptions are incomplete, uninformative, or have out-of-context content, compromising developer workflows and hindering AI-based generation models trained on commit messages and original descriptions as ""ground truth."" This study examines the prevalence of ""noisy"" PRs and evaluates their impact on state-of-the-art description generation models. To do so, we propose four cleaning heuristics to filter noise from an initial dataset of 169K+ PRs drawn from 513 GitHub repositories. We train four models-BART, T5, PRSummarizer, and iTAPE-on both raw and cleaned datasets. Performance is measured via ROUGE-1, ROUGE-2, and ROUGE-L metrics, alongside a manual evaluation to assess description quality improvements from a human perspective. Cleaning the dataset yields significant gains: average F1 improvements of 8.6% (ROUGE-1), 8.7% (ROUGE-2), and 8.5% (ROUGE-L). Manual assessment confirms higher readability and relevance in descriptions generated by the best-performing model, BART when trained on cleaned data. Dataset refinement markedly enhances PR description generation, offering a foundation for more accurate AI-driven tools and guidelines to assist developers in crafting high-quality PR descriptions.",http://arxiv.org/abs/2505.01120v1,IS,Quantitative
Information Freshness in Dynamic Gossip Networks,"We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.",http://arxiv.org/abs/2504.18504v1,IS,Mixed
"""Ohhh, He's the Boss!"": Unpacking Power Dynamics Among Developers, Designers, and End-Users in FLOSS Usability","Addressing usability in free, libre, and open-source software (FLOSS) is a challenging issue, particularly due to a long-existing ""by developer, for developer"" mentality. Engaging designers and end-users to work with developers can help improve its usability, but unequal power dynamics among those stakeholder roles must be mitigated. To explore how the power of different FLOSS stakeholders manifests and can be mediated during collaboration, we conducted eight design workshops with different combinations of key FLOSS stakeholders (i.e., developers, designers, and end-users). Leveraging existing theories on Dimensions of Power, we revealed how participants navigate existing role-based power structures through resource utilization, knowledge gap management, and experience referencing. We also observed that participants exhibited diverse behaviors confirming and challenging the status quo of FLOSS usability. Overall, our results contribute to a comprehensive understanding of the power dynamics among FLOSS stakeholders, providing valuable insights into ways to balance their power to improve FLOSS usability. Our work also serves as an exemplar of using design workshops as a research method to study power dynamics during collaboration that are usually hidden in the field.",http://arxiv.org/abs/2504.15494v1,IS,Mixed
Certus: A domain specific language for confidence assessment in assurance cases,"Assurance cases (ACs) are prepared to argue that a system has satisfied critical quality attributes. Many methods exist to assess confidence in ACs, including quantitative methods that represent confidence numerically. While quantitative methods are attractive in principle, existing methods suffer from issues related to interpretation, subjectivity, scalability, dialectic reasoning, and trustworthiness, which have limited their adoption. This paper introduces Certus, a domain specific language for quantitative confidence assessment. In Certus, users describe their confidence with fuzzy sets, which allow them to represent their judgment using vague, but linguistically meaningful terminology. Certus includes syntax to specify confidence propagation using expressions that can be easily inspected by users. To demonstrate the concept of the language, Certus is applied to a worked example from the automotive domain.",http://arxiv.org/abs/2505.01894v1,IS,Quantitative
Automatically Generating Single-Responsibility Unit Tests,"Automatic test generation aims to save developers time and effort by producing test suites with reasonably high coverage and fault detection. However, the focus of search-based generation tools in maximizing coverage leaves other properties, such as test quality, coincidental. The evidence shows that developers remain skeptical of using generated tests as they face understandability challenges. Generated tests do not follow a defined structure while evolving, which can result in tests that contain method calls to improve coverage but lack a clear relation to the generated assertions. In my doctoral research, I aim to investigate the effects of providing a pre-process structure to the generated tests, based on the single-responsibility principle to favor the identification of the focal method under test. To achieve this, we propose to implement different test representations for evolution and evaluate their impact on coverage, fault detection, and understandability. We hypothesize that improving the structure of generated tests will report positive effects on the tests' understandability without significantly affecting the effectiveness. We aim to conduct a quantitative analysis of this proposed approach as well as a developer evaluation of the understandability of these tests.",http://arxiv.org/abs/2504.06431v2,IS,Quantitative
Exit the Code: A Model for Understanding Career Abandonment Intention Among Software Developers,"Background. Career abandonment, the process in which professionals leave the activity, assuming positions in another area, among software developers involves frustration with the lost investment and emotional and financial costs, even though being beneficial for the human being, depending on personal context. Previous studies have identified work-related motivators for career abandonment, such as the threat of obsolescence, unstable requirements, and low code quality, though these factors have primarily been examined in former developers. The relationship between these motivators and the intention to abandon among currently active developers remains unexplored. Goal. This article investigates the relationship between key work-related motivators and currently active software developers intention to abandon their careers. Method. We employed a quantitative approach, surveying 221 software developers to validate a theoretical model for career abandonment intention, based on an adaptation of the Investment Model, which incorporates satisfaction with technical aspects of the profession as well as the intention to abandon. Findings. Exploratory and confirmatory factor analyses, through structural equation modeling (SEM), provided robust support for the adapted Investment Model in explaining software developers intention to abandon their careers. Moreover, career commitment significantly impacts the intention to leave the profession, being positively influenced by satisfaction with technical work-related factors and negatively influenced by career alternatives and career investment. Conclusion. The paper offers valuable insights for organizational leaders and research, potentially guiding retention strategies to better support developers, and the adoption of theoretical models to explain career abandonment.",http://arxiv.org/abs/2503.04460v1,IS,Quantitative
VoxLogicA UI: Supporting Declarative Medical Image Analysis,"This Master's Thesis in Computer Science dives into the design and creation of a user-friendly interface for VoxLogicA, an image analysis tool using spatial model checking with a focus on neuroimaging. The research tackles the problem of existing tools being too complex, which makes them hard for medical professionals and researchers to use. By using spatial logic, the goal is to make these powerful analytical tools more practical and accessible in real-world clinical settings. The main objectives are to design a modern web interface that's easy to use, build it with the latest web technologies (e.g. Svelte and Niivue), and test its effectiveness through user studies and real-world case analyses.",http://arxiv.org/abs/2504.13846v1,IS,Quantitative
Enhancing Large Language Model Efficiencyvia Symbolic Compression: A Formal Approach Towards Interpretability,"Large language models (LLMs) face significant token efficiency bottlenecks in code generation and logical reasoning tasks, a challenge that directly impacts inference cost and model interpretability. This paper proposes a formal framework based on symbolic compression,integrating combinatory logic, information-theoretic optimal encoding, and context-aware inference techniques to achieve a step-change improvement in token efficiency while preserving semantic integrity. We establish a mathematical framework within a functional programming paradigm, derive the quantitative relationship between symbolic density and model interpretability, and propose a differentiable compression factor metric to evaluate encoding efficiency. Furthermore, we leverage parameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost application of the GAEL language. Experimental results show that this method achieves a 78.3% token compression rate in code generation tasks while improving logical traceability by 62% through structural explicitness. This research provides new theoretical tools for efficient inference in LLMs and opens a symbolic path for modelinterpretability research.",http://arxiv.org/abs/2501.18657v1,IS,Quantitative
A Stochastic Geometry Based Techno-Economic Analysis of RIS-Assisted Cellular Networks,"Reconfigurable intelligent surfaces (RISs) are a promising technology for enhancing cellular network performance and yielding additional value to network operators. This paper proposes a techno-economic analysis of RIS-assisted cellular networks to guide operators in deciding between deploying additional RISs or base stations (BS). We assume a relative cost model that considers the total cost of ownership (TCO) of deploying additional nodes, either BSs or RISs. We assume a return on investment (RoI) that is proportional to the system's spectral efficiency. The latter is evaluated based on a stochastic geometry model that gives an integral formula for the ergodic rate in cellular networks equipped with RISs. The marginal RoI for any investment strategy is determined by the partial derivative of this integral expression with respect to node densities. We investigate two case studies: throughput enhancement and coverage hole mitigation. These examples demonstrate how operators could determine the optimal investment strategy in scenarios defined by the current densities of BSs and RISs, and their relative costs. Numerical results illustrate the evolution of ergodic rates based on the proposed investment strategy, demonstrating the investment decision-making process while considering technological and economic factors. This work quantitatively demonstrates that strategically investing in RISs can offer better system-level benefits than solely investing in BS densification.",http://arxiv.org/abs/2501.12037v1,IS,Quantitative
Leader Rotation Is Not Enough: Scrutinizing Leadership Democracy of Chained BFT Consensus,"With the growing popularity of blockchains, modern chained BFT protocols combining chaining and leader rotation to obtain better efficiency and leadership democracy have received increasing interest. Although the efficiency provisions of chained BFT protocols have been thoroughly analyzed, the leadership democracy has received little attention in prior work. In this paper, we scrutinize the leadership democracy of four representative chained BFT protocols, especially under attack. To this end, we propose a unified framework with two evaluation metrics, i.e., chain quality and censorship resilience, and quantitatively analyze chosen protocols through the Markov Decision Process (MDP). With this framework, we further examine the impact of two key components, i.e., voting pattern and leader rotation on leadership democracy. Our results indicate that leader rotation is not enough to provide the leadership democracy guarantee; an adversary could utilize the design, e.g., voting pattern, to deteriorate the leadership democracy significantly. Based on the analysis results, we propose customized countermeasures for three evaluated protocols to improve their leadership democracy with only slight protocol overhead and no change of consensus rules. We also discuss future directions toward building more democratic chained BFT protocols.",http://arxiv.org/abs/2501.02970v1,IS,Quantitative
"Towards One-shot Federated Learning: Advances, Challenges, and Future Directions","One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.",http://arxiv.org/abs/2505.02426v1,IS,Quantitative
An instrument to measure factors that constitute the socio-technical context of testing experience,"We consider testing a cooperative and social practice that is shaped by the tools developers use, the tests they write, and their mindsets and human needs. This work is one part of a project that explores the human- and socio-technical context of testing through the lens of those interwoven elements: test suite and tools as technical infrastructure and collaborative factors and motivation as mindset. Drawing on empirical observations of previous work, this survey examines how these factors relate to each other. We want to understand which combination of factors can help developers strive and make the most of their ambitions to leverage the potential that software testing practices have. In this report, we construct a survey instrument to measure the factors that constitute the socio-technical context of testing experience. In addition, we state our hypotheses about how these factors impact testing experience and explain the considerations and process that led to the construction of the survey questions.",http://arxiv.org/abs/2505.01171v1,IS,Mixed
Modeling Communication Perception in Development Teams Using Monte Carlo Methods,"Software development is a collaborative task involving diverse development teams, where toxic communication can negatively impact team mood and project success. Mood surveys enable the early detection of underlying tensions or dissatisfaction within development teams, allowing communication issues to be addressed before they escalate, fostering a positive and productive work environment. The mood can be surveyed indirectly by analyzing the text-based communication of the team. However, emotional subjectivity leads to varying sentiment interpretations across team members; a statement perceived neutrally by one developer might be seen as problematic by another developer with a different conversational culture. Early identification of perception volatility can help prevent misunderstandings and enhance team morale while safeguarding the project. This paper analyzes the diversity of perceptions within arbitrary development teams and determines how many team members should report their sentiment to accurately reflect the team's mood. Through a Monte Carlo experiment involving 45 developers, we present a preliminary mathematical model to calculate the minimum agreement among a subset of developers based on the whole team's agreement. This model can guide leadership in mood assessment, demonstrating that omitting even a single member in an average-sized 7-member team can misrepresent the overall mood. Therefore, including all developers in mood surveying is recommended to ensure a reliable evaluation of the team's mood.",http://arxiv.org/abs/2504.17610v1,IS,Quantitative
Creating benchmarkable components to measure the quality ofAI-enhanced developer tools,"In the AI community, benchmarks to evaluate model quality are well established, but an equivalent approach to benchmarking products built upon generative AI models is still missing. This has had two consequences. First, it has made teams focus on model quality over the developer experience, while successful products combine both. Second, product team have struggled to answer questions about their products in relation to their competitors. In this case study, we share: (1) our process to create robust, enterprise-grade and modular components to support the benchmarking of the developer experience (DX) dimensions of our team's AI for code offerings, and (2) the components we have created to do so, including demographics and attitudes towards AI surveys, a benchmarkable task, and task and feature surveys. By doing so, we hope to lower the barrier to the DX benchmarking of genAI-enhanced code products.",http://arxiv.org/abs/2504.12211v1,IS,Mixed
Who Speaks for Ethics? How Demographics Shape Ethical Advocacy in Software Development,"The integration of ethics into software development faces significant challenges due to market fundamentalism in organizational practices, where profit often takes precedence over ethical considerations. Additionally, the critical influence of practitioners' individual backgrounds on ethical decision-making remains underexplored, highlighting a gap in comprehensive research. This is especially essential to understand due to the demographic imbalance in software roles. This study investigates ethical concerns in software development, focusing on how they are perceived, prioritized, and addressed by demographically different practitioners. By surveying 217 software practitioners across diverse roles, industries, and countries, we identify critical barriers to ethical integration and examine practitioners' capacity to mitigate these issues. Our findings reveal pronounced demographic disparities, with marginalized groups - including women, BIPOC, and disabled individuals - reporting ethical concerns at higher frequencies. Notably, marginalized practitioners demonstrated heightened sensitivity to ethical implementation and greater empowerment to address them. However, practitioners overall often lack the support needed to address ethical challenges effectively. These insights underscore the urgent need for reforms in software education and development processes that center on diverse perspectives. Such reforms are essential to advancing ethical integration in software development and ensuring responsible computing practices in an increasingly complex technological landscape.",http://arxiv.org/abs/2504.10276v1,IS,Quantitative
From Teacher to Colleague: How Coding Experience Shapes Developer Perceptions of AI Tools,"AI-assisted development tools promise productivity gains and improved code quality, yet their adoption among developers remains inconsistent. Prior research suggests that professional expertise influences technology adoption, but its role in shaping developers' perceptions of AI tools is unclear. We analyze survey data from 3380 developers to examine how coding experience relates to AI awareness, adoption, and the roles developers assign to AI in their workflow. Our findings reveal that coding experience does not predict AI adoption but significantly influences mental models of AI's role. Experienced developers are more likely to perceive AI as a junior colleague, a content generator, or assign it no role, whereas less experienced developers primarily view AI as a teacher. These insights suggest that AI tools must align with developers' expertise levels to drive meaningful adoption.",http://arxiv.org/abs/2504.13903v1,IS,Quantitative
Beyond A Single AI Cluster: A Survey of Decentralized LLM Training,"The emergence of large language models (LLMs) has revolutionized AI development, yet the resource demands beyond a single cluster or even datacenter, limiting accessibility to well-resourced organizations. Decentralized training has emerged as a promising paradigm to leverage dispersed resources across clusters, datacenters and regions, offering the potential to democratize LLM development for broader communities. As the first comprehensive exploration of this emerging field, we present decentralized LLM training as a resource-driven paradigm and categorize existing efforts into community-driven and organizational approaches. We further clarify this through: (1) a comparison with related paradigms, (2) a characterization of decentralized resources, and (3) a taxonomy of recent advancements. We also provide up-to-date case studies and outline future directions to advance research in decentralized LLM training.",http://arxiv.org/abs/2503.11023v2,IS,Quantitative
User-UAV Association for Dynamic User in mmWave Communication for eMBB and URLLC,"In unmanned aerial vehicle (UAV) assisted millimeter wave (mmWave) communication, appropriate user-UAV association is crucial for improving system performance. In mmWave communication, user throughput largely depends on the line of sight (LoS) connectivity with the UAV, which in turn depends on the mobility pattern of the users. Moreover, different traffic types like enhanced mobile broadband (eMBB) and ultra reliable low latency communication (URLLC) may require different types of LoS connectivity. Existing user-UAV association policies do not consider the user mobility during a time interval and different LoS requirements of different traffic types. In this paper, we consider both of them and develop a user association policy in the presence of building blockages. First, considering a simplified scenario, we have analytically established the LoS area, which is the region where users will experience seamless LoS connectivity for eMBB traffic, and the LoS radius, which is the radius of the largest circle within which the user gets uninterrupted LoS services for URLLC traffic. Then, for a more complex scenario, we present a geometric shadow polygon-based method to compute LoS area and LoS radius. Finally, we associate eMBB and URLLC users, with the UAVs from which they get the maximum average throughput based on LoS area and maximum LoS radius respectively. We show that our approach outperforms the existing discretization based and maximum throughput based approaches.",http://arxiv.org/abs/2505.17948v1,IS,Mixed
An Autonomy Loop for Dynamic HPC Job Time Limit Adjustment,"High Performance Computing (HPC) systems rely on fixed user-provided estimates of job time limits. These estimates are often inaccurate, resulting in inefficient resource use and the loss of unsaved work if a job times out shortly before reaching its next checkpoint. This work proposes a novel feedback-driven autonomy loop that dynamically adjusts HPC job time limits based on checkpoint progress reported by applications. Our approach monitors checkpoint intervals and queued jobs, enabling informed decisions to either early cancel a job after its last completed checkpoint or extend the time limit sufficiently to accommodate the next checkpoint. The objective is to minimize tail waste, that is, the computation that occurs between the last checkpoint and the termination of a job, which is not saved and hence wasted. Through experiments conducted on a subset of a production workload trace, we show a 95% reduction of tail waste, which equates to saving approximately 1.3% of the total CPU time that would otherwise be wasted. We propose various policies that combine early cancellation and time limit extension, achieving tail waste reduction while improving scheduling metrics such as weighted average job wait time. This work contributes an autonomy loop for improved scheduling in HPC environments, where system job schedulers and applications collaborate to significantly reduce resource waste and improve scheduling performance.",http://arxiv.org/abs/2505.05927v1,IS,Quantitative
JustinANN: Realistic Test Generation for Java Programs Driven by Annotations,"Automated test case generation is important. However, the automatically generated test input does not always make sense, and the automated assertion is difficult to validate against the program under test. In this paper, we propose JustinANN, a flexible and scalable tool to generate test cases for Java programs, providing realistic test inputs and assertions. We have observed that, in practice, Java programs contain a large number of annotations from programs, which can be considered as part of the user specification. We design a systematic annotation set with 7 kinds of annotations and 4 combination rules based on them to modify complex Java objects. Annotations that modify the fields or return variables of methods can be used to generate assertions that represent the true intent of the program, and the ones that modify the input parameters can be used to generate test inputs that match the real business requirement. We have conducted experiments to evaluate the approach on open source Java programs. The results show that the annotations and their combinations designed in this paper are compatible with existing annotations; our approach is easier to generate test data in, on and outside the boundaries of the requirement domain; and it also helps to find program defects.",http://arxiv.org/abs/2505.05715v1,IS,Quantitative
PUDTune: Multi-Level Charging for High-Precision Calibration in Processing-Using-DRAM,"Recently, practical analog in-memory computing has been realized using unmodified commercial DRAM modules. The underlying Processing-Using-DRAM (PUD) techniques enable high-throughput bitwise operations directly within DRAM arrays. However, the presence of inherent error-prone columns hinders PUD's practical adoption. While selectively using only error-free columns would ensure reliability, this approach significantly reduces PUD's computational throughput. This paper presents PUDTune, a novel high-precision calibration technique for increasing the number of error-free columns in PUD. PUDTune compensates for errors by applying pre-identified column-specific offsets to PUD operations. By leveraging multi-level charge states of DRAM cells, PUDTune generates fine-grained and wide-range offset variations despite the limited available rows. Our experiments with DDR4 DRAM demonstrate that PUDTune increases the number of error-free columns by 1.81$\times$ compared to conventional implementations, improving addition and multiplication throughput by 1.88$\times$ and 1.89$\times$ respectively.",http://arxiv.org/abs/2505.05266v1,IS,Quantitative
Can You Mimic Me? Exploring the Use of Android Record & Replay Tools in Debugging,"Android User Interface (UI) testing is a critical research area due to the ubiquity of apps and the challenges faced by developers. Record and replay (R&R) tools facilitate manual and automated UI testing by recording UI actions to execute test scenarios and replay bugs. These tools typically support (i) regression testing, (ii) non-crashing functional bug reproduction, and (iii) crashing bug reproduction. However, prior work only examines these tools in fragmented settings, lacking a comprehensive evaluation across common use cases. We address this gap by conducting an empirical study on using R&R tools to record and replay non-crashing failures, crashing bugs, and feature-based user scenarios, and explore combining R&R with automated input generation (AIG) tools to replay crashing bugs. Our study involves one industrial and three academic R&R tools, 34 scenarios from 17 apps, 90 non-crashing failures from 42 apps, and 31 crashing bugs from 17 apps. Results show that 17% of scenarios, 38% of non-crashing bugs, and 44% of crashing bugs cannot be reliably recorded and replayed, mainly due to action interval resolution, API incompatibility, and Android tooling limitations. Our findings highlight key future research directions to enhance the practical application of R&R tools.",http://arxiv.org/abs/2504.20237v1,IS,Quantitative
Safe to Stay: Psychological Safety Sustains Participation in Pull-based Open Source Projects,"Background: Psychological safety refers to the belief that team members can speak up or make mistakes without fear of negative consequences. While it is recognized as important in traditional software teams, its role in open-source software development remains understudied. Open-source contributors often collaborate without formal roles or structures, where interpersonal relationships can significantly influence participation. Code review, a central and collaborative activity in modern software development, offers a valuable context for observing such team interactions. Aims: This study investigates whether team-level psychological safety, inferred from code review activities, is associated with contributors' sustained participation in open-source projects. Method: Using data from 60,684 pull requests across multiple repositories, we developed a psychological safety index based on observable cues such as merge decisions, comment activity, interaction diversity, and mentions. We analyzed the relationship between this index and contributors' short-term (within 1 year) and long-term (over 4--5 years) sustained participation using three logistic regression models. Results: Contributors are more likely to remain active in repositories with higher levels of psychological safety. Psychological safety is positively associated with both short-term and long-term sustained participation. However, prior participation emerges as a stronger predictor of future engagement, reducing the effect of psychological safety when accounted for. Conclusions: This study introduces a scalable, data-driven approach to measuring psychological safety through pull request data and provides new empirical evidence of its relevance in sustaining participation within open-source development.",http://arxiv.org/abs/2504.17510v2,IS,Quantitative
Identifying and Replicating Code Patterns Driving Performance Regressions in Software Systems,"Context: Performance regressions negatively impact execution time and memory usage of software systems. Nevertheless, there is a lack of systematic methods to evaluate the effectiveness of performance test suites. Performance mutation testing, which introduces intentional defects (mutants) to measure and enhance fault-detection capabilities, is promising but underexplored. A key challenge is understanding if generated mutants accurately reflect real-world performance issues. Goal: This study evaluates and extends mutation operators for performance testing. Its objectives include (i) collecting existing performance mutation operators, (ii) introducing new operators from real-world code changes that impact performance, and (iii) evaluating these operators on real-world systems to see if they effectively degrade performance. Method: To this aim, we will (i) review the literature to identify performance mutation operators, (ii) conduct a mining study to extract patterns of code changes linked to performance regressions, (iii) propose new mutation operators based on these patterns, and (iv) apply and evaluate the operators to assess their effectiveness in exposing performance degradations. Expected Outcomes: We aim to provide an enriched set of mutation operators for performance testing, helping developers and researchers identify harmful coding practices and design better strategies to detect and prevent performance regressions.",http://arxiv.org/abs/2504.05851v1,IS,Quantitative
BGTplanner: Maximizing Training Accuracy for Differentially Private Federated Recommenders via Strategic Privacy Budget Allocation,"To mitigate the rising concern about privacy leakage, the federated recommender (FR) paradigm emerges, in which decentralized clients co-train the recommendation model without exposing their raw user-item rating data. The differentially private federated recommender (DPFR) further enhances FR by injecting differentially private (DP) noises into clients. Yet, current DPFRs, suffering from noise distortion, cannot achieve satisfactory accuracy. Various efforts have been dedicated to improving DPFRs by adaptively allocating the privacy budget over the learning process. However, due to the intricate relation between privacy budget allocation and model accuracy, existing works are still far from maximizing DPFR accuracy. To address this challenge, we develop BGTplanner (Budget Planner) to strategically allocate the privacy budget for each round of DPFR training, improving overall training performance. Specifically, we leverage the Gaussian process regression and historical information to predict the change in recommendation accuracy with a certain allocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is harnessed to make privacy budget allocation decisions by reconciling the current improvement and long-term privacy constraints. Our extensive experimental results on real datasets demonstrate that \emph{BGTplanner} achieves an average improvement of 6.76\% in training performance compared to state-of-the-art baselines.",http://arxiv.org/abs/2412.02934v1,IS,Quantitative
Investigating the Impact of Interpersonal Challenges on Feeling Welcome in OSS,"The sustainability of open source software (OSS) projects hinges on contributor retention. Interpersonal challenges can inhibit a feeling of welcomeness among contributors, particularly from underrepresented groups, which impacts their decision to continue with the project. How much this impact is, varies among individuals, underlining the importance of a thorough understanding of their effects. Here, we investigate the effects of interpersonal challenges on the sense of welcomeness among diverse populations within OSS, through the diversity lenses of gender, race, and (dis)ability. We analyzed the large-scale Linux Foundation Diversity and Inclusion survey (n = 706) to model a theoretical framework linking interpersonal challenges with the sense of welcomeness through Structural Equation Models Partial Least Squares (PLS-SEM). We then examine the model to identify the impact of these challenges on different demographics through Multi-Group Analysis (MGA). Finally, we conducted a regression analysis to investigate how differently people from different demographics experience different types of interpersonal challenges. Our findings confirm the negative association between interpersonal challenges and the feeling of welcomeness in OSS, with this relationship being more pronounced among gender minorities and people with disabilities. We found that different challenges have unique impacts on how people feel welcomed, with variations across gender, race, and disability groups. We also provide evidence that people from gender minorities and with disabilities are more likely to experience interpersonal challenges than their counterparts, especially when we analyze stalking, sexual harassment, and doxxing. Our insights benefit OSS communities, informing potential strategies to improve the landscape of interpersonal relationships, ultimately fostering more inclusive and welcoming communities.",http://arxiv.org/abs/2411.01601v1,IS,Quantitative
A Zoned Storage Optimized Flash Cache on ZNS SSDs,"Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block interface penalties of flash-based SSDs. It is a good opportunity for flash cache to address cache throughput and write amplification (WA) issues by fully controlling data allocation and garbage collection via zone-based interfaces. However, there are several critical challenges that need to be addressed including zone-interface compatibility, data management of large zone size, and a better tradeoff between throughput, cache hit ratio, and WA. In this paper, we present Z-CacheLib, a zoned storage optimized flash cache on ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs with low mapping and operational overhead, and 2) a novel zCache Engine with cross-layer optimizations to resolve the throughput regression and WA issues of garbage collection, which consists of delayed data eviction with virtual over-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU, and a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that Z-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and almost no WA compared to CacheLib with compatible regular SSDs, demonstrating benefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X throughput and 92% WA reduction compared with F2FS-based scheme.",http://arxiv.org/abs/2410.11260v1,IS,Quantitative
An Analysis of XML Compression Efficiency,"XML simplifies data exchange among heterogeneous computers, but it is notoriously verbose and has spawned the development of many XML-specific compressors and binary formats. We present an XML test corpus and a combined efficiency metric integrating compression ratio and execution speed. We use this corpus and linear regression to assess 14 general-purpose and XML-specific compressors relative to the proposed metric. We also identify key factors when selecting a compressor. Our results show XMill or WBXML may be useful in some instances, but a general-purpose compressor is often the best choice.",http://arxiv.org/abs/2410.07603v1,IS,Quantitative
Ecosystem-wide influences on pull request decisions: insights from NPM,"The pull-based development model facilitates global collaboration within open-source software projects. However, whereas it is increasingly common for software to depend on other projects in their ecosystem, most research on the pull request decision-making process explored factors within projects, not the broader software ecosystem they comprise. We uncover ecosystem-wide factors that influence pull request acceptance decisions. We collected a dataset of approximately 1.8 million pull requests and 2.1 million issues from 20,052 GitHub projects within the NPM ecosystem. Of these, 98% depend on another project in the dataset, enabling studying collaboration across dependent projects. We employed social network analysis to create a collaboration network in the ecosystem, and mixed effects logistic regression and random forest techniques to measure the impact and predictive strength of the tested features. We find that gaining experience within the software ecosystem through active participation in issue-tracking systems, submitting pull requests, and collaborating with pull request integrators and experienced developers benefits all open-source contributors, especially project newcomers. These results are complemented with an exploratory qualitative analysis of 538 pull requests. We find that developers with ecosystem experience make different contributions than users without. Zooming in on a subset of 111 pull requests with clear ecosystem involvement, we find 3 overarching and 10 specific reasons why developers involve ecosystem projects in their pull requests. The results show that combining ecosystem-wide factors with features studied in previous work to predict the outcome of pull requests reached an overall F1 score of 0.92. However, the outcomes of pull requests submitted by newcomers are harder to predict.",http://arxiv.org/abs/2410.14695v2,IS,Mixed
"Exploring turnover, retention and growth in an OSS Ecosystem","The Gentoo ecosystem has evolved significantly over 23 years, highlighting the critical impact of developer sentiment on workforce dynamics such as turnover, retention, and growth. While prior research has explored sentiment at the project level, sentiment-driven dynamics at the component level remain underexplored, particularly in their implications for software stability. This study investigates the interplay between developer sentiment and workforce dynamics in Gentoo. The primary objectives are to (1) compare workforce metrics (turnover, retention, and growth rates) between sentiment-positive (SP) and sentiment-negative (SN) components, (2) examine temporal trends across three time phases, and (3) analyze the impact of these dynamics on software stability. A mixed-method approach was employed, integrating sentiment analysis of mailing lists and commit histories using the SentiStrength-SE tool. Workforce metrics were statistically analyzed using Pearson Correlation Matrix and Mann-Whitney U tests. The analysis focused on the most SP and SN components in the ecosystem. SN components exhibited higher retention rates but slower growth and turnover compared to SP components, which showed dynamic contributor behavior but reduced long-term stability. Temporal analysis revealed significant variations in workforce dynamics over three phases, with developer retention correlating positively with modifications in both sentiment groups. Tailored strategies are necessary for managing sentiment-driven dynamics in OSS projects. Improving \textit{adaptability} in SN components, and \textit{continuity} in SP components, could improve project sustainability and innovation. This study contributes to a nuanced understanding of sentiment's role in workforce behavior and software stability within OSS ecosystems.",http://arxiv.org/abs/2504.16483v1,IS,Quantitative
"Faster Releases, Fewer Risks: A Study on Maven Artifact Vulnerabilities and Lifecycle Management","In modern software ecosystems, dependency management plays a critical role in ensuring secure and maintainable applications. However, understanding the relationship between release practices and their impact on vulnerabilities and update cycles remains a challenge. In this study, we analyze the release histories of 10,000 Maven artifacts, covering over 203,000 releases and 1.7 million dependencies. We evaluate how release speed affects software security and lifecycle. Our results show an inverse relationship between release speed and dependency outdatedness. Artifacts with more frequent releases maintain significantly shorter outdated times. We also find that faster release cycles are linked to fewer CVEs in dependency chains, indicating a strong negative correlation. These findings emphasize the importance of accelerated release strategies in reducing security risks and ensuring timely updates. Our research provides valuable insights for software developers, maintainers, and ecosystem managers.",http://arxiv.org/abs/2503.24349v1,IS,Mixed
How Execution Features Relate to Failures: An Empirical Study and Diagnosis Approach,"Fault localization is a fundamental aspect of debugging, aiming to identify code regions likely responsible for failures. Traditional techniques primarily correlate statement execution with failures, yet program behavior is influenced by diverse execution features-such as variable values, branch conditions, and definition-use pairs-that can provide richer diagnostic insights. In an empirical study of 310 bugs across 20 projects, we analyzed 17 execution features and assessed their correlation with failure outcomes. Our findings suggest that fault localization benefits from a broader range of execution features: (1) Scalar pairs exhibit the strongest correlation with failures; (2) Beyond line executions, def-use pairs and functions executed are key indicators for fault localization; and (3) Combining multiple features enhances effectiveness compared to relying solely on individual features. Building on these insights, we introduce a debugging approach to diagnose failure circumstances. The approach extracts fine-grained execution features and trains a decision tree to differentiate passing and failing runs. From this model, we derive a diagnosis that pinpoints faulty locations and explains the underlying causes of the failure. Our evaluation demonstrates that the generated diagnoses achieve high predictive accuracy, reinforcing their reliability. These interpretable diagnoses empower developers to efficiently debug software by providing deeper insights into failure causes.",http://arxiv.org/abs/2502.18664v1,IS,Quantitative
Two-Fold Byzantine Fault Tolerance Algorithm: Byzantine Consensus in Blockchain,"Blockchain technology offers a decentralized and secure method for storing and authenticating data, rendering it well-suited for various applications such as digital currencies, supply chain management, and voting systems. However, the decentralized nature of blockchain also exposes it to vulnerabilities, particularly Byzantine faults, which arise when nodes in the network behave maliciously or encounter unexpected failures. Such incidents can result in inconsistencies within the blockchain and, in extreme scenarios, lead to a breakdown in consensus. Byzantine fault-tolerant consensus algorithms are crafted to tackle this challenge by ensuring that network nodes can agree on the blockchain's state even in the presence of faulty or malicious nodes. To bolster the system's resilience against these faults, it is imperative to detect them within the system. However, our examination of existing literature reveals a prevalent assumption: solutions typically operate under constraints regarding the number of faulty nodes. Such constraints confine the proposed solutions to ideal environments, limiting their practical applicability. In response, we propose a novel approach inspired by social paradigms, employing a trusted and fully monitored communication sub-process to detect Byzantine nodes. Upon detection, these nodes can be either disregarded in the consensus-building process, subjected to penalties, or undergo modifications as per the system's policy. Finally, we statistically demonstrate that our approach achieves a detection probability that exceeds 95\% for Byzantine nodes. In essence, our methodology ensures that if Byzantine nodes exhibit malicious behavior, healthy nodes can identify them with a confidence level of 95\%.",http://arxiv.org/abs/2504.16267v1,IS,Quantitative
Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning,"The last few decades have witnessed a rapid increase in IoT devices owing to their wide range of applications, such as smart healthcare monitoring systems, smart cities, and environmental monitoring. A critical task in IoT networks is sensing and transmitting information over the network. The IoT nodes gather data by sensing the environment and then transmit this data to a destination node via multi-hop communication, following some routing protocols. These protocols are usually designed to optimize possibly contradictory objectives, such as maximizing packet delivery ratio and energy efficiency. While most literature has focused on optimizing a static objective that remains unchanged, many real-world IoT applications require adapting to rapidly shifting priorities. For example, in monitoring systems, some transmissions are time-critical and require a high priority on low latency, while other transmissions are less urgent and instead prioritize energy efficiency. To meet such dynamic demands, we propose novel dynamic and distributed routing based on multiobjective Q-learning that can adapt to changes in preferences in real-time. Our algorithm builds on ideas from both multi-objective optimization and Q-learning. We also propose a novel greedy interpolation policy scheme to take near-optimal decisions for unexpected preference changes. The proposed scheme can approximate and utilize the Pareto-efficient solutions for dynamic preferences, thus utilizing past knowledge to adapt to unpredictable preferences quickly during runtime. Simulation results show that the proposed scheme outperforms state-of-the-art algorithms for various exploration strategies, preference variation patterns, and important metrics like overall reward, energy efficiency, and packet delivery ratio.",http://arxiv.org/abs/2505.00918v1,IS,Quantitative
Manifesto from Dagstuhl Perspectives Workshop 24452 -- Reframing Technical Debt,"This is the Dagstuhl Perspectives Workshop 24452 manifesto on Reframing Technical Debt. The manifesto begins with a one-page summary of Values, Beliefs, and Principles. It then elaborates on each Value, Belief, and Principle to explain their rationale and clarify their meaning. Subsequently, the paper describes the current landscape of Technical Debt Management methods and tools and explains why the current practice is inadequate and where current research falls short. The current landscape is organized into five major topics: Technical Debt as Value-Creation, Tooling, Data Collection, the role of Architecture, and Socio-Technical Aspects. Finally, the paper outlines a roadmap to realize the stated principles, with concrete milestones to be addressed by researchers, software practitioners, and tool vendors. The manifesto is signed by the workshop participants.",http://arxiv.org/abs/2505.13009v1,IS,Quantitative
Evaluation of Indoor/Outdoor Sharing in the Unlicensed 6 GHz Band,"Standard Power (SP) Wi-Fi 6E in the U.S. is just beginning to be deployed outdoors in the shared but unlicensed 6 GHz band under the control of an Automated Frequency Coordination (AFC) system to protect incumbents, while low-power-indoor (LPI) usage has been steadily increasing over the past 2 years. In this paper, we present the first comprehensive measurements and analyses of a SP Wi-Fi 6E deployment at the University of Notre Dame's football stadium, with 902 access points and a seating capacity of 80,000, coexisting with LPI deployments in adjacent buildings. Measurement campaigns were conducted during and after games, outdoors and indoors to fully characterize the performance of SP Wi-Fi 6E, interactions between SP and LPI and potential for interference to incumbents. Our main conclusions are: (i) in a very short time of about 2 months, the percentage of Wi-Fi 6E client connections is already 14% indicating rapid adoption, (ii) dense SP operation outdoors can negatively impact LPI deployments indoors, depending on building loss, indicating the need to carefully consider hybrid indoor-outdoor sharing deployments, and (iii) spectrum analyzer results indicate an aggregate signal level increase of approximately 10 dB in a Wi-Fi channel during peak usage which could potentially lead to interference since the AFC does not consider aggregate interference when allocating permitted power levels. These results from real-world deployments can inform spectrum policy in other bands where similar sharing mechanisms are being considered, such as 7.125 - 8.4 GHz.",http://arxiv.org/abs/2505.18359v1,IS,Quantitative
Crypto-Economic Analysis of Web3 Funding Programs Using the Grant Maturity Framework,"Web3 grant programs are evolving mechanisms aimed at supporting innovation within the blockchain ecosystem, yet little is known on about their effectiveness. This paper proposes the concept of maturity to fill this gap and introduces the Grant Maturity Framework (GMF), a mixed-methods model for evaluating the maturity of Web3 grant programs. The GMF provides a systematic approach to assessing the structure, governance, and impact of Web3 grants, applied here to four prominent Ethereum layer-two (L2) grant programs: Arbitrum, Optimism, Mantle, and Taiko. By evaluating these programs using the GMF, the study categorizes them into four maturity stages, ranging from experimental to advanced. The findings reveal that Arbitrum's Long-Term Incentive Pilot Program (LTIPP) and Optimism's Mission Rounds show higher maturity, while Mantle and Taiko are still in their early stages. The research concludes by discussing the user-centric development of a Web3 grant management platform aimed at improving the maturity and effectiveness of Web3 grant management processes based on the findings from the GMF. This work contributes to both practical and theoretical knowledge on Web3 grant program evaluation and tooling, providing a valuable resource for Web3 grant operators and stakeholders.",http://arxiv.org/abs/2505.06801v1,IS,Quantitative
DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization,"Personalized diffusion models have shown remarkable success in Text-to-Image (T2I) generation by enabling the injection of user-defined concepts into diverse contexts. However, balancing concept fidelity with contextual alignment remains a challenging open problem. In this work, we propose an RL-based approach that leverages the diverse outputs of T2I models to address this issue. Our method eliminates the need for human-annotated scores by generating a synthetic paired dataset for DPO-like training using external quality metrics. These better-worse pairs are specifically constructed to improve both concept fidelity and prompt adherence. Moreover, our approach supports flexible adjustment of the trade-off between image fidelity and textual alignment. Through multi-step training, our approach outperforms a naive baseline in convergence speed and output quality. We conduct extensive qualitative and quantitative analysis, demonstrating the effectiveness of our method across various architectures and fine-tuning techniques. The source code can be found at https://github.com/ControlGenAI/DreamBoothDPO.",http://arxiv.org/abs/2505.20975v1,IT,Mixed
Reasoning LLMs are Wandering Solution Explorers,"Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.",http://arxiv.org/abs/2505.20296v1,IT,Mixed
THiNK: Can Large Language Models Think-aloud?,"Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.",http://arxiv.org/abs/2505.20184v1,IT,Qualitative
Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement,"Underwater images are often affected by complex degradations such as light absorption, scattering, color casts, and artifacts, making enhancement critical for effective object detection, recognition, and scene understanding in aquatic environments. Existing methods, especially diffusion-based approaches, typically rely on synthetic paired datasets due to the scarcity of real underwater references, introducing bias and limiting generalization. Furthermore, fine-tuning these models can degrade learned priors, resulting in unrealistic enhancements due to domain shifts. To address these challenges, we propose UDAN-CLIP, an image-to-image diffusion framework pre-trained on synthetic underwater datasets and enhanced with a customized classifier based on vision-language model, a spatial attention module, and a novel CLIP-Diffusion loss. The classifier preserves natural in-air priors and semantically guides the diffusion process, while the spatial attention module focuses on correcting localized degradations such as haze and low contrast. The proposed CLIP-Diffusion loss further strengthens visual-textual alignment and helps maintain semantic consistency during enhancement. The proposed contributions empower our UDAN-CLIP model to perform more effective underwater image enhancement, producing results that are not only visually compelling but also more realistic and detail-preserving. These improvements are consistently validated through both quantitative metrics and qualitative visual comparisons, demonstrating the model's ability to correct distortions and restore natural appearance in challenging underwater conditions.",http://arxiv.org/abs/2505.19895v1,IT,Mixed
StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation,"In the current research landscape, multimodal autoregressive (AR) models have shown exceptional capabilities across various domains, including visual understanding and generation. However, complex tasks such as style-aligned text-to-image generation present significant challenges, particularly in data acquisition. In analogy to instruction-following tuning for image editing of AR models, style-aligned generation requires a reference style image and prompt, resulting in a text-image-to-image triplet where the output shares the style and semantics of the input. However, acquiring large volumes of such triplet data with specific styles is considerably more challenging than obtaining conventional text-to-image data used for training generative models. To address this issue, we propose StyleAR, an innovative approach that combines a specially designed data curation method with our proposed AR models to effectively utilize text-to-image binary data for style-aligned text-to-image generation. Our method synthesizes target stylized data using a reference style image and prompt, but only incorporates the target stylized image as the image modality to create high-quality binary data. To facilitate binary data training, we introduce a CLIP image encoder with a perceiver resampler that translates the image input into style tokens aligned with multimodal tokens in AR models and implement a style-enhanced token technique to prevent content leakage which is a common issue in previous work. Furthermore, we mix raw images drawn from large-scale text-image datasets with stylized images to enhance StyleAR's ability to extract richer stylistic features and ensure style consistency. Extensive qualitative and quantitative experiments demonstrate our superior performance.",http://arxiv.org/abs/2505.19874v1,IT,Mixed
InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory,"Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud. To enable human-understandable diagnostics of model failures, an ideal critical subset should be *faithful* (preserving points that causally influence predictions) and *conceptually coherent* (forming semantically meaningful structures that align with human perception). We propose InfoCons, an explanation framework that applies information-theoretic principles to decompose the point cloud into 3D concepts, enabling the examination of their causal effect on model predictions with learnable priors. We evaluate InfoCons on synthetic datasets for classification, comparing it qualitatively and quantitatively with four baselines. We further demonstrate its scalability and flexibility on two real-world datasets and in two applications that utilize critical scores of PC.",http://arxiv.org/abs/2505.19820v1,IT,Mixed
SETBVE: Quality-Diversity Driven Exploration of Software Boundary Behaviors,"Software systems exhibit distinct behaviors based on input characteristics, and failures often occur at the boundaries between input domains. Traditional Boundary Value Analysis (BVA) relies on manual heuristics, while automated Boundary Value Exploration (BVE) methods typically optimize a single quality metric, risking a narrow and incomplete survey of boundary behaviors. We introduce SETBVE, a customizable, modular framework for automated black-box BVE that leverages Quality-Diversity (QD) optimization to systematically uncover and refine a broader spectrum of boundaries. SETBVE maintains an archive of boundary pairs organized by input- and output-based behavioral descriptors. It steers exploration toward underrepresented regions while preserving high-quality boundary pairs and applies local search to refine candidate boundaries. In experiments with ten integer-based functions, SETBVE outperforms the baseline in diversity, boosting archive coverage by 37 to 82 percentage points. A qualitative analysis reveals that SETBVE identifies boundary candidates the baseline misses. While the baseline method typically plateaus in both diversity and quality after 30 seconds, SETBVE continues to improve in 600-second runs, demonstrating better scalability. Even the simplest SETBVE configurations perform well in identifying diverse boundary behaviors. Our findings indicate that balancing quality with behavioral diversity can help identify more software edge-case behaviors than quality-focused approaches.",http://arxiv.org/abs/2505.19736v1,IT,Mixed
Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically,"Cross-lingual alignment in pretrained language models (LMs) has enabled efficient transfer in text-based LMs. Such an alignment has also been observed in speech foundation models. However, it remains an open question whether findings and methods from text-based cross-lingual alignment apply to speech. Building on prior work on spoken translation retrieval, we perform pronunciation-controlled experiments to observe if cross-lingual alignment can indeed occur in such models on a semantic basis, instead of relying on phonetic similarities. Our findings indicate that even in the absence of phonetic cues, spoken translation retrieval accuracy remains relatively stable. We follow up with a controlled experiment on a word-level dataset of cross-lingual synonyms and near-homophones, confirming the existence of both phonetic and semantic knowledge in the encoder. Finally, we qualitatively examine the transcriptions produced by early exiting the encoder, where we observe that speech translation produces semantic errors that are characterized by phonetic similarities to corresponding words in the source language. We apply this insight from early exiting to speech recognition in seven low-resource languages unsupported by the Whisper model, and achieve improved accuracy in all languages examined, particularly for languages with transparent orthographies.",http://arxiv.org/abs/2505.19606v1,IT,Mixed
Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation,"Time consumption and the complexity of manual layout design make automated layout generation a critical task, especially for multiple applications across different mobile devices. Existing graph-based layout generation approaches suffer from limited generative capability, often resulting in unreasonable and incompatible outputs. Meanwhile, vision based generative models tend to overlook the original structural information, leading to component intersections and overlaps. To address these challenges, we propose an Aggregation Structural Representation (ASR) module that integrates graph networks with large language models (LLMs) to preserve structural information while enhancing generative capability. This novel pipeline utilizes graph features as hierarchical prior knowledge, replacing the traditional Vision Transformer (ViT) module in multimodal large language models (MLLM) to predict full layout information for the first time. Moreover, the intermediate graph matrix used as input for the LLM is human editable, enabling progressive, human centric design generation. A comprehensive evaluation on the RICO dataset demonstrates the strong performance of ASR, both quantitatively using mean Intersection over Union (mIoU), and qualitatively through a crowdsourced user study. Additionally, sampling on relational features ensures diverse layout generation, further enhancing the adaptability and creativity of the proposed approach.",http://arxiv.org/abs/2505.19554v1,IT,Mixed
Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments,"As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.",http://arxiv.org/abs/2505.18927v1,IT,Mixed
EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models,"Vision-language retrieval (VLR) has attracted significant attention in both academia and industry, which involves using text (or images) as queries to retrieve corresponding images (or text). However, existing methods often neglect the rich visual semantics knowledge of entities, thus leading to incorrect retrieval results. To address this problem, we propose the Entity Visual Description enhanced CLIP (EvdCLIP), designed to leverage the visual knowledge of entities to enrich queries. Specifically, since humans recognize entities through visual cues, we employ a large language model (LLM) to generate Entity Visual Descriptions (EVDs) as alignment cues to complement textual data. These EVDs are then integrated into raw queries to create visually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced queries may introduce noise or low-quality expansions, we develop a novel, trainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW utilizes EVD knowledge and the generative capabilities of the language model to effectively rewrite queries. With our specialized training strategy, EaRW can generate high-quality and low-noise EVD-enhanced queries. Extensive quantitative and qualitative experiments on image-text retrieval benchmarks validate the superiority of EvdCLIP on vision-language retrieval tasks.",http://arxiv.org/abs/2505.18594v1,IT,Mixed
ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts,"Reasoning Video Object Segmentation is a challenging task, which generates a mask sequence from an input video and an implicit, complex text query. Existing works probe into the problem by finetuning Multimodal Large Language Models (MLLM) for segmentation-based output, while still falling short in difficult cases on videos given temporally-sensitive queries, primarily due to the failure to integrate temporal and spatial information. In this paper, we propose ThinkVideo, a novel framework which leverages the zero-shot Chain-of-Thought (CoT) capability of MLLM to address these challenges. Specifically, ThinkVideo utilizes the CoT prompts to extract object selectivities associated with particular keyframes, then bridging the reasoning image segmentation model and SAM2 video processor to output mask sequences. The ThinkVideo framework is training-free and compatible with closed-source MLLMs, which can be applied to Reasoning Video Instance Segmentation. We further extend the framework for online video streams, where the CoT is used to update the object of interest when a better target starts to emerge and becomes visible. We conduct extensive experiments on video object segmentation with explicit and implicit queries. The results show that ThinkVideo significantly outperforms previous works in both cases, qualitatively and quantitatively.",http://arxiv.org/abs/2505.18561v1,IT,Mixed
Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark,"Recent advances in large reasoning models (LRMs) show strong performance in structured domains such as mathematics and programming; however, they often lack pedagogical coherence and realistic teaching behaviors. To bridge this gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use through three innovations: (1) a distillation-based pipeline that filters and refines model outputs for instruction-tuning, (2) the Well-balanced Educational Benchmark (WBEB), which evaluates performance across subject knowledge, pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and (3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting teacher-style reasoning. Our mixed-method evaluation combines quantitative metrics with qualitative analysis, providing the first systematic assessment of LRMs' pedagogical strengths and limitations.",http://arxiv.org/abs/2505.18467v1,IT,Mixed
"Small Models, Smarter Learning: The Power of Joint Task Training","The ability of a model to learn a task depends strongly on both the task difficulty and the model size. We aim to understand how task difficulty relates to the minimum number of parameters required for learning specific tasks in small transformer models. Our study focuses on the ListOps dataset, which consists of nested mathematical operations. We gradually increase task difficulty by introducing new operations or combinations of operations into the training data. We observe that sum modulo n is the hardest to learn. Curiously, when combined with other operations such as maximum and median, the sum operation becomes easier to learn and requires fewer parameters. We show that joint training not only improves performance but also leads to qualitatively different model behavior. We show evidence that models trained only on SUM might be memorizing and fail to capture the number structure in the embeddings. In contrast, models trained on a mixture of SUM and other operations exhibit number-like representations in the embedding space, and a strong ability to distinguish parity. Furthermore, the SUM-only model relies more heavily on its feedforward layers, while the jointly trained model activates the attention mechanism more. Finally, we show that learning pure SUM can be induced in models below the learning threshold of pure SUM, by pretraining them on MAX+MED. Our findings indicate that emergent abilities in language models depend not only on model size, but also the training curriculum.",http://arxiv.org/abs/2505.18369v1,IT,Mixed
UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions,"Background: Bone surface reconstruction plays a critical role in computer-assisted orthopedic surgery. Compared to traditional imaging modalities such as CT and MRI, ultrasound offers a radiation-free, cost-effective, and portable alternative. Continuous bone surface reconstruction can be employed for many clinical applications. However, due to the inherent limitations of ultrasound imaging, B-mode ultrasound typically capture only partial bone surfaces. Existing reconstruction methods struggle with such incomplete data, leading to artifacts and increased reconstruction errors. Effective techniques for accurately reconstructing thin and open bone surfaces from real-world 3D ultrasound volumes remain lacking. Methods: We propose UltraBoneUDF, a self-supervised framework designed for reconstructing open bone surfaces from ultrasound using neural Unsigned Distance Functions. To enhance reconstruction quality, we introduce a novel global feature extractor that effectively fuses ultrasound-specific image characteristics. Additionally, we present a novel loss function based on local tangent plane optimization that substantially improves surface reconstruction quality. UltraBoneUDF and baseline models are extensively evaluated on four open-source datasets. Results: Qualitative results highlight the limitations of the state-of-the-art methods for open bone surface reconstruction and demonstrate the effectiveness of UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms competing methods across all evaluated datasets for both open and closed bone surface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the UltraBones100k dataset (39.6\% improvement compared to the SOTA), 0.23 mm on the OpenBoneCT dataset (69.3\% improvement), 0.18 mm on the ClosedBoneCT dataset (70.2\% improvement), and 0.05 mm on the Prostate dataset (55.3\% improvement).",http://arxiv.org/abs/2505.17912v1,IT,Mixed
TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis,"Diffusion-based scene text synthesis has progressed rapidly, yet existing methods commonly rely on additional visual conditioning modules and require large-scale annotated data to support multilingual generation. In this work, we revisit the necessity of complex auxiliary modules and further explore an approach that simultaneously ensures glyph accuracy and achieves high-fidelity scene integration, by leveraging diffusion models' inherent capabilities for contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework that enables multilingual scene text synthesis. The advantages of TextFlux can be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates the need for OCR encoders (additional visual conditioning modules) that are specifically used to extract visual text-related features. (2) Strong multilingual scalability. TextFlux is effective in low-resource multilingual settings, and achieves strong performance in newly added languages with fewer than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with only 1% of the training data required by competing methods. (4) Controllable multi-line text generation. TextFlux offers flexible multi-line synthesis with precise line-level control, outperforming methods restricted to single-line or rigid layouts. Extensive experiments and visualizations demonstrate that TextFlux outperforms previous methods in both qualitative and quantitative evaluations.",http://arxiv.org/abs/2505.17778v1,IT,Mixed
ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation,"Recent advances in diffusion models have significantly improved text-to-face generation, but achieving fine-grained control over facial features remains a challenge. Existing methods often require training additional modules to handle specific controls such as identity, attributes, or age, making them inflexible and resource-intensive. We propose ExpertGen, a training-free framework that leverages pre-trained expert models such as face recognition, facial attribute recognition, and age estimation networks to guide generation with fine control. Our approach uses a latent consistency model to ensure realistic and in-distribution predictions at each diffusion step, enabling accurate guidance signals to effectively steer the diffusion process. We show qualitatively and quantitatively that expert models can guide the generation process with high precision, and multiple experts can collaborate to enable simultaneous control over diverse facial aspects. By allowing direct integration of off-the-shelf expert models, our method transforms any such model into a plug-and-play component for controllable face generation.",http://arxiv.org/abs/2505.17256v1,IT,Mixed
FB-RAG: Improving RAG with Forward and Backward Lookup,"The performance of Retrieval Augmented Generation (RAG) systems relies heavily on the retriever quality and the size of the retrieved context. A large enough context ensures that the relevant information is present in the input context for the LLM, but also incorporates irrelevant content that has been shown to confuse the models. On the other hand, a smaller context reduces the irrelevant information, but it often comes at the risk of losing important information necessary to answer the input question. This duality is especially challenging to manage for complex queries that contain little information to retrieve the relevant chunks from the full context. To address this, we present a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on a combination of backward lookup (overlap with the query) and forward lookup (overlap with candidate reasons and answers) to retrieve specific context chunks that are the most relevant for answering the input query. Our evaluations on 9 datasets from two leading benchmarks show that FB-RAG consistently outperforms RAG and Long Context baselines developed recently for these benchmarks. We further show that FB-RAG can improve performance while reducing latency. We perform qualitative analysis of the strengths and shortcomings of our approach, providing specific insights to guide future work.",http://arxiv.org/abs/2505.17206v1,IT,Mixed
A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents,"The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.",http://arxiv.org/abs/2505.15108v1,IT,Qualitative
ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding,"The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.",http://arxiv.org/abs/2505.15046v2,IT,Mixed
MedBLIP: Fine-tuning BLIP for Medical Image Captioning,"Medical image captioning is a challenging task that requires generating clinically accurate and semantically meaningful descriptions of radiology images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini and ViT-GPT2 show strong performance on natural image datasets, they often produce generic or imprecise captions when applied to specialized medical domains. In this project, we explore the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning. We compare the fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific fine-tuning on BLIP significantly improves performance across both quantitative and qualitative evaluation metrics. We also visualize decoder cross-attention maps to assess interpretability and conduct an ablation study to evaluate the contributions of encoder-only and decoder-only fine-tuning. Our findings highlight the importance of targeted adaptation for medical applications and suggest that decoder-only fine-tuning (encoder-frozen) offers a strong performance baseline with 5% lower training time than full fine-tuning, while full model fine-tuning still yields the best results overall.",http://arxiv.org/abs/2505.14726v1,IT,Mixed
ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models,"Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.",http://arxiv.org/abs/2505.13444v1,IT,Mixed
VesselGPT: Autoregressive Modeling of Vascular Geometry,"Anatomical trees are critical for clinical diagnosis and treatment planning, yet their complex and diverse geometry make accurate representation a significant challenge. Motivated by the latest advances in large language models, we introduce an autoregressive method for synthesizing anatomical trees. Our approach first embeds vessel structures into a learned discrete vocabulary using a VQ-VAE architecture, then models their generation autoregressively with a GPT-2 model. This method effectively captures intricate geometries and branching patterns, enabling realistic vascular tree synthesis. Comprehensive qualitative and quantitative evaluations reveal that our technique achieves high-fidelity tree reconstruction with compact discrete representations. Moreover, our B-spline representation of vessel cross-sections preserves critical morphological details that are often overlooked in previous' methods parameterizations. To the best of our knowledge, this work is the first to generate blood vessels in an autoregressive manner. Code, data, and trained models will be made available.",http://arxiv.org/abs/2505.13318v1,IT,Mixed
Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance,"Multimodal pathology-genomic analysis has become increasingly prominent in cancer survival prediction. However, existing studies mainly utilize multi-instance learning to aggregate patch-level features, neglecting the information loss of contextual and hierarchical details within pathology images. Furthermore, the disparity in data granularity and dimensionality between pathology and genomics leads to a significant modality imbalance. The high spatial resolution inherent in pathology data renders it a dominant role while overshadowing genomics in multimodal integration. In this paper, we propose a multimodal survival prediction framework that incorporates hypergraph learning to effectively capture both contextual and hierarchical details from pathology images. Moreover, it employs a modality rebalance mechanism and an interactive alignment fusion strategy to dynamically reweight the contributions of the two modalities, thereby mitigating the pathology-genomics imbalance. Quantitative and qualitative experiments are conducted on five TCGA datasets, demonstrating that our model outperforms advanced methods by over 3.4\% in C-Index performance.",http://arxiv.org/abs/2505.11997v2,IT,Mixed
When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research,"Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.",http://arxiv.org/abs/2505.11855v1,IT,Mixed
HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class,"Large language models (LLMs) have shown remarkable progress in mathematical problem-solving, but evaluation has largely focused on problems that have exact analytical solutions or involve formal proofs, often overlooking approximation-based problems ubiquitous in applied science and engineering. To fill this gap, we build on prior work and present HARDMath2, a dataset of 211 original problems covering the core topics in an introductory graduate applied math class, including boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics of oscillatory integrals. This dataset was designed and verified by the students and instructors of a core graduate applied mathematics course at Harvard. We build the dataset through a novel collaborative environment that challenges students to write and refine difficult problems consistent with the class syllabus, peer-validate solutions, test different models, and automatically check LLM-generated solutions against their own answers and numerical ground truths. Evaluation results show that leading frontier models still struggle with many of the problems in the dataset, highlighting a gap in the mathematical reasoning skills of current LLMs. Importantly, students identified strategies to create increasingly difficult problems by interacting with the models and exploiting common failure modes. This back-and-forth with the models not only resulted in a richer and more challenging benchmark but also led to qualitative improvements in the students' understanding of the course material, which is increasingly important as we enter an age where state-of-the-art language models can solve many challenging problems across a wide domain of fields.",http://arxiv.org/abs/2505.11774v1,IT,Mixed
Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures,"Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.",http://arxiv.org/abs/2505.11726v1,IT,Mixed
AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery,"Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.",http://arxiv.org/abs/2505.21499v1,IT,Quantitative
Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance,"Large Language Models (LLMs) are known to process information using a proficient internal language consistently, referred to as latent language, which may differ from the input or output languages. However, how the discrepancy between the latent language and the input and output language affects downstream task performance remains largely unexplored. While many studies research the latent language of LLMs, few address its importance in influencing task performance. In our study, we hypothesize that thinking in latent language consistently enhances downstream task performance. To validate this, our work varies the input prompt languages across multiple downstream tasks and analyzes the correlation between consistency in latent language and task performance. We create datasets consisting of questions from diverse domains such as translation and geo-culture, which are influenced by the choice of latent language. Experimental results across multiple LLMs on translation and geo-culture tasks, which are sensitive to the choice of language, indicate that maintaining consistency in latent language is not always necessary for optimal downstream task performance. This is because these models adapt their internal representations near the final layers to match the target language, reducing the impact of consistency on overall performance.",http://arxiv.org/abs/2505.21458v1,IT,Quantitative
Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning,"Multimodal feature reconstruction is a promising approach for 3D anomaly detection, leveraging the complementary information from dual modalities. We further advance this paradigm by utilizing multi-modal mentor learning, which fuses intermediate features to further distinguish normal from feature differences. To address these challenges, we propose a novel method called Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared features of different modalities, Mentor3AD can extract more effective features and guide feature reconstruction, ultimately improving detection performance. Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges features extracted from RGB and 3D modalities to create a mentor feature. Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate cross-modal reconstruction, supported by the mentor feature. Lastly, we introduce a Voting Module (VM) to more accurately generate the final anomaly score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies have verified the effectiveness of the proposed method.",http://arxiv.org/abs/2505.21420v1,IT,Quantitative
PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense,"As malicious users increasingly employ phonetic substitution to evade hate speech detection, researchers have investigated such strategies. However, two key challenges remain. First, existing studies have overlooked the Korean language, despite its vulnerability to phonetic perturbations due to its phonographic nature. Second, prior work has primarily focused on constructing datasets rather than developing architectural defenses. To address these challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH) that exploits the phonological characteristics of the Korean writing system, and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the detector's robustness by incorporating phonetic information at the architectural level. Our experimental results demonstrate the effectiveness of our proposed methods on both perturbed and unperturbed datasets, suggesting that they not only improve detection performance but also reflect realistic adversarial behaviors employed by malicious users.",http://arxiv.org/abs/2505.21380v1,IT,Quantitative
XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration,"Recent advancements in vision-language models (VLMs) have spurred increased interest in Device-Control Agents (DC agents), such as utilizing in-the-wild device control to manage graphical user interfaces. Conventional methods for assessing the capabilities of DC agents, such as computing step-wise action accuracy and overall task success rates, provide a macroscopic view of DC agents' performance; however, they fail to offer microscopic insights into potential errors that may occur in real-world applications. Conducting a finer-grained performance evaluation of DC agents presents significant challenges. This study introduces a new perspective on evaluation methods for DC agents by proposing the XBOUND evaluation method, which employs the calculation of a novel Explore Metric to delineate the capability boundaries of DC agents. Compared to previous evaluation methods, XBOUND focuses on individual states to assess the proficiency of DC agents in mastering these states. Furthermore, we have developed a ``pseudo'' episode tree dataset derived from Android Control test data. Utilizing this dataset and XBOUND, we comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the overall and specific performance across five common tasks. Additionally, we select representative cases to highlight the current deficiencies and limitations inherent in both series. Code is available at https://github.com/sqzhang-lazy/XBOUND.",http://arxiv.org/abs/2505.21279v1,IT,Quantitative
Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings,"Large Language Models (LLMs) recently achieved great success in medical text summarization by simply using in-context learning. However, these recent efforts do not perform fine-grained evaluations under difficult settings where LLMs might fail. They typically report performance scores over the entire dataset. Through our benchmarking study, we show that LLMs show a significant performance drop for data points with high concentration of out-of-vocabulary (OOV) words or with high novelty. Vocabulary adaptation is an intuitive solution to this vocabulary mismatch issue where the LLM vocabulary gets updated with certain expert domain (here, medical) words or subwords. An interesting finding from our study is that Llama-3.1, even with a vocabulary size of around 128K tokens, still faces over-fragmentation issue with medical words. To that end, we show vocabulary adaptation helps improve the LLM summarization performance even in difficult settings. Through extensive experimentation of multiple vocabulary adaptation strategies, two continual pretraining strategies, and three benchmark medical summarization datasets, we gain valuable insights into the role of vocabulary adaptation strategies for customizing LLMs to the medical domain. We also performed a human evaluation study with medical experts where they found that vocabulary adaptation results in more relevant and faithful summaries. Our codebase is made publicly available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.",http://arxiv.org/abs/2505.21242v1,IT,Quantitative
Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities,"The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.",http://arxiv.org/abs/2505.21191v1,IT,Quantitative
Exploring the Latent Capacity of LLMs for One-Step Text Generation,"A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.",http://arxiv.org/abs/2505.21189v1,IT,Quantitative
PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing,"To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.",http://arxiv.org/abs/2505.21184v1,IT,Quantitative
A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction,"Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.",http://arxiv.org/abs/2505.21109v1,IT,Mixed
Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking,"Low-Altitude Economic Networking (LAENet) aims to support diverse flying applications below 1,000 meters by deploying various aerial vehicles for flexible and cost-effective aerial networking. However, complex decision-making, resource constraints, and environmental uncertainty pose significant challenges to the development of the LAENet. Reinforcement learning (RL) offers a potential solution in response to these challenges but has limitations in generalization, reward design, and model stability. The emergence of large language models (LLMs) offers new opportunities for RL to mitigate these limitations. In this paper, we first present a tutorial about integrating LLMs into RL by using the capacities of generation, contextual understanding, and structured reasoning of LLMs. We then propose an LLM-enhanced RL framework for the LAENet in terms of serving the LLM as information processor, reward designer, decision-maker, and generator. Moreover, we conduct a case study by using LLMs to design a reward function to improve the learning performance of RL in the LAENet. Finally, we provide a conclusion and discuss future work.",http://arxiv.org/abs/2505.21045v1,IT,Qualitative
Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation,"Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS.",http://arxiv.org/abs/2505.21033v1,IT,Mixed
Articulatory strategy in vowel production as a basis for speaker discrimination,"The way speakers articulate is well known to be variable across individuals while at the same time subject to anatomical and biomechanical constraints. In this study, we ask whether articulatory strategy in vowel production can be sufficiently speaker-specific to form the basis for speaker discrimination. We conducted Generalised Procrustes Analyses of tongue shape data from 40 English speakers from the North West of England, and assessed the speaker-discriminatory potential of orthogonal tongue shape features within the framework of likelihood ratios. Tongue size emerged as the individual dimension with the strongest discriminatory power, while tongue shape variation in the more anterior part of the tongue generally outperformed tongue shape variation in the posterior part. When considered in combination, shape-only information may offer comparable levels of speaker specificity to size-and-shape information, but only when features do not exhibit speaker-level co-variation.",http://arxiv.org/abs/2505.20995v1,IT,Mixed
Evaluating and Steering Modality Preferences in Multimodal Large Language Model,"Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a \textbf{MC\textsuperscript{2}} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.",http://arxiv.org/abs/2505.20977v1,IT,Quantitative
Label Leakage in Federated Inertial-based Human Activity Recognition,"While prior work has shown that Federated Learning updates can leak sensitive information, label reconstruction attacks, which aim to recover input labels from shared gradients, have not yet been examined in the context of Human Activity Recognition (HAR). Given the sensitive nature of activity labels, this study evaluates the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets. Our findings show that the number of activity classes, sampling strategy, and class imbalance are critical factors influencing the extent of label leakage, with reconstruction accuracies reaching up to 90% on two benchmark datasets, even for trained models. Moreover, we find that Local Differential Privacy techniques such as gradient noise and clipping offer only limited protection, as certain attacks still reliably infer both majority and minority class labels. We conclude by offering practical recommendations for the privacy-aware deployment of federated HAR systems and identify open challenges for future research. Code to reproduce our experiments is publicly available via github.com/mariusbock/leakage_har.",http://arxiv.org/abs/2505.20924v1,IT,Quantitative
Can LLMs Learn to Map the World from Local Descriptions?,"Recent advances in Large Language Models (LLMs) have demonstrated strong capabilities in tasks such as code and mathematics. However, their potential to internalize structured spatial knowledge remains underexplored. This study investigates whether LLMs, grounded in locally relative human observations, can construct coherent global spatial cognition by integrating fragmented relational descriptions. We focus on two core aspects of spatial cognition: spatial perception, where models infer consistent global layouts from local positional relationships, and spatial navigation, where models learn road connectivity from trajectory data and plan optimal paths between unconnected locations. Experiments conducted in a simulated urban environment demonstrate that LLMs not only generalize to unseen spatial relationships between points of interest (POIs) but also exhibit latent representations aligned with real-world spatial distributions. Furthermore, LLMs can learn road connectivity from trajectory descriptions, enabling accurate path planning and dynamic spatial awareness during navigation.",http://arxiv.org/abs/2505.20874v1,IT,Mixed
AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset,"Identifying factors that make ad text attractive is essential for advertising success. This study proposes AdParaphrase v2.0, a dataset for ad text paraphrasing, containing human preference data, to enable the analysis of the linguistic factors and to support the development of methods for generating attractive ad texts. Compared with v1.0, this dataset is 20 times larger, comprising 16,460 ad text paraphrase pairs, each annotated with preference data from ten evaluators, thereby enabling a more comprehensive and reliable analysis. Through the experiments, we identified multiple linguistic features of engaging ad texts that were not observed in v1.0 and explored various methods for generating attractive ad texts. Furthermore, our analysis demonstrated the relationships between human preference and ad performance, and highlighted the potential of reference-free metrics based on large language models for evaluating ad text attractiveness. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.",http://arxiv.org/abs/2505.20826v1,IT,Quantitative
REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing,"Short videos are an effective tool for promoting contents and improving knowledge accessibility. While existing extractive video summarization methods struggle to produce a coherent narrative, existing abstractive methods cannot `quote' from the input videos, i.e., inserting short video clips in their outputs. In this work, we explore novel video editing models for generating shorts that feature a coherent narrative with embedded video insertions extracted from a long input video. We propose a novel retrieval-embedded generation framework that allows a large language model to quote multimodal resources while maintaining a coherent narrative. Our proposed REGen system first generates the output story script with quote placeholders using a finetuned large language model, and then uses a novel retrieval model to replace the quote placeholders by selecting a video clip that best supports the narrative from a pool of candidate quotable video clips. We examine the proposed method on the task of documentary teaser generation, where short interview insertions are commonly used to support the narrative of a documentary. Our objective evaluations show that the proposed method can effectively insert short video clips while maintaining a coherent narrative. In a subjective survey, we show that our proposed method outperforms existing abstractive and extractive approaches in terms of coherence, alignment, and realism in teaser generation.",http://arxiv.org/abs/2505.18880v1,IT,Mixed
How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors,"Providing personalized, detailed feedback at scale in large undergraduate STEM courses remains a persistent challenge. We present an empirically evaluated practice exam system that integrates AI generated feedback with targeted textbook references, deployed in a large introductory biology course. Our system encourages metacognitive behavior by asking students to explain their answers and declare their confidence. It uses OpenAI's GPT-4o to generate personalized feedback based on this information, while directing them to relevant textbook sections. Through interaction logs from consenting participants across three midterms (541, 342, and 413 students respectively), totaling 28,313 question-student interactions across 146 learning objectives, along with 279 surveys and 23 interviews, we examined the system's impact on learning outcomes and engagement. Across all midterms, feedback types showed no statistically significant performance differences, though some trends suggested potential benefits. The most substantial impact came from the required confidence ratings and explanations, which students reported transferring to their actual exam strategies. About 40 percent of students engaged with textbook references when prompted by feedback -- far higher than traditional reading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5), with 82.1 percent reporting increased confidence on practiced midterm topics, and 73.4 percent indicating they could recall and apply specific concepts. Our findings suggest that embedding structured reflection requirements may be more impactful than sophisticated feedback mechanisms.",http://arxiv.org/abs/2505.13381v1,IT,Mixed
GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation,"Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted \textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines.",http://arxiv.org/abs/2505.12888v1,IT,Mixed
"Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents","The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgent eliminates centralized control and reduces human intervention, addressing key trust concerns inherent in centralized AI systems. However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.",http://arxiv.org/abs/2505.09757v1,IT,Mixed
Mitigating Configuration Differences Between Development and Production Environments: A Catalog of Strategies,"Context: The Configuration Management of the development and production environments is an important aspect of IT operations. However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime. Objective: In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments. Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues. Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data. These participants shed some light on the current practices, processes, challenges, or issues they have encountered. Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences. These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies. Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment. In addition, they can also ensure compliance with industry standards and regulations.",http://arxiv.org/abs/2505.09392v2,IT,Qualitative
CoVoL: A Cooperative Vocabulary Learning Game for Children with Autism,"Children with Autism commonly face difficulties in vocabulary acquisition, which can have an impact on their social communication. Using digital tools for vocabulary learning can prove beneficial for these children, as they can provide a predictable environment and effective individualized feedback. While existing work has explored the use of technology-assisted vocabulary learning for children with Autism, no study has incorporated turn-taking to facilitate learning and use of vocabulary similar to that used in real-world social contexts. To address this gap, we propose the design of a cooperative two-player vocabulary learning game, CoVoL. CoVoL allows children to engage in game-based vocabulary learning useful for real-world social communication scenarios. We discuss our first prototype and its evaluation. Additionally, we present planned features which are based on feedback obtained through ten interviews with researchers and therapists, as well as an evaluation plan for the final release of CoVoL.",http://arxiv.org/abs/2505.08515v1,IT,Qualitative
SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts,"Democratic societies need accessible, reliable information. Videos and Podcasts have established themselves as the medium of choice for civic dissemination, but also as carriers of misinformation. The emerging Science Communication Knowledge Infrastructure (SciCom KI) curating non-textual media is still fragmented and not adequately equipped to scale against the content flood. Our work sets out to support the SciCom KI with a central, collaborative platform, the SciCom Wiki, to facilitate FAIR (findable, accessible, interoperable, reusable) media representation and the fact-checking of their content, particularly for videos and podcasts. Building an open-source service system centered around Wikibase, we survey requirements from 53 stakeholders, refine these in 11 interviews, and evaluate our prototype based on these requirements with another 14 participants. To address the most requested feature, fact-checking, we developed a neurosymbolic computational fact-checking approach, converting heterogenous media into knowledge graphs. This increases machine-readability and allows comparing statements against equally represented ground-truth. Our computational fact-checking tool was iteratively evaluated through 10 expert interviews, a public user survey with 43 participants verified the necessity and usability of our tool. Overall, our findings identified several needs to systematically support the SciCom KI. The SciCom Wiki, as a FAIR digital library complementing our neurosymbolic computational fact-checking framework, was found suitable to address the raised requirements. Further, we identified that the SciCom KI is severely underdeveloped regarding FAIR knowledge and related systems facilitating its collaborative creation and curation. Our system can provide a central knowledge node, yet a collaborative effort is required to scale against the imminent (mis-)information flood.",http://arxiv.org/abs/2505.07912v1,IT,Mixed
AI in Money Matters,"In November 2022, Europe and the world by and large were stunned by the birth of a new large language model : ChatGPT. Ever since then, both academic and populist discussions have taken place in various public spheres such as LinkedIn and X(formerly known as Twitter) with the view to both understand the tool and its benefits for the society. The views of real actors in professional spaces, especially in regulated industries such as finance and law have been largely missing. We aim to begin to close this gap by presenting results from an empirical investigation conducted through interviews with professional actors in the Fintech industry. The paper asks the question, how and to what extent are large language models in general and ChatGPT in particular being adopted and used in the Fintech industry? The results show that while the fintech experts we spoke with see a potential in using large language models in the future, a lot of questions marks remain concerning how they are policed and therefore might be adopted in a regulated industry such as Fintech. This paper aims to add to the existing academic discussing around large language models, with a contribution to our understanding of professional viewpoints.",http://arxiv.org/abs/2505.07393v1,IT,Mixed
Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift,"In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling.",http://arxiv.org/abs/2505.02257v1,IT,Mixed
TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments,"Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.",http://arxiv.org/abs/2504.21851v1,IT,Mixed
MAGI: Multi-Agent Guided Interview for Psychiatric Assessment,"Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.",http://arxiv.org/abs/2504.18260v1,IT,Mixed
AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How,"Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social Computing requires the examination of ethical and social norms to ensure safe incorporation into human life. We conducted a mixed-method study, including an online survey with 111 participants and an interview study with 38 experts, to investigate the AI ethics and social norms in ChatGPT as everyday life tools. This study aims to evaluate whether ChatGPT in an empirical context operates following ethics and social norms, which is critical for understanding actions in industrial and academic research and achieving machine ethics. The findings of this study provide initial insights into six important aspects of AI ethics, including bias, trustworthiness, security, toxicology, social norms, and ethical data. Significant obstacles related to transparency and bias in unsupervised data collection methods are identified as ChatGPT's ethical concerns.",http://arxiv.org/abs/2504.18044v1,IT,Mixed
Beyond Text: Characterizing Domain Expert Needs in Document Research,"Working with documents is a key part of almost any knowledge work, from contextualizing research in a literature review to reviewing legal precedent. Recently, as their capabilities have expanded, primarily text-based NLP systems have often been billed as able to assist or even automate this kind of work. But to what extent are these systems able to model these tasks as experts conceptualize and perform them now? In this study, we interview sixteen domain experts across two domains to understand their processes of document research, and compare it to the current state of NLP systems. We find that our participants processes are idiosyncratic, iterative, and rely extensively on the social context of a document in addition its content; existing approaches in NLP and adjacent fields that explicitly center the document as an object, rather than as merely a container for text, tend to better reflect our participants' priorities, though they are often less accessible outside their research communities. We call on the NLP community to more carefully consider the role of the document in building useful tools that are accessible, personalizable, iterative, and socially aware.",http://arxiv.org/abs/2504.12495v1,IT,Qualitative
"Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process","As generative AI tools like ChatGPT become integral to everyday writing, critical questions arise about how to preserve writers' sense of agency and ownership when using these tools. Yet, a systematic understanding of how AI assistance affects different aspects of the writing process - and how this shapes writers' agency - remains underexplored. To address this gap, we conducted a systematic review of 109 HCI papers using the PRISMA approach. From this literature, we identify four overarching design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback - mapped across the four key cognitive processes in writing: planning, translating, reviewing, and monitoring. We complement this analysis with interviews of 15 writers across diverse domains. Our findings reveal that writers' desired levels of AI intervention vary across the writing process: content-focused writers (e.g., academics) prioritize ownership during planning, while form-focused writers (e.g., creatives) value control over translating and reviewing. Writers' preferences are also shaped by contextual goals, values, and notions of originality and authorship. By examining when ownership matters, what writers want to own, and how AI interactions shape agency, we surface both alignment and gaps between research and user needs. Our findings offer actionable design guidance for developing human-centered writing tools for co-writing with AI, on human terms.",http://arxiv.org/abs/2504.12488v1,IT,Qualitative
Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions,"Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses during the early phases of survey design. While previous studies have examined whether models can reflect individual opinions or attitudes, we argue that a \emph{higher-order} binding of virtual personas requires successfully approximating not only the opinions of a user as an identified member of a group, but also the nuanced ways in which that user perceives and evaluates those outside the group. In particular, faithfully simulating how humans perceive different social groups is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories"" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies. Altogether, our work extends the applicability of LLMs beyond estimating individual self-opinions, enabling their use in a broader range of human studies.",http://arxiv.org/abs/2504.11673v1,IT,Mixed
Deep Generative Model-Based Generation of Synthetic Individual-Specific Brain MRI Segmentations,"To the best of our knowledge, all existing methods that can generate synthetic brain magnetic resonance imaging (MRI) scans for a specific individual require detailed structural or volumetric information about the individual's brain. However, such brain information is often scarce, expensive, and difficult to obtain. In this paper, we propose the first approach capable of generating synthetic brain MRI segmentations -- specifically, 3D white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations -- for individuals using their easily obtainable and often readily available demographic, interview, and cognitive test information. Our approach features a novel deep generative model, CSegSynth, which outperforms existing prominent generative models, including conditional variational autoencoder (C-VAE), conditional generative adversarial network (C-GAN), and conditional latent diffusion model (C-LDM). We demonstrate the high quality of our synthetic segmentations through extensive evaluations. Also, in assessing the effectiveness of the individual-specific generation, we achieve superior volume prediction, with mean absolute errors of only 36.44mL, 29.20mL, and 35.51mL between the ground-truth WM, GM, and CSF volumes of test individuals and those volumes predicted based on generated individual-specific segmentations, respectively.",http://arxiv.org/abs/2504.12352v2,IT,Qualitative
CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical Interview Dialogue Reconstruction and Evaluation,"Large language models (LLMs) hold great promise for assisting clinical interviews due to their fluent interactive capabilities and extensive medical knowledge. However, the lack of high-quality interview dialogue data and widely accepted evaluation methods has significantly impeded this process. So we propose CliniChat, a framework that integrates multi-source knowledge to enable LLMs to simulate real-world clinical interviews. It consists of two modules: Clini-Recon and Clini-Eval, each responsible for reconstructing and evaluating interview dialogues, respectively. By incorporating three sources of knowledge, Clini-Recon transforms clinical notes into systematic, professional, and empathetic interview dialogues. Clini-Eval combines a comprehensive evaluation metric system with a two-phase automatic evaluation approach, enabling LLMs to assess interview performance like experts. We contribute MedQA-Dialog, a high-quality synthetic interview dialogue dataset, and CliniChatGLM, a model specialized for clinical interviews. Experimental results demonstrate that CliniChatGLM's interview capabilities undergo a comprehensive upgrade, particularly in history-taking, achieving state-of-the-art performance.",http://arxiv.org/abs/2504.10418v1,IT,Mixed
AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents,"A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.",http://arxiv.org/abs/2504.09723v2,IT,Mixed
UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents,"Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\textbf{LLM Agent}) research inspired us to design \textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.",http://arxiv.org/abs/2504.09407v2,IT,Mixed
"""It's not a representation of me"": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services","Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.",http://arxiv.org/abs/2504.09346v1,IT,Mixed
"From ""Worse is Better"" to Better: Lessons from a Mixed Methods Study of Ansible's Challenges","Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed methods approach, we investigate challenges, obstacles, and issues faced by practitioners. We analyze 59,157 posts from Stack Overflow, Reddit, and the Ansible Forum to identify common pain points, complemented by 16 semi-structured interviews with practitioners of varying expertise levels. Based on our findings, we propose four main recommendations to improve Ansible: 1) refactoring to mitigate performance issues, 2) restructuring higher-level language concepts, 3) improved debugging and error reporting tools, and 4) better documentation and learning resources. By highlighting the real-world struggles of Ansible users, we provide actionable insights for tool designers, educators, and the broader IaC community, contributing to a deeper understanding of the trade-offs inherent in IaC tools.",http://arxiv.org/abs/2504.08678v1,IT,Mixed
"Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment","Depression is a prevalent mental health disorder that is difficult to detect early due to subjective symptom assessments. Recent advancements in large language models have offered efficient and cost-effective approaches for this objective. In this study, we evaluated the performance of four LLMs in depression detection using clinical interview data. We selected the best performing model and further tested it in the severity evaluation scenario and knowledge enhanced scenario. The robustness was evaluated in complex diagnostic scenarios using a dataset comprising 51074 statements from six different mental disorders. We found that DeepSeek V3 is the most reliable and cost-effective model for depression detection, performing well in both zero-shot and few-shot scenarios, with zero-shot being the most efficient choice. The evaluation of severity showed low agreement with the human evaluator, particularly for mild depression. The model maintains stably high AUCs for detecting depression in complex diagnostic scenarios. These findings highlight DeepSeek V3s strong potential for text-based depression detection in real-world clinical applications. However, they also underscore the need for further refinement in severity assessment and the mitigation of potential biases to enhance clinical reliability.",http://arxiv.org/abs/2504.04891v1,IT,Mixed
When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing,"Large language models (LLMs) have been actively applied in the mental health field. Recent research shows the promise of LLMs in applying psychotherapy, especially motivational interviewing (MI). However, there is a lack of studies investigating how language models understand MI ethics. Given the risks that malicious actors can use language models to apply MI for unethical purposes, it is important to evaluate their capability of differentiating ethical and unethical MI practices. Thus, this study investigates the ethical awareness of LLMs in MI with multiple experiments. Our findings show that LLMs have a moderate to strong level of knowledge in MI. However, their ethical standards are not aligned with the MI spirit, as they generated unethical responses and performed poorly in detecting unethical responses. We proposed a Chain-of-Ethic prompt to mitigate those risks and improve safety. Finally, our proposed strategy effectively improved ethical MI response generation and detection performance. These findings highlight the need for safety evaluations and guidelines for building ethical LLM-powered psychotherapy.",http://arxiv.org/abs/2503.23566v1,IT,Mixed
Socially Constructed Treatment Plans: Analyzing Online Peer Interactions to Understand How Patients Navigate Complex Medical Conditions,"When faced with complex and uncertain medical conditions (e.g., cancer, mental health conditions, recovery from substance dependency), millions of patients seek online peer support. In this study, we leverage content analysis of online discourse and ethnographic studies with clinicians and patient representatives to characterize how treatment plans for complex conditions are ""socially constructed."" Specifically, we ground online conversation on medication-assisted recovery treatment to medication guidelines and subsequently surface when and why people deviate from the clinical guidelines. We characterize the implications and effectiveness of socially constructed treatment plans through in-depth interviews with clinical experts. Finally, given the enthusiasm around AI-powered solutions for patient communication, we investigate whether and how socially constructed treatment-related knowledge is reflected in a state-of-the-art large language model (LLM). Leveraging a novel mixed-method approach, this study highlights critical research directions for patient-centered communication in online health communities.",http://arxiv.org/abs/2503.21986v1,IT,Qualitative
TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews,"Thematic analysis (TA) is a widely used qualitative approach for uncovering latent meanings in unstructured text data. TA provides valuable insights in healthcare but is resource-intensive. Large Language Models (LLMs) have been introduced to perform TA, yet their applications in healthcare remain unexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis framework using Multi-Agent LLMs for clinical interviews. We leverage the scalability and coherence of multi-agent systems through structured conversations between agents and coordinate the expertise of cardiac experts in TA. Using interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we demonstrate that TAMA outperforms existing LLM-assisted TA approaches, achieving higher thematic hit rate, coverage, and distinctiveness. TAMA demonstrates strong potential for automated TA in clinical settings by leveraging multi-agent LLM systems with human-in-the-loop integration by enhancing quality while significantly reducing manual workload.",http://arxiv.org/abs/2503.20666v1,IT,Qualitative
Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder,"Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum disorders. Recently, clinical estimates of the severity of disorganized thinking have been shown to correlate with measures of how difficult speech transcripts would be for large language models (LLMs) to predict. However, LLMs' deployment challenges -- including privacy concerns, computational and financial costs, and lack of transparency of training data -- limit their clinical utility. We investigate whether smaller neural language models can serve as effective alternatives for detecting positive formal thought disorder, using the same sliding window based perplexity measurements that proved effective with larger models. Surprisingly, our results show that smaller models are more sensitive to linguistic differences associated with formal thought disorder than their larger counterparts. Detection capability declines beyond a certain model size and context length, challenging the common assumption of ``bigger is better'' for LLM-based applications. Our findings generalize across audio diaries and clinical interview speech samples from individuals with psychotic symptoms, suggesting a promising direction for developing efficient, cost-effective, and privacy-preserving screening tools that can be deployed in both clinical and naturalistic settings.",http://arxiv.org/abs/2503.20103v1,IT,Mixed
Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties,"Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our \href{https://github.com/jiyounglee-0523/TransEnV}{code} and \href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets} are publicly available.",http://arxiv.org/abs/2505.20875v1,IT,Quantitative
Frame-Level Captions for Long Video Generation with Complex Multi Scenes,"Generating long videos that can show complex stories, like movie scenes from scripts, has great promise and offers much more than short clips. However, current methods that use autoregression with diffusion models often struggle because their step-by-step process naturally leads to a serious error accumulation (drift). Also, many existing ways to make long videos focus on single, continuous scenes, making them less useful for stories with many events and changes. This paper introduces a new approach to solve these problems. First, we propose a novel way to annotate datasets at the frame-level, providing detailed text guidance needed for making complex, multi-scene long videos. This detailed guidance works with a Frame-Level Attention Mechanism to make sure text and video match precisely. A key feature is that each part (frame) within these windows can be guided by its own distinct text prompt. Our training uses Diffusion Forcing to provide the model with the ability to handle time flexibly. We tested our approach on difficult VBench 2.0 benchmarks (""Complex Plots"" and ""Complex Landscapes"") based on the WanX2.1-T2V-1.3B model. The results show our method is better at following instructions in complex, changing scenes and creates high-quality long videos. We plan to share our dataset annotation methods and trained models with the research community. Project page: https://zgctroy.github.io/frame-level-captions .",http://arxiv.org/abs/2505.20827v1,IT,Quantitative
FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information,"We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.",http://arxiv.org/abs/2505.20650v1,IT,Qualitative
Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach,"Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at https://github.com/azencot-group/ImagenFew.",http://arxiv.org/abs/2505.20446v1,IT,Quantitative
OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction,"Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role's voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms. Code and dataset are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.",http://arxiv.org/abs/2505.20277v1,IT,Quantitative
GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining,"The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora. Existing domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks. This paper introduces Group Robust Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously. GRAPE dynamically adjusts sampling weights across source domains (domain weights) while concurrently modulating task weights that quantify the relative importance of each individual target task. This adaptive process prioritizes tasks based on their learning difficulty throughout training. We formulate this interleaved reweighting mechanism as a minimax optimization problem: The inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; The outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning performance across 6 benchmarks. Furthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.",http://arxiv.org/abs/2505.20380v1,IT,Quantitative
Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue,"Existing Task-Oriented Dialogue (TOD) systems primarily focus on single-session dialogues, limiting their effectiveness in long-term memory augmentation. To address this challenge, we introduce a MS-TOD dataset, the first multi-session TOD dataset designed to retain long-term memory across sessions, enabling fewer turns and more efficient task completion. This defines a new benchmark task for evaluating long-term memory in multi-session TOD. Based on this new dataset, we propose a Memory-Active Policy (MAP) that improves multi-session dialogue efficiency through a two-stage approach. 1) Memory-Guided Dialogue Planning retrieves intent-aligned history, identifies key QA units via a memory judger, refines them by removing redundant questions, and generates responses based on the reconstructed memory. 2) Proactive Response Strategy detects and correct errors or omissions, ensuring efficient and accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on response quality and effectiveness of the proactive strategy. Experiments on MS-TOD demonstrate that MAP significantly improves task success and turn efficiency in multi-session scenarios, while maintaining competitive performance on conventional single-session tasks.",http://arxiv.org/abs/2505.20231v1,IT,Quantitative
Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations,"Limited access to mental healthcare, extended wait times, and increasing capabilities of Large Language Models (LLMs) has led individuals to turn to LLMs for fulfilling their mental health needs. However, examining the multi-turn mental health conversation capabilities of LLMs remains under-explored. Existing evaluation frameworks typically focus on diagnostic accuracy and win-rates and often overlook alignment with patient-specific goals, values, and personalities required for meaningful conversations. To address this, we introduce MedAgent, a novel framework for synthetically generating realistic, multi-turn mental health sensemaking conversations and use it to create the Mental Health Sensemaking Dialogue (MHSD) dataset, comprising over 2,200 patient-LLM conversations. Additionally, we present MultiSenseEval, a holistic framework to evaluate the multi-turn conversation abilities of LLMs in healthcare settings using human-centric criteria. Our findings reveal that frontier reasoning models yield below-par performance for patient-centric communication and struggle at advanced diagnostic capabilities with average score of 31%. Additionally, we observed variation in model performance based on patient's persona and performance drop with increasing turns in the conversation. Our work provides a comprehensive synthetic data generation framework, a dataset and evaluation framework for assessing LLMs in multi-turn mental health conversations.",http://arxiv.org/abs/2505.20201v1,IT,Quantitative
Visual Abstract Thinking Empowers Multimodal Reasoning,"Images usually convey richer detail than text, but often include redundant information which potentially downgrades multimodal reasoning performance. When faced with lengthy or complex messages, humans tend to employ abstract thinking to convert them into simple and concise abstracts. Inspired by this cognitive strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking paradigm that prompts Multimodal Large Language Models (MLLMs) with visual abstract instead of explicit verbal thoughts or elaborate guidance, permitting a more concentrated visual reasoning mechanism. Explicit thinking, such as Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity of reasoning process via inserting verbose intermediate steps, external knowledge or visual information. In contrast, VAT reduces redundant visual information and encourages models to focus their reasoning on more essential visual elements. Experimental results show that VAT consistently empowers different models, and achieves an average gain of 17% over GPT-4o baseline by employing diverse types of visual abstracts, demonstrating that VAT can enhance visual reasoning abilities for MLLMs regarding conceptual, structural and relational reasoning tasks. VAT is also compatible with CoT in knowledge-intensive multimodal reasoning tasks. These findings highlight the effectiveness of visual reasoning via abstract thinking and encourage further exploration of more diverse reasoning paradigms from the perspective of human cognition.",http://arxiv.org/abs/2505.20164v1,IT,Quantitative
Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models,"Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, yet they often struggle with trivially simple visual tasks. In this work, we focus on the domain of basic 2D Euclidean geometry and systematically categorize the fundamental, indivisible visual perception skills, which we refer to as atomic visual skills. We then introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find that they struggle with these tasks, despite being trivial for adult humans. Our findings highlight the need for purpose-built datasets to train and evaluate VLMs on atomic, rather than composite, visual perception tasks.",http://arxiv.org/abs/2505.20021v1,IT,Quantitative
On the class of coding optimality of human languages and the origins of Zipf's law,"Here we present a new class of optimality for coding systems. Members of that class are separated linearly from optimal coding and thus exhibit Zipf's law, namely a power-law distribution of frequency ranks. Whithin that class, Zipf's law, the size-rank law and the size-probability law form a group-like structure. We identify human languages that are members of the class. All languages showing sufficient agreement with Zipf's law are potential members of the class. In contrast, there are communication systems in other species that cannot be members of that class for exhibiting an exponential distribution instead but dolphins and humpback whales might. We provide a new insight into plots of frequency versus rank in double logarithmic scale. For any system, a straight line in that scale indicates that the lengths of optimal codes under non-singular coding and under uniquely decodable encoding are separated by a linear function whose slope is the exponent of Zipf's law. For systems under compression and constrained to be uniquely decodable, such a straight line may indicate that the system is coding close to optimality. Our findings provide support for the hypothesis that Zipf's law originates from compression.",http://arxiv.org/abs/2505.20015v1,IT,Mixed
Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction,"Human mobility prediction is crucial for applications ranging from location-based recommendations to urban planning, which aims to forecast users' next location visits based on historical trajectories. Despite the severe long-tailed distribution of locations, the problem of long-tailed mobility prediction remains largely underexplored. Existing long-tailed learning methods primarily focus on rebalancing the skewed distribution at the data, model, or class level, neglecting to exploit the spatiotemporal semantics of locations. To address this gap, we propose the first plug-and-play framework for long-tailed mobility prediction in an exploitation and exploration manner, named \textbf{A}daptive \textbf{LO}cation \textbf{H}ier\textbf{A}rchy learning (ALOHA). First, we construct city-tailored location hierarchy based on Large Language Models (LLMs) by exploiting Maslow's theory of human motivation to design Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics. Second, we optimize the location hierarchy predictions by Gumbel disturbance and node-wise adaptive weights within the hierarchical tree structure. Experiments on state-of-the-art models across six datasets demonstrate the framework's consistent effectiveness and generalizability, which strikes a well balance between head and tail locations. Weight analysis and ablation studies reveal the optimization differences of each component for head and tail locations. Furthermore, in-depth analyses of hierarchical distance and case study demonstrate the effective semantic guidance from the location hierarchy. Our code will be made publicly available.",http://arxiv.org/abs/2505.19965v1,IT,Mixed
The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants,"As proprietary giants increasingly dominate the race for ever-larger language models, a pressing question arises for the open-source community: can smaller models remain competitive across a broad range of tasks? In this paper, we present the Avengers--a simple recipe that effectively leverages the collective intelligence of open-source, smaller language models. Our framework is built upon four lightweight operations: (i) embedding: encode queries using a text embedding model; (ii) clustering: group queries based on their semantic similarity; (iii) scoring: scores each model's performance within each cluster; and (iv) voting: improve outputs via repeated sampling and voting. At inference time, each query is embedded and assigned to its nearest cluster. The top-performing model(s) within that cluster are selected to generate the response using the Self-Consistency or its multi-model variant. Remarkably, with 10 open-source models (~7B parameters each), the Avengers collectively outperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code, logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the Avengers delivers superior out-of-distribution generalization, and remains robust across various embedding models, clustering algorithms, ensemble strategies, and values of its sole parameter--the number of clusters. We have open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers",http://arxiv.org/abs/2505.19797v1,IT,Quantitative
Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition,"Visual Emotion Recognition (VER) is a critical yet challenging task aimed at inferring emotional states of individuals based on visual cues. However, existing works focus on single domains, e.g., realistic images or stickers, limiting VER models' cross-domain generalizability. To fill this gap, we introduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER) task, which aims to generalize visual emotion recognition from the source domain (e.g., realistic images) to the low-resource target domain (e.g., stickers) in an unsupervised manner. Compared to the conventional unsupervised domain adaptation problems, UCDVER presents two key challenges: a significant emotional expression variability and an affective distribution shift. To mitigate these issues, we propose the Knowledge-aligned Counterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically, KCDP leverages a VLM to align emotional representations in a shared knowledge space and guides diffusion models for improved visual affective perception. Furthermore, a Counterfactual-Enhanced Language-image Emotional Alignment (CLIEA) method generates high-quality pseudo-labels for the target domain. Extensive experiments demonstrate that our model surpasses SOTA models in both perceptibility and generalization, e.g., gaining 12% improvements over the SOTA VER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver.",http://arxiv.org/abs/2505.19694v1,IT,Quantitative
"Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation","Automatic related work generation (RWG) can save people's time and effort when writing a draft of related work section (RWS) for further revision. However, existing methods for RWG always suffer from shallow comprehension due to taking the limited portions of references papers as input and isolated explanation for each reference due to ineffective capturing the relationships among them. To address these issues, we focus on full-text-based RWG task and propose a novel multi-agent framework. Our framework consists of three agents: a selector that decides which section of the papers is going to read next, a reader that digests the selected section and updates a shared working memory, and a writer that generates RWS based on the final curated memory. To better capture the relationships among references, we also propose two graph-aware strategies for selector, enabling to optimize the reading order with constrains of the graph structure. Extensive experiments demonstrate that our framework consistently improves performance across three base models and various input configurations. The graph-aware selectors outperform alternative selectors, achieving state-of-the-art results. The code and data are available at https://github.com/1190200817/Full_Text_RWG.",http://arxiv.org/abs/2505.19647v1,IT,Quantitative
MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios,"Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.",http://arxiv.org/abs/2505.21333v1,IT,Quantitative
MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs,"Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.",http://arxiv.org/abs/2505.21327v1,IT,Quantitative
PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems,"Although Automatic Speech Recognition (ASR) systems have become an integral part of modern technology, their evaluation remains challenging, particularly for low-resource languages such as Persian. This paper introduces Persian Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to address this gap by incorporating diverse linguistic and acoustic conditions. We evaluate ten ASR systems, including state-of-the-art commercial and open-source models, to examine performance variations and inherent biases. Additionally, we conduct an in-depth analysis of Persian ASR transcriptions, identifying key error types and proposing a novel metric that weights substitution errors. This metric enhances evaluation robustness by reducing the impact of minor and partial errors, thereby improving the precision of performance assessment. Our findings indicate that while ASR models generally perform well on standard Persian, they struggle with regional accents, children's speech, and specific linguistic challenges. These results highlight the necessity of fine-tuning and incorporating diverse, representative training datasets to mitigate biases and enhance overall ASR performance. PSRB provides a valuable resource for advancing ASR research in Persian and serves as a framework for developing benchmarks in other low-resource languages. A subset of the PSRB dataset is publicly available at https://huggingface.co/datasets/PartAI/PSRB.",http://arxiv.org/abs/2505.21230v1,IT,Quantitative
Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis,"Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre- training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research",http://arxiv.org/abs/2505.21138v1,IT,Quantitative
Efficient Large Language Model Inference with Neural Block Linearization,"The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.",http://arxiv.org/abs/2505.21077v1,IT,Quantitative
DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding,"Multimodal large language models have demonstrated remarkable capabilities in visual understanding, but their application to long-term Earth observation analysis remains limited, primarily focusing on single-temporal or bi-temporal imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework for analyzing long-term urban dynamics through remote sensing imagery. Our suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42 megacities in the U.S. from 2005 to 2023, organized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding tasks, from fundamental change detection (pixel-level) to quantitative analyses (regional-level) and comprehensive urban narratives (scene-level), capturing diverse urban dynamics including expansion/transformation patterns, disaster assessment, and environmental challenges. We evaluate 17 state-of-the-art multimodal large language models and reveal their limitations in long-term temporal understanding and quantitative analysis. These challenges motivate the creation of DVL-Instruct, a specialized instruction-tuning dataset designed to enhance models' capabilities in multi-temporal Earth observation. Building upon this dataset, we develop DVLChat, a baseline model capable of both image-level question-answering and pixel-level segmentation, facilitating a comprehensive understanding of city dynamics through language interactions.",http://arxiv.org/abs/2505.21076v1,IT,Mixed
Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection,"Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.",http://arxiv.org/abs/2505.20956v1,IT,Quantitative
Cooperation of Experts: Fusing Heterogeneous Information with Large Margin,"Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at https://github.com/strangeAlan/CoE.",http://arxiv.org/abs/2505.20853v1,IT,Quantitative
Concealment of Intent: A Game-Theoretic Analysis,"As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.",http://arxiv.org/abs/2505.20841v1,IT,Quantitative
Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models,"Inverse problems (IPs) involve reconstructing signals from noisy observations. Traditional approaches often rely on handcrafted priors, which can fail to capture the complexity of real-world data. The advent of pre-trained generative models has introduced new paradigms, offering improved reconstructions by learning rich priors from data. Among these, diffusion models (DMs) have emerged as a powerful framework, achieving remarkable reconstruction performance across numerous IPs. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approach under appropriate conditions and validate its superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers.",http://arxiv.org/abs/2505.20789v1,IT,Mixed
Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline,"Multilingual large language models (LLMs) often exhibit factual inconsistencies across languages, with significantly better performance in factual recall tasks in English than in other languages. The causes of these failures, however, remain poorly understood. Using mechanistic analysis techniques, we uncover the underlying pipeline that LLMs employ, which involves using the English-centric factual recall mechanism to process multilingual queries and then translating English answers back into the target language. We identify two primary sources of error: insufficient engagement of the reliable English-centric mechanism for factual recall, and incorrect translation from English back into the target language for the final answer. To address these vulnerabilities, we introduce two vector interventions, both independent of languages and datasets, to redirect the model toward better internal paths for higher factual consistency. Our interventions combined increase the recall accuracy by over 35 percent for the lowest-performing language. Our findings demonstrate how mechanistic insights can be used to unlock latent multilingual capabilities in LLMs.",http://arxiv.org/abs/2505.20546v1,IT,Quantitative
Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding,"Understanding online conversations has attracted research attention with the growth of social networks and online discussion forums. Content analysis of posts and replies in online conversations is difficult because each individual utterance is usually short and may implicitly refer to other posts within the same conversation. Thus, understanding individual posts requires capturing the conversational context and dependencies between different parts of a conversation tree and then encoding the context dependencies between posts and comments/replies into the language model. To this end, we propose a general-purpose mechanism to discover appropriate conversational context for various aspects about an online post in a conversation, such as whether it is informative, insightful, interesting or funny. Specifically, we design two families of Conversation Kernels, which explore different parts of the neighborhood of a post in the tree representing the conversation and through this, build relevant conversational context that is appropriate for each task being considered. We apply our developed method to conversations crawled from slashdot.org, which allows users to apply highly different labels to posts, such as 'insightful', 'funny', etc., and therefore provides an ideal experimental platform to study whether a framework such as Conversation Kernels is general-purpose and flexible enough to be adapted to disparately different conversation understanding tasks.",http://arxiv.org/abs/2505.20482v1,IT,Mixed
Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming,"Automating robust hypothesis generation in open environments is pivotal for AI cognition. We introduce a novel framework integrating a multi-agent system, powered by Large Language Models (LLMs), with Inductive Logic Programming (ILP). Our system's LLM agents autonomously define a structured symbolic vocabulary (predicates) and relational templates , i.e., \emph{language bias} directly from raw textual data. This automated symbolic grounding (the construction of the language bias), traditionally an expert-driven bottleneck for ILP, then guides the transformation of text into facts for an ILP solver, which inductively learns interpretable rules. This approach overcomes traditional ILP's reliance on predefined symbolic structures and the noise-sensitivity of pure LLM methods. Extensive experiments in diverse, challenging scenarios validate superior performance, paving a new path for automated, explainable, and verifiable hypothesis generation.",http://arxiv.org/abs/2505.21486v1,IT,Quantitative
Automated Privacy Information Annotation in Large Language Model Interactions,"Users interacting with large language models (LLMs) under their real identifiers often unknowingly risk disclosing private information. Automatically notifying users whether their queries leak privacy and which phrases leak what private information has therefore become a practical need. Existing privacy detection methods, however, were designed for different objectives and application scenarios, typically tagging personally identifiable information (PII) in anonymous content. In this work, to support the development and evaluation of privacy detection models for LLM interactions that are deployable on local user devices, we construct a large-scale multilingual dataset with 249K user queries and 154K annotated privacy phrases. In particular, we build an automated privacy annotation pipeline with cloud-based strong LLMs to automatically extract privacy phrases from dialogue datasets and annotate leaked information. We also design evaluation metrics at the levels of privacy leakage, extracted privacy phrase, and privacy information. We further establish baseline methods using light-weight LLMs with both tuning-free and tuning-based methods, and report a comprehensive evaluation of their performance. Evaluation results reveal a gap between current performance and the requirements of real-world LLM applications, motivating future research into more effective local privacy detection methods grounded in our dataset.",http://arxiv.org/abs/2505.20910v1,IT,Quantitative
"Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models","Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.",http://arxiv.org/abs/2505.20753v1,IT,Quantitative
Ten Principles of AI Agent Economics,"The rapid rise of AI-based autonomous agents is transforming human society and economic systems, as these entities increasingly exhibit human-like or superhuman intelligence. From excelling at complex games like Go to tackling diverse general-purpose tasks with large language and multimodal models, AI agents are evolving from specialized tools into dynamic participants in social and economic ecosystems. Their autonomy and decision-making capabilities are poised to impact industries, professions, and human lives profoundly, raising critical questions about their integration into economic activities, potential ethical concerns, and the balance between their utility and safety. To address these challenges, this paper presents ten principles of AI agent economics, offering a framework to understand how AI agents make decisions, influence social interactions, and participate in the broader economy. Drawing on economics, decision theory, and ethics, we explore fundamental questions, such as whether AI agents might evolve from tools into independent entities, their impact on labor markets, and the ethical safeguards needed to align them with human values. These principles build on existing economic theories while accounting for the unique traits of AI agents, providing a roadmap for their responsible integration into human systems. Beyond theoretical insights, this paper highlights the urgency of future research into AI trustworthiness, ethical guidelines, and regulatory oversight. As we enter a transformative era, this work serves as both a guide and a call to action, ensuring AI agents contribute positively to human progress while addressing risks tied to their unprecedented capabilities.",http://arxiv.org/abs/2505.20273v1,IT,Mixed
Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks,"Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.",http://arxiv.org/abs/2505.20047v1,IT,Mixed
Expectations and Reality: Why an enterprise software system didn't work as planned,"Over two decades, we and other research groups have found that ethnographic and social analyses of work settings can provide insights useful to the process of system analysis and design. Despite this, ethnographic and social analyses have not been widely assimilated into industry practice. Practitioners tend to address sociotechnical factors in an ad-hoc manner, often post-implementation, once system use or outcome has become problematic. In response to this, we have developed a lightweight qualitative approach to provide insights to ameliorate problematic system deployments. Unlike typical ethnographies and social analyses of work activity that inform systems analysis and design; we argue that analysis of intentional and structural factors to inform system deployment and integration can have a shorter time duration and yet can provide actionable insights. We evaluate our approach using a case study of a problematic enterprise document manage-ment system within a multinational systems engineering organization. Our find-ings are of academic and practical significance as our approach demonstrates that structural-intentional analysis scales to enable the timely analysis of large-scale system deployments.",http://arxiv.org/abs/1104.1370v1,IT,Qualitative
Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations,"Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.",http://arxiv.org/abs/2505.20052v1,IT,Quantitative
Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat,"Purpose: To develop a bilingual multimodal visual question answering (VQA) benchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts and associated captions published between January 1, 2016, and December 31, 2024, were collected from WeChat Official Accounts. Based on these captions, bilingual question-answer (QA) pairs in Chinese and English were generated using GPT-4o-mini. QA pairs were categorized into six subsets by question type and language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN, Single-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark was used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included 3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548 conditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0 Flash achieved the highest overall accuracy (0.548), outperforming GPT-4o (0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led in both Chinese (0.546) and English subsets (0.550). Subset-specific performance showed Gemini 2.0 Flash excelled in Binary_CN (0.687), Single-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked highest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382), and Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study presents the first bilingual VQA benchmark for ophthalmology, distinguished by its real-world context and inclusion of multiple examinations per patient. The dataset reflects authentic clinical decision-making scenarios and enables quantitative evaluation of VLMs, supporting the development of accurate, specialized, and trustworthy AI systems for eye care.",http://arxiv.org/abs/2505.19624v1,IT,Quantitative
LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models,"Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. In this survey, we conduct a data-driven, semi-automated review of research on limitations of LLM (LLLMs) from 2022 to 2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we identify 14,648 relevant papers using keyword filtering, LLM-based classification, validated against expert labels, and topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs research grows even faster, reaching over 30% of LLM papers by late 2024. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2024. We release a dataset of annotated abstracts and a validated methodology, and offer a quantitative view of trends in LLM limitations research.",http://arxiv.org/abs/2505.19240v1,IT,Quantitative
Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models,"We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework that models large language model generation as a controlled dynamical system evolving on a low_dimensional semantic manifold. By casting latent_state updates as discrete time Euler approximations of continuous dynamics, we map intrinsic energy_driven flows and context_dependent forces onto Transformer components (residual connections, attention, feed-forward networks). Leveraging Lyapunov stability theory We define three empirical metrics (state continuity, clustering quality, topological persistence) that quantitatively link latent_trajectory properties to text fluency, grammaticality, and semantic coherence. Extensive experiments across decoding parameters validate DMET's predictions and yield principled guidelines for balancing creativity and consistency in text generation.",http://arxiv.org/abs/2505.20340v1,IT,Quantitative
HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model,"Face restoration has achieved remarkable advancements through the years of development. However, ensuring that restored facial images exhibit high fidelity, preserve authentic features, and avoid introducing artifacts or biases remains a significant challenge. This highlights the need for models that are more ""honest"" in their reconstruction from low-quality inputs, accurately reflecting original characteristics. In this work, we propose HonestFace, a novel approach designed to restore faces with a strong emphasis on such honesty, particularly concerning identity consistency and texture realism. To achieve this, HonestFace incorporates several key components. First, we propose an identity embedder to effectively capture and preserve crucial identity features from both the low-quality input and multiple reference faces. Second, a masked face alignment method is presented to enhance fine-grained details and textural authenticity, thereby preventing the generation of patterned or overly synthetic textures and improving overall clarity. Furthermore, we present a new landmark-based evaluation metric. Based on affine transformation principles, this metric improves the accuracy compared to conventional L2 distance calculations for facial feature alignment. Leveraging these contributions within a one-step diffusion model framework, HonestFace delivers exceptional restoration results in terms of facial fidelity and realism. Extensive experiments demonstrate that our approach surpasses existing state-of-the-art methods, achieving superior performance in both visual quality and quantitative assessments. The code and pre-trained models will be made publicly available at https://github.com/jkwang28/HonestFace .",http://arxiv.org/abs/2505.18469v1,IT,Quantitative
Generative Distribution Embeddings,"Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).",http://arxiv.org/abs/2505.18150v1,IT,Quantitative
Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio,"Advances in AI, particularly LLMs, have dramatically shortened drug discovery cycles by up to 40% and improved molecular target identification. However, these innovations also raise dual-use concerns by enabling the design of toxic compounds. Prompting Moremi Bio Agent without the safety guardrails to specifically design novel toxic substances, our study generated 1020 novel toxic proteins and 5,000 toxic small molecules. In-depth computational toxicity assessments revealed that all the proteins scored high in toxicity, with several closely matching known toxins such as ricin, diphtheria toxin, and disintegrin-based snake venom proteins. Some of these novel agents showed similarities with other several known toxic agents including disintegrin eristostatin, metalloproteinase, disintegrin triflavin, snake venom metalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk assessments and scenario analyses, we identify dual-use capabilities in current LLM-enabled biodesign pipelines and propose multi-layered mitigation strategies. The findings from this toxicity assessment challenge claims that large language models (LLMs) are incapable of designing bioweapons. This reinforces concerns about the potential misuse of LLMs in biodesign, posing a significant threat to research and development (R&D). The accessibility of such technology to individuals with limited technical expertise raises serious biosecurity risks. Our findings underscore the critical need for robust governance and technical safeguards to balance rapid biotechnological innovation with biosecurity imperatives.",http://arxiv.org/abs/2505.17154v1,IT,Quantitative
On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction,"Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.",http://arxiv.org/abs/2505.20068v1,IT,Quantitative
Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications,"Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.",http://arxiv.org/abs/2505.19837v1,IT,Quantitative
Large Language Models for Planning: A Comprehensive and Systematic Survey,"Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.",http://arxiv.org/abs/2505.19683v1,IT,Quantitative
A Survey of LLM $\times$ DATA,"The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.",http://arxiv.org/abs/2505.18458v2,IT,Quantitative
Simulating Macroeconomic Expectations using LLM Agents,"We introduce a novel framework for simulating macroeconomic expectation formation using Large Language Model-Empowered Agents (LLM Agents). By constructing thousands of LLM Agents equipped with modules for personal characteristics, prior expectations, and knowledge, we replicate a survey experiment involving households and experts on inflation and unemployment. Our results show that although the expectations and thoughts generated by LLM Agents are more homogeneous than those of human participants, they still effectively capture key heterogeneity across agents and the underlying drivers of expectation formation. Furthermore, a module-ablation exercise highlights the critical role of prior expectations in simulating such heterogeneity. This approach complements traditional survey methods and offers new insights into AI behavioral science in macroeconomic research.",http://arxiv.org/abs/2505.17648v1,IT,Quantitative
"Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions","LLM-based digital twin simulation, where large language models are used to emulate individual human behavior, holds great promise for research in AI, social science, and digital experimentation. However, progress in this area has been hindered by the scarcity of real, individual-level datasets that are both large and publicly available. This lack of high-quality ground truth limits both the development and validation of digital twin methodologies. To address this gap, we introduce a large-scale, public dataset designed to capture a rich and holistic view of individual human behavior. We survey a representative sample of $N = 2,058$ participants (average 2.42 hours per person) in the US across four waves with 500 questions in total, covering a comprehensive battery of demographic, psychological, economic, personality, and cognitive measures, as well as replications of behavioral economics experiments and a pricing survey. The final wave repeats tasks from earlier waves to establish a test-retest accuracy baseline. Initial analyses suggest the data are of high quality and show promise for constructing digital twins that predict human behavior well at the individual and aggregate levels. By making the full dataset publicly available, we aim to establish a valuable testbed for the development and benchmarking of LLM-based persona simulations. Beyond LLM applications, due to its unique breadth and scale the dataset also enables broad social science research, including studies of cross-construct correlations and heterogeneous treatment effects.",http://arxiv.org/abs/2505.17479v1,IT,Quantitative
From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation,"High-fidelity 3D reconstruction is critical for aerial inspection tasks such as infrastructure monitoring, structural assessment, and environmental surveying. While traditional photogrammetry techniques enable geometric modeling, they lack semantic interpretability, limiting their effectiveness for automated inspection workflows. Recent advances in neural rendering and 3D Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but similarly lack scene-level understanding. In this work, we present a UAV-based pipeline that extends Feature-3DGS for language-guided 3D segmentation. We leverage LSeg-based feature fields with CLIP embeddings to generate heatmaps in response to language prompts. These are thresholded to produce rough segmentations, and the highest-scoring point is then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view renderings. Our results highlight the strengths and limitations of various feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful structure in large-scale outdoor environments. We demonstrate that this hybrid approach enables flexible, language-driven interaction with photorealistic 3D reconstructions, opening new possibilities for semantic aerial inspection and scene understanding.",http://arxiv.org/abs/2505.17402v1,IT,Quantitative
Deep Learning-Driven Ultra-High-Definition Image Restoration: A Survey,"Ultra-high-definition (UHD) image restoration aims to specifically solve the problem of quality degradation in ultra-high-resolution images. Recent advancements in this field are predominantly driven by deep learning-based innovations, including enhancements in dataset construction, network architecture, sampling strategies, prior knowledge integration, and loss functions. In this paper, we systematically review recent progress in UHD image restoration, covering various aspects ranging from dataset construction to algorithm design. This serves as a valuable resource for understanding state-of-the-art developments in the field. We begin by summarizing degradation models for various image restoration subproblems, such as super-resolution, low-light enhancement, deblurring, dehazing, deraining, and desnowing, and emphasizing the unique challenges of their application to UHD image restoration. We then highlight existing UHD benchmark datasets and organize the literature according to degradation types and dataset construction methods. Following this, we showcase major milestones in deep learning-driven UHD image restoration, reviewing the progression of restoration tasks, technological developments, and evaluations of existing methods. We further propose a classification framework based on network architectures and sampling strategies, helping to clearly organize existing methods. Finally, we share insights into the current research landscape and propose directions for further advancements. A related repository is available at https://github.com/wlydlut/UHD-Image-Restoration-Survey.",http://arxiv.org/abs/2505.16161v1,IT,Quantitative
Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey,"With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.",http://arxiv.org/abs/2505.15957v2,IT,Quantitative
Graph Foundation Models: A Comprehensive Survey,"Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.",http://arxiv.org/abs/2505.15116v1,IT,Quantitative
Editing Across Languages: A Survey of Multilingual Knowledge Editing,"While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks,summarize key findings on method effectiveness and transfer patterns, identify challenges in cross-lingual propagation, and highlight open problems related to language anisotropy, evaluation coverage, and edit scalability. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs.",http://arxiv.org/abs/2505.14393v1,IT,Quantitative
"Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives","Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.",http://arxiv.org/abs/2505.14361v1,IT,Quantitative
Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey,"Plane geometry problem solving (PGPS) has recently gained significant attention as a benchmark to assess the multi-modal reasoning capabilities of large vision-language models. Despite the growing interest in PGPS, the research community still lacks a comprehensive overview that systematically synthesizes recent work in PGPS. To fill this gap, we present a survey of existing PGPS studies. We first categorize PGPS methods into an encoder-decoder framework and summarize the corresponding output formats used by their encoders and decoders. Subsequently, we classify and analyze these encoders and decoders according to their architectural designs. Finally, we outline major challenges and promising directions for future research. In particular, we discuss the hallucination issues arising during the encoding phase within encoder-decoder architectures, as well as the problem of data leakage in current PGPS benchmarks.",http://arxiv.org/abs/2505.14340v1,IT,Quantitative
Integration of TinyML and LargeML: A Survey of 6G and Beyond,"The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management. Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.",http://arxiv.org/abs/2505.15854v1,IT,Quantitative
The AI Gap: How Socioeconomic Status Affects Language Technology Interactions,"Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.",http://arxiv.org/abs/2505.12158v2,IT,Quantitative
A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?,"Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.",http://arxiv.org/abs/2505.10924v2,IT,Quantitative
Private Transformer Inference in MLaaS: A Survey,"Transformer models have revolutionized AI, powering applications like content generation and sentiment analysis. However, their deployment in Machine Learning as a Service (MLaaS) raises significant privacy concerns, primarily due to the centralized processing of sensitive user data. Private Transformer Inference (PTI) offers a solution by utilizing cryptographic techniques such as secure multi-party computation and homomorphic encryption, enabling inference while preserving both user data and model privacy. This paper reviews recent PTI advancements, highlighting state-of-the-art solutions and challenges. We also introduce a structured taxonomy and evaluation framework for PTI, focusing on balancing resource efficiency with privacy and bridging the gap between high-performance inference and data privacy.",http://arxiv.org/abs/2505.10315v1,IT,Quantitative
ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models,"Currently, a prevalent approach for enhancing Vision-Language Models (VLMs) performance is to encode both the high-resolution version and the thumbnail of an image simultaneously. While effective, this method generates a large number of image tokens. When combined with the widely used Rotary Position Embedding (RoPE), its long-term decay property hinders the interaction between high-resolution tokens and thumbnail tokens, as well as between text and image. To address these issues, we propose ID-Align, which alleviates these problems by reordering position IDs. In this method, high-resolution tokens inherit IDs from their corresponding thumbnail token while constraining the overexpansion of positional indices. Our experiments conducted within the LLaVA-Next framework demonstrate that ID-Align achieves significant improvements, including a 6.09% enhancement on MMBench's relation reasoning tasks and notable gains across multiple benchmarks. Our code is available at the following link: https://github.com/zooblastlbz/ID-Align.",http://arxiv.org/abs/2505.21465v1,IT,Quantitative
OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers,"Lip synchronization is the task of aligning a speaker's lip movements in video with corresponding speech audio, and it is essential for creating realistic, expressive video content. However, existing methods often rely on reference frames and masked-frame inpainting, which limit their robustness to identity consistency, pose variations, facial occlusions, and stylized content. In addition, since audio signals provide weaker conditioning than visual cues, lip shape leakage from the original video will affect lip sync quality. In this paper, we present OmniSync, a universal lip synchronization framework for diverse visual scenarios. Our approach introduces a mask-free training paradigm using Diffusion Transformer models for direct frame editing without explicit masks, enabling unlimited-duration inference while maintaining natural facial dynamics and preserving character identity. During inference, we propose a flow-matching-based progressive noise initialization to ensure pose and identity consistency, while allowing precise mouth-region editing. To address the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance strength over time and space. We also establish the AIGC-LipSync Benchmark, the first evaluation suite for lip synchronization in diverse AI-generated videos. Extensive experiments demonstrate that OmniSync significantly outperforms prior methods in both visual quality and lip sync accuracy, achieving superior results in both real-world and AI-generated videos.",http://arxiv.org/abs/2505.21448v1,IT,Quantitative
"Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling","Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.",http://arxiv.org/abs/2505.21399v1,IT,Quantitative
ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision,"Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings; however, they require labeled query-document pairs for fine-tuning. This poses a significant challenge in MHQA due to the high variability of queries (reformulated) questions throughout the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without labeled documents. ReSCORE leverages large language models to capture each documents relevance to the question and consistency with the correct answer and use them to train a retriever within an iterative question-answering framework. Experiments on three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval, and in turn, the state-of-the-art MHQA performance. Our implementation is available at: https://leeds1219.github.io/ReSCORE.",http://arxiv.org/abs/2505.21250v1,IT,Quantitative
Is Hyperbolic Space All You Need for Medical Anomaly Detection?,"Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at https://hyperbolic-anomalies.github.io",http://arxiv.org/abs/2505.21228v1,IT,Quantitative
Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation,"Large Language Models (LLMs) enhanced with external knowledge retrieval, an approach known as Retrieval-Augmented Generation (RAG), have shown strong performance in open-domain question answering. However, RAG systems remain susceptible to hallucinations: factually incorrect outputs that may arise either from inconsistencies in the model's internal knowledge or incorrect use of the retrieved context. Existing approaches often conflate factuality with faithfulness to the retrieved context, misclassifying factually correct statements as hallucinations if they are not directly supported by the retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval Augmented UNcertainty Quantification), a novel method for hallucination detection in RAG outputs. FRANQ applies different Uncertainty Quantification (UQ) techniques to estimate factuality based on whether a statement is faithful to the retrieved context or not. To evaluate FRANQ and other UQ techniques for RAG, we present a new long-form Question Answering (QA) dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging examples. Extensive experiments on long- and short-form QA across multiple datasets and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing methods.",http://arxiv.org/abs/2505.21072v1,IT,Quantitative
Agent-Environment Alignment via Automated Interface Generation,"Large language model (LLM) agents have shown impressive reasoning capabilities in interactive decision-making tasks. These agents interact with environment through intermediate interfaces, such as predefined action spaces and interaction rules, which mediate the perception and action. However, mismatches often happen between the internal expectations of the agent regarding the influence of its issued actions and the actual state transitions in the environment, a phenomenon referred to as \textbf{agent-environment misalignment}. While prior work has invested substantially in improving agent strategies and environment design, the critical role of the interface still remains underexplored. In this work, we empirically demonstrate that agent-environment misalignment poses a significant bottleneck to agent performance. To mitigate this issue, we propose \textbf{ALIGN}, an \underline{A}uto-A\underline{l}igned \underline{I}nterface \underline{G}e\underline{n}eration framework that alleviates the misalignment by enriching the interface. Specifically, the ALIGN-generated interface enhances both the static information of the environment and the step-wise observations returned to the agent. Implemented as a lightweight wrapper, this interface achieves the alignment without modifying either the agent logic or the environment code. Experiments across multiple domains including embodied tasks, web navigation and tool-use, show consistent performance improvements, with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile, ALIGN-generated interface can generalize across different agent architectures and LLM backbones without interface regeneration. Code and experimental results are available at https://github.com/THUNLP-MT/ALIGN.",http://arxiv.org/abs/2505.21055v1,IT,Mixed
RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes,"Neural fields (NFs) have demonstrated remarkable performance in scene reconstruction, powering various tasks such as novel view synthesis. However, existing NF methods relying on RGB or LiDAR inputs often exhibit severe fragility to adverse weather, particularly when applied in outdoor scenarios like autonomous driving. In contrast, millimeter-wave radar is inherently robust to environmental changes, while unfortunately, its integration with NFs remains largely underexplored. Besides, as outdoor driving scenarios frequently involve moving objects, making spatiotemporal modeling essential for temporally consistent novel view synthesis. To this end, we introduce RF4D, a radar-based neural field framework specifically designed for novel view synthesis in outdoor dynamic scenes. RF4D explicitly incorporates temporal information into its representation, significantly enhancing its capability to model moving objects. We further introduce a feature-level flow module that predicts latent temporal offsets between adjacent frames, enforcing temporal coherence in dynamic scene modeling. Moreover, we propose a radar-specific power rendering formulation closely aligned with radar sensing physics, improving synthesis accuracy and interoperability. Extensive experiments on public radar datasets demonstrate the superior performance of RF4D in terms of radar measurement synthesis quality and occupancy estimation accuracy, achieving especially pronounced improvements in dynamic outdoor scenarios.",http://arxiv.org/abs/2505.20967v1,IT,Quantitative
Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization,"Sound source localization (SSL) is a critical technology for determining the position of sound sources in complex environments. However, existing methods face challenges such as high computational costs and precise calibration requirements, limiting their deployment in dynamic or resource-constrained environments. This paper introduces a novel 3D SSL framework, which uses sparse cross-attention, pretraining, and adaptive signal coherence metrics, to achieve accurate and computationally efficient localization with fewer input microphones. The framework is also fault-tolerant to unreliable or even unknown microphone position inputs, ensuring its applicability in real-world scenarios. Preliminary experiments demonstrate its scalability for multi-source localization without requiring additional hardware. This work advances SSL by balancing the model's performance and efficiency and improving its robustness for real-world scenarios.",http://arxiv.org/abs/2505.20961v1,IT,Quantitative
Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation,"Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.",http://arxiv.org/abs/2505.20897v1,IT,Quantitative
Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization,"Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.",http://arxiv.org/abs/2505.20881v1,IT,Quantitative
AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding,"Hallucination remains a major challenge in multimodal large language models (MLLMs). To address this, various contrastive decoding (CD) methods have been proposed that contrasts original logits with hallucinated logits generated from perturbed inputs. While CD has shown promise in vision-language models (VLMs), it is not well-suited for AV-LLMs, where hallucinations often emerge from both unimodal and cross-modal combinations involving audio, video, and language. These intricate interactions call for a more adaptive and modality-aware decoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding (AVCD)-a novel, training-free decoding framework designed to model trimodal interactions and suppress modality-induced hallucinations in AV-LLMs. Unlike previous CD methods in VLMs that corrupt a fixed modality, AVCD leverages attention distributions to dynamically identify less dominant modalities and applies attentive masking to generate perturbed output logits. To support CD in a trimodal setting, we also reformulate the original CD framework to jointly handle audio, visual, and textual inputs. Finally, to improve efficiency, we introduce entropy-guided adaptive decoding, which selectively skips unnecessary decoding steps based on the model's confidence in its predictions. Extensive experiments demonstrate that AVCD consistently outperforms existing decoding methods. Especially, on the AVHBench dataset, it improves accuracy by 6% for VideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and generalizability.",http://arxiv.org/abs/2505.20862v1,IT,Quantitative
Multi-Domain Explainability of Preferences,"Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated end-to-end method for generating local and global concept-based explanations of preferences across multiple domains. Our method employs an LLM to discover concepts that differentiate between chosen and rejected responses and represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two novel application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work provides a new paradigm for explainability in the era of LLMs.",http://arxiv.org/abs/2505.20088v1,IT,Quantitative
UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models,"Large Language Models (LLMs) have become indispensable tools across various applications, making it more important than ever to ensure the quality and the trustworthiness of their outputs. This has led to growing interest in uncertainty quantification (UQ) methods for assessing the reliability of LLM outputs. Many existing UQ techniques rely on token probabilities, which inadvertently introduces a bias with respect to the length of the output. While some methods attempt to account for this, we demonstrate that such biases persist even in length-normalized approaches. To address the problem, here we propose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing procedure that regresses uncertainty scores on output length and uses the residuals as corrected, length-invariant estimates. Our method is post-hoc, model-agnostic, and applicable to a range of UQ measures. Through extensive evaluation on machine translation, summarization, and question-answering tasks, we demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally length-normalized UQ methods uncertainty estimates across multiple metrics and models.",http://arxiv.org/abs/2505.19060v1,IT,Mixed
Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding,"Large language models (LLMs) suffer from high inference latency due to the auto-regressive decoding process. Speculative decoding accelerates inference by generating multiple draft tokens using a lightweight model and verifying them in parallel. However, existing verification methods rely heavily on distributional consistency while overlooking semantic correctness, thereby limiting the potential speedup of speculative decoding. While some methods employ additional models for relaxed verification of draft tokens, they often fail to generalize effectively to more diverse or open-domain settings. In this work, we propose Reflective Verification, a training-free and semantics-aware approach that achieves a better trade-off between correctness and efficiency. Specifically, we leverage the inherent reflective capacity of LLMs to semantically assess the correctness of draft tokens in parallel during verification. Using prompt-based probing, we obtain both the original and reflective distributions of draft tokens in a single forward pass. The fusion of these distributions enables semantic-level verification of draft tokens that incorporates both consistency and correctness. Experiments across multiple domain benchmarks and model scales demonstrate that our method significantly increases the acceptance length of draft tokens without compromising model performance. Furthermore, we find that the proposed Reflective Verification is orthogonal to existing statistical verification methods, and their combination yields additional 5$ im$15\% improvements in decoding speed.",http://arxiv.org/abs/2505.18629v1,IT,Quantitative
RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection,"Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detection is analogous to aerial oriented object detection, where angle-based methods are recognized for being affected by discontinuities in their loss functions. Drawing inspiration from this domain, we propose Restricted Quadrilateral Representation to define 3D regression targets. RQR3D regresses the smallest horizontal bounding box encapsulating the oriented box, along with the offsets between the corners of these two boxes, thereby transforming the oriented object detection problem into a keypoint regression task. RQR3D is compatible with any 3D object detection approach. We employ RQR3D within an anchor-free single-stage object detection method and introduce an objectness head to address class imbalance problem. Furthermore, we introduce a simplified radar fusion backbone that eliminates the need for voxel grouping and processes the BEV-mapped point cloud with standard 2D convolutions, rather than sparse convolutions. Extensive evaluations on the nuScenes dataset demonstrate that RQR3D achieves state-of-the-art performance in camera-radar 3D object detection, outperforming the previous best method by +4% in NDS and +2.4% in mAP, and significantly reducing the translation and orientation errors, which are crucial for safe autonomous driving. These consistent gains highlight the robustness, precision, and real-world readiness of our approach.",http://arxiv.org/abs/2505.17732v1,IT,Quantitative
Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering,"Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: https://gaozhongpai.github.io/renderfm/.",http://arxiv.org/abs/2505.17338v1,IT,Quantitative
Liouville PDE-based sliced-Wasserstein flow for fair regression,"The sliced Wasserstein flow (SWF), a nonparametric and implicit generative gradient flow, is applied to fair regression. We have improved the SWF in a few aspects. First, the stochastic diffusive term from the Fokker-Planck equation-based Monte Carlo is transformed to Liouville partial differential equation (PDE)-based transport with density estimation, however, without the diffusive term. Now, the computation of the Wasserstein barycenter is approximated by the SWF barycenter with the prescription of Kantorovich potentials for the induced gradient flow to generate its samples. These two efforts improve the convergence in training and testing SWF and SWF barycenters with reduced variance. Applying the generative SWF barycenter for fair regression demonstrates competent profiles in the accuracy-fairness Pareto curves.",http://arxiv.org/abs/2505.17204v1,IT,Quantitative
On the use of Graphs for Satellite Image Time Series,"The Earth's surface is subject to complex and dynamic processes, ranging from large-scale phenomena such as tectonic plate movements to localized changes associated with ecosystems, agriculture, or human activity. Satellite images enable global monitoring of these processes with extensive spatial and temporal coverage, offering advantages over in-situ methods. In particular, resulting satellite image time series (SITS) datasets contain valuable information. To handle their large volume and complexity, some recent works focus on the use of graph-based techniques that abandon the regular Euclidean structure of satellite data to work at an object level. Besides, graphs enable modelling spatial and temporal interactions between identified objects, which are crucial for pattern detection, classification and regression tasks. This paper is an effort to examine the integration of graph-based methods in spatio-temporal remote-sensing analysis. In particular, it aims to present a versatile graph-based pipeline to tackle SITS analysis. It focuses on the construction of spatio-temporal graphs from SITS and their application to downstream tasks. The paper includes a comprehensive review and two case studies, which highlight the potential of graph-based approaches for land cover mapping and water resource forecasting. It also discusses numerous perspectives to resolve current limitations and encourage future developments.",http://arxiv.org/abs/2505.16685v1,IT,Quantitative
LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions,"High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.",http://arxiv.org/abs/2505.17134v1,IT,Quantitative
Large Language Models Implicitly Learn to See and Hear Just By Reading,"This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",http://arxiv.org/abs/2505.17091v1,IT,Quantitative
FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain,"Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we determine the most informative ones. The key idea in our method is to select examples that maximize information gain, measured by the Hessian of the log-likelihood of the LLM. We approximate it efficiently by linearizing the LLM at the last layer using multinomial logistic regression models. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, and back our claims with both quantitative results and an LLM evaluation.",http://arxiv.org/abs/2505.14826v1,IT,Quantitative
AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings,"Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities.This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.",http://arxiv.org/abs/2505.14664v1,IT,Quantitative
TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring,"Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.",http://arxiv.org/abs/2505.14577v1,IT,Quantitative
A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations,"We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.",http://arxiv.org/abs/2505.14106v2,IT,Quantitative
Computational Efficiency under Covariate Shift in Kernel Ridge Regression,"This paper addresses the covariate shift problem in the context of nonparametric regression within reproducing kernel Hilbert spaces (RKHSs). Covariate shift arises in supervised learning when the input distributions of the training and test data differ, presenting additional challenges for learning. Although kernel methods have optimal statistical properties, their high computational demands in terms of time and, particularly, memory, limit their scalability to large datasets. To address this limitation, the main focus of this paper is to explore the trade-off between computational efficiency and statistical accuracy under covariate shift. We investigate the use of random projections where the hypothesis space consists of a random subspace within a given RKHS. Our results show that, even in the presence of covariate shift, significant computational savings can be achieved without compromising learning performance.",http://arxiv.org/abs/2505.14083v1,IT,Quantitative
Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator,"LLM-as-Benchmark-Generator methods have been widely studied as a supplement to human annotators for scalable evaluation, while the potential biases within this paradigm remain underexplored. In this work, we systematically define and validate the phenomenon of inflated performance in models evaluated on their self-generated benchmarks, referred to as self-bias, and attribute it to sub-biases arising from question domain, language style, and wrong labels. On this basis, we propose Silencer, a general framework that leverages the heterogeneity between multiple generators at both the sample and benchmark levels to neutralize bias and generate high-quality, self-bias-silenced benchmark. Experimental results across various settings demonstrate that Silencer can suppress self-bias to near zero, significantly improve evaluation effectiveness of the generated benchmark (with an average improvement from 0.655 to 0.833 in Pearson correlation with high-quality human-annotated benchmark), while also exhibiting strong generalizability.",http://arxiv.org/abs/2505.20738v1,IT,Quantitative
Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors,"Continual learning (CL) enables deep networks to acquire new knowledge while avoiding catastrophic forgetting. The powerful generalization ability of pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training (CLIP) model, has inspired a range of CL methods targeting new and specialized tasks, providing rich multi-modal embeddings that support lightweight, incremental prompt tuning. Existing methods often rely on complex designs built upon specific assumptions, such as intricate regularization schemes for prompt pools, specialized routing mechanisms, or multi-stage incrementations, that introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's intrinsic capabilities. In this paper, we propose a concise CL approach for CLIP based on incremental prompt tuning that fully exploits its multi-modal structure and the stability of textual representations. Our method, Textual Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely as static classifiers, as in existing methods, but as stable anchors to guide the learning of visual prompts, thereby shaping the embedding space (i.e., TPPT-V). We show that our bidirectional supervision strategy enables more effective learning of new knowledge while reducing forgetting. To further close the vision-language gap during CL, we jointly optimizes visual and textual prompts (i.e., TPPT-VT). We also introduce a relational diversity regularization on the textual anchors to prevent embedding space collapse and mitigate correlated forgetting. Extensive experiments and analyses demonstrate the effectiveness of our proposed approach, highlighting the benefits of leveraging CLIP's intrinsic guidance for continual adaptation.",http://arxiv.org/abs/2505.20680v1,IT,Quantitative
Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction,"Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.",http://arxiv.org/abs/2505.20036v1,IT,Quantitative
DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response,"Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.",http://arxiv.org/abs/2505.19973v1,IT,Quantitative
MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models,"Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",http://arxiv.org/abs/2505.19959v1,IT,Quantitative
ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs,"Large Language Models (LLMs) are widely used in Spoken Language Understanding (SLU). Recent SLU models process audio directly by adapting speech input into LLMs for better multimodal learning. A key consideration for these models is the cross-modal alignment between text and audio modalities, which is a telltale sign as to whether or not LLM is able to associate semantic meaning to audio segments. While various methods exist for fusing these modalities, there is no standard metric to evaluate alignment quality in LLMs. In this work, we propose a new metric, ALAS (Automatic Latent Alignment Score). Our study examines the correlation between audio and text representations across transformer layers, for two different tasks (Spoken Question Answering and Emotion Recognition). We showcase that our metric behaves as expected across different layers and different tasks.",http://arxiv.org/abs/2505.19937v1,IT,Mixed
Two Causally Related Needles in a Video Haystack,"Evaluating the video understanding capabilities of Video-Language Models (VLMs) remains a significant challenge. We propose a long-context video understanding benchmark, Causal2Needles, that assesses two crucial abilities insufficiently evaluated by existing benchmarks: (1) the ability to extract information from two separate locations in a long video and understand them jointly, and (2) the ability to model the world in terms of cause and effect in human behaviors. Specifically, Causal2Needles introduces 2-needle questions, which require extracting information from both the cause and effect human-behavior events in a long video and the associated narration text. To prevent textual bias, these questions comprise two complementary formats: one asking to identify the video clip containing the answer, and one asking for the textual description of an unrelated visual detail from that video clip. Our experiments reveal that models excelling in pre-existing benchmarks struggle with 2-needle visual grounding, and the model performance is negatively correlated with the distance between the two needles. These findings highlight critical limitations in current VLMs.",http://arxiv.org/abs/2505.19853v1,IT,Quantitative
Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models,"Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation. Our results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion.",http://arxiv.org/abs/2505.19676v1,IT,Quantitative
Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression,"Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.",http://arxiv.org/abs/2505.19433v1,IT,Quantitative
"VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation","Ensuring that large language models (LLMs) can effectively assess, detect, explain, and remediate software vulnerabilities is critical for building robust and secure software systems. We introduce VADER, a human-evaluated benchmark designed explicitly to assess LLM performance across four key vulnerability-handling dimensions: assessment, detection, explanation, and remediation. VADER comprises 174 real-world software vulnerabilities, each carefully curated from GitHub repositories and annotated by security experts. For each vulnerability case, models are tasked with identifying the flaw, classifying it using Common Weakness Enumeration (CWE), explaining its underlying cause, proposing a patch, and formulating a test plan. Using a one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7 Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and human security experts evaluated each response according to a rigorous scoring rubric emphasizing remediation (quality of the code fix, 50%), explanation (20%), and classification and test plan (30%) according to a standardized rubric. Our results show that current state-of-the-art LLMs achieve only moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with others in the 49-54% range, indicating ample room for improvement. Notably, remediation quality is strongly correlated (Pearson r > 0.97) with accurate classification and test plans, suggesting that models that effectively categorize vulnerabilities also tend to fix them well. VADER's comprehensive dataset, detailed evaluation rubrics, scoring tools, and visualized results with confidence intervals are publicly released, providing the community with an interpretable, reproducible benchmark to advance vulnerability-aware LLMs. All code and data are available at: https://github.com/AfterQuery/vader",http://arxiv.org/abs/2505.19395v1,IT,Quantitative
Language Models Surface the Unwritten Code of Science and Society,"This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's ""unwritten code"" - such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 computer science conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g. theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. This shift reveals the primacy of scientific myths about intrinsic properties driving scientific excellence rather than extrinsic contextualization and storytelling that influence conceptions of relevance and significance. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. We discuss the broad applicability of the framework, leveraging LLMs as diagnostic tools to surface the tacit codes underlying human society, enabling more precisely targeted responsible AI.",http://arxiv.org/abs/2505.18942v2,IT,Qualitative
Next-token pretraining implies in-context learning,"We argue that in-context learning (ICL) predictably arises from standard self-supervised next-token pretraining, rather than being an exotic emergent property. This work establishes the foundational principles of this emergence by focusing on in-distribution ICL, demonstrating how models necessarily adapt to context when trained on token sequences, especially from non-ergodic sources. Our information-theoretic framework precisely predicts these in-distribution ICL dynamics (i.e., context-dependent loss reduction). We verify this with experiments using synthetic datasets of differing types of correlational structure, reproducing characteristic phenomena like phase transitions in training loss for induction head formation and power-law scaling of in-context loss. We further show that a model's in-context performance on any task is mathematically coupled to the ensemble of tasks seen in pretraining, offering a fundamental explanation, grounded in architecture- and modality-independent principles, for such inference-time learning.",http://arxiv.org/abs/2505.18373v1,IT,Quantitative
DART$^3$: Leveraging Distance for Test Time Adaptation in Person Re-Identification,"Person re-identification (ReID) models are known to suffer from camera bias, where learned representations cluster according to camera viewpoints rather than identity, leading to significant performance degradation under (inter-camera) domain shifts in real-world surveillance systems when new cameras are added to camera networks. State-of-the-art test-time adaptation (TTA) methods, largely designed for classification tasks, rely on classification entropy-based objectives that fail to generalize well to ReID, thus making them unsuitable for tackling camera bias. In this paper, we introduce DART$^3$, a TTA framework specifically designed to mitigate camera-induced domain shifts in person ReID. DART$^3$ (Distance-Aware Retrieval Tuning at Test Time) leverages a distance-based objective that aligns better with image retrieval tasks like ReID by exploiting the correlation between nearest-neighbor distance and prediction error. Unlike prior ReID-specific domain adaptation methods, DART$^3$ requires no source data, architectural modifications, or retraining, and can be deployed in both fully black-box and hybrid settings. Empirical evaluations on multiple ReID benchmarks indicate that DART$^3$ and DART$^3$ LITE, a lightweight alternative to the approach, consistently outperforms state-of-the-art TTA baselines, making for a viable option to online learning to mitigate the adverse effects of camera bias.",http://arxiv.org/abs/2505.18337v1,IT,Quantitative
Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling,"Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines.",http://arxiv.org/abs/2505.17384v1,IT,Quantitative
On Multilingual Encoder Language Model Compression for Low-Resource Languages,"In this paper, we combine two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming for extremely compressing multilingual encoder-only language models for low-resource languages. Our novel approach systematically combines existing techniques and takes them to the extreme, reducing layer depth, feed-forward hidden size, and intermediate layer embedding size to create significantly smaller monolingual models while retaining essential language-specific knowledge. We achieve compression rates of up to 92% with only a marginal performance drop of 2-10% in four downstream tasks, including sentiment analysis, topic classification, named entity recognition, and part-of-speech tagging, across three low-resource languages. Notably, the performance degradation correlates with the amount of language-specific data in the teacher model, with larger datasets resulting in smaller performance losses. Additionally, we conduct extensive ablation studies to identify best practices for multilingual model compression using these techniques.",http://arxiv.org/abs/2505.16956v1,IT,Quantitative
Next Token Perception Score: Analytical Assessment of your LLM Perception Skills,"Autoregressive pretraining has become the de facto paradigm for learning general-purpose representations in large language models (LLMs). However, linear probe performance across downstream perception tasks shows substantial variability, suggesting that features optimized for next-token prediction do not consistently transfer well to downstream perception tasks. We demonstrate that representations learned via autoregression capture features that may lie outside the subspaces most informative for perception. To quantify the (mis)alignment between autoregressive pretraining and downstream perception, we introduce the Next Token Perception Score (NTPS)-a score derived under a linear setting that measures the overlap between autoregressive and perception feature subspaces. This metric can be easily computed in closed form from pretrained representations and labeled data, and is proven to both upper- and lower-bound the excess loss. Empirically, we show that NTPS correlates strongly with linear probe accuracy across 12 diverse NLP datasets and eight pretrained models ranging from 270M to 8B parameters, confirming its utility as a measure of alignment. Furthermore, we show that NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA aligning representations to perception tasks enhances subspace overlap and thus improves downstream performance. More importantly, we find that NTPS reliably predicts the additional accuracy gains attained by LoRA finetuning thereby providing a lightweight prescreening tool for LoRA adaptation. Our results offer both theoretical insights and practical tools for analytically assessing LLM perception skills.",http://arxiv.org/abs/2505.17169v1,IT,Quantitative
Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability,"As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.",http://arxiv.org/abs/2505.16789v1,IT,Quantitative
Single Domain Generalization for Few-Shot Counting via Universal Representation Matching,"Few-shot counting estimates the number of target objects in an image using only a few annotated exemplars. However, domain shift severely hinders existing methods to generalize to unseen scenarios. This falls into the realm of single domain generalization that remains unexplored in few-shot counting. To solve this problem, we begin by analyzing the main limitations of current methods, which typically follow a standard pipeline that extract the object prototypes from exemplars and then match them with image feature to construct the correlation map. We argue that existing methods overlook the significance of learning highly generalized prototypes. Building on this insight, we propose the first single domain generalization few-shot counting model, Universal Representation Matching, termed URM. Our primary contribution is the discovery that incorporating universal vision-language representations distilled from a large scale pretrained vision-language model into the correlation construction process substantially improves robustness to domain shifts without compromising in domain performance. As a result, URM achieves state-of-the-art performance on both in domain and the newly introduced domain generalization setting.",http://arxiv.org/abs/2505.16778v1,IT,Quantitative
Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval,"Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in geographic information interpretation, disaster monitoring, and urban planning by establishing semantic associations between image and textual descriptions. Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language Pre-training (VLP) models typically adopt symmetric adapter structures for exploring cross-modal correlations. However, the strong discriminative nature of text modality may dominate the optimization process and inhibits image representation learning. The nonnegligible imbalanced cross-modal optimization remains a bottleneck to enhancing the model performance. To address this issue, this study proposes a Representation Discrepancy Bridging (RDB) method for the RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is designed to enable modality-specific optimization and improve feature alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text Semantic Adapter (TSA). VEA mines fine-grained image features by Differential Attention (DA) mechanism, while TSA identifies key textual semantics through Hierarchical Attention (HA) mechanism. On the other hand, this study extends the traditional single-task retrieval framework to a dual-task optimization framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves cross-modal alignment robustness through an adaptive weighted combination of cross-modal, classification, and exponential moving average consistency constraints. Experiments on RSICD and RSITMD datasets show that the proposed RDB method achieves a 6%-11% improvement in mR metrics compared to state-of-the-art PEFT methods and a 1.15%-2% improvement over the full fine-tuned GeoRSCLIP model.",http://arxiv.org/abs/2505.16756v1,IT,Quantitative
Holes in Latent Space: Topological Signatures Under Adversarial Influence,"Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.",http://arxiv.org/abs/2505.20435v1,IT,Quantitative
ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining,"Large language model pretraining is compute-intensive, yet many tokens contribute marginally to learning, resulting in inefficiency. We introduce Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that improves training efficiency and distributional robustness by performing online token-level batch selection. ESLM leverages per-token statistics (e.g., entropy or loss) and applies value-at-risk thresholding to retain only the most informative tokens per batch. This data-centric mechanism reshapes the training loss, prioritizing high-risk tokens and eliminating redundant gradient computation. We frame ESLM as a bilevel game: the model competes with a masking adversary that selects worst-case token subsets under a constrained thresholding rule. In the loss-based setting, ESLM recovers conditional value-at-risk loss minimization, providing a principled connection to distributionally robust optimization. We extend our approach to Ada-ESLM, which adaptively tunes the selection confidence during training. Experiments on GPT-2 pretraining show that ESLM significantly reduces training FLOPs while maintaining or improving both perplexity and downstream performance compared to baselines. Our approach also scales across model sizes, pretraining corpora, and integrates naturally with knowledge distillation.",http://arxiv.org/abs/2505.19893v1,IT,Quantitative
VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis,"Stock price prediction remains a complex and high-stakes task in financial analysis, traditionally addressed using statistical models or, more recently, language models. In this work, we introduce VISTA (Vision-Language Inference for Stock Time-series Analysis), a novel, training-free framework that leverages Vision-Language Models (VLMs) for multi-modal stock forecasting. VISTA prompts a VLM with both textual representations of historical stock prices and their corresponding line charts to predict future price values. By combining numerical and visual modalities in a zero-shot setting and using carefully designed chain-of-thought prompts, VISTA captures complementary patterns that unimodal approaches often miss. We benchmark VISTA against standard baselines, including ARIMA and text-only LLM-based prompting methods. Experimental results show that VISTA outperforms these baselines by up to 89.83%, demonstrating the effectiveness of multi-modal inference for stock time-series analysis and highlighting the potential of VLMs in financial forecasting tasks without requiring task-specific training.",http://arxiv.org/abs/2505.18570v1,IT,Quantitative
Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking,"Refractive errors are among the most common visual impairments globally, yet their diagnosis often relies on active user participation and clinical oversight. This study explores a passive method for estimating refractive power using two eye movement recording techniques: electrooculography (EOG) and video-based eye tracking. Using a publicly available dataset recorded under varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to classify refractive power from unimodal (EOG or eye tracking) and multimodal configuration. We assess performance in both subject-dependent and subject-independent settings to evaluate model personalization and generalizability across individuals. Results show that the multimodal model consistently outperforms unimodal models, achieving the highest average accuracy in both settings: 96.207\% in the subject-dependent scenario and 8.882\% in the subject-independent scenario. However, generalization remains limited, with classification accuracy only marginally above chance in the subject-independent evaluations. Statistical comparisons in the subject-dependent setting confirmed that the multimodal model significantly outperformed the EOG and eye-tracking models. However, no statistically significant differences were found in the subject-independent setting. Our findings demonstrate both the potential and current limitations of eye movement data-based refractive error estimation, contributing to the development of continuous, non-invasive screening methods using EOG signals and eye-tracking data.",http://arxiv.org/abs/2505.18538v1,IT,Quantitative
An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs,"Recent advances in Large Language Models (LLMs) have led to the widespread adoption of third-party inference services, raising critical privacy concerns. Existing methods of performing private third-party inference, such as Secure Multiparty Computation (SMPC), often rely on cryptographic methods. However, these methods are thousands of times slower than standard unencrypted inference, and fail to scale to large modern LLMs. Therefore, recent lines of work have explored the replacement of expensive encrypted nonlinear computations in SMPC with statistical obfuscation methods - in particular, revealing permuted hidden states to the third parties, with accompanying strong claims of the difficulty of reversal into the unpermuted states. In this work, we begin by introducing a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs. We then show that extensions of our attack are nearly perfectly effective in reversing permuted hidden states of LLMs, demonstrating the insecurity of three recently proposed privacy schemes. We further dissect the shortcomings of prior theoretical `proofs' of permuation security which allow our attack to succeed. Our findings highlight the importance of rigorous security analysis in privacy-preserving LLM inference.",http://arxiv.org/abs/2505.18332v1,IT,Quantitative
Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning,"This study presents a novel approach for decomposing urban water demand patterns using Skewed Gaussian Distributions (SGD) to derive behavioral insights and support operational planning. Hourly demand profiles contain critical information for both long-term infrastructure design and daily operations, influencing network pressures, water quality, energy consumption, and overall reliability. By breaking down each daily demand curve into a baseline component and distinct peak components, the proposed SGD method characterizes each peak with interpretable parameters, including peak amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby reconstructing the observed pattern and uncovering latent usage dynamics. This detailed peak-level decomposition enables both operational applications, e.g. anomaly and leakage detection, real-time demand management, and strategic analyses, e.g. identifying behavioral shifts, seasonal influences, or policy impacts on consumption patterns. Unlike traditional symmetric Gaussian or purely statistical time-series models, SGDs explicitly capture asymmetric peak shapes such as sharp morning surges followed by gradual declines, improving the fidelity of synthetic pattern generation and enhancing the detection of irregular consumption behavior. The method is demonstrated on several real-world datasets, showing that SGD outperforms symmetric Gaussian models in reconstruction accuracy, reducing root-mean-square error by over 50% on average, while maintaining physical interpretability. The SGD framework can also be used to construct synthetic demand scenarios by designing daily peak profiles with chosen characteristics. All implementation code is publicly available at: https://github.com/Relkayam/water-demand-decomposition-sgd",http://arxiv.org/abs/2505.18245v1,IT,Quantitative
"Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP","Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs. This study explores whether a Retrieval Augmented Generation (RAG) approach using only the most relevant text segments can match the performance of processing entire clinical notes with large context LLMs. We begin by splitting clinical documents into smaller chunks, converting them into vector embeddings, and storing these in a FAISS index. We then retrieve the top 4,000 words most pertinent to the classification query and feed these consolidated segments into an LLM. We evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication identification task. Metrics such as AUC ROC, precision, recall, and F1 showed no statistically significant differences between the RAG based approach and whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can significantly reduce token usage without sacrificing classification accuracy, providing a scalable and cost effective solution for analyzing lengthy clinical documents.",http://arxiv.org/abs/2505.20320v1,IT,Quantitative
"ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling","Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically designed to synthesize realistic, event triggered, and symbolic sensor sequences for ambient assistive environments. ADLGen integrates a decoder only Transformer with sign based symbolic temporal encoding, and a context and layout aware sampling mechanism to guide generation toward semantically rich and physically plausible sensor event sequences. To enhance semantic fidelity and correct structural inconsistencies, we further incorporate a large language model into an automatic generate evaluate refine loop, which verifies logical, behavioral, and temporal coherence and generates correction rules without manual intervention or environment specific tuning. Through comprehensive experiments with novel evaluation metrics, ADLGen is shown to outperform baseline generators in statistical fidelity, semantic richness, and downstream activity recognition, offering a scalable and privacy-preserving solution for ADL data synthesis.",http://arxiv.org/abs/2505.17987v1,IT,Quantitative
BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models,"The advent of universal time series forecasting models has revolutionized zero-shot forecasting across diverse domains, yet the critical role of data diversity in training these models remains underexplored. Existing large-scale time series datasets often suffer from inherent biases and imbalanced distributions, leading to suboptimal model performance and generalization. To address this gap, we introduce BLAST, a novel pre-training corpus designed to enhance data diversity through a balanced sampling strategy. First, BLAST incorporates 321 billion observations from publicly available datasets and employs a comprehensive suite of statistical metrics to characterize time series patterns. Then, to facilitate pattern-oriented sampling, the data is implicitly clustered using grid-based partitioning. Furthermore, by integrating grid sampling and grid mixup techniques, BLAST ensures a balanced and representative coverage of diverse patterns. Experimental results demonstrate that models pre-trained on BLAST achieve state-of-the-art performance with a fraction of the computational resources and training tokens required by existing methods. Our findings highlight the pivotal role of data diversity in improving both training efficiency and model performance for the universal forecasting task.",http://arxiv.org/abs/2505.17871v2,IT,Mixed
C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models,"Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.",http://arxiv.org/abs/2505.17773v1,IT,Quantitative
Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning,"Objective: Perceptual voice quality assessment plays a critical role in diagnosing and monitoring voice disorders by providing standardized evaluation of vocal function. Traditionally, this process relies on expert raters utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are inherently subjective and susceptible to inter-rater variability, motivating the need for automated and objective assessment methods. Methods: We propose Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to capture high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we present VOQANet+, which integrates handcrafted acoustic features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings. Results: Sentence-based input yields stronger performance than vowel-based input, especially at the patient level. VOQANet consistently outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even better and maintains robustness under noisy conditions. Conclusion: Combining SFM embeddings with domain-informed acoustic features improves interpretability and resilience. Significance: VOQANet+ shows strong potential for deployment in real-world and telehealth settings, addressing the limitations of subjective perceptual assessments with an interpretable and noise-resilient solution.",http://arxiv.org/abs/2505.21356v1,IT,Quantitative
Towards Fully FP8 GEMM LLM Training at Scale,"Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.",http://arxiv.org/abs/2505.20524v1,IT,Quantitative
GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation,"Fine-tuning for large language models (LLMs) typically requires substantial amounts of high-quality supervised data, which is both costly and labor-intensive to acquire. While synthetic data generation has emerged as a promising solution, existing approaches frequently suffer from factual inaccuracies, insufficient long-tail coverage, simplistic knowledge structures, and homogenized outputs. To address these challenges, we introduce GraphGen, a knowledge graph-guided framework designed for three key question-answering (QA) scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by constructing a fine-grained knowledge graph from the source text. It then identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data. Experimental results on knowledge-intensive tasks under closed-book settings demonstrate that GraphGen outperforms conventional synthetic data methods, offering a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning. The code and data are publicly available at https://github.com/open-sciencelab/GraphGen.",http://arxiv.org/abs/2505.20416v1,IT,Quantitative
Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?,"To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. Our metric enables future works towards this universal form of LLM uncertainties.",http://arxiv.org/abs/2505.20295v1,IT,Mixed
How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation,"Large language models (LLMs) have demonstrated strong performance in general-purpose machine translation, but their effectiveness in complex, domain-sensitive translation tasks remains underexplored. Recent advancements in Large Reasoning Models (LRMs), raise the question of whether structured reasoning can enhance translation quality across diverse domains. In this work, we compare the performance of LRMs with traditional LLMs across 15 representative domains and four translation directions. Our evaluation considers various factors, including task difficulty, input length, and terminology density. We use a combination of automatic metrics and an enhanced MQM-based evaluation hierarchy to assess translation quality. Our findings show that LRMs consistently outperform traditional LLMs in semantically complex domains, especially in long-text and high-difficulty translation scenarios. Moreover, domain-adaptive prompting strategies further improve performance by better leveraging the reasoning capabilities of LRMs. These results highlight the potential of structured reasoning in MDMT tasks and provide valuable insights for optimizing translation systems in domain-sensitive contexts.",http://arxiv.org/abs/2505.19987v1,IT,Quantitative
DocMEdit: Towards Document-Level Model Editing,"Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document-level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce \benchmarkname, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.",http://arxiv.org/abs/2505.19572v1,IT,Quantitative
Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models,"Large proprietary language models exhibit strong causal reasoning abilities that smaller open-source models struggle to replicate. We introduce a novel framework for distilling causal explanations that transfers causal reasoning skills from a powerful teacher model to a compact open-source model. The key idea is to train the smaller model to develop causal reasoning abilities by generating structured cause-and-effect explanations consistent with those of the teacher model. To evaluate the quality of the student-generated explanations, we introduce a new metric called Causal Explanation Coherence (CEC) to assess the structural and logical consistency of causal reasoning. This metric uses sentence-level semantic alignment to measure how well each part of the generated explanation corresponds to the teacher's reference, capturing both faithfulness and coverage of the underlying causal chain. Our framework and the CEC metric provide a principled foundation for training smaller models to perform robust causal reasoning and for systematically assessing the coherence of explanations in language model outputs.",http://arxiv.org/abs/2505.19511v1,IT,Mixed
CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation,"Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.",http://arxiv.org/abs/2505.19502v1,IT,Quantitative
Vision Transformers with Self-Distilled Registers,"Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly ""absorb"" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.",http://arxiv.org/abs/2505.21501v1,IT,Quantitative
Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion,"Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver up to a 34x end-to-end speedup without compromising accuracy. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains.",http://arxiv.org/abs/2505.21467v1,IT,Mixed
Learning Annotation Consensus for Continuous Emotion Recognition,"In affective computing, datasets often contain multiple annotations from different annotators, which may lack full agreement. Typically, these annotations are merged into a single gold standard label, potentially losing valuable inter-rater variability. We propose a multi-annotator training approach for continuous emotion recognition (CER) that seeks a consensus across all annotators rather than relying on a single reference label. Our method employs a consensus network to aggregate annotations into a unified representation, guiding the main arousal-valence predictor to better reflect collective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach outperforms traditional methods that unify annotations into a single label. This underscores the benefits of fully leveraging multi-annotator data in emotion recognition and highlights its applicability across various fields where annotations are abundant yet inconsistent.",http://arxiv.org/abs/2505.21196v1,IT,Quantitative
Model as Loss: A Self-Consistent Training Paradigm,"Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training. The Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.",http://arxiv.org/abs/2505.21156v1,IT,Quantitative
Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations,"Retrieving semantically similar but visually distinct contents has been a critical capability in visual search systems. In this work, we aim to tackle this problem with Visual Product Graph (VPG), leveraging high-performance infrastructure for storage and state-of-the-art computer vision models for image understanding. VPG is built to be an online real-time retrieval system that enables navigation from individual products to composite scenes containing those products, along with complementary recommendations. Our system not only offers contextual insights by showcasing how products can be styled in a context, but also provides recommendations for complementary products drawn from these inspirations. We discuss the essential components for building the Visual Product Graph, along with the core computer vision model improvements across object detection, foundational visual embeddings, and other visual signals. Our system achieves a 78.8% extremely similar@1 in end-to-end human relevance evaluations, and a 6% module engagement rate. The ""Ways to Style It"" module, powered by the Visual Product Graph technology, is deployed in production at Pinterest.",http://arxiv.org/abs/2505.21454v1,IT,Quantitative
Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs,"Today's cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives.",http://arxiv.org/abs/2505.21419v1,IT,Quantitative
AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping,"Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at urlhttps://github.com/flyakon/AgriFM.",http://arxiv.org/abs/2505.21357v1,IT,Quantitative
Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages,"What happens when an English Fairytaler is fine-tuned on Indian languages? We evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare: (i) training from scratch, (ii) fine-tuning English F5 on Indian data, and (iii) fine-tuning on both Indian and English data to prevent forgetting. Fine-tuning with only Indian data proves most effective and the resultant IN-F5 is a near-human polyglot; that enables speakers of one language (e.g., Odia) to fluently speak in another (e.g., Hindi). Our results show English pretraining aids low-resource TTS in reaching human parity. To aid progress in other low-resource languages, we study data-constrained setups and arrive at a compute optimal strategy. Finally, we show IN-F5 can synthesize unseen languages like Bhojpuri and Tulu using a human-in-the-loop approach for zero-resource TTS via synthetic data generation.",http://arxiv.org/abs/2505.20693v1,IT,Mixed
Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design,"In AI-empowered poster design, content-aware layout generation is crucial for the on-image arrangement of visual-textual elements, e.g., logo, text, and underlay. To perceive the background images, existing work demanded a high parameter count that far exceeds the size of available training data, which has impeded the model's real-time performance and generalization ability. To address these challenges, we proposed a patch-level data summarization and augmentation approach, vividly named Scan-and-Print. Specifically, the scan procedure selects only the patches suitable for placing element vertices to perform fine-grained perception efficiently. Then, the print procedure mixes up the patches and vertices across two image-layout pairs to synthesize over 100% new samples in each epoch while preserving their plausibility. Besides, to facilitate the vertex-level operations, a vertex-based layout representation is introduced. Extensive experimental results on widely used benchmarks demonstrated that Scan-and-Print can generate visually appealing layouts with state-of-the-art quality while dramatically reducing computational bottleneck by 95.2%.",http://arxiv.org/abs/2505.20649v1,IT,Quantitative
FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement,"The integration of large language models (LLMs) with function calling has emerged as a crucial capability for enhancing their practical utility in real-world applications. However, effectively combining reasoning processes with accurate function execution remains a significant challenge. Traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls, leading to suboptimal performance. To address these limitations, we introduce FunReason, a novel framework that enhances LLMs' function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason leverages LLMs' natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. The SRML approach dynamically balances the contribution of reasoning processes and function call accuracy during training, addressing the inherent trade-off between these two critical aspects. FunReason achieves performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning. FunReason provides a comprehensive solution for enhancing LLMs' function calling capabilities by introducing a balanced training methodology and a data refinement pipeline. For code and dataset, please refer to our repository at GitHub https://github.com/BingguangHao/FunReason",http://arxiv.org/abs/2505.20192v1,IT,Quantitative
Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling,"Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.",http://arxiv.org/abs/2505.19609v1,IT,Quantitative
Analyzing Biases in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework,"We present a critical discourse analysis of the 2024 U.S. presidential debates, examining Donald Trump's rhetorical strategies in his interactions with Joe Biden and Kamala Harris. We introduce a novel annotation framework, BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically extends the DAMSL framework to capture bias driven and adversarial discourse features in political communication. BEADS includes a domain and language agnostic set of tags that model ideological framing, emotional appeals, and confrontational tactics. Our methodology compares detailed human annotation with zero shot ChatGPT assisted tagging on verified transcripts from the Trump and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our analysis shows that Trump consistently dominated in key categories: Challenge and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias, and Perceived Dismissiveness. These findings underscore his use of emotionally charged and adversarial rhetoric to control the narrative and influence audience perception. In this work, we establish BEADS as a scalable and reproducible framework for critical discourse analysis across languages, domains, and political contexts.",http://arxiv.org/abs/2505.19515v2,IT,Qualitative
Communication-Efficient Multi-Device Inference Acceleration for Transformer Models,"Transformer models power many AI applications but suffer from high inference latency, limiting their use in real-time settings. Multi-device inference can reduce latency by parallelizing computation. Yet, existing methods require high inter-device bandwidth, making them impractical for bandwidth-constrained environments. We propose ASTRA, a communication-efficient framework that accelerates Transformer inference through a novel integration of sequence parallelism and a Mixed-Precision Attention mechanism designed to minimize inter-device communication. ASTRA compresses non-local token embeddings via vector quantization and preserves task accuracy through two optimizations, Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X speedups over single-device inference and up to 15.25X speedups over state-of-the-art multi-device inferences, while operating under bandwidths as low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.",http://arxiv.org/abs/2505.19342v1,IT,Quantitative
MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search,"Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.",http://arxiv.org/abs/2505.19209v1,IT,Quantitative
Interpretable Graph Learning Over Sets of Temporally-Sparse Data,"Real-world medical data often includes measurements from multiple signals that are collected at irregular and asynchronous time intervals. For example, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling of different attributes occur in other domains, such as monitoring of large systems using event log files or the spread of fake news on social networks. Effectively learning from such data requires models that can handle sets of temporally sparse and heterogeneous signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design model for learning over irregular sets of temporal signals. Our method achieves state-of-the-art performance in real-world medical tasks, including a 4-point increase in the AUROC score of in-hospital mortality prediction, compared to existing methods. We further showcase GMAN's flexibility by applying it to a fake news detection task. We demonstrate how its interpretability capabilities, including node-level, graph-level, and subset-level importance, allow for transition phases detection and gaining medical insights with real-world high-stakes implications. Finally, we provide theoretical insights on GMAN expressive power.",http://arxiv.org/abs/2505.19193v1,IT,Quantitative
Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing,"While posit format offers superior dynamic range and accuracy for transprecision computing, its adoption in RISC-V processors is hindered by the lack of a unified solution for lightweight, precision-scalable, and IEEE-754 arithmetic compatible hardware implementation. To address these challenges, we enhance RISC-V processors by 1) integrating dedicated posit codecs into the original FPU for lightweight implementation, 2) incorporating multi/mixed-precision support with dynamic exponent size for precision-scalability, and 3) reusing and customizing ISA extensions for IEEE-754 compatible posit operations. Our comprehensive evaluation spans the modified FPU, RISC-V core, and SoC levels. It demonstrates that our implementation achieves 47.9% LUTs and 57.4% FFs reduction compared to state-of-the-art posit-enabled RISC-V processors, while achieving up to 2.54$\times$ throughput improvement in various GEMM kernels.",http://arxiv.org/abs/2505.19096v1,IT,Quantitative
Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings,"Selective state-space models have achieved great success in long-sequence modeling. However, their capacity for language representation, especially in complex hierarchical reasoning tasks, remains underexplored. Most large language models rely on flat Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this limitation, we propose Hierarchical Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved nature of hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via cosine and sine-based mapping) with ""learnable"" curvature, optimized with a combined hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning. This makes it well-suited for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. We evaluated our HiM with four linguistic and medical datasets for mixed-hop prediction and multi-hop inference tasks. Experimental results demonstrated that: 1) Both HiM models effectively capture hierarchical relationships for four ontological datasets, surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic distinctions with higher h-norms, while HiM-Lorentz provides more stable, compact, and hierarchy-preserving embeddings favoring robustness over detail.",http://arxiv.org/abs/2505.18973v2,IT,Quantitative
The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation,"Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.",http://arxiv.org/abs/2505.18759v1,IT,Quantitative
MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images,"Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled image classification and prevalence diversity (abundant vs. sparse) in long-tailed image classification. Similarly, medical images in universal lesion detection (ULD) exhibit substantial variations in image quality, encompassing attributes such as clarity and label correctness. How to effectively leverage training images with diverse qualities becomes a problem in learning deep models. Conventional training mechanisms, such as self-paced curriculum learning (SCL) and online hard example mining (OHEM), relieve this problem by reweighting images with high loss values. Despite their success, these methods still confront two challenges: (i) the loss-based measure of sample hardness is imprecise, preventing optimum handling of different cases, and (ii) there exists under-utilization in SCL or over-utilization OHEM with the identified hard samples. To address these issues, this paper revisits the minibatch sampling (MBS), a technique widely used in deep network training but largely unexplored concerning the handling of diverse-quality training samples. We discover that the samples within a minibatch influence each other during training; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS) method to optimize the use of training samples with diverse qualities. MoMBS introduces a measure that takes both loss and uncertainty into account to surpass a sole reliance on loss and allows for a more refined categorization of high-loss samples by distinguishing them as either poorly labeled and under represented or well represented and overfitted. We prioritize under represented samples as the main gradient contributors in a minibatch and keep them from the negative influences of poorly labeled or overfitted samples with a mixed-order minibatch sampling design.",http://arxiv.org/abs/2505.18741v1,IT,Mixed
ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery,"Soil organic carbon (SOC) is a critical indicator of soil health, but its accurate estimation from satellite imagery is hindered in vegetated regions due to spectral contamination from plant cover, which obscures soil reflectance and reduces model reliability. This study proposes the Reflectance Transformation Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework designed to reconstruct accurate bare soil reflectance from vegetated soil satellite observations. By learning the spectral transformation between vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and corresponding Landsat 8 imagery, we trained multiple learning-based models on both original and ReflectGAN-reconstructed reflectance inputs. Models trained on ReflectGAN outputs consistently outperformed those using existing vegetation correction methods. For example, the best-performing model (RF) achieved an $R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\% reduction in RMSE, and a 43\% improvement in RPD compared to the best existing method (PMM-SU). The performance of the models with ReflectGAN is also better compared to their counterparts when applied to another dataset, i.e., Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to improve SOC estimation accuracy in vegetated landscapes, supporting more reliable soil monitoring.",http://arxiv.org/abs/2505.18546v1,IT,Mixed
Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications,"Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.",http://arxiv.org/abs/2505.18488v1,IT,Quantitative
NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities,"Enhancing the linguistic capabilities of Large Language Models (LLMs) to include low-resource languages is a critical research area. Current research directions predominantly rely on synthetic data generated by translating English corpora, which, while demonstrating promising linguistic understanding and translation abilities, often results in models aligned with source language culture. These models frequently fail to represent the cultural heritage and values of local communities. This work proposes a methodology to create both synthetic and retrieval-based pre-training data tailored to a specific community, considering its (i) language, (ii) cultural heritage, and (iii) cultural values. We demonstrate our methodology using Egyptian and Moroccan dialects as testbeds, chosen for their linguistic and cultural richness and current underrepresentation in LLMs. As a proof-of-concept, we develop NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities, incorporating their language, cultural heritage, and values. Our results on various understanding, translation, and cultural and values alignment benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar size and performs on par with larger models. We share our methods, data, and models with the community to promote the inclusion and coverage of more diverse communities in LLM development.",http://arxiv.org/abs/2505.18383v1,IT,Quantitative
RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification,"Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactX and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.",http://arxiv.org/abs/2505.18380v1,IT,Quantitative
RetroChat: Designing for the Preservation of Past Digital Experiences,"Rapid changes in social networks have transformed the way people express themselves, turning past neologisms, values, and mindsets embedded in these expressions into online heritage. How can we preserve these expressions as cultural heritage? Instead of traditional archiving methods for static material, we designed an interactive and experiential form of archiving for Chinese social networks. Using dialogue data from 2000-2010 on early Chinese social media, we developed a GPT-driven agent within a retro chat interface, emulating the language and expression style of the period for interaction. Results from a qualitative study with 18 participants show that the design captures the past chatting experience and evokes memory flashbacks and nostalgia feeling through conversation. Participants, particularly those familiar with the era, adapted their language to match the agent's chatting style. This study explores how the design of preservation methods for digital experiences can be informed by experiential representations supported by generative tools.",http://arxiv.org/abs/2505.17208v1,IT,Qualitative
Two Empirical Studies on Audiovisual Semiotics of Uncertainty,"There exists limited theoretical guidance on integrating visualization and sonification. In this paper, we address this gap by investigating audiovisual semiotics for uncertainty representation: joining uncertainty visualization and sonification to combine audiovisual channels for enhancing users' perception of uncertainty. We conducted two preregistered crowd-sourced user studies. First, we assessed suitable audio/visual pairs. Then, we investigated audiovisual mappings of uncertainty. Here, we use probability as it is an easily communicated aspect of uncertainty. We analyzed the participants' preferences and reaction times in both user studies. Additionally, we explored the strategies employed by participants through qualitative analysis. Our results reveal audiovisual mappings that lead to particularly strong preferences and low reaction times. Furthermore, we found that preferred audio/visual pairs are not necessarily suitable audiovisual mappings of uncertainty. For example, while pitch paired with brightness was preferred as a pair, it was not well suited as a mapping for uncertainty. We recommend audiovisual mappings of uncertainty that lead to low reaction times and high preferences in both user studies. This paper presents guidelines to anyone seeking to employ audiovisual representations for uncertainty, contributing to enhancing the perception of uncertainty.",http://arxiv.org/abs/2505.14379v1,IT,Qualitative
"Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI","Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment, potentially leading to delayed diagnoses and inadequate treatments. This study evaluates the capabilities and limitations of Large Language Models (LLMs) in generating accurate, safe, and accessible cancer-related information to support patient understanding. We evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Our approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that general-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness. Our findings indicate a duality between domain-specific knowledge and safety in health communications. The results highlight the need for intentional model design with targeted improvements, particularly in mitigating harm and bias, and improving safety and affectiveness. This study provides a comprehensive evaluation of LLMs for cancer communication, offering critical insights for improving AI-generated health content and informing future development of accurate, safe, and accessible digital health tools.",http://arxiv.org/abs/2505.10472v1,IT,Mixed
Will Your Next Pair Programming Partner Be Human? An Empirical Evaluation of Generative AI as a Collaborative Teammate in a Semester-Long Classroom Setting,"Generative AI (GenAI), especially Large Language Models (LLMs), is rapidly reshaping both programming workflows and computer science education. Many programmers now incorporate GenAI tools into their workflows, including for collaborative coding tasks such as pair programming. While prior research has demonstrated the benefits of traditional pair programming and begun to explore GenAI-assisted coding, the role of LLM-based tools as collaborators in pair programming remains underexamined. In this work, we conducted a mixed-methods study with 39 undergraduate students to examine how GenAI influences collaboration, learning, and performance in pair programming. Specifically, students completed six in-class assignments under three conditions: Traditional Pair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming with GenAI (SAI). They used both LLM-based inline completion tools (e.g., GitHub Copilot) and LLM-based conversational tools (e.g., ChatGPT). Our results show that students in PAI achieved the highest assignment scores, whereas those in SAI attained the lowest. Additionally, students' attitudes toward LLMs' programming capabilities improved significantly after collaborating with LLM-based tools, and preferences were largely shaped by the perceived usefulness for completing assignments and learning programming skills, as well as the quality of collaboration. Our qualitative findings further reveal that while students appreciated LLM-based tools as valuable pair programming partners, they also identified limitations and had different expectations compared to human teammates. Our study provides one of the first empirical evaluations of GenAI as a pair programming collaborator through a comparison of three conditions (PP, PAI, and SAI). We also discuss the design implications and pedagogical considerations for future GenAI-assisted pair programming approaches.",http://arxiv.org/abs/2505.08119v1,IT,Mixed
A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive Fiction to Reduce Stigma Toward Dirty Work,"Occupations referred to as ""dirty work"" often face entrenched social stigma, which adversely affects the mental health of workers in these fields and impedes occupational equity. In this study, we propose a novel Interactive Fiction (IF) framework powered by Large Language Models (LLMs) to encourage perspective-taking and reduce biases against these stigmatized yet essential roles. Through an experiment with participants (n = 100) across four such occupations, we observed a significant increase in participants' understanding of these occupations, as well as a high level of empathy and a strong sense of connection to individuals in these roles. Additionally, qualitative interviews with participants (n = 15) revealed that the LLM-based perspective-taking IF enhanced immersion, deepened emotional resonance and empathy toward ""dirty work,"" and allowed participants to experience a sense of professional fulfillment in these occupations. However, participants also highlighted ongoing challenges, such as limited contextual details generated by the LLM and the unintentional reinforcement of existing stereotypes. Overall, our findings underscore that an LLM-based perspective-taking IF framework offers a promising and scalable strategy for mitigating stigma and promoting social equity in marginalized professions.",http://arxiv.org/abs/2505.05786v1,IT,Mixed
Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models,"This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.",http://arxiv.org/abs/2505.04650v1,IT,Mixed
Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A Randomized Study with 90+ Novice Counselors,"Training more counselors, from clinical students to peer supporters, can help meet the demand for accessible mental health support; however, current training approaches remain resource-intensive and difficult to scale effectively. Large Language Models (LLMs) offer promising solutions for growing counseling skills training through simulated practice and automated feedback. Despite successes in aligning LLMs with expert-counselor annotations, we do not know whether LLM-based counseling training tools -- such as AI patients that simulate real-world challenges and generative AI feedback with suggested alternatives and rationales -- actually lead to improvements in novice counselor skill development. We develop CARE, an LLM-simulated practice and feedback system, and randomize 94 novice counselors to practice using an AI patient, either alone or with AI feedback, measuring changes in their behavioral performance, self-assessments, and qualitative learning takeaways. Our results show the practice-and-feedback group improved in their use of reflections and questions (d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI patient alone did not show improvements, and in the case of empathy, actually had worse uses across time (d=$-$0.52, p=0.001) and when compared against the practice-and-feedback group (d=0.72, p=0.001). Participants' qualitative self-reflections revealed key differences: the practice-and-feedback group adopted a client-centered approach involving listening to and validating feelings, while the practice-alone group remained solution-oriented but delayed offering suggestions until gathering more information. Overall, these results suggest that LLM-based training systems can promote effective skill development, but that combining both simulated practice and structured feedback is critical.",http://arxiv.org/abs/2505.02428v1,IT,Qualitative
Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models,"Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. We present the Data Therapist, a web-based tool that helps domain experts externalize this implicit knowledge through a mixed-initiative process combining iterative Q&A with interactive annotation. Powered by a large language model, the system analyzes user-supplied datasets, prompts users with targeted questions, and allows annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. We evaluated the tool in a qualitative study involving expert pairs from Molecular Biology, Accounting, Political Science, and Usable Security. The study revealed recurring patterns in how experts reason about their data and highlights areas where AI support can improve visualization design.",http://arxiv.org/abs/2505.00455v2,IT,Mixed
"Crafting a Personal Journaling Practice: Negotiating Ecosystems of Materials, Personal Context, and Community in Analog Journaling","Analog journaling has grown in popularity, with journaling on paper encompassing a range of motivations, styles, and practices including planning, habit-tracking, and reflecting. Journalers develop strong personal preferences around the tools they use, the ideas they capture, and the layout in which they represent their ideas and memories. Understanding how analog journaling practices are individually shaped and crafted over time is critical to supporting the varied benefits associated with journaling, including improved mental health and positive support for identity development. To understand this development, we qualitatively analyzed publicly-shared journaling content from YouTube and Instagram and interviewed 11 journalers. We report on our identification of the journaling ecosystem in which journaling practices are shaped by materials, personal context, and communities, sharing how this ecosystem plays a role in the practices and identities of journalers as they customize their journaling routine to best suit their personal goals. Using these insights, we discuss design opportunities for how future tools can better align with and reflect the rich affordances and practices of journaling on paper.",http://arxiv.org/abs/2504.19767v1,IT,Qualitative
Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment,"Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits.",http://arxiv.org/abs/2504.17204v1,IT,Qualitative
Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges,"Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing ""ground truth"", thereby reducing system hallucinations. A crucial factor in RAG evaluation is ""support"", whether the information in the cited documents supports the answer. To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate that for 56% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72% in the manual with post-editing condition. Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.",http://arxiv.org/abs/2504.15205v1,IT,Qualitative
Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers,"This paper explores the design of a propaganda detection tool using Large Language Models (LLMs). Acknowledging the inherent biases in AI models, especially in political contexts, we investigate how these biases might be leveraged to enhance critical thinking in news consumption. Countering the typical view of AI biases as detrimental, our research proposes strategies of user choice and personalization in response to a user's political stance, applying psychological concepts of confirmation bias and cognitive dissonance. We present findings from a qualitative user study, offering insights and design recommendations (bias awareness, personalization and choice, and gradual introduction of diverse perspectives) for AI tools in propaganda detection.",http://arxiv.org/abs/2504.14522v1,IT,Qualitative
Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study,"High-stakes domains like cyber operations need responsible and trustworthy AI methods. While large language models (LLMs) are becoming increasingly popular in these domains, they still suffer from hallucinations. This research paper provides learning outcomes from a case study with LinkQ, an open-source natural language interface that was developed to combat hallucinations by forcing an LLM to query a knowledge graph (KG) for ground-truth data during question-answering (QA). We conduct a quantitative evaluation of LinkQ using a well-known KGQA dataset, showing that the system outperforms GPT-4 but still struggles with certain question categories - suggesting that alternative query construction strategies will need to be investigated in future LLM querying systems. We discuss a qualitative study of LinkQ with two domain experts using a real-world cybersecurity KG, outlining these experts' feedback, suggestions, perceived limitations, and future opportunities for systems like LinkQ.",http://arxiv.org/abs/2504.12422v1,IT,Mixed
Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence,"Vaccine hesitancy and misinformation are significant barriers to achieving widespread vaccination coverage. Smaller public health departments may lack the expertise or resources to craft effective vaccine messaging. This paper explores the potential of ChatGPT-augmented messaging to promote confidence in vaccination uptake. We conducted a survey in which participants chose between pairs of vaccination messages and assessed which was more persuasive and to what extent. In each pair, one message was the original, and the other was augmented by ChatGPT. At the end of the survey, participants were informed that half of the messages had been generated by ChatGPT. They were then asked to provide both quantitative and qualitative responses regarding how knowledge of a message's ChatGPT origin affected their impressions. Overall, ChatGPT-augmented messages were rated slightly higher than the original messages. These messages generally scored better when they were longer. Respondents did not express major concerns about ChatGPT-generated content, nor was there a significant relationship between participants' views on ChatGPT and their message ratings. Notably, there was a correlation between whether a message appeared first or second in a pair and its score. These results point to the potential of ChatGPT to enhance vaccine messaging, suggesting a promising direction for future research on human-AI collaboration in public health communication.",http://arxiv.org/abs/2504.09857v1,IT,Mixed
Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries,"The ubiquity and widespread use of digital and online technologies have transformed mental health support, with online mental health communities (OMHCs) providing safe spaces for peer support. More recently, generative AI and large language models (LLMs) have introduced new possibilities for scalable, around-the-clock mental health assistance that could potentially augment and supplement the capabilities of OMHCs. Although genAI shows promise in delivering immediate and personalized responses, their effectiveness in replicating the nuanced, experience-based support of human peers remains an open question. In this study, we harnessed 24,114 posts and 138,758 online community (OC) responses from 55 OMHCs on Reddit. We prompted several state-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts, and compared their (AI) responses to human-written (OC) responses based on a variety of linguistic measures across psycholinguistics and lexico-semantics. Our findings revealed that AI responses are more verbose, readable, and analytically structured, but lack linguistic diversity and personal narratives inherent in human-human interactions. Through a qualitative examination, we found validation as well as complementary insights into the nature of AI responses, such as its neutrality of stance and the absence of seeking back-and-forth clarifications. We discuss the ethical and practical implications of integrating generative AI into OMHCs, advocating for frameworks that balance AI's scalability and timeliness with the irreplaceable authenticity, social interactiveness, and expertise of human connections that form the ethos of online support communities.",http://arxiv.org/abs/2504.09271v1,IT,Qualitative
Connecting Feedback to Choice: Understanding Educator Preferences in GenAI vs. Human-Created Lesson Plans in K-12 Education -- A Comparative Analysis,"As generative AI (GenAI) models are increasingly explored for educational applications, understanding educator preferences for AI-generated lesson plans is critical for their effective integration into K-12 instruction. This exploratory study compares lesson plans authored by human curriculum designers, a fine-tuned LLaMA-2-13b model trained on K-12 content, and a customized GPT-4 model to evaluate their pedagogical quality across multiple instructional measures: warm-up activities, main tasks, cool-down activities, and overall quality. Using a large-scale preference study with K-12 math educators, we examine how preferences vary across grade levels and instructional components. We employ both qualitative and quantitative analyses. The raw preference results indicate that human-authored lesson plans are generally favored, particularly for elementary education, where educators emphasize student engagement, scaffolding, and collaborative learning. However, AI-generated models demonstrate increasing competitiveness in cool-down tasks and structured learning activities, particularly in high school settings. Beyond quantitative results, we conduct thematic analysis using LDA and manual coding to identify key factors influencing educator preferences. Educators value human-authored plans for their nuanced differentiation, real-world contextualization, and student discourse facilitation. Meanwhile, AI-generated lesson plans are often praised for their structure and adaptability for specific instructional tasks. Findings suggest a human-AI collaborative approach to lesson planning, where GenAI can serve as an assistive tool rather than a replacement for educator expertise in lesson planning. This study contributes to the growing discourse on responsible AI integration in education, highlighting both opportunities and challenges in leveraging GenAI for curriculum development.",http://arxiv.org/abs/2504.05449v1,IT,Mixed
Frontier AI's Impact on the Cybersecurity Landscape,"As frontier AI advances rapidly, understanding its impact on cybersecurity and inherent risks is essential to ensuring safe AI evolution (e.g., guiding risk mitigation and informing policymakers). While some studies review AI applications in cybersecurity, none of them comprehensively discuss AI's future impacts or provide concrete recommendations for navigating its safe and secure usage. This paper presents an in-depth analysis of frontier AI's impact on cybersecurity and establishes a systematic framework for risk assessment and mitigation. To this end, we first define and categorize the marginal risks of frontier AI in cybersecurity and then systemically analyze the current and future impacts of frontier AI in cybersecurity, qualitatively and quantitatively. We also discuss why frontier AI likely benefits attackers more than defenders in the short term from equivalence classes, asymmetry, and economic impact. Next, we explore frontier AI's impact on future software system development, including enabling complex hybrid systems while introducing new risks. Based on our findings, we provide security recommendations, including constructing fine-grained benchmarks for risk assessment, designing AI agents for defenses, building security mechanisms and provable defenses for hybrid systems, enhancing pre-deployment security testing and transparency, and strengthening defenses for users. Finally, we present long-term research questions essential for understanding AI's future impacts and unleashing its defensive capabilities.",http://arxiv.org/abs/2504.05408v2,IT,Mixed
Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation,"Our study introduces an automated system leveraging large language models (LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving effective praise, 2. reacting to errors, 3. determining what students know, 4. helping students manage inequity, and 5. responding to negative self-talk. Using a public dataset from the Teacher-Student Chatroom Corpus, our system classifies each tutoring strategy as either being employed as desired or undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use of these strategies and analyze tutoring dialogues. The results show that for the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to 0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is effective at excluding incorrect classifications but struggles to consistently identify the correct strategy. The strategy \textit{helping students manage inequity} showed the highest performance with a TNR of 0.738 and Recall of 0.432. The study highlights the potential of LLMs in tutoring strategy analysis and outlines directions for future improvements, including incorporating more advanced models for more nuanced feedback.",http://arxiv.org/abs/2504.13882v1,IT,Quantitative
Exploring undercurrents of learning tensions in an LLM-enhanced landscape: A student-centered qualitative perspective on LLM vs Search,"Large language models (LLMs) are transforming how students learn by providing readily available tools that can quickly augment or complete various learning activities with non-trivial performance. Similar paradigm shifts have occurred in the past with the introduction of search engines and Wikipedia, which replaced or supplemented traditional information sources such as libraries and books. This study investigates the potential for LLMs to represent the next shift in learning, focusing on their role in information discovery and synthesis compared to existing technologies, such as search engines. Using a within-subjects, counterbalanced design, participants learned new topics using a search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews explored students' reflections, preferences, pain points, and overall perceptions. We present analysis of their responses that show nuanced insights into when, why, and how students prefer LLMs over search engines, offering implications for educators, policymakers, and technology developers navigating the evolving educational landscape.",http://arxiv.org/abs/2504.02622v1,IT,Qualitative
TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research,"Terpenoids are a crucial class of natural products that have been studied for over 150 years, but their interdisciplinary nature (spanning chemistry, pharmacology, and biology) complicates knowledge integration. To address this, the authors developed TeroSeek, a curated knowledge base (KB) built from two decades of terpenoid literature, coupled with an AI-powered question-answering chatbot and web service. Leveraging a retrieval-augmented generation (RAG) framework, TeroSeek provides structured, high-quality information and outperforms general-purpose large language models (LLMs) in terpenoid-related queries. It serves as a domain-specific expert tool for multidisciplinary research and is publicly available at http://teroseek.qmclab.com.",http://arxiv.org/abs/2505.20663v1,IT,Mixed
MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval,"Existing Zero-Shot Composed Image Retrieval (ZS-CIR) methods typically train adapters that convert reference images into pseudo-text tokens, which are concatenated with the modifying text and processed by frozen text encoders in pretrained VLMs or LLMs. While this design leverages the strengths of large pretrained models, it only supervises the adapter to produce encoder-compatible tokens that loosely preserve visual semantics. Crucially, it does not directly optimize the composed query representation to capture the full intent of the composition or to align with the target semantics, thereby limiting retrieval performance, particularly in cases involving fine-grained or complex visual transformations. To address this problem, we propose MLLM-Guided VLM Fine-Tuning with Joint Inference (MVFT-JI), a novel approach that leverages a pretrained multimodal large language model (MLLM) to construct two complementary training tasks using only unlabeled images: target text retrieval taskand text-to-image retrieval task. By jointly optimizing these tasks, our method enables the VLM to inherently acquire robust compositional retrieval capabilities, supported by the provided theoretical justifications and empirical validation. Furthermore, during inference, we further prompt the MLLM to generate target texts from composed queries and compute retrieval scores by integrating similarities between (i) the composed query and candidate images, and (ii) the MLLM-generated target text and candidate images. This strategy effectively combines the VLM's semantic alignment strengths with the MLLM's reasoning capabilities.",http://arxiv.org/abs/2505.19707v1,IT,Quantitative
EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding,"Achieving political consensus is crucial yet challenging for the effective functioning of social governance. However, although frontier AI systems represented by large language models (LLMs) have developed rapidly in recent years, their capabilities on this scope are still understudied. In this paper, we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality deliberation records of the European Parliament over 13 years, ranging from 2009 to 2022, to evaluate the ability of LLMs to reach political consensus among divergent party positions across diverse parliament settings. Specifically, EuroCon incorporates four factors to build each simulated parliament setting: specific political issues, political goals, participating parties, and power structures based on seat distribution. We also develop an evaluation framework for EuroCon to simulate real voting outcomes in different parliament settings, assessing whether LLM-generated resolutions meet predefined political goals. Our experimental results demonstrate that even state-of-the-art models remain undersatisfied with complex tasks like passing resolutions by a two-thirds majority and addressing security issues, while revealing some common strategies LLMs use to find consensus under different power structures, such as prioritizing the stance of the dominant party, highlighting EuroCon's promise as an effective platform for studying LLMs' ability to find political consensus.",http://arxiv.org/abs/2505.19558v1,IT,Quantitative
Enhancing CTR Prediction with De-correlated Expert Networks,"Modeling feature interactions is essential for accurate click-through rate (CTR) prediction in advertising systems. Recent studies have adopted the Mixture-of-Experts (MoE) approach to improve performance by ensembling multiple feature interaction experts. These studies employ various strategies, such as learning independent embedding tables for each expert or utilizing heterogeneous expert architectures, to differentiate the experts, which we refer to expert \emph{de-correlation}. However, it remains unclear whether these strategies effectively achieve de-correlated experts. To address this, we propose a De-Correlated MoE (D-MoE) framework, which introduces a Cross-Expert De-Correlation loss to minimize expert correlations.Additionally, we propose a novel metric, termed Cross-Expert Correlation, to quantitatively evaluate the expert de-correlation degree. Based on this metric, we identify a key finding for MoE framework design: \emph{different de-correlation strategies are mutually compatible, and progressively employing them leads to reduced correlation and enhanced performance}.Extensive experiments have been conducted to validate the effectiveness of D-MoE and the de-correlation principle. Moreover, online A/B testing on Tencent's advertising platforms demonstrates that D-MoE achieves a significant 1.19\% Gross Merchandise Volume (GMV) lift compared to the Multi-Embedding MoE baseline.",http://arxiv.org/abs/2505.17925v1,IT,Quantitative
Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs,"Large Language Models (LLMs) augmented with retrieval mechanisms have demonstrated significant potential in fact-checking tasks by integrating external knowledge. However, their reliability decreases when confronted with conflicting evidence from sources of varying credibility. This paper presents the first systematic evaluation of Retrieval-Augmented Generation (RAG) models for fact-checking in the presence of conflicting evidence. To support this study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for \textbf{Fact}-Checking) (Dataset available at https://github.com/zoeyyes/CONFACT), a novel dataset comprising questions paired with conflicting information from various sources. Extensive experiments reveal critical vulnerabilities in state-of-the-art RAG methods, particularly in resolving conflicts stemming from differences in media source credibility. To address these challenges, we investigate strategies to integrate media background information into both the retrieval and generation stages. Our results show that effectively incorporating source credibility significantly enhances the ability of RAG models to resolve conflicting evidence and improve fact-checking performance.",http://arxiv.org/abs/2505.17762v1,IT,Quantitative
JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks,"Generative AI offers potential for educational support, but often lacks pedagogical grounding and awareness of the student's learning context. Furthermore, researching student interactions with these tools within authentic learning environments remains challenging. To address this, we present JELAI, an open-source platform architecture designed to integrate fine-grained Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly within a Jupyter Notebook environment. JELAI employs a modular, containerized design featuring JupyterLab extensions for telemetry and chat, alongside a central middleware handling LA processing and context-aware LLM prompt enrichment. This architecture enables the capture of integrated code interaction and chat data, facilitating real-time, context-sensitive AI scaffolding and research into student behaviour. We describe the system's design, implementation, and demonstrate its feasibility through system performance benchmarks and two proof-of-concept use cases illustrating its capabilities for logging multi-modal data, analysing help-seeking patterns, and supporting A/B testing of AI configurations. JELAI's primary contribution is its technical framework, providing a flexible tool for researchers and educators to develop, deploy, and study LA-informed AI tutoring within the widely used Jupyter ecosystem.",http://arxiv.org/abs/2505.17593v1,IT,Quantitative
VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR,"Many decision-making tasks, where both accuracy and efficiency matter, still require human supervision. For example, tasks like traffic officers reviewing hour-long dashcam footage or researchers screening conference videos can benefit from concise summaries that reduce cognitive load and save time. Yet current vision-language models (VLMs) often produce verbose, redundant outputs that hinder task performance. Existing video caption evaluation depends on costly human annotations and overlooks the summaries' utility in downstream tasks. We address these gaps with Video-to-text Information Bottleneck Evaluation (VIBE), an annotation-free method that scores VLM outputs using two metrics: grounding (how well the summary aligns with visual content) and utility (how informative it is for the task). VIBE selects from randomly sampled VLM outputs by ranking them according to the two scores to support effective human decision-making. Human studies on LearningPaper24, SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE consistently improve performance-boosting task accuracy by up to 61.23% and reducing response time by 75.77% compared to naive VLM summaries or raw video.",http://arxiv.org/abs/2505.17423v1,IT,Quantitative
Towards medical AI misalignment: a preliminary study,"Despite their staggering capabilities as assistant tools, often exceeding human performances, Large Language Models (LLMs) are still prone to jailbreak attempts from malevolent users. Although red teaming practices have already identified and helped to address several such jailbreak techniques, one particular sturdy approach involving role-playing (which we named `Goofy Game') seems effective against most of the current LLMs safeguards. This can result in the provision of unsafe content, which, although not harmful per se, might lead to dangerous consequences if delivered in a setting such as the medical domain. In this preliminary and exploratory study, we provide an initial analysis of how, even without technical knowledge of the internal architecture and parameters of generative AI models, a malicious user could construct a role-playing prompt capable of coercing an LLM into producing incorrect (and potentially harmful) clinical suggestions. We aim to illustrate a specific vulnerability scenario, providing insights that can support future advancements in the field.",http://arxiv.org/abs/2505.18212v1,IT,Quantitative
Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing,"The proliferation of Large Language Model (LLM)-based Graphical User Interface (GUI) agents in web browsing scenarios present complex unintended consequences (UCs). This paper characterizes three UCs from three perspectives: phenomena, influence and mitigation, drawing on social media analysis (N=221 posts) and semi-structured interviews (N=14). Key phenomenon for UCs include agents' deficiencies in comprehending instructions and planning tasks, challenges in executing accurate GUI interactions and adapting to dynamic interfaces, the generation of unreliable or misaligned outputs, and shortcomings in error handling and feedback processing. These phenomena manifest as influences from unanticipated actions and user frustration, to privacy violations and security vulnerabilities, and further to eroded trust and wider ethical concerns. Our analysis also identifies user-initiated mitigation, such as technical adjustments and manual oversight, and provides implications for designing future LLM-based GUI agents that are robust, user-centric, and transparent, fostering a crucial balance between automation and human oversight.",http://arxiv.org/abs/2505.09875v2,IT,Qualitative
BizChat: Scaffolding AI-Powered Business Planning for Small Business Owners Across Digital Skill Levels,"Generative AI can help small business owners automate tasks, increase efficiency, and improve their bottom line. However, despite the seemingly intuitive design of systems like ChatGPT, significant barriers remain for those less comfortable with technology. To address these disparities, prior work highlights accessory skills -- beyond prompt engineering -- users must master to successfully adopt generative AI including keyboard shortcuts, editing skills, file conversions, and browser literacy. Building on a design workshop series and 15 interviews with small businesses, we introduce BizChat, a large language model (LLM)-powered web application that helps business owners across digital skills levels write their business plan -- an essential but often neglected document. To do so, BizChat's interface embodies three design considerations inspired by learning sciences: ensuring accessibility to users with less digital skills while maintaining extensibility to power users (""low-floor-high-ceiling""), providing in situ micro-learning to support entrepreneurial education (""just-in-time learning""), and framing interaction around business activities (""contextualized technology introduction""). We conclude with plans for a future BizChat deployment.",http://arxiv.org/abs/2505.08493v2,IT,Qualitative
Perspectives on Capturing Emotional Expressiveness in Sign Language,"Significant advances have been made in our ability to understand and generate emotionally expressive content such as text and speech, yet comparable progress in sign language technologies remain limited. While computational approaches to sign language translation have focused on capturing lexical content, the emotional dimensions of sign language communication remain largely unexplored. Through semi-structured interviews with eight sign language users across Singapore, Sri Lanka and the United States, including both Deaf and Hard of hearing (DHH) and hearing signers, we investigate how emotions are expressed and perceived in sign languages. Our findings highlight the role of both manual and non-manual elements in emotional expression, revealing universal patterns as well as individual and cultural variations in how signers communicate emotions. We identify key challenges in capturing emotional nuance for sign language translation, and propose design considerations for developing more emotionally-aware sign language technologies. This work contributes to both theoretical understanding of emotional expression in sign language and practical development of interfaces to better serve diverse signing communities.",http://arxiv.org/abs/2505.08072v1,IT,Qualitative
The Experience of Running: Recommending Routes Using Sensory Mapping in Urban Environments,"Depending on the route, runners may experience frustration, freedom, or fulfilment. However, finding routes that are conducive to the psychological experience of running remains an unresolved task in the literature. In a mixed-method study, we interviewed 7 runners to identify themes contributing to running experience, and quantitatively examined these themes in an online survey with 387 runners. Using Principal Component Analysis on the survey responses, we developed a short experience sampling questionnaire that captures the three most important dimensions of running experience: \emph{performance \& achievement}, \emph{environment}, and \emph{mind \& social connectedness}. Using path preferences obtained from the online survey, we clustered them into two types of routes: \emph{scenic} (associated with nature and greenery) and \emph{urban} (characterized by the presence of people); and developed a routing engine for path recommendations. We discuss challenges faced in developing the routing engine, and provide guidelines to integrate it into mobile and wearable running apps.",http://arxiv.org/abs/2505.05817v1,IT,Mixed
The Impact of Large Language Models on K-12 Education in Rural India: A Thematic Analysis of Student Volunteer's Perspectives,"AI-driven education, particularly Large Language Models (LLMs), has the potential to address learning disparities in rural K-12 schools. However, research on AI adoption in rural India remains limited, with existing studies focusing primarily on urban settings. This study examines the perceptions of volunteer teachers on AI integration in rural education, identifying key challenges and opportunities. Through semi-structured interviews with 23 volunteer educators in Rajasthan and Delhi, we conducted a thematic analysis to explore infrastructure constraints, teacher preparedness, and digital literacy gaps. Findings indicate that while LLMs could enhance personalized learning and reduce teacher workload, barriers such as poor connectivity, lack of AI training, and parental skepticism hinder adoption. Despite concerns over over-reliance and ethical risks, volunteers emphasize that AI should be seen as a complementary tool rather than a replacement for traditional teaching. Given the potential benefits, LLM-based tutors merit further exploration in rural classrooms, with structured implementation and localized adaptations to ensure accessibility and equity.",http://arxiv.org/abs/2505.03163v1,IT,Qualitative
AI-Based Speaking Assistant: Supporting Non-Native Speakers' Speaking in Real-Time Multilingual Communication,"Non-native speakers (NNSs) often face speaking challenges in real-time multilingual communication, such as struggling to articulate their thoughts. To address this issue, we developed an AI-based speaking assistant (AISA) that provides speaking references for NNSs based on their input queries, task background, and conversation history. To explore NNSs' interaction with AISA and its impact on NNSs' speaking during real-time multilingual communication, we conducted a mixed-method study involving a within-subject experiment and follow-up interviews. In the experiment, two native speakers (NSs) and one NNS formed a team (31 teams in total) and completed two collaborative tasks--one with access to the AISA and one without. Overall, our study revealed four types of AISA input patterns among NNSs, each reflecting different levels of effort and language preferences. Although AISA did not improve NNSs' speaking competence, follow-up interviews revealed that it helped improve the logical flow and depth of their speech. Moreover, the additional multitasking introduced by AISA, such as entering and reviewing system output, potentially elevated NNSs' workload and anxiety. Based on these observations, we discuss the pros and cons of implementing tools to assist NNS in real-time multilingual communication and offer design recommendations.",http://arxiv.org/abs/2505.01678v1,IT,Mixed
Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots,"Personalized support is essential to fulfill individuals' emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.",http://arxiv.org/abs/2504.12943v2,IT,Qualitative
PlanGlow: Personalized Study Planning with an Explainable and Controllable LLM-Driven System,"Personal development through self-directed learning is essential in today's fast-changing world, but many learners struggle to manage it effectively. While AI tools like large language models (LLMs) have the potential for personalized learning planning, they face issues such as transparency and hallucinated information. To address this, we propose PlanGlow, an LLM-based system that generates personalized, well-structured study plans with clear explanations and controllability through user-centered interactions. Through mixed methods, we surveyed 28 participants and interviewed 10 before development, followed by a within-subject experiment with 24 participants to evaluate PlanGlow's performance, usability, controllability, and explainability against two baseline systems: a GPT-4o-based system and Khan Academy's Khanmigo. Results demonstrate that PlanGlow significantly improves usability, explainability, and controllability. Additionally, two educational experts assessed and confirmed the quality of the generated study plans. These findings highlight PlanGlow's potential to enhance personalized learning and address key challenges in self-directed learning.",http://arxiv.org/abs/2504.12452v1,IT,Mixed
The Robotability Score: Enabling Harmonious Robot Navigation on Urban Streets,"This paper introduces the Robotability Score ($R$), a novel metric that quantifies the suitability of urban environments for autonomous robot navigation. Through expert interviews and surveys, we identify and weigh key features contributing to R for wheeled robots on urban streets. Our findings reveal that pedestrian density, crowd dynamics and pedestrian flow are the most critical factors, collectively accounting for 28% of the total score. Computing robotability across New York City yields significant variation; the area of highest R is 3.0 times more ""robotable"" than the area of lowest R. Deployments of a physical robot on high and low robotability areas show the adequacy of the score in anticipating the ease of robot navigation. This new framework for evaluating urban landscapes aims to reduce uncertainty in robot deployment while respecting established mobility patterns and urban planning principles, contributing to the discourse on harmonious human-robot environments.",http://arxiv.org/abs/2504.11163v1,IT,Mixed
"Entertainers Between Real and Virtual -- Investigating Viewer Interaction, Engagement, and Relationships with Avatarized Virtual Livestreamers","Virtual YouTubers (VTubers) are avatar-based livestreamers that are voiced and played by human actors. VTubers have been popular in East Asia for years and have more recently seen widespread international growth. Despite their emergent popularity, research has been scarce into the interactions and relationships that exist between avatarized VTubers and their viewers, particularly in contrast to non-avatarized streamers. To address this gap, we performed in-depth interviews with self-reported VTuber viewers (n=21). Our findings first reveal that the avatarized nature of VTubers fosters new forms of theatrical engagement, as factors of the virtual blend with the real to create a mixture of fantasy and realism in possible livestream interactions. Avatarization furthermore results in a unique audience perception regarding the identity of VTubers - an identity which comprises a dynamic, distinct mix of the real human (the voice actor/actress) and the virtual character. Our findings suggest that each of these dual identities both individually and symbiotically affect viewer interactions and relationships with VTubers. Whereas the performer's identity mediates social factors such as intimacy, relatability, and authenticity, the virtual character's identity offers feelings of escapism, novelty in interactions, and a sense of continuity beyond the livestream. We situate our findings within existing livestreaming literature to highlight how avatarization drives unique, character-based interactions as well as reshapes the motivations and relationships that viewers form with livestreamers. Finally, we provide suggestions and recommendations for areas of future exploration to address the challenges involved in present livestreamed avatarized entertainment.",http://arxiv.org/abs/2504.09018v1,IT,Qualitative
Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications,"Military weapon systems and command-and-control infrastructure augmented by artificial intelligence (AI) have seen rapid development and deployment in recent years. However, the sociotechnical impacts of AI on combat systems, military decision-making, and the norms of warfare have been understudied. We focus on a specific subset of lethal autonomous weapon systems (LAWS) that use AI for targeting or battlefield decisions. We refer to this subset as AI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they introduce novel risks -- including unanticipated escalation, poor reliability in unfamiliar environments, and erosion of human oversight -- all of which threaten both military effectiveness and the openness of AI research. These risks cannot be addressed by high-level policy alone; effective regulation must be grounded in the technical behavior of AI models. We argue that AI researchers must be involved throughout the regulatory lifecycle. Thus, we propose a clear, behavior-based definition of AI-LAWS -- systems that introduce unique risks through their use of modern AI -- as a foundation for technically grounded regulation, given that existing frameworks do not distinguish them from conventional LAWS. Using this definition, we propose several technically-informed policy directions and invite greater participation from the AI research community in military AI policy discussions.",http://arxiv.org/abs/2505.18371v1,IT,Mixed
Cultural Awareness in Vision-Language Models: A Cross-Country Exploration,"Vision-Language Models (VLMs) are increasingly deployed in diverse cultural contexts, yet their internal biases remain poorly understood. In this work, we propose a novel framework to systematically evaluate how VLMs encode cultural differences and biases related to race, gender, and physical traits across countries. We introduce three retrieval-based tasks: (1) Race to Country retrieval, which examines the association between individuals from specific racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and Black) and different countries; (2) Personal Traits to Country retrieval, where images are paired with trait-based prompts (e.g., Smart, Honest, Criminal, Violent) to investigate potential stereotypical associations; and (3) Physical Characteristics to Country retrieval, focusing on visual attributes like skinny, young, obese, and old to explore how physical appearances are culturally linked to nations. Our findings reveal persistent biases in VLMs, highlighting how visual representations may inadvertently reinforce societal stereotypes.",http://arxiv.org/abs/2505.20326v1,IT,Mixed
DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval,"Composed Image Retrieval (CIR) aims to retrieve target images from a gallery based on a reference image and modification text as a combined query. Recent approaches focus on balancing global information from two modalities and encode the query into a unified feature for retrieval. However, due to insufficient attention to fine-grained details, these coarse fusion methods often struggle with handling subtle visual alterations or intricate textual instructions. In this work, we propose DetailFusion, a novel dual-branch framework that effectively coordinates information across global and detailed granularities, thereby enabling detail-enhanced CIR. Our approach leverages atomic detail variation priors derived from an image editing dataset, supplemented by a detail-oriented optimization strategy to develop a Detail-oriented Inference Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically fuses global and detailed features based on fine-grained information of each unique multimodal query. Extensive experiments and ablation analyses not only demonstrate that our method achieves state-of-the-art performance on both CIRR and FashionIQ datasets but also validate the effectiveness and cross-domain adaptability of detail enhancement for CIR.",http://arxiv.org/abs/2505.17796v1,IT,Quantitative
MAPS: A Multilingual Benchmark for Global Agent Performance and Security,"Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the global accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into ten diverse languages, resulting in 805 unique tasks and 8,855 total language-specific instances. Our benchmark suite enables a systematic analysis of how multilingual contexts affect agent performance and robustness. Empirically, we observe consistent degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes a standardized evaluation framework, encouraging future research towards equitable, reliable, and globally accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS",http://arxiv.org/abs/2505.15935v1,IT,Quantitative
AMAQA: A Metadata-based QA Dataset for RAG Systems,"Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering. The dataset is available at https://anonymous.4open.science/r/AMAQA-5D0D/",http://arxiv.org/abs/2505.13557v1,IT,Quantitative
"MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark","Document retrieval is an important task for search and Retrieval-Augmented Generation (RAG) applications. Large Language Models (LLMs) have contributed to improving the accuracy of text-based document retrieval. However, documents with complex layout and visual elements like tables, charts and infographics are not perfectly represented in textual format. Recently, image-based document retrieval pipelines have become popular, which use visual large language models (VLMs) to retrieve relevant page images given a query. Current evaluation benchmarks on visual document retrieval are limited, as they primarily focus only English language, rely on synthetically generated questions and offer a small corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual document retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and is an extension of the MIRACL dataset, a popular benchmark to evaluate text-based multilingual retrieval pipelines. MIRACL was built using a human-intensive annotation process to generate high-quality questions. In order to reduce MIRACL-VISION corpus size to make evaluation more compute friendly while keeping the datasets challenging, we have designed a method for eliminating the ""easy"" negatives from the corpus. We conducted extensive experiments comparing MIRACL-VISION with other benchmarks, using popular public text and image models. We observe a gap in state-of-the-art VLM-based embedding models on multilingual capabilities, with up to 59.7% lower retrieval accuracy than a text-based retrieval models. Even for the English language, the visual models retrieval accuracy is 12.1% lower compared to text-based models. MIRACL-VISION is a challenging, representative, multilingual evaluation benchmark for visual retrieval pipelines and will help the community build robust models for document retrieval.",http://arxiv.org/abs/2505.11651v2,IT,Quantitative
Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation,"Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks.",http://arxiv.org/abs/2505.10940v2,IT,Quantitative
Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns,"As the use of Large Language Models (LLMs) by students, lecturers and researchers becomes more prevalent, universities - like other organizations - are pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer accessible pre-trained models, customizable to specific (business) needs. While most studies prioritize data, model, or infrastructure adaptations (e.g., model fine-tuning), we focus on user-salient customizations, like interface changes and corporate branding, which we argue influence users' trust and usage patterns. This study serves as a functional prequel to a large-scale field study in which we examine how students and employees at a German university perceive and use their institution's customized LLMaaS compared to ChatGPT. The goals of this prequel are to stimulate discussions on psychological effects of LLMaaS customizations and refine our research approach through feedback. Our forthcoming findings will deepen the understanding of trust dynamics in LLMs, providing practical guidance for organizations considering LLMaaS deployment.",http://arxiv.org/abs/2505.10490v1,IT,Mixed
"Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility","The legal compliance and safety of different Human-in-the-loop (HITL) setups for AI can vary greatly. This manuscript aims to identify new ways of choosing between such setups, and shows that there is an unavoidable trade-off between the attribution of legal responsibility and the technical explainability of AI. We begin by using the notion of oracle machines from computability theory to formalise different HITL setups, distinguishing between trivial human monitoring, single endpoint human action, and highly involved interaction between the human(s) and the AI. These correspond to total functions, many-one reductions, and Turing reductions respectively. A taxonomy categorising HITL failure modes is then presented, highlighting the limitations on what any HITL setup can actually achieve. Our approach then identifies oversights from UK and EU legal frameworks, which focus on certain HITL setups which may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding unnecessary and unproductive human ""scapegoating"". Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures which are often out of the humans' control. This opens up a new analytic perspective on the challenges arising in the creation of HITL setups, helping inform AI developers and lawmakers on designing HITL to better achieve their desired outcomes.",http://arxiv.org/abs/2505.10426v1,IT,Mixed
GlobalMood: A cross-cultural benchmark for music emotion recognition,"Human annotations of mood in music are essential for music generation and recommender systems. However, existing datasets predominantly focus on Western songs with mood terms derived from English, which may limit generalizability across diverse linguistic and cultural backgrounds. To address this, we introduce `GlobalMood', a novel cross-cultural benchmark dataset comprising 1,180 songs sampled from 59 countries, with large-scale annotations collected from 2,519 individuals across five culturally and linguistically distinct locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing predefined mood categories, we implement a bottom-up, participant-driven approach to organically elicit culturally specific music-related mood terms. We then recruit another pool of human participants to collect 988,925 ratings for these culture-specific descriptors. Our analysis confirms the presence of a valence-arousal structure shared across cultures, yet also reveals significant divergences in how certain mood terms, despite being dictionary equivalents, are perceived cross-culturally. State-of-the-art multimodal models benefit substantially from fine-tuning on our cross-culturally balanced dataset, as evidenced by improved alignment with human evaluations - particularly in non-English contexts. More broadly, our findings inform the ongoing debate on the universality versus cultural specificity of emotional descriptors, and our methodology can contribute to other multimodal and cross-lingual research.",http://arxiv.org/abs/2505.09539v1,IT,Quantitative
Towards the Automated Extraction and Refactoring of NoSQL Schemas from Application Code,"In this paper, we present a static code analysis strategy to extract logical schemas from NoSQL applications. Our solution is based on a model-driven reverse engineering process composed of a chain of platform-independent model transformations. The extracted schema conforms to the \uschema{} unified metamodel, which can represent both NoSQL and relational schemas. To support this process, we define a metamodel capable of representing the core elements of object-oriented languages. Application code is first injected into a code model, from which a control flow model is derived. This, in turn, enables the generation of a model representing both data access operations and the structure of stored data. From these models, the \uschema{} logical schema is inferred. Additionally, the extracted information can be used to identify refactoring opportunities. We illustrate this capability through the detection of join-like query patterns and the automated application of field duplication strategies to eliminate expensive joins. All stages of the process are described in detail, and the approach is validated through a round-trip experiment in which a application using a MongoDB store is automatically generated from a predefined schema. The inferred schema is then compared to the original to assess the accuracy of the extraction process.",http://arxiv.org/abs/2505.20230v1,IT,Quantitative
Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval,"Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.",http://arxiv.org/abs/2505.19356v1,IT,Quantitative
Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms,"Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",http://arxiv.org/abs/2505.20322v1,IT,Quantitative
AI Literacy for Legal AI Systems: A practical approach,"Legal AI systems are increasingly being adopted by judicial and legal system deployers and providers worldwide to support a range of applications. While they offer potential benefits such as reducing bias, increasing efficiency, and improving accountability, they also pose significant risks, requiring a careful balance between opportunities, and legal and ethical development and deployment. AI literacy, as a legal requirement under the EU AI Act and a critical enabler of ethical AI for deployers and providers, could be a tool to achieve this. The article introduces the term ""legal AI systems"" and then analyzes the concept of AI literacy and the benefits and risks associated with these systems. This analysis is linked to a broader AI-L concept for organizations that deal with legal AI systems. The outcome of the article, a roadmap questionnaire as a practical tool for developers and providers to assess risks, benefits, and stakeholder concerns, could be useful in meeting societal and regulatory expectations for legal AI.",http://arxiv.org/abs/2505.18006v1,IT,Quantitative
VisTopics: A Visual Semantic Unsupervised Approach to Topic Modeling of Video and Image Data,"Understanding visual narratives is crucial for examining the evolving dynamics of media representation. This study introduces VisTopics, a computational framework designed to analyze large-scale visual datasets through an end-to-end pipeline encompassing frame extraction, deduplication, and semantic clustering. Applying VisTopics to a dataset of 452 NBC News videos resulted in reducing 11,070 frames to 6,928 deduplicated frames, which were then semantically analyzed to uncover 35 topics ranging from political events to environmental crises. By integrating Latent Dirichlet Allocation with caption-based semantic analysis, VisTopics demonstrates its potential to unravel patterns in visual framing across diverse contexts. This approach enables longitudinal studies and cross-platform comparisons, shedding light on the intersection of media, technology, and public discourse. The study validates the method's reliability through human coding accuracy metrics and emphasizes its scalability for communication research. By bridging the gap between visual representation and semantic meaning, VisTopics provides a transformative tool for advancing the methodological toolkit in computational media studies. Future research may leverage VisTopics for comparative analyses across media outlets or geographic regions, offering insights into the shifting landscapes of media narratives and their societal implications.",http://arxiv.org/abs/2505.14868v1,IT,Mixed
VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and Management,"The dynamic landscape of cybersecurity demands precise and scalable solutions for vulnerability management in heterogeneous systems, where configuration-specific vulnerabilities are often misidentified due to inconsistent data in databases like the National Vulnerability Database (NVD). Inaccurate Common Platform Enumeration (CPE) data in NVD further leads to false positives and incomplete vulnerability retrieval. Informed by our systematic analysis of CPE and CVEdeails data, revealing more than 50% vendor name inconsistencies, we propose VulCPE, a framework that standardizes data and models configuration dependencies using a unified CPE schema (uCPE), entity recognition, relation extraction, and graph-based modeling. VulCPE achieves superior retrieval precision (0.766) and coverage (0.926) over existing tools. VulCPE ensures precise, context-aware vulnerability management, enhancing cyber resilience.",http://arxiv.org/abs/2505.13895v1,IT,Quantitative
Cold-Start Recommendation with Knowledge-Guided Retrieval-Augmented Generation,"Cold-start items remain a persistent challenge in recommender systems due to their lack of historical user interactions, which collaborative models rely on. While recent zero-shot methods leverage large language models (LLMs) to address this, they often struggle with sparse metadata and hallucinated or incomplete knowledge. We propose ColdRAG, a retrieval-augmented generation approach that builds a domain-specific knowledge graph dynamically to enhance LLM-based recommendation in cold-start scenarios, without requiring task-specific fine-tuning. ColdRAG begins by converting structured item attributes into rich natural-language profiles, from which it extracts entities and relationships to construct a unified knowledge graph capturing item semantics. Given a user's interaction history, it scores edges in the graph using an LLM, retrieves candidate items with supporting evidence, and prompts the LLM to rank them. By enabling multi-hop reasoning over this graph, ColdRAG grounds recommendations in verifiable evidence, reducing hallucinations and strengthening semantic connections. Experiments on three public benchmarks demonstrate that ColdRAG surpasses existing zero-shot baselines in both Recall and NDCG. This framework offers a practical solution to cold-start recommendation by combining knowledge-graph reasoning with retrieval-augmented LLM generation.",http://arxiv.org/abs/2505.20773v1,IT,Quantitative
Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering,"Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration.Key findings reveal advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems. Our benchmark and code are available at https://mmdocrag.github.io/MMDocRAG/.",http://arxiv.org/abs/2505.16470v1,IT,Quantitative
Social Sycophancy: A Broader Understanding of LLM Sycophancy,"A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue.",http://arxiv.org/abs/2505.13995v1,IT,Quantitative
ConflictLens: LLM-Based Conflict Resolution Training in Romantic Relationship,"Romantic conflicts are often rooted in deep psychological factors such as coping styles, emotional responses, and communication habits. Existing systems tend to address surface-level behaviors or isolated events, offering limited support for understanding the underlying dynamics. We present ConflictLens, an interactive system that leverages psychological theory and large language models (LLMs) to help individuals analyze and reflect on the deeper mechanisms behind their conflicts. The system provides multi-level strategy recommendations and guided dialogue exercises, including annotation, rewriting, and continuation tasks. A case study demonstrates how ConflictLens supports emotional insight, improves relational understanding, and fosters more constructive communication. This work offers a novel approach to supporting self-awareness and growth in romantic relationships.",http://arxiv.org/abs/2505.11715v1,IT,Qualitative
Fairness-Utility Trade-off via Wasserstein Projection,"Ensuring fairness in data-driven decision-making is a critical concern, but existing fairness constraints often involve trade-offs with overall utility. We propose a fairness framework that enforces strong demographic parity-related fairness criteria (with $\epsilon$-tolerance) in propensity score allocation while guaranteeing a minimum total utility. This approach balances equity and utility by calibrating propensity scores to satisfy fairness criteria and optimizing outcomes without incurring unacceptable losses in performance. Grounded in a binary treatment and sensitive attribute setting under causal fairness setup, our method provides a principled mechanism to address fairness while transparently managing associated economic and social costs, offering a practical approach for designing equitable policies in diverse decision-making contexts. Building on this, we provide theoretical guarantee for our proposed utility-constrained fairness evaluation framework, and we formalize a hypothesis testing framework to help practitioners assess whether the desired fairness-utility trade-off is achieved.",http://arxiv.org/abs/2505.11678v1,IT,Quantitative
X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation,"The ability to imitate realistic facial expressions is essential for humanoid robots engaged in affective human-robot communication. However, the lack of datasets containing diverse humanoid facial expressions with proper annotations hinders progress in realistic humanoid facial expression imitation. To address these challenges, we introduce X2C (Anything to Control), a dataset featuring nuanced facial expressions for realistic humanoid imitation. With X2C, we contribute: 1) a high-quality, high-diversity, large-scale dataset comprising 100,000 (image, control value) pairs. Each image depicts a humanoid robot displaying a diverse range of facial expressions, annotated with 30 control values representing the ground-truth expression configuration; 2) X2CNet, a novel human-to-humanoid facial expression imitation framework that learns the correspondence between nuanced humanoid expressions and their underlying control values from X2C. It enables facial expression imitation in the wild for different human performers, providing a baseline for the imitation task, showcasing the potential value of our dataset; 3) real-world demonstrations on a physical humanoid robot, highlighting its capability to advance realistic humanoid facial expression imitation. Code and Data: https://lipzh5.github.io/X2CNet/",http://arxiv.org/abs/2505.11146v1,IT,Quantitative
"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement","The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",http://arxiv.org/abs/2505.08245v1,IT,Quantitative
LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning,"Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.",http://arxiv.org/abs/2505.07437v1,IT,Quantitative
Future of Home-living: Designing Smart Spaces for Modern Domestic Life,"The evolution of smart home technologies, particularly agentic ones such as conversational agents, robots, and virtual avatars, is reshaping our understanding of home and domestic life. This shift highlights the complexities of modern domestic life, with the household landscape now featuring diverse cohabiting units like co-housing and communal living arrangements. These agentic technologies present specific design challenges and opportunities as they become integrated into everyday routines and activities. Our workshop envisions smart homes as dynamic, user-shaped spaces, focusing on the integration of these technologies into daily life. We aim to explore how these technologies transform household dynamics, especially through boundary fluidity, by uniting researchers and practitioners from fields such as design, sociology, and ethnography. Together, we will develop a taxonomy of challenges and opportunities, providing a structured perspective on the integration of agentic technologies and their impact on contemporary living arrangements.",http://arxiv.org/abs/2407.15956v1,IT,Qualitative
Using Topic Models to Mine Everyday Object Usage Routines Through Connected IoT Sensors,"With the tremendous progress in sensing and IoT infrastructure, it is foreseeable that IoT systems will soon be available for commercial markets, such as in people's homes. In this paper, we present a deployment study using sensors attached to household objects to capture the resourcefulness of three individuals. The concept of resourcefulness highlights the ability of humans to repurpose objects spontaneously for a different use case than was initially intended. It is a crucial element for human health and wellbeing, which is of great interest for various aspects of HCI and design research. Traditionally, resourcefulness is captured through ethnographic practice. Ethnography can only provide sparse and often short duration observations of human experience, often relying on participants being aware of and remembering behaviours or thoughts they need to report on. Our hypothesis is that resourcefulness can also be captured through continuously monitoring objects being used in everyday life. We developed a system that can record object movement continuously and deployed them in homes of three elderly people for over two weeks. We explored the use of probabilistic topic models to analyze the collected data and identify common patterns.",http://arxiv.org/abs/1807.04343v1,IT,Qualitative
Living Without a Mobile Phone: An Autoethnography,"This paper presents an autoethnography of my experiences living without a mobile phone. What started as an experiment motivated by a personal need to reduce stress, has resulted in two voluntary mobile phone breaks spread over nine years (i.e., 2002-2008 and 2014-2017). Conducting this autoethnography is the means to assess if the lack of having a phone has had any real impact in my life. Based on formative and summative analyses, four meaningful units or themes were identified (i.e., social relationships, everyday work, research career, and location and security), and judged using seven criteria for successful ethnography from existing literature. Furthermore, I discuss factors that allow me to make the choice of not having a mobile phone, as well as the relevance that the lessons gained from not having a mobile phone have on the lives of people who are involuntarily disconnected from communication infrastructures.",http://arxiv.org/abs/1804.04833v1,IT,Mixed
Capturing the Connections: Unboxing Internet of Things Devices,"Based upon a study of how to capture data from Internet of Things (IoT) devices, this paper explores the challenges for data centric design ethnography. Often purchased to perform specific tasks, IoT devices exist in a complex ecosystem. This paper describes a study that used a variety of methods to capture the interactions an IoT device engaged in when it was first setup. The complexity of the study that is explored through the annotated documentation across video and router activity, presents the ethnographic challenges that designers face in an age of connected things.",http://arxiv.org/abs/1708.00076v1,IT,Qualitative
The Durability and Fragility of Knowledge Infrastructures: Lessons Learned from Astronomy,"Infrastructures are not inherently durable or fragile, yet all are fragile over the long term. Durability requires care and maintenance of individual components and the links between them. Astronomy is an ideal domain in which to study knowledge infrastructures, due to its long history, transparency, and accumulation of observational data over a period of centuries. Research reported here draws upon a long-term study of scientific data practices to ask questions about the durability and fragility of infrastructures for data in astronomy. Methods include interviews, ethnography, and document analysis. As astronomy has become a digital science, the community has invested in shared instruments, data standards, digital archives, metadata and discovery services, and other relatively durable infrastructure components. Several features of data practices in astronomy contribute to the fragility of that infrastructure. These include different archiving practices between ground- and space-based missions, between sky surveys and investigator-led projects, and between observational and simulated data. Infrastructure components are tightly coupled, based on international agreements. However, the durability of these infrastructures relies on much invisible work - cataloging, metadata, and other labor conducted by information professionals. Continual investments in care and maintenance of the human and technical components of these infrastructures are necessary for sustainability.",http://arxiv.org/abs/1611.00055v1,IT,Mixed
Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models,"Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such ""test awareness"" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.",http://arxiv.org/abs/2505.14617v2,IT,Quantitative
Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases,"The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.",http://arxiv.org/abs/2505.12183v1,IT,Quantitative
Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data,"Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.",http://arxiv.org/abs/2504.21634v1,IT,Quantitative
CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews,"Consumers often heavily rely on online product reviews, analyzing both quantitative ratings and textual descriptions to assess product quality. However, existing research hasn't adequately addressed how to systematically encourage the creation of comprehensive reviews that capture both customers sentiment and detailed product feature analysis. This paper presents CPR, a novel methodology that leverages the power of Large Language Models (LLMs) and Topic Modeling to guide users in crafting insightful and well-rounded reviews. Our approach employs a three-stage process: first, we present users with product-specific terms for rating; second, we generate targeted phrase suggestions based on these ratings; and third, we integrate user-written text through topic modeling, ensuring all key aspects are addressed. We evaluate CPR using text-to-text LLMs, comparing its performance against real-world customer reviews from Walmart. Our results demonstrate that CPR effectively identifies relevant product terms, even for new products lacking prior reviews, and provides sentiment-aligned phrase suggestions, saving users time and enhancing reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU score over baseline methods, further supported by manual evaluation of generated phrases. We conclude by discussing potential extensions and future research directions.",http://arxiv.org/abs/2504.13993v1,IT,Quantitative
The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides,"The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This study presents the first large-scale, systematic examination of this discourse using an innovative hybrid AI-driven methodology, combining large language models with structured human validation to analyse thousands of scholarly abstracts spanning 25 years. By integrating Large Language Models (LLMs) with expert oversight, we developed a quantitative framework for tracking epistemic shifts in contested medical fields, with applications to other content analysis domains. Our analysis revealed a progressive transition from infection-based models of Lyme disease to immune-mediated explanations for persistent symptoms. This study offers new empirical insights into the structural and epistemic forces shaping Lyme disease research, providing a scalable and replicable methodology for analysing discourse, while underscoring the value of AI-assisted methodologies in social science and medical research.",http://arxiv.org/abs/2504.08777v1,IT,Mixed
A Survey on Large Language Models in Multimodal Recommender Systems,"Multimodal recommender systems (MRS) integrate heterogeneous user and item data, such as text, images, and structured information, to enhance recommendation performance. The emergence of large language models (LLMs) introduces new opportunities for MRS by enabling semantic reasoning, in-context learning, and dynamic input handling. Compared to earlier pre-trained language models (PLMs), LLMs offer greater flexibility and generalisation capabilities but also introduce challenges related to scalability and model accessibility. This survey presents a comprehensive review of recent work at the intersection of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques. We propose a novel taxonomy to characterise integration patterns, identify transferable techniques from related recommendation domains, provide an overview of evaluation metrics and datasets, and point to possible future directions. We aim to clarify the emerging role of LLMs in multimodal recommendation and support future research in this rapidly evolving field.",http://arxiv.org/abs/2505.09777v1,IT,Quantitative
AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques,"Natural disasters, including earthquakes, wildfires and cyclones, bear a huge risk on human lives as well as infrastructure assets. An effective response to disaster depends on the ability to rapidly and efficiently assess the intensity of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence (GenAI) presents a breakthrough solution, capable of combining knowledge from multiple types and sources of data, simulating realistic scenarios of disaster, and identifying emerging trends at a speed previously unimaginable. In this paper, we present a comprehensive review on the prospects of AI and GenAI in damage assessment for various natural disasters, highlighting both its strengths and limitations. We talk about its application to multimodal data such as text, image, video, and audio, and also cover major issues of data privacy, security, and ethical use of the technology during crises. The paper also recognizes the threat of Generative AI misuse, in the form of dissemination of misinformation and for adversarial attacks. Finally, we outline avenues of future research, emphasizing the need for secure, reliable, and ethical Generative AI systems for disaster management in general. We believe that this work represents the first comprehensive survey of Gen-AI techniques being used in the field of Disaster Assessment and Response.",http://arxiv.org/abs/2505.08202v1,IT,Quantitative
Must Read: A Systematic Survey of Computational Persuasion,"Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.",http://arxiv.org/abs/2505.07775v1,IT,Quantitative
Proof Assistants for Teaching: a Survey,"In parallel to the ever-growing usage of mechanized proofs in diverse areas of mathematics and computer science, proof assistants are used more and more for education. This paper surveys previous work related to the use of proof assistants for (mostly undergraduate) teaching. This includes works where the authors report on their experiments using proof assistants to teach logic, mathematics or computer science, as well as designs or adaptations of proof assistants for teaching. We provide an overview of both tutoring systems that have been designed for teaching proof and proving, or general-purpose proof assistants that have been adapted for education, adding user interfaces and/or dedicated input or output languages.",http://arxiv.org/abs/2505.13472v1,IT,Quantitative
Sick of being driven? -- Prevalence and modulating factors of carsickness in the European population in context of automated driving,"As in automated driving the driver becomes a passenger, carsickness might reduce comfort for susceptible individuals. Insights in the prevalence of carsickness and its modulating factors are considered useful for the development of automated vehicles to mitigate or prevent its occurrence. An online survey was conducted with N = 3999 participants in Spain, Sweden, Poland, and Germany. 30% of participants reported to have already experienced carsickness as adult. The frequency of carsickness was modulated not only by demographic factors (country, gender, age), but also by frequency of being a passenger, type of non-driving related task, road type, and the seating position in car. Furthermore, the efficiency of applied countermeasures, temporal aspects of carsickness development, as well as the relation of carsickness with the acceptability of automated driving and the effect on subjective fitness to drive was investigated. The results are discussed with focus on automated driving.",http://arxiv.org/abs/2505.04210v2,IT,Quantitative
Methodological Foundations for AI-Driven Survey Question Generation,"This paper presents a methodological framework for using generative AI in educational survey research. We explore how Large Language Models (LLMs) can generate adaptive, context-aware survey questions and introduce the Synthetic Question-Response Analysis (SQRA) framework, which enables iterative testing and refinement of AI-generated prompts prior to deployment with human participants. Guided by Activity Theory, we analyze how AI tools mediate participant engagement and learning, and we examine ethical issues such as bias, privacy, and transparency. Through sentiment, lexical, and structural analyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the alignment and effectiveness of these questions. Our findings highlight the promise and limitations of AI-driven survey instruments, emphasizing the need for robust prompt engineering and validation to support trustworthy, scalable, and contextually relevant data collection in engineering education.",http://arxiv.org/abs/2505.01150v1,IT,Quantitative
Investigating Middle School Students Question-Asking and Answer-Evaluation Skills When Using ChatGPT for Science Investigation,"Generative AI (GenAI) tools such as ChatGPT allow users, including school students without prior AI expertise, to explore and address a wide range of tasks. Surveys show that most students aged eleven and older already use these tools for school-related activities. However, little is known about how they actually use GenAI and how it impacts their learning. This study addresses this gap by examining middle school students ability to ask effective questions and critically evaluate ChatGPT responses, two essential skills for active learning and productive interactions with GenAI. 63 students aged 14 to 15 were tasked with solving science investigation problems using ChatGPT. We analyzed their interactions with the model, as well as their resulting learning outcomes. Findings show that students often over-relied on ChatGPT in both the question-asking and answer-evaluation phases. Many struggled to use clear questions aligned with task goals and had difficulty judging the quality of responses or knowing when to seek clarification. As a result, their learning performance remained moderate: their explanations of the scientific concepts tended to be vague, incomplete, or inaccurate, even after unrestricted use of ChatGPT. This pattern held even in domains where students reported strong prior knowledge. Furthermore, students self-reported understanding and use of ChatGPT were negatively associated with their ability to select effective questions and evaluate responses, suggesting misconceptions about the tool and its limitations. In contrast, higher metacognitive skills were positively linked to better QA-related skills. These findings underscore the need for educational interventions that promote AI literacy and foster question-asking strategies to support effective learning with GenAI.",http://arxiv.org/abs/2505.01106v1,IT,Quantitative
LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects,"With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",http://arxiv.org/abs/2504.19838v2,IT,Quantitative
A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task,"Knowledge-based Vision Question Answering (KB-VQA) extends general Vision Question Answering (VQA) by not only requiring the understanding of visual and textual inputs but also extensive range of knowledge, enabling significant advancements across various real-world applications. KB-VQA introduces unique challenges, including the alignment of heterogeneous information from diverse modalities and sources, the retrieval of relevant knowledge from noisy or large-scale repositories, and the execution of complex reasoning to infer answers from the combined context. With the advancement of Large Language Models (LLMs), KB-VQA systems have also undergone a notable transformation, where LLMs serve as powerful knowledge repositories, retrieval-augmented generators and strong reasoners. Despite substantial progress, no comprehensive survey currently exists that systematically organizes and reviews the existing KB-VQA methods. This survey aims to fill this gap by establishing a structured taxonomy of KB-VQA approaches, and categorizing the systems into main stages: knowledge representation, knowledge retrieval, and knowledge reasoning. By exploring various knowledge integration techniques and identifying persistent challenges, this work also outlines promising future research directions, providing a foundation for advancing KB-VQA models and their applications.",http://arxiv.org/abs/2504.17547v1,IT,Quantitative
Cloud based DevOps Framework for Identifying Risk Factors of Hospital Utilization,"A scalable and reliable system is required to analyze the National Health and Nutrition Examination Survey (NHANES) data efficiently to understand hospital utilization risk factors. This study aims to investigate the integration of continuous integration and deployment (CI/CD) practices in data science workflows, specifically focusing on analyzing NHANES data to identify the prevalence of diabetes, obesity, and cardiovascular diseases. An end-to-end cloud-based DevOps framework is proposed for data analysis which examines risk factors associated with hospital utilization and evaluates key hospital utilization metrics. We have also highlighted the modular structure of the framework that can be generalized for any other domains beyond healthcare. In the framework, an online data update method is provided which can be extended further using both real and synthetic data. As such, the framework can be especially useful for sparse dataset domains such as environmental science, robotics, cybersecurity, and cultural heritage and arts.",http://arxiv.org/abs/2504.14097v1,IT,Quantitative
Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems,"Recent years have witnessed extensive exploration of Large Language Models (LLMs) on the field of Recommender Systems (RS). There are currently two commonly used strategies to enable LLMs to have recommendation capabilities: 1) The ""Guidance-Only"" strategy uses in-context learning to exploit and amplify the inherent semantic understanding and item recommendation capabilities of LLMs; 2) The ""Tuning-Only"" strategy uses supervised fine-tuning (SFT) to fine-tune LLMs with the aim of fitting them to real recommendation data. However, neither of these strategies can effectively bridge the gap between the knowledge space of LLMs and recommendation, and their performance do not meet our expectations. To better enable LLMs to learn recommendation knowledge, we combine the advantages of the above two strategies and proposed a novel ""Guidance+Tuning"" method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of curriculum learning. It first employs self-distillation to construct an auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it further utilizes a self-adaptive curriculum scheduler to enable LLMs to gradually learn from simpler data (self-distilled data) to more challenging data (real RS data). Extensive experiments demonstrate that SOFT significantly enhances the recommendation accuracy (37.59\% on average) of LLM-based methods. The code is available via https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E",http://arxiv.org/abs/2505.20771v1,IT,Quantitative
Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval,"Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a compositional query, consisting of a reference image and a modifying text-without relying on annotated training data. Existing approaches often generate a synthetic target text using large language models (LLMs) to serve as an intermediate anchor between the compositional query and the target image. Models are then trained to align the compositional query with the generated text, and separately align images with their corresponding texts using contrastive learning. However, this reliance on intermediate text introduces error propagation, as inaccuracies in query-to-text and text-to-image mappings accumulate, ultimately degrading retrieval performance. To address these problems, we propose a novel framework by employing a Multimodal Reasoning Agent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries by directly constructing triplets, <reference image, modification text, target image>, using only unlabeled image data. By training on these synthetic triplets, our model learns to capture the relationships between compositional queries and candidate images directly. Extensive experiments on three standard CIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ dataset, our method improves Average R@10 by at least 7.5\% over existing baselines; on CIRR, it boosts R@1 by 9.6\%; and on CIRCO, it increases mAP@5 by 9.5\%.",http://arxiv.org/abs/2505.19952v1,IT,Quantitative
Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model,"Large Language Models (LLMs) have garnered significant attention in Recommendation Systems (RS) due to their extensive world knowledge and robust reasoning capabilities. However, a critical challenge lies in enabling LLMs to effectively comprehend and extract insights from massive user behaviors. Current approaches that directly leverage LLMs for user interest learning face limitations in handling long sequential behaviors, effectively extracting interest, and applying interest in practical scenarios. To address these issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture diverse interests and interest evolution of user. CUBE divides user lifelong behaviors into multiple chunks and learns the interest and interest evolution within each chunk in a cascading manner. HTS generates candidate interests through hierarchical expansion and searches for the optimal interest with process rating model to ensure information gain for each behavior chunk. Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate interests from multiple behavior chunks, constructing a comprehensive representation of user lifelong interests. The representation can be embedded into any recommendation model to enhance performance. Extensive experiments demonstrate the effectiveness of our approach, showing that it surpasses state-of-the-art methods.",http://arxiv.org/abs/2505.19505v1,IT,Quantitative
Aligning Web Query Generation with Ranking Objectives via Direct Preference Optimization,"Neural retrieval models excel in Web search, but their training requires substantial amounts of labeled query-document pairs, which are costly to obtain. With the widespread availability of Web document collections like ClueWeb22, synthetic queries generated by large language models offer a scalable alternative. Still, synthetic training queries often vary in quality, which leads to suboptimal downstream retrieval performance. Existing methods typically filter out noisy query-document pairs based on signals from an external re-ranker. In contrast, we propose a framework that leverages Direct Preference Optimization (DPO) to integrate ranking signals into the query generation process, aiming to directly optimize the model towards generating high-quality queries that maximize downstream retrieval effectiveness. Experiments show higher ranker-assessed relevance between query-document pairs after DPO, leading to stronger downstream performance on the MS~MARCO benchmark when compared to baseline models trained with synthetic data.",http://arxiv.org/abs/2505.19307v1,IT,Quantitative
DLF: Enhancing Explicit-Implicit Interaction via Dynamic Low-Order-Aware Fusion for CTR Prediction,"Click-through rate (CTR) prediction is a critical task in online advertising and recommender systems, relying on effective modeling of feature interactions. Explicit interactions capture predefined relationships, such as inner products, but often suffer from data sparsity, while implicit interactions excel at learning complex patterns through non-linear transformations but lack inductive biases for efficient low-order modeling. Existing two-stream architectures integrate these paradigms but face challenges such as limited information sharing, gradient imbalance, and difficulty preserving low-order signals in sparse CTR data. We propose a novel framework, Dynamic Low-Order-Aware Fusion (DLF), which addresses these limitations through two key components: a Residual-Aware Low-Order Interaction Network (RLI) and a Network-Aware Attention Fusion Module (NAF). RLI explicitly preserves low-order signals while mitigating redundancy from residual connections, and NAF dynamically integrates explicit and implicit representations at each layer, enhancing information sharing and alleviating gradient imbalance. Together, these innovations balance low-order and high-order interactions, improving model expressiveness. Extensive experiments on public datasets demonstrate that DLF achieves state-of-the-art performance in CTR prediction, addressing key limitations of existing models. The implementation is publicly available at https://github.com/USTC-StarTeam/DLF.",http://arxiv.org/abs/2505.19182v1,IT,Quantitative
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering,"As recommendation services scale rapidly and their deployment now commonly involves resource-constrained edge devices, GNN-based recommender systems face significant challenges, including high embedding storage costs and runtime latency from graph propagations. Our previous work, LEGCF, effectively reduced embedding storage costs but struggled to maintain recommendation performance under stricter storage limits. Additionally, LEGCF did not address the extensive runtime computation costs associated with graph propagation, which involves heavy multiplication and accumulation operations (MACs). These challenges consequently hinder effective training and inference on resource-constrained edge devices. To address these limitations, we propose Lightweight Embeddings with Rewired Graph for Graph Collaborative Filtering (LERG), an improved extension of LEGCF. LERG retains LEGCFs compositional codebook structure but introduces quantization techniques to reduce the storage cost, enabling the inclusion of more meta-embeddings within the same storage. To optimize graph propagation, we pretrain the quantized compositional embedding table using the full interaction graph on resource-rich servers, after which a fine-tuning stage is engaged to identify and prune low-contribution entities via a gradient-free binary integer programming approach, constructing a rewired graph that excludes these entities (i.e., user/item nodes) from propagating signals. The quantized compositional embedding table with selective embedding participation and sparse rewired graph are transferred to edge devices which significantly reduce computation memory and inference time. Experiments on three public benchmark datasets, including an industry-scale dataset, demonstrate that LERG achieves superior recommendation performance while dramatically reducing storage and computation costs for graph-based recommendation services.",http://arxiv.org/abs/2505.18999v1,IT,Quantitative
Weaver: Interweaving SQL and LLM for Table Reasoning,"Querying tables with unstructured data is challenging due to the presence of text (or image), either embedded in the table or in external paragraphs, which traditional SQL struggles to process, especially for tasks requiring semantic reasoning. While Large Language Models (LLMs) excel at understanding context, they face limitations with long input sequences. Existing approaches that combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting their adaptability to complex queries. To address these issues, we introduce Weaver , a modular pipeline that dynamically integrates SQL and LLMs for table-based question answering (TableQA). Weaver generates a flexible, step-by-step plan that combines SQL for structured data retrieval with LLMs for semantic processing. By decomposing complex queries into manageable subtasks, Weaver improves accuracy and generalization. Our experiments show that Weaver consistently outperforms state-of-the-art methods across four TableQA datasets, reducing both API calls and error rates.",http://arxiv.org/abs/2505.18961v1,IT,Quantitative
Challenges for artificial cognitive systems,"The declared goal of this paper is to fill this gap: ""... cognitive systems research needs questions or challenges that define progress. The challenges are not (yet more) predictions of the future, but a guideline to what are the aims and what would constitute progress."" -- the quotation being from the project description of EUCogII, the project for the European Network for Cognitive Systems within which this formulation of the 'challenges' was originally developed (http://www.eucognition.org). So, we stick out our neck and formulate the challenges for artificial cognitive systems. These challenges are articulated in terms of a definition of what a cognitive system is: a system that learns from experience and uses its acquired knowledge (both declarative and practical) in a flexible manner to achieve its own goals.",http://arxiv.org/abs/2505.20339v1,IT,Mixed
FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding,"In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag",http://arxiv.org/abs/2505.17330v1,IT,Quantitative
Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement,"Large language models (LLMs) encounter difficulties in knowledge-intensive multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract and represent rationale evidence. The current methods often extract semantically relevant but logically irrelevant evidence, resulting in flawed reasoning and inaccurate responses. We propose a two-way evidence self-alignment (TW-ESA) module, which utilizes the mutual alignment between strict reasoning and LLM reasoning to enhance its understanding of the causal logic of evidence, thereby addressing the first challenge. Another challenge is how to utilize the rationale evidence and LLM's intrinsic knowledge for accurate reasoning when the evidence contains uncertainty. We propose a dual-gated reasoning enhancement (DGR) module to gradually fuse useful knowledge of LLM within strict reasoning, which can enable the model to perform accurate reasoning by focusing on causal elements in the evidence and exhibit greater robustness. The two modules are collaboratively trained in a unified framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based fine-tuning methods, with remarkable average improvements of 4% in exact match (EM) and 5% in F1 score. The implementation code is available at https://anonymous.4open.science/r/ESA-DGR-2BF8.",http://arxiv.org/abs/2505.16806v1,IT,Quantitative
Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation,"Embedding-Based Retrieval (EBR) is an important technique in modern search engines, enabling semantic match between search queries and relevant results. However, search logging data on platforms like Facebook Marketplace lacks the diversity and details needed for effective EBR model training, limiting the models' ability to capture nuanced search patterns. To address this challenge, we propose Aug2Search, an EBR-based framework leveraging synthetic data generated by Generative AI (GenAI) models, in a multimodal and multitask approach to optimize query-product relevance. This paper investigates the capabilities of GenAI, particularly Large Language Models (LLMs), in generating high-quality synthetic data, and analyzing its impact on enhancing EBR models. We conducted experiments using eight Llama models and 100 million data points from Facebook Marketplace logs. Our synthetic data generation follows three strategies: (1) generate queries, (2) enhance product listings, and (3) generate queries from enhanced listings. We train EBR models on three different datasets: sampled engagement data or original data ((e.g., ""Click"" and ""Listing Interactions"")), synthetic data, and a mixture of both engagement and synthetic data to assess their performance across various training sets. Our findings underscore the robustness of Llama models in producing synthetic queries and listings with high coherence, relevance, and diversity, while maintaining low levels of hallucination. Aug2Search achieves an improvement of up to 4% in ROC_AUC with 100 million synthetic data samples, demonstrating the effectiveness of our approach. Moreover, our experiments reveal that with the same volume of training data, models trained exclusively on synthetic data often outperform those trained on original data only or a mixture of original and synthetic data.",http://arxiv.org/abs/2505.16065v1,IT,Quantitative
Low Rank Learning for Offline Query Optimization,"Recent deployments of learned query optimizers use expensive neural networks and ad-hoc search policies. To address these issues, we introduce \textsc{LimeQO}, a framework for offline query optimization leveraging low-rank learning to efficiently explore alternative query plans with minimal resource usage. By modeling the workload as a partially observed, low-rank matrix, we predict unobserved query plan latencies using purely linear methods, significantly reducing computational overhead compared to neural networks. We formalize offline exploration as an active learning problem, and present simple heuristics that reduces a 3-hour workload to 1.5 hours after just 1.5 hours of exploration. Additionally, we propose a transductive Tree Convolutional Neural Network (TCNN) that, despite higher computational costs, achieves the same workload reduction with only 0.5 hours of exploration. Unlike previous approaches that place expensive neural networks directly in the query processing ``hot'' path, our approach offers a low-overhead solution and a no-regressions guarantee, all without making assumptions about the underlying DBMS. The code is available in \href{https://github.com/zixy17/LimeQO}{https://github.com/zixy17/LimeQO}.",http://arxiv.org/abs/2504.06399v1,IT,Quantitative
Auto-Configuring Entity Resolution Pipelines,"The same real-world entity (e.g., a movie, a restaurant, a person) may be described in various ways on different datasets. Entity Resolution (ER) aims to find such different descriptions of the same entity, this way improving data quality and, therefore, data value. However, an ER pipeline typically involves several steps (e.g., blocking, similarity estimation, clustering), with each step requiring its own configurations and tuning. The choice of the best configuration, among a vast number of possible combinations, is a dataset-specific and labor-intensive task both for novice and expert users, while it often requires some ground truth knowledge of real matches. In this work, we examine ways of automatically configuring a state of-the-art end-to-end ER pipeline based on pre-trained language models under two settings: (i) When ground truth is available. In this case, sampling strategies that are typically used for hyperparameter optimization can significantly restrict the search of the configuration space. We experimentally compare their relative effectiveness and time efficiency, applying them to ER pipelines for the first time. (ii) When no ground truth is available. In this case, labelled data extracted from other datasets with available ground truth can be used to train a regression model that predicts the relative effectiveness of parameter configurations. Experimenting with 11 ER benchmark datasets, we evaluate the relative performance of existing techniques that address each problem, but have not been applied to ER before.",http://arxiv.org/abs/2503.13226v1,IT,Quantitative
From G-Factor to A-Factor: Establishing a Psychometric Framework for AI Literacy,"This research addresses the growing need to measure and understand AI literacy in the context of generative AI technologies. Through three sequential studies involving a total of 517 participants, we establish AI literacy as a coherent, measurable construct with significant implications for education, workforce development, and social equity. Study 1 (N=85) revealed a dominant latent factor - termed the ""A-factor"" - that accounts for 44.16% of variance across diverse AI interaction tasks. Study 2 (N=286) refined the measurement tool by examining four key dimensions of AI literacy: communication effectiveness, creative idea generation, content evaluation, and step-by-step collaboration, resulting in an 18-item assessment battery. Study 3 (N=146) validated this instrument in a controlled laboratory setting, demonstrating its predictive validity for real-world task performance. Results indicate that AI literacy significantly predicts performance on complex, language-based creative tasks but shows domain specificity in its predictive power. Additionally, regression analyses identified several significant predictors of AI literacy, including cognitive abilities (IQ), educational background, prior AI experience, and training history. The multidimensional nature of AI literacy and its distinct factor structure provide evidence that effective human-AI collaboration requires a combination of general and specialized abilities. These findings contribute to theoretical frameworks of human-AI collaboration while offering practical guidance for developing targeted educational interventions to promote equitable access to the benefits of generative AI technologies.",http://arxiv.org/abs/2503.16517v1,IT,Quantitative
Fact-checking with Generative AI: A Systematic Cross-Topic Examination of LLMs Capacity to Detect Veracity of Political Information,"The purpose of this study is to assess how large language models (LLMs) can be used for fact-checking and contribute to the broader debate on the use of automated means for veracity identification. To achieve this purpose, we use AI auditing methodology that systematically evaluates performance of five LLMs (ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google Gemini) using prompts regarding a large set of statements fact-checked by professional journalists (16,513). Specifically, we use topic modeling and regression analysis to investigate which factors (e.g. topic of the prompt or the LLM type) affect evaluations of true, false, and mixed statements. Our findings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy than other models, overall performance across models remains modest. Notably, the results indicate that models are better at identifying false statements, especially on sensitive topics such as COVID-19, American political controversies, and social issues, suggesting possible guardrails that may enhance accuracy on these topics. The major implication of our findings is that there are significant challenges for using LLMs for factchecking, including significant variation in performance across different LLMs and unequal quality of outputs for specific topics which can be attributed to deficits of training data. Our research highlights the potential and limitations of LLMs in political fact-checking, suggesting potential avenues for further improvements in guardrails as well as fine-tuning.",http://arxiv.org/abs/2503.08404v1,IT,Quantitative
Toward Filling a Critical Knowledge Gap: Charting the Interactions of Age with Task and Visualization,"We present the results of a study comparing the performance of younger adults (YA) and people in late adulthood (PLA) across ten low-level analysis tasks and five basic visualizations, employing Bayesian regression to aggregate and model participant performance. We analyzed performance at the task level and across combinations of tasks and visualizations, reporting measures of performance at aggregate and individual levels. These analyses showed that PLA on average required more time to complete tasks while demonstrating comparable accuracy. Furthermore, at the individual level, PLA exhibited greater heterogeneity in task performance as well as differences in best-performing visualization types for some tasks. We contribute empirical knowledge on how age interacts with analysis task and visualization type and use these results to offer actionable insights and design recommendations for aging-inclusive visualization design. We invite the visualization research community to further investigate aging-aware data visualization. Supplementary materials can be found at https://osf.io/a7xtz/.",http://arxiv.org/abs/2503.02699v1,IT,Quantitative
Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers,"This study examines the potential of large language models (LLMs) to augment the academic peer review process by reliably evaluating the quality of economics research without introducing systematic bias. We conduct one of the first large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5, Gemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we use nonparametric binscatter and linear regression techniques to analyze over 29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals excluded from the training data of current LLMs, along with a set of AI-generated submissions. The results show that LLMs consistently distinguish between higher- and lower-quality research based solely on textual content, producing quality gradients that closely align with established journal prestige measures. Claude and Gemma perform exceptionally well in capturing these gradients, while GPT excels in detecting AI-generated content. The second experiment comprises 8,910 evaluations designed to assess whether LLMs replicate human like biases in single blind reviews. By systematically varying author gender, institutional affiliation, and academic prominence across 330 papers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings to submissions from top male authors and elite institutions relative to the same papers presented anonymously. These results emphasize the importance of excluding author-identifying information when deploying LLMs in editorial screening. Overall, our findings provide compelling evidence and practical guidance for integrating LLMs into peer review to enhance efficiency, improve accuracy, and promote equity in the publication process of economics research.",http://arxiv.org/abs/2502.00070v2,IT,Quantitative
Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model,"Heart Failure (HF) affects millions of Americans and leads to high readmission rates, posing significant healthcare challenges. While Social Determinants of Health (SDOH) such as socioeconomic status and housing stability play critical roles in health outcomes, they are often underrepresented in structured EHRs and hidden in unstructured clinical notes. This study leverages advanced large language models (LLMs) to extract SDOHs from clinical text and uses logistic regression to analyze their association with HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited transportation) linked to readmission risk, this work also offers actionable insights for reducing readmissions and improving patient care.",http://arxiv.org/abs/2502.12158v1,IT,Quantitative
The Impact of Student Writing Assessment Literacy on Psychological Factors: An Ordinal Logistic Regression Analysis,"Previous studies have shown that enhanced student assessment literacy can lead to improvements in academic performance in EFL (English as a Foreign Language) writing. Additionally, psychological factors such as self-efficacy, achievement motivation, and writing anxiety significantly influence EFL writing outcomes. However, the relationship between student writing assessment literacy (SWAL) and these psychological factors remains unclear. The present study aims to explore how SWAL affects psychological factors in the Chinese EFL context. Data were collected from 103 Chinese undergraduate EFL students using four questionnaires: the Student Writing Assessment Literacy Scale (SWAL), the Self-Efficacy for Writing Scale (SEWS), the Achievement Goal Questionnaire (AGQ), and the Second Language Writing Anxiety Inventory (SLWAI). Ordinal logistic regression was employed to analyze the data. The results indicated that higher levels of SWAL were positively associated with writing self-efficacy and achievement motivation, while negatively related to writing anxiety. These findings have significant pedagogical implications for second language (L2) writing instructions, emphasizing the importance of integrating SWAL training into writing instruction to enhance students' writing experiences and outcomes.",http://arxiv.org/abs/2502.00004v1,IT,Quantitative
Chat Bankman-Fried: an Exploration of LLM Alignment in Finance,"Advancements in large language models (LLMs) have renewed concerns about AI alignment - the consistency between human and AI goals and values. As various jurisdictions enact legislation on AI safety, the concept of alignment must be defined and measured across different domains. This paper proposes an experimental framework to assess whether LLMs adhere to ethical and legal standards in the relatively unexplored context of finance. We prompt twelve LLMs to impersonate the CEO of a financial institution and test their willingness to misuse customer assets to repay outstanding corporate debt. Beginning with a baseline configuration, we adjust preferences, incentives and constraints, analyzing the impact of each adjustment with logistic regression. Our findings reveal significant heterogeneity in the baseline propensity for unethical behavior of LLMs. Factors such as risk aversion, profit expectations, and regulatory environment consistently influence misalignment in ways predicted by economic theory, although the magnitude of these effects varies across LLMs. This paper highlights both the benefits and limitations of simulation-based, ex post safety testing. While it can inform financial authorities and institutions aiming to ensure LLM safety, there is a clear trade-off between generality and cost.",http://arxiv.org/abs/2411.11853v3,IT,Quantitative
ListenNet: A Lightweight Spatio-Temporal Enhancement Nested Network for Auditory Attention Detection,"Auditory attention detection (AAD) aims to identify the direction of the attended speaker in multi-speaker environments from brain signals, such as Electroencephalography (EEG) signals. However, existing EEG-based AAD methods overlook the spatio-temporal dependencies of EEG signals, limiting their decoding and generalization abilities. To address these issues, this paper proposes a Lightweight Spatio-Temporal Enhancement Nested Network (ListenNet) for AAD. The ListenNet has three key components: Spatio-temporal Dependency Encoder (STDE), Multi-scale Temporal Enhancement (MSTE), and Cross-Nested Attention (CNA). The STDE reconstructs dependencies between consecutive time windows across channels, improving the robustness of dynamic pattern extraction. The MSTE captures temporal features at multiple scales to represent both fine-grained and long-range temporal patterns. In addition, the CNA integrates hierarchical features more effectively through novel dynamic attention mechanisms to capture deep spatio-temporal correlations. Experimental results on three public datasets demonstrate the superiority of ListenNet over state-of-the-art methods in both subject-dependent and challenging subject-independent settings, while reducing the trainable parameter count by approximately 7 times. Code is available at:https://github.com/fchest/ListenNet.",http://arxiv.org/abs/2505.10348v1,IT,Quantitative
Learning Item Representations Directly from Multimodal Features for Effective Recommendation,"Conventional multimodal recommender systems predominantly leverage Bayesian Personalized Ranking (BPR) optimization to learn item representations by amalgamating item identity (ID) embeddings with multimodal features. Nevertheless, our empirical and theoretical findings unequivocally demonstrate a pronounced optimization gradient bias in favor of acquiring representations from multimodal features over item ID embeddings. As a consequence, item ID embeddings frequently exhibit suboptimal characteristics despite the convergence of multimodal feature parameters. Given the rich informational content inherent in multimodal features, in this paper, we propose a novel model (i.e., LIRDRec) that learns item representations directly from these features to augment recommendation performance. Recognizing that features derived from each modality may capture disparate yet correlated aspects of items, we propose a multimodal transformation mechanism, integrated with modality-specific encoders, to effectively fuse features from all modalities. Moreover, to differentiate the influence of diverse modality types, we devise a progressive weight copying fusion module within LIRDRec. This module incrementally learns the weight assigned to each modality in synthesizing the final user or item representations. Finally, we utilize the powerful visual understanding of Multimodal Large Language Models (MLLMs) to convert the item images into texts and extract semantics embeddings upon the texts via LLMs. Empirical evaluations conducted on five real-world datasets validate the superiority of our approach relative to competing baselines. It is worth noting the proposed model, equipped with embeddings extracted from MLLMs and LLMs, can further improve the recommendation accuracy of NDCG@20 by an average of 4.21% compared to the original embeddings.",http://arxiv.org/abs/2505.04960v1,IT,Quantitative
Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences,"The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations.",http://arxiv.org/abs/2504.17146v3,IT,Mixed
Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models,"Masculine defaults are widely recognized as a significant type of gender bias, but they are often unseen as they are under-researched. Masculine defaults involve three key parts: (i) the cultural context, (ii) the masculine characteristics or behaviors, and (iii) the reward for, or simply acceptance of, those masculine characteristics or behaviors. In this work, we study discourse-based masculine defaults, and propose a twofold framework for (i) the large-scale discovery and analysis of gendered discourse words in spoken content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the measurement of the gender bias associated with these gendered discourse words in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus our study on podcasts, a popular and growing form of social media, analyzing 15,117 podcast episodes. We analyze correlations between gender and discourse words -- discovered via LDA and BERTopic -- to automatically form gendered discourse word lists. We then study the prevalence of these gendered discourse words in domain-specific contexts, and find that gendered discourse-based masculine defaults exist in the domains of business, technology/politics, and video games. Next, we study the representation of these gendered discourse words from a state-of-the-art LLM embedding model from OpenAI, and find that the masculine discourse words have a more stable and robust representation than the feminine discourse words, which may result in better system performance on downstream tasks for men. Hence, men are rewarded for their discourse patterns with better system performance by one of the state-of-the-art language models -- and this embedding disparity is a representational harm and a masculine default.",http://arxiv.org/abs/2504.11431v1,IT,Quantitative
Quantifying Emotional Arousal through Pupillary Response: A Novel Approach for Isolating the Luminosity Effect and Predicting Affective States,"Researchers have long recognized pupil response as a potential objective indicator of emotional arousal; however, confounding factors, particularly luminosity of stimuli and the ambient environment, have limited its usefulness in detecting emotions. This study presents a new approach to isolate and remove the effect of luminosity on pupil dilation, obtaining the component of pupil dilation due only to emotional arousal. Our model predicts the pupil size due to luminosity only as a function of the screen luminosity and adapts to individual differences in pupil response to light, different types and configurations of monitors by using a calibration procedure. The predicted pupil size has an average correlation with the measured pupil size of 0.76, an R2 of 0.58, and a normalized root mean square error (NRMSE) of 0.14. Here, we demonstrate that our model can be used simply to calculate emotional arousal. We showed 32 video clips with different content and emotional intensity to 47 participants, who, after each video, reported their level of emotional arousal. We then calculated the pupil size due only to luminosity and subtracted it from the total recorded pupil size, obtaining the component due only to emotional arousal. From the latter, we predicted the arousal of each participant for each video. We obtained an average correlation between predicted and self-reported arousal of 0.65, an R2 of 0.43, and an NRMSE of 0.27. Instead, using the measured pupil size, without subtracting the component due to luminosity, we obtained dramatically worse results. an average correlation between the predicted and self-reported arousal of 0.26, an R2 of 0.09, and an NRMSE of 0.42. Our results highlight that separating the emotional and luminosity components from pupillary responses is critical to accurately and precisely predicting arousal.",http://arxiv.org/abs/2504.13886v1,IT,Mixed
Geospatial and Symbolic Hypothesis for the Foundation of Tenochtitlan Based on Digital Elevation Analysis of the Valley of Mexico,"This paper proposes a novel hypothesis about the foundation of Tenochtitlan by combining digital elevation modeling with historical and symbolic analysis. Using geospatial data from EarthExplorer, we simulate various historical water levels in the Valley of Mexico. The resulting lake configurations reveal possible locations for ancient settlements near now-vanished shorelines, suggesting a dynamic transformation of sacred geography that aligns with key Mexica myths. We identify Santa Mar\'ia Aztahuacan as a strong candidate for the historical Aztlan and propose a reinterpretation of foundational codices in light of geomythical correlations.",http://arxiv.org/abs/2504.03787v1,IT,Quantitative
Do Chinese models speak Chinese languages?,"The release of top-performing open-weight LLMs has cemented China's role as a leading force in AI development. Do these models support languages spoken in China? Or do they speak the same languages as Western models? Comparing multilingual capabilities is important for two reasons. First, language ability provides insights into pre-training data curation, and thus into resource allocation and development priorities. Second, China has a long history of explicit language policy, varying between inclusivity of minority languages and a Mandarin-first policy. To test whether Chinese LLMs today reflect an agenda about China's languages, we test performance of Chinese and Western open-source LLMs on Asian regional and Chinese minority languages. Our experiments on Information Parity and reading comprehension show Chinese models' performance across these languages correlates strongly (r=0.93) with Western models', with the sole exception being better Mandarin. Sometimes, Chinese models cannot identify languages spoken by Chinese minorities such as Kazakh and Uyghur, even though they are good at French and German. These results provide a window into current development priorities, suggest options for future development, and indicate guidance for end users.",http://arxiv.org/abs/2504.00289v2,IT,Quantitative
LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries,"Query expansion (QE) enhances retrieval by incorporating relevant terms, with large language models (LLMs) offering an effective alternative to traditional rule-based and statistical methods. However, LLM-based QE suffers from a fundamental limitation: it often fails to generate relevant knowledge, degrading search performance. Prior studies have focused on hallucination, yet its underlying cause--LLM knowledge deficiencies--remains underexplored. This paper systematically examines two failure cases in LLM-based QE: (1) when the LLM lacks query knowledge, leading to incorrect expansions, and (2) when the query is ambiguous, causing biased refinements that narrow search coverage. We conduct controlled experiments across multiple datasets, evaluating the effects of knowledge and query ambiguity on retrieval performance using sparse and dense retrieval models. Our results reveal that LLM-based QE can significantly degrade the retrieval effectiveness when knowledge in the LLM is insufficient or query ambiguity is high. We introduce a framework for evaluating QE under these conditions, providing insights into the limitations of LLM-based retrieval augmentation.",http://arxiv.org/abs/2505.12694v1,IT,Quantitative
Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom,"Knowledge Distillation (KD) is one of the approaches to reduce the size of Large Language Models (LLMs). A LLM with smaller number of model parameters (student) is trained to mimic the performance of a LLM of a larger size (teacher model) on a specific task. For domain-specific tasks, it is not clear if teacher or student model, or both, must be considered for domain adaptation. In this work, we study this problem from perspective of telecom domain Question-Answering (QA) task. We systematically experiment with Supervised Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to KD. We design experiments to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. Multi-faceted evaluation of the distillation using 14 different metrics (N-gram, embedding and LLM-based metrics) is considered. Experimental results show that SFT of teacher improves performance of distilled model when both models have same vocabulary, irrespective of algorithm and metrics. Overall, SFT of both teacher and student results in better performance across all metrics, although the statistical significance of the same depends on the vocabulary of the teacher models.",http://arxiv.org/abs/2504.20000v1,IT,Quantitative
Prompt-Hacking: The New p-Hacking?,"As Large Language Models (LLMs) become increasingly embedded in empirical research workflows, their use as analytical tools raises pressing concerns for scientific integrity. This opinion paper draws a parallel between ""prompt-hacking"", the strategic tweaking of prompts to elicit desirable outputs from LLMs, and the well-documented practice of ""p-hacking"" in statistical analysis. We argue that the inherent biases, non-determinism, and opacity of LLMs make them unsuitable for data analysis tasks demanding rigor, impartiality, and reproducibility. We emphasize how researchers may inadvertently, or even deliberately, adjust prompts to confirm hypotheses while undermining research validity. We advocate for a critical view of using LLMs in research, transparent prompt documentation, and clear standards for when LLM use is appropriate. We discuss how LLMs can replace traditional analytical methods, whereas we recommend that LLMs should only be used with caution, oversight, and justification.",http://arxiv.org/abs/2504.14571v1,IT,Quantitative
Design Priorities in Digital Gateways: A Comparative Study of Authentication and Usability in Academic Library Alliances,"Purpose: This study examines the design and functionality of university library login pages across academic alliances (IVY Plus, BTAA, JULAC, JVU) to identify how these interfaces align with institutional priorities and user needs. It explores consensus features, design variations, and emerging trends in authentication, usability, and security. Methodology: A multi-method approach was employed: screenshots and HTML files from 46 institutions were analyzed through categorization, statistical analysis, and comparative evaluation. Features were grouped into authentication mechanisms, usability, security/compliance, and library-specific elements. Findings: Core functionalities (e.g., ID/password, privacy policies) were consistent across alliances. Divergences emerged in feature emphasis: mature alliances (e.g., BTAA) prioritized resource accessibility with streamlined interfaces, while emerging consortia (e.g., JVU) emphasized cybersecurity (IP restrictions, third-party integrations). Usability features, particularly multilingual support, drove cross-alliance differences. The results highlighted regional and institutional influences, with older alliances favoring simplicity and newer ones adopting security-centric designs. Originality/Value: This is the first systematic comparison of login page designs across academic alliances, offering insights into how regional, technological, and institutional factors shape digital resource access. Findings inform best practices for balancing security, usability, and accessibility in library interfaces. **Keywords**: Academic library consortia, Login page design, User authentication, User experience, Security compliance.",http://arxiv.org/abs/2504.13404v1,IT,Mixed
Text Chunking for Document Classification for Urban System Management using Large Language Models,"Urban systems are managed using complex textual documentation that need coding and analysis to set requirements and evaluate built environment performance. This paper contributes to the study of applying large-language models (LLM) to qualitative coding activities to reduce resource requirements while maintaining comparable reliability to humans. Qualitative coding and assessment face challenges like resource limitations and bias, accuracy, and consistency between human evaluators. Here we report the application of LLMs to deductively code 10 case documents on the presence of 17 digital twin characteristics for the management of urban systems. We utilize two prompting methods to compare the semantic processing of LLMs with human coding efforts: whole text analysis and text chunk analysis using OpenAI's GPT-4o, GPT-4o-mini, and o1-mini models. We found similar trends of internal variability between methods and results indicate that LLMs may perform on par with human coders when initialized with specific deductive coding contexts. GPT-4o, o1-mini and GPT-4o-mini showed significant agreement with human raters when employed using a chunking method. The application of both GPT-4o and GPT-4o-mini as an additional rater with three manual raters showed statistically significant agreement across all raters, indicating that the analysis of textual documents is benefited by LLMs. Our findings reveal nuanced sub-themes of LLM application suggesting LLMs follow human memory coding processes where whole-text analysis may introduce multiple meanings. The novel contributions of this paper lie in assessing the performance of OpenAI GPT models and introduces the chunk-based prompting approach, which addresses context aggregation biases by preserving localized context.",http://arxiv.org/abs/2504.00274v1,IT,Mixed
Launching Insights: A Pilot Study on Leveraging Real-World Observational Data from the Mayo Clinic Platform to Advance Clinical Research,"Backgrounds: Artificial intelligence (AI) is transforming healthcare, yet translating AI models from theoretical frameworks to real-world clinical applications remains challenging. The Mayo Clinic Platform (MCP) was established to address these challenges by providing a scalable ecosystem that integrates real-world multiple modalities data from multiple institutions, advanced analytical tools, and secure computing environments to support clinical research and AI development. Methods: In this study, we conducted four research projects leveraging MCP's data infrastructure and analytical capabilities to demonstrate its potential in facilitating real-world evidence generation and AI-driven clinical insights. Utilizing MCP's tools and environment, we facilitated efficient cohort identification, data extraction, and subsequent statistical or AI-powered analyses. Results: The results underscore MCP's role in accelerating translational research by offering de-identified, standardized real-world data and facilitating AI model validation across diverse healthcare settings. Compared to Mayo's internal Electronic Health Record (EHR) data, MCP provides broader accessibility, enhanced data standardization, and multi-institutional integration, making it a valuable resource for both internal and external researchers. Conclusion: Looking ahead, MCP is well-positioned to transform clinical research through its scalable ecosystem, effectively bridging the divide between AI innovation and clinical deployment. Future investigations will build upon this foundation, further exploring MCP's capacity to advance precision medicine and enhance patient outcomes.",http://arxiv.org/abs/2504.16090v1,IT,Quantitative
HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings,"Implementation of digital health systems in low-middle-income countries (LMICs) often fails due to a lack of evaluations that take into account infrastructure limitations, local policies, and community readiness. We introduce HOT-FIT-BR, a contextual evaluation framework that expands the HOT-FIT model with three new dimensions: (1) Infrastructure Index to measure electricity/internet availability, (2) Policy Compliance Layer to ensure regulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community Engagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR is 58% more sensitive to detecting problems than HOT-FIT, especially in rural areas with an Infra Index <3. The framework has also proven adaptive to the context of other LMICs such as India and Kenya through local parameter adjustments.",http://arxiv.org/abs/2505.20585v1,IT,Quantitative
Towards Reliable Large Audio Language Model,"Recent advancements in large audio language models (LALMs) have demonstrated impressive results and promising prospects in universal understanding and reasoning across speech, music, and general sound. However, these models still lack the ability to recognize their knowledge boundaries and refuse to answer questions they don't know proactively. While there have been successful attempts to enhance the reliability of LLMs, reliable LALMs remain largely unexplored. In this paper, we systematically investigate various approaches towards reliable LALMs, including training-free methods such as multi-modal chain-of-thought (MCoT), and training-based methods such as supervised fine-tuning (SFT). Besides, we identify the limitations of previous evaluation metrics and propose a new metric, the Reliability Gain Index (RGI), to assess the effectiveness of different reliable methods. Our findings suggest that both training-free and training-based methods enhance the reliability of LALMs to different extents. Moreover, we find that awareness of reliability is a ""meta ability"", which can be transferred across different audio modalities, although significant structural and content differences exist among sound, music, and speech.",http://arxiv.org/abs/2505.19294v1,IT,Mixed
Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach,"Large language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely. Existing safety evaluations primarily rely on context-independent metrics - such as factuality, bias, or toxicity - overlooking the fact that the same response may carry divergent risks depending on the user's background or condition. We introduce personalized safety to fill this gap and present PENGUIN - a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by 43.2%, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISE - a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard.",http://arxiv.org/abs/2505.18882v1,IT,Quantitative
Justified Evidence Collection for Argument-based AI Fairness Assurance,"It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.",http://arxiv.org/abs/2505.08064v1,IT,Qualitative
Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume,"Reliable street-level traffic volume data, covering multiple modes of transportation, helps urban planning by informing decisions on infrastructure improvements, traffic management, and public transportation. Yet, traffic sensors measuring traffic volume are typically scarcely located, due to their high deployment and maintenance costs. To address this, interpolation methods can estimate traffic volumes at unobserved locations using available data. Graph Neural Networks have shown strong performance in traffic volume forecasting, particularly on highways and major arterial networks. Applying them to urban settings, however, presents unique challenges: urban networks exhibit greater structural diversity, traffic volumes are highly overdispersed with many zeros, the best way to account for spatial dependencies remains unclear, and sensor coverage is often very sparse. We introduce the Graph Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume estimation approach. GNNUI employs a masking algorithm to learn interpolation, integrates node features to capture functional roles, and uses a loss function tailored to zero-inflated traffic distributions. In addition to the model, we introduce two new open, large-scale urban traffic volume benchmarks, covering different transportation modes: Strava cycling data from Berlin and New York City taxi data. GNNUI outperforms recent, some graph-based, interpolation methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence) and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE rises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong performance under extreme data scarcity, common in real-world urban settings. We also examine how graph connectivity choices influence model accuracy.",http://arxiv.org/abs/2505.06292v1,IT,Quantitative
Advancing Remote and Continuous Cardiovascular Patient Monitoring through a Novel and Resource-efficient IoT-Driven Framework,"Cardiovascular diseases are a leading cause of fatalities worldwide, often occurring suddenly with limited time for intervention. Current healthcare monitoring systems for cardiac patients rely heavily on hospitalization, which can be impractical for continuous monitoring. This paper presents a novel IoT-based solution for remote, real-time tracking of critical cardiac metrics, addressing the pressing need for accessible and continuous healthcare, particularly for the aging population in Pakistan. The proposed IoT kit measures essential parameters such as body temperature, heart rate (HR), blood pressure (BP), oxygen saturation (SPO2), and electrocardiography (ECG). A key innovation of the system is its integration with a cloud-based application, enabling constant remote monitoring and incorporating an alarm mechanism to alert medical professionals for timely intervention, reducing the risk of catastrophic incidents. The system was tested in a clinical environment with 20 participants, demonstrating results closely aligned with those obtained using standard medical devices. The findings validate the system's potential for reliable remote monitoring, offering a significant step forward in proactive cardiac healthcare management. This novel approach combines IoT technology with cloud-based applications to provide a cost-effective and efficient solution for reducing unexpected fatalities among cardiac patients.",http://arxiv.org/abs/2505.03409v1,IT,Mixed
Benchmarking Poisoning Attacks against Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has proven effective in mitigating hallucinations in large language models by incorporating external knowledge during inference. However, this integration introduces new security vulnerabilities, particularly to poisoning attacks. Although prior work has explored various poisoning strategies, a thorough assessment of their practical threat to RAG systems remains missing. To address this gap, we propose the first comprehensive benchmark framework for evaluating poisoning attacks on RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10 expanded variants, along with 13 poisoning attack methods and 7 defense mechanisms, representing a broad spectrum of existing techniques. Using this benchmark, we conduct a comprehensive evaluation of all included attacks and defenses across the full dataset spectrum. Our findings show that while existing attacks perform well on standard QA datasets, their effectiveness drops significantly on the expanded versions. Moreover, our results demonstrate that various advanced RAG architectures, such as sequential, branching, conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning attacks. Notably, current defense techniques fail to provide robust protection, underscoring the pressing need for more resilient and generalizable defense strategies.",http://arxiv.org/abs/2505.18543v1,IT,Quantitative
MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning,"Despite the widespread exploration of Retrieval-Augmented Generation (RAG), its deployment in enterprises for domain-specific datasets remains limited due to poor answer accuracy. These corpora, often shielded behind firewalls in private enterprise knowledge bases, having complex, domain-specific terminology, rarely seen by LLMs during pre-training; exhibit significant semantic variability across domains (like networking, military, or legal, etc.), or even within a single domain like medicine, and thus result in poor context precision for RAG systems. Currently, in such situations, fine-tuning or RAG with fine-tuning is attempted, but these approaches are slow, expensive, and lack generalization for accuracy as the new domain-specific data emerges. We propose an approach for Enterprise Search that focuses on enhancing the retriever for a domain-specific corpus through hybrid query indexes and metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata generation pipeline using key concepts, topics, and acronyms, and then creates a metadata-enriched hybrid index with boosted search queries. This approach avoids overfitting and generalizes effectively across domains. On the PubMedQA benchmark for the biomedical domain, the proposed method achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results without fine-tuning and sets a new benchmark for zero-shot results while outperforming much larger models like GPT3.5. The results are even comparable to the best fine-tuned models on this dataset, and we further demonstrate the robustness and scalability of the approach by evaluating it on other Q&A datasets like SQuAD, NQ etc.",http://arxiv.org/abs/2505.18247v1,IT,Quantitative
Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs,"Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data.",http://arxiv.org/abs/2505.17217v1,IT,Quantitative
LifeIR at the NTCIR-18 Lifelog-6 Task,"In recent years, sharing lifelogs recorded through wearable devices such as sports watches and GoPros, has gained significant popularity. Lifelogs involve various types of information, including images, videos, and GPS data, revealing users' lifestyles, dietary patterns, and physical activities. The Lifelog Semantic Access Task(LSAT) in the NTCIR-18 Lifelog-6 Challenge focuses on retrieving relevant images from a large scale of users' lifelogs based on textual queries describing an action or event. It serves users' need to find images about a scenario in the historical moments of their lifelogs. We propose a multi-stage pipeline for this task of searching images with texts, addressing various challenges in lifelog retrieval. Our pipeline includes: filtering blurred images, rewriting queries to make intents clearer, extending the candidate set based on events to include images with temporal connections, and reranking results using a multimodal large language model(MLLM) with stronger relevance judgment capabilities. The evaluation results of our submissions have shown the effectiveness of each stage and the entire pipeline.",http://arxiv.org/abs/2505.20987v1,IT,Mixed
In-memory Incremental Maintenance of Provenance Sketches [extended version],"Provenance-based data skipping compactly over-approximates the provenance of a query using so-called provenance sketches and utilizes such sketches to speed-up the execution of subsequent queries by skipping irrelevant data. However, a sketch captured at some time in the past may become stale if the data has been updated subsequently. Thus, there is a need to maintain provenance sketches. In this work, we introduce In-Memory incremental Maintenance of Provenance sketches (IMP), a framework for maintaining sketches incrementally under updates. At the core of IMP is an incremental query engine for data annotated with sketches that exploits the coarse-grained nature of sketches to enable novel optimizations. We experimentally demonstrate that IMP significantly reduces the cost of sketch maintenance, thereby enabling the use of provenance sketches for a broad range of workloads that involve updates.",http://arxiv.org/abs/2505.20683v1,IT,Quantitative
MatTools: Benchmarking Large Language Models for Materials Science Tools,"Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.",http://arxiv.org/abs/2505.10852v1,IT,Quantitative
Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy,"Long-distance relationships (LDRs) have become more common in the last few decades, primarily among young adults pursuing educational or employment opportunities. A common way for couples in LDRs to spend time together is by playing multiplayer video games, which are often a shared hobby and therefore a preferred joint activity. However, games are relatively understudied in the context of relational maintenance for LDRs. In this work, we used a mixed-methods approach to collect data on the experiences of 13 couples in LDRs who frequently play games together. We investigated different values around various game mechanics and modalities and found significant differences in couple play styles, and also detail how couples appropriate game mechanics to express affection to each other virtually. We also created prototypes and design implications based on couples' needs surrounding the lack of physical sensation and memorabilia storage in most popular games.",http://arxiv.org/abs/2505.09509v1,IT,Mixed
InfoVids: Reimagining the Viewer Experience with Alternative Visualization-Presenter Relationships,"Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.",http://arxiv.org/abs/2505.03164v1,IT,Mixed
"Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes","Independent learners often struggle with sustaining focus and emotional regulation in unstructured or distracting settings. Although some rely on ambient aids such as music, ASMR, or visual backgrounds to support concentration, these tools are rarely integrated into cohesive, learner-centered systems. Moreover, existing educational technologies focus primarily on content adaptation and feedback, overlooking the emotional and sensory context in which learning takes place. Large language models have demonstrated powerful multimodal capabilities including the ability to generate and adapt text, audio, and visual content. Educational research has yet to fully explore their potential in creating personalized audiovisual learning environments. To address this gap, we introduce an AI-powered system that uses LLMs to generate personalized multisensory study environments. Users select or generate customized visual themes (e.g., abstract vs. realistic, static vs. animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs. novel sounds) to create immersive settings aimed at reducing distraction and enhancing emotional stability. Our primary research question investigates how combinations of personalized audiovisual elements affect learner cognitive load and engagement. Using a mixed-methods design that incorporates biometric measures and performance outcomes, this study evaluates the effectiveness of LLM-driven sensory personalization. The findings aim to advance emotionally responsive educational technologies and extend the application of multimodal LLMs into the sensory dimension of self-directed learning.",http://arxiv.org/abs/2505.03033v1,IT,Mixed
Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values,"The design and implementation of unit tests is a complex task many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. An optimized prompt was developed, that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision. This work highlights the importance of manual qualitative analysis as an essential complement to automation in unit test evaluation.",http://arxiv.org/abs/2505.09830v1,IT,Mixed
PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull Request Outcomes,"The rapid adoption of large language models (LLMs) like ChatGPT in software development has introduced new ways for developers to interact with AI, particularly in pull request workflows. While prior research has examined AI-generated code quality, there is limited understanding of how ChatGPT is utilized in real-world pull request decision-making and how its suggestions influence patch integration and rejection. To explore these aspects, we analyze self-admitted ChatGPT usage (SACU), where developers explicitly disclose their reliance on ChatGPT within pull request discussions. Our study examines 338 pull requests (285 merged, 53 closed) across 255 GitHub repositories, containing 645 ChatGPT-generated code snippets and 3,486 patches. We introduce PatchTrack, a classification tool that determines whether ChatGPT-generated patches were applied (PA, 115 cases), not applied (PN, 64 cases), or not suggested (NE, 106 cases). Our findings reveal that full adoption of ChatGPT-generated code is rare, developers frequently modify or selectively integrate AI-generated patches to align with project constraints, with a median integration rate of 25%. Through qualitative analysis, we identify key factors influencing patch integration and pull request rejection, including scope misalignment, maintainability concerns, redundant solutions, and procedural barriers such as incomplete documentation or administrative policies. By providing empirical insights into ChatGPT's role in pull request workflows, this study informs developers, maintainers, and educators on the evolving use of generative AI in collaborative software development. It also lays the groundwork for future research on optimizing AI-assisted development, improving transparency in AI adoption, and enhancing patch integration workflows.",http://arxiv.org/abs/2505.07700v1,IT,Mixed
A Defect Taxonomy for Infrastructure as Code: A Replication Study,"Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.",http://arxiv.org/abs/2505.01568v2,IT,Mixed
From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering,"Requirements Engineering (RE) is essential for developing complex and regulated software projects. Given the challenges in transforming stakeholder inputs into consistent software designs, Qualitative Data Analysis (QDA) provides a systematic approach to handling free-form data. However, traditional QDA methods are time-consuming and heavily reliant on manual effort. In this paper, we explore the use of Large Language Models (LLMs), including GPT-4, Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs' performance in inductive (zero-shot) and deductive (one-shot, few-shot) annotation tasks, revealing that GPT-4 achieves substantial agreement with human analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7, while zero-shot performance remains limited. Detailed, context-rich prompts significantly improve annotation accuracy and consistency, particularly in deductive scenarios, and GPT-4 demonstrates high reliability across repeated runs. These findings highlight the potential of LLMs to support QDA in RE by reducing manual effort while maintaining annotation quality. The structured labels automatically provide traceability of requirements and can be directly utilized as classes in domain models, facilitating systematic software design.",http://arxiv.org/abs/2504.19384v1,IT,Qualitative
Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation,"Despite recent advances in Large Language Models (LLMs) for code generation, the quality of LLM-generated code still faces significant challenges. One significant issue is code repetition, which refers to the model's tendency to generate structurally redundant code, resulting in inefficiencies and reduced readability. To address this, we conduct the first empirical study to investigate the prevalence and nature of repetition across 19 state-of-the-art code LLMs using three widely-used benchmarks. Our study includes both quantitative and qualitative analyses, revealing that repetition is pervasive and manifests at various granularities and extents, including character, statement, and block levels. We further summarize a taxonomy of 20 repetition patterns. Building on our findings, we propose DeRep, a rule-based technique designed to detect and mitigate repetition in generated code. We evaluate DeRep using both open-source benchmarks and in an industrial setting. Our results demonstrate that DeRep significantly outperforms baselines in reducing repetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3, rep-line, and sim-line metrics) and enhancing code quality (with a Pass@1 increase of 208.3% over greedy search). Furthermore, integrating DeRep improves the performance of existing repetition mitigation methods, with Pass@1 improvements ranging from 53.7% to 215.7%.",http://arxiv.org/abs/2504.12608v1,IT,Mixed
Code Generation with Small Language Models: A Deep Evaluation on Codeforces,"Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws.",http://arxiv.org/abs/2504.07343v1,IT,Qualitative
"Open, Small, Rigmarole -- Evaluating Llama 3.2 3B's Feedback for Programming Exercises","Large Language Models (LLMs) have been subject to extensive research in the past few years. This is particularly true for the potential of LLMs to generate formative programming feedback for novice learners at university. In contrast to Generative AI (GenAI) tools based on LLMs, such as GPT, smaller and open models have received much less attention. Yet, they offer several benefits, as educators can let them run on a virtual machine or personal computer. This can help circumvent some major concerns applicable to other GenAI tools and LLMs (e. g., data protection, lack of control over changes, privacy). Therefore, this study explores the feedback characteristics of the open, lightweight LLM Llama 3.2 (3B). In particular, we investigate the models' responses to authentic student solutions to introductory programming exercises written in Java. The generated output is qualitatively analyzed to help evaluate the feedback's quality, content, structure, and other features. The results provide a comprehensive overview of the feedback capabilities and serious shortcomings of this open, small LLM. We further discuss the findings in the context of previous research on LLMs and contribute to benchmarking recently available GenAI tools and their feedback for novice learners of programming. Thereby, this work has implications for educators, learners, and tool developers attempting to utilize all variants of LLMs (including open, and small models) to generate formative feedback and support learning.",http://arxiv.org/abs/2504.01054v1,IT,Qualitative
"Closing the Chain: How to reduce your risk of being SolarWinds, Log4j, or XZ Utils","Software supply chain frameworks, such as the US NIST Secure Software Development Framework (SSDF), detail what tasks software development organizations should adopt to reduce security risk. However, to further reduce the risk of similar attacks occurring, framework adopters (i.e., software organizations) would benefit from knowing what tasks mitigate attack techniques the attackers are currently using to help organizations prioritize and to indicate current framework task gaps that leave organizations vulnerable to attacks. The goal of this study is to aid software supply chain framework adopters in reducing the risk of attacks by systematically mapping the attack techniques used in the SolarWinds, Log4j, and XZ Utils attacks to mitigating framework tasks. We qualitatively analyzed 106 Cyber Threat Intelligence (CTI) reports of the 3 attacks to gather the attack techniques. We then systematically constructed a mapping between attack techniques and the 73 tasks enumerated in 10 software supply chain frameworks. Afterward, we established and ranked priority tasks that mitigate attack techniques. The three mitigation tasks with the highest scores are role-based access control, system monitoring, and boundary protection. Additionally, three mitigation tasks were missing from all ten frameworks, including sustainable open-source software and environmental scanning tools. Thus, software products would still be vulnerable to software supply chain attacks even if organizations adopted all recommended tasks.",http://arxiv.org/abs/2503.12192v1,IT,Qualitative
Reducing Friction in Cloud Migration of Services,"Public cloud services are integral to modern software development, offering scalability and flexibility to organizations. Based on customer requests, a large-scale product development organization considered migrating the microservice-based product deployments of a large customer to a public cloud provider. We conducted an exploratory single-case study, utilizing quantitative and qualitative data analysis to understand how and why deployment costs would change when transitioning the product from a private to a public cloud environment while preserving the software architecture. We also isolated the major factors driving the changes in deployment costs. We found that switching to the customer-chosen public cloud provider would increase costs by up to 50\%, even when sharing some resources between deployments, and limiting the use of expensive cloud services such as security log analyzers. A large part of the cost was related to the sizing and license costs of the existing relational database, which was running on Virtual Machines in the cloud. We also found that existing system integrators, using the product via its API, were likely to use the product inefficiently, in many cases causing at least 10\% more load to the system than needed. From a deployment cost perspective, successful migration to a public cloud requires considering the entire system architecture, including services like relational databases, value-added cloud services, and enabled product features. Our study highlights the importance of leveraging end-to-end usage data to assess and manage these cost drivers effectively, especially in environments with elastic costs, such as public cloud deployments.",http://arxiv.org/abs/2503.07169v1,IT,Mixed
Streamlining Security Vulnerability Triage with Large Language Models,"Bug triaging for security vulnerabilities is a critical part of software maintenance, ensuring that the most pressing vulnerabilities are addressed promptly to safeguard system integrity and user data. However, the process is resource-intensive and comes with challenges, including classifying software vulnerabilities, assessing their severity, and managing a high volume of bug reports. In this paper, we present CASEY, a novel approach that leverages Large Language Models (in our case, the GPT model) that automates the identification of Common Weakness Enumerations (CWEs) of security bugs and assesses their severity. CASEY employs prompt engineering techniques and incorporates contextual information at varying levels of granularity to assist in the bug triaging process. We evaluated CASEY using an augmented version of the National Vulnerability Database (NVD), employing quantitative and qualitative metrics to measure its performance across CWE identification, severity assessment, and their combined analysis. CASEY achieved a CWE identification accuracy of 68%, a severity identification accuracy of 73.6%, and a combined accuracy of 51.2% for identifying both. These results demonstrate the potential of LLMs in identifying CWEs and severity levels, streamlining software vulnerability management, and improving the efficiency of security vulnerability triaging workflows.",http://arxiv.org/abs/2501.18908v1,IT,Mixed
Insights from Publishing Open Data in Industry-Academia Collaboration,"Effective data management and sharing are critical success factors in industry-academia collaboration. This paper explores the motivations and lessons learned from publishing open data sets in such collaborations. Through a survey of participants in a European research project that published 13 data sets, and an analysis of metadata from almost 281 thousand datasets in Zenodo, we collected qualitative and quantitative results on motivations, achievements, research questions, licences and file types. Through inductive reasoning and statistical analysis we found that planning the data collection is essential, and that only few datasets (2.4%) had accompanying scripts for improved reuse. We also found that authors are not well aware of the importance of licences or which licence to choose. Finally, we found that data with a synthetic origin, collected with simulations and potentially mixed with real measurements, can be very meaningful, as predicted by Gartner and illustrated by many datasets collected in our research project.",http://arxiv.org/abs/2501.14841v1,IT,Mixed
Do Automated Fixes Truly Mitigate Smart Contract Exploits?,"Automated Program Repair (APR) for smart contract security promises to automatically mitigate smart contract vulnerabilities responsible for billions in financial losses. However, the true effectiveness of this research in addressing smart contract exploits remains uncharted territory. This paper bridges this critical gap by introducing a novel and systematic experimental framework for evaluating exploit mitigation of program repair tools for smart contracts. We qualitatively and quantitatively analyze 20 state-of-the-art APR tools using a dataset of 143 vulnerable smart contracts, for which we manually craft 91 executable exploits. We are the very first to define and measure the essential ""exploit mitigation rate"" , giving researchers and practitioners a real sense of effectiveness of cutting edge techniques. Our findings reveal substantial disparities in the state of the art, with an exploit mitigation rate ranging from a low of 29% to a high of 74%. Our study identifies systemic limitations, such as inconsistent functionality preservation, that must be addressed in future research on program repair for smart contracts.",http://arxiv.org/abs/2501.04600v3,IT,Mixed
Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE,"This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at https://doi.org/10.6084/m9.figshare.26367139.",http://arxiv.org/abs/2412.14306v3,IT,Mixed
Pre-Release Experimentation in Indie Game Development: An Interview Survey,"[Background] The game industry faces fierce competition and games are developed on short deadlines and tight budgets. Continuously testing and experimenting with new ideas and features is essential in validating and guiding development toward market viability and success. Such continuous experimentation (CE) requires user data, which is often limited in early development stages. This challenge is further exacerbated for independent (indie) game companies with limited resources. [Aim] We wanted to gain insights into CE practices in pre-release indie game development. [Method] We performed an exploratory interview survey with 10 indie game developers from different companies and synthesised findings through an iterative coding process. [Results] We present a CE framework for game development that highlights key parts to consider when planning and implementing an experiment and note that pre-release experimentation is centred on qualitative data. Time and resource constraints impose limits on the type and extent of experimentation and playtesting that indie companies can perform, e.g. due to limited access to participants, biases and representativeness of the target audience. [Conclusions] Our results outline challenges and practices for conducting experiments with limited user data in early stages of indie game development, and may be of value also for larger game companies, and for software intensive organisations in other industries.",http://arxiv.org/abs/2411.17183v1,IT,Mixed
A Flexible Infrastructure-Sharing 5G Network Architecture Based on Network Slicing and Roaming,"The sharing of mobile network infrastructure has become a key topic with the introduction of 5G due to the high costs of deploying such infrastructures, with neutral host models coupled with features such as network function virtualization (NFV) and network slicing emerging as viable solutions for the challenges in this area. With this in mind, this work presents the design, implementation, and test of a flexible infrastructure-sharing 5G network architecture capable of providing services to any type of client, whether an operator or not. The proposed architecture leverages 5G's network slicing for traffic isolation and compliance with the policies of different clients, with roaming employed for the authentication of users of operator clients. The proposed architecture was implemented and tested in a simulation environment using the UERANSIM and Open5GS open-source tools. Qualitative tests successfully validated the authentication and the traffic isolation features provided by the slices for the two types of clients. Results also demonstrate that the proposed architecture has a positive impact on the performance of the neutral host network infrastructure, achieving 61.8% higher throughput and 96.8% lower packet loss ratio (PLR) in a scenario sharing the infrastructure among four clients and eight users when compared to a single client with all the network resources.",http://arxiv.org/abs/2411.15505v1,IT,Mixed
CI/CD Configuration Practices in Open-Source Android Apps: An Empirical Study,"Continuous Integration and Continuous Delivery (CI/CD) is a well-established practice that automatically builds, tests, packages, and deploys software systems. To adopt CI/CD, software developers need to configure their projects using dedicated YML configuration files. Mobile apps have distinct characteristics with respect to CI/CD practices, such as testing on various emulators and deploying to app stores. However, little is known about the challenges and added value of adopting CI/CD in mobile apps and how developers maintain such a practice. In this paper, we conduct an empirical study on CI/CD practices in 2,557 Android apps adopting four popular CI/CD services, namely GitHub Actions, Travis CI, CircleCI, and GitLab CI/CD. We also compare our findings with those reported in prior research on general CI/CD practices to situate them within broader trends. We observe a lack of commonality and standardization across CI/CD services and Android apps, leading to complex YML configurations and associated maintenance efforts. We also observe that CI/CD configurations focus primarily on the build setup, with around half of the projects performing standard testing and only 9% incorporating deployment. In addition, we find that CI/CD configurations are changed bi-monthly on average, with frequent maintenance correlating with active issue tracking, project size/age, and community engagement. Our qualitative analysis of commits uncovered 11 themes in CI/CD maintenance activities, with over a third of the changes focusing on improving workflows and fixing build issues, whereas another third involves updating the build environment, tools, and dependencies. Our study emphasizes the necessity for automation and AI-powered tools to improve CI/CD processes for mobile apps and advocates creating adaptable open-source tools to efficiently manage resources, especially in testing and deployment.",http://arxiv.org/abs/2411.06077v2,IT,Mixed
Load Balancing in Strongly Inhomogeneous Simulations -- a Vlasiator Case Study,"Parallelization is a necessity for large-scale simulations due to the amount of data processed. In this article we investigate different load balancing methods using Vlasiator, a global magnetospheric simulation as our case study. The theoretical basis for load balancing is the (hyper)graph partitioning problem, modeling simulation units as vertices and their data dependencies as edges. As it is an NP-hard problem, heuristics are necessary for dynamic runtime balancing. We consider first hypergraph partitioning via an algorithm called parallel hypergraph partitioner (PHG); this is done by partitioning a simplified grid and then attempting to optimize the solution on the finer grid. The second and third are the geometric methods of recursive coordinate bisection (RCB) and recursive inertial bisection (RIB). Finally we consider the method of Hilbert space filling curves (HSFC). The algorithm projects simulation cells along a Hilbert curve and makes cuts along the curve. This works well due to the excellent locality of Hilbert curves, and can be optimized further by choice of curve. We introduce and investigate six three-dimensional Hilbert curves in total. Our findings on runs of two different scales indicate the HSFC method provides optimal load balance, followed by RIB and PHG methods and finally by RCB. Of the Hilbert curves evaluated, the Beta curve outperformed the most commonly used curve by a few percent.",http://arxiv.org/abs/2505.20908v1,IT,Mixed
Evaluating Large Language Models for Code Review,"Context: Code reviews are crucial for software quality. Recent AI advances have allowed large language models (LLMs) to review and fix code; now, there are tools that perform these reviews. However, their reliability and accuracy have not yet been systematically evaluated. Objective: This study compares different LLMs' performance in detecting code correctness and suggesting improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated code blocks of varying correctness, along with 164 canonical code blocks from the HumanEval benchmark. To simulate the code review task objectively, we expected LLMs to assess code correctness and improve the code if needed. We ran experiments with different configurations and reported on the results. Results: With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code correctness 68.50% and 63.89% of the time, respectively, and corrected the code 67.83% and 54.26% of the time for the 492 code blocks of varying correctness. Without problem descriptions, performance declined. The results for the 164 canonical code blocks differed, suggesting that performance depends on the type of code. Conclusion: LLM code reviews can help suggest improvements and assess correctness, but there is a risk of faulty outputs. We propose a process that involves humans, called the ""Human in the loop LLM Code Review"" to promote knowledge sharing while mitigating the risk of faulty outputs.",http://arxiv.org/abs/2505.20206v1,IT,Quantitative
From Few to Many Faults: Adaptive Byzantine Agreement with Optimal Communication,"Achieving agreement among distributed parties is a fundamental task in modern systems, underpinning applications such as consensus in blockchains, coordination in cloud infrastructure, and fault tolerance in critical services. However, this task can be communication-intensive, often requiring a large number of messages to be exchanged, especially in the presence of Byzantine faults, making efficiency a central challenge in the design of practical agreement protocols. In this paper, we study the problem of Strong Byzantine Agreement and establish tight upper and lower bounds on communication complexity, parameterized by the actual number of Byzantine faults. Specifically, for a system of $n$ parties tolerating up to $t$ Byzantine faults, out of which only $f \leq t$ are actually faulty, we obtain the following results: In the partially synchronous setting, we present the first Byzantine Agreement protocol that achieves adaptive communication complexity of $\mathcal{O}(n + t \cdot f)$ words, which is asymptotically optimal. Our protocol has an optimal resilience of $t < n/3$. In the asynchronous setting, we prove a lower bound of $\Omega(n + t^2)$ on the expected number of messages, and design an almost matching protocol with an optimal resilience that solves agreement with $\mathcal{O}((n + t^2)\cdot \log n)$ words. Our main technical contribution in the asynchronous setting is the utilization of a bipartite expander graph that allows for low-cost information dissemination.",http://arxiv.org/abs/2505.19989v1,IT,Mixed
A Cost-efficient Credit-Based Shaper Deployment Framework for Time-Sensitive Networks,"Time-sensitive networks are designed to meet stringent Quality of Service (QoS) requirements for mixed-criticality traffic with diverse performance demands. Ensuring deterministic guarantees for such traffic while reducing deployment costs remains a significant challenge. This paper proposes a cost-efficient partial deployment strategy for Time Sensitive Networking (TSN) devices within legacy Ethernet network. At the core of our approach is the Credit-Based Shaper (CBS), a key TSN scheduling mechanism. Unlike cost-prohibitive full CBS deployment, our approach selectively integrates CBS where it is most needed to enhance performance while reducing costs. Combining Network Calculus for schedulability verification and a heuristic optimization method for CBS configuration and placement, our proposal minimizes deployment costs while improving schedulability for medium-priority traffic and mitigating blocking delays for high-priority traffic. The feasibility and benefits of our approach are validated on a realistic automotive TSN use case with up to 70% of reduction in TSN devices requirements compared to a full deployment.",http://arxiv.org/abs/2505.19771v1,IT,Quantitative
Evaluation and Performance Analysis of the Ryu Controller in Various Network Scenarios,"Software-defined networking (SDN) represents a revolutionary shift in network technology by decoupling the data plane from the control plane.}In this architecture, all network decision-making processes are centralized in a controller, meaning each switch receives routing information from the controller and forwards network packets accordingly. This clearly highlights the crucial role that controllers play in the overall performance of SDN. Ryu is one of the most widely used SDN controllers, known for its ease of use in research due to its support for Python programming. This makes Ryu a suitable option for experimental and academic studies. In this research, we evaluate the performance of the Ryu controller based on various network metrics and across different network topologies. For experimental analysis, we use Mininet, a powerful network emulation tool that enables the creation of diverse network structures and the connection of switches to controllers. To facilitate the experiments, we developed a Python-based script that executes various network scenarios, connects to different controllers, and captures and stores the results. This study not only provides a comprehensive performance evaluation of the Ryu controller but also paves the way for evaluating other SDN controllers in future research.",http://arxiv.org/abs/2505.19290v1,IT,Quantitative
BSAGIoT: A Bayesian Security Aspect Graph for Internet of Things (IoT),"IoT is a dynamic network of interconnected things that communicate and exchange data, where security is a significant issue. Previous studies have mainly focused on attack classifications and open issues rather than presenting a comprehensive overview on the existing threats and vulnerabilities. This knowledge helps analyzing the network in the early stages even before any attack takes place. In this paper, the researchers have proposed different security aspects and a novel Bayesian Security Aspects Dependency Graph for IoT (BSAGIoT) to illustrate their relations. The proposed BSAGIoT is a generic model applicable to any IoT network and contains aspects from five categories named data, access control, standard, network, and loss. This proposed Bayesian Security Aspect Graph (BSAG) presents an overview of the security aspects in any given IoT network. The purpose of BSAGIoT is to assist security experts in analyzing how a successful compromise and/or a failed breach could impact the overall security and privacy of the respective IoT network. In addition, root cause identification of security challenges, how they affect one another, their impact on IoT networks via topological sorting, and risk assessment could be achieved. Hence, to demonstrate the feasibility of the proposed method, experimental results with various scenarios has been presented, in which the security aspects have been quantified based on the network configurations. The results indicate the impact of the aspects on each other and how they could be utilized to mitigate and/or eliminate the security and privacy deficiencies in IoT networks.",http://arxiv.org/abs/2505.19283v1,IT,Quantitative
From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?,"Post-processing is crucial for the automatic evaluation of LLMs in fill-in-the-middle (FIM) code generation due to the frequent presence of extraneous code in raw outputs. This extraneous generation suggests a lack of awareness regarding output boundaries, requiring truncation for effective evaluation. The determination of an optimal truncation strategy, however, often proves intricate, particularly when the scope includes several programming languages. This study investigates the necessity of post-processing instruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning significantly enhances FIM code generation, enabling LLMs to generate code that seamlessly integrates with the surrounding context. Evaluating our fine-tuned \texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and SAFIM benchmarks demonstrates improved performances without post-processing, especially when the \emph{middle} consist of complete lines. However, post-processing of the LLM outputs remains necessary when the \emph{middle} is a random span of code.",http://arxiv.org/abs/2505.18789v1,IT,Quantitative
ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source Software Supply Chain,"Many critical information technology and cyber-physical systems rely on a supply chain of open-source software projects. OSS project maintainers often integrate contributions from external actors. While maintainers can assess the correctness of a change request, assessing a change request's cybersecurity implications is challenging. To help maintainers make this decision, we propose that the open-source ecosystem should incorporate Actor Reputation Metrics (ARMS). This capability would enable OSS maintainers to assess a prospective contributor's cybersecurity reputation. To support the future instantiation of ARMS, we identify seven generic security signals from industry standards; map concrete metrics from prior work and available security tools, describe study designs to refine and assess the utility of ARMS, and finally weigh its pros and cons.",http://arxiv.org/abs/2505.18760v1,IT,Mixed
Neutral-Hosts In The Shared Mid-Bands: Addressing Indoor Cellular Performance,"The 3.55 - 3.7 GHz Citizens Broadband Radio Service (CBRS) band in the U.S., shared with incumbent Navy radars, is witnessing increasing deployments both indoors and outdoors using a shared, licensed model. Among the many use-cases of such private networks is the indoor neutral-host, where cellular customers of Mobile Network Operators (MNOs) can be seamlessly served indoors over CBRS with improved performance, since building loss reduces the indoor signal strength of mid-band 5G cellular signals considerably. In this paper, we present the first detailed measurements and analyses of a real-world deployment of an indoor private network serving as a neutral-host in the CBRS band serving two MNOs. Our findings demonstrate significant advantages: (i) minimal outdoor interference from the CBRS network due to over 22 dB median penetration loss, ensuring compatibility with incumbent users; (ii) substantial indoor performance gains with up to 535$\times$ and 33$\times$ median downlink and uplink throughput improvements, respectively, compared to the worst-performing MNO; (iii) reduced uplink transmit power for user devices (median 12 dB reduction), increasing energy efficiency; and (iv) significant capacity offload from the MNO network (median 233 resource blocks/slot freed in 5G), allowing MNOs to better serve outdoor users. These results highlight the potential of low-power indoor CBRS deployments to improve performance, increase spectrum efficiency, and support coexistence with current and future incumbents, e.g., the 3.1 - 3.45 GHz band being considered for sharing with federal incumbents in the U.S.",http://arxiv.org/abs/2505.18360v1,IT,Quantitative
EtherBee: A Global Dataset of Ethereum Node Performance Measurements Coupled with Honeypot Interactions and Full Network Sessions,"We introduce EtherBee, a global dataset integrating detailed Ethereum node metrics, network traffic metadata, and honeypot interaction logs collected from ten geographically diverse vantage points over three months. By correlating node data with granular network sessions and security events, EtherBee provides unique insights into benign and malicious activity, node stability, and network-level threats in the Ethereum peer-to-peer network. A case study shows how client-based optimizations can unintentionally concentrate the network geographically, impacting resilience and censorship resistance. We publicly release EtherBee to promote further investigations into performance, reliability, and security in decentralized networks.",http://arxiv.org/abs/2505.18290v1,IT,Mixed
An Empirical Analysis of Vulnerability Detection Tools for Solidity Smart Contracts Using Line Level Manually Annotated Vulnerabilities,"The rapid adoption of blockchain technology highlighted the importance of ensuring the security of smart contracts due to their critical role in automated business logic execution on blockchain platforms. This paper provides an empirical evaluation of automated vulnerability analysis tools specifically designed for Solidity smart contracts. Leveraging the extensive SmartBugs 2.0 framework, which includes 20 analysis tools, we conducted a comprehensive assessment using an annotated dataset of 2,182 instances we manually annotated with line-level vulnerability labels. Our evaluation highlights the detection effectiveness of these tools in detecting various types of vulnerabilities, as categorized by the DASP TOP 10 taxonomy. We evaluated the effectiveness of a Large Language Model-based detection method on two popular datasets. In this case, we obtained inconsistent results with the two datasets, showing unreliable detection when analyzing real-world smart contracts. Our study identifies significant variations in the accuracy and reliability of different tools and demonstrates the advantages of combining multiple detection methods to improve vulnerability identification. We identified a set of 3 tools that, combined, achieve up to 76.78\% found vulnerabilities taking less than one minute to run, on average. This study contributes to the field by releasing the largest dataset of manually analyzed smart contracts with line-level vulnerability annotations and the empirical evaluation of the greatest number of tools to date.",http://arxiv.org/abs/2505.15756v1,IT,Quantitative
UniSTPA: A Safety Analysis Framework for End-to-End Autonomous Driving,"As autonomous driving technology continues to advance, end-to-end models have attracted considerable attention owing to their superior generalisation capability. Nevertheless, such learning-based systems entail numerous safety risks throughout development and on-road deployment, and existing safety-analysis methods struggle to identify these risks comprehensively. To address this gap, we propose the Unified System Theoretic Process Analysis (UniSTPA) framework, which extends the scope of STPA from the operational phase to the entire lifecycle of an end-to-end autonomous driving system, including information gathering, data preparation, closed loop training, verification, and deployment. UniSTPA performs hazard analysis not only at the component level but also within the model's internal layers, thereby enabling fine-grained assessment of inter and intra module interactions. Using a highway Navigate on Autopilot function as a case study, UniSTPA uncovers multi-stage hazards overlooked by conventional approaches including scene design defects, sensor fusion biases, and internal model flaws, through multi-level causal analysis, traces these hazards to deeper issues such as data quality, network architecture, and optimisation objectives. The analysis result are used to construct a safety monitoring and safety response mechanism that supports continuous improvement from hazard identification to system optimisation. The proposed framework thus offers both theoretical and practical guidance for the safe development and deployment of end-to-end autonomous driving systems.",http://arxiv.org/abs/2505.15005v1,IT,Qualitative
Sibling Prefixes: Identifying Similarities in IPv4 and IPv6 Prefixes,"Since the standardization of IPv6 in 1998, both versions of the Internet Protocol have coexisted in the Internet. Clients usually run algorithms such as Happy Eyeballs, to decide whether to connect to an IPv4 or IPv6 endpoint for dual-stack domains. To identify whether two addresses belong to the same device or service, researchers have proposed different forms of alias resolution techniques. Similarly, one can also form siblings of IPv4 and IPv6 addresses belonging to the same device. Traditionally, all of these approaches have focused on individual IP addresses. In this work, we propose the concept of ""sibling prefixes"", where we extend the definition of an IPv4-IPv6 sibling to two IP prefixe-one IPv4 prefix and its sibling IPv6 prefix. We present a technique based on large-scale DNS resolution data to identify 76k IPv4-IPv6 sibling prefixes. We find sibling prefixes to be relatively stable over time. We present SP-Tuner algorithm to tune the CIDR size of sibling prefixes and improve the perfect match siblings from 52% to 82%. For more than half of sibling prefixes, the organization names for their IPv4 and IPv6 origin ASes are identical, and 60% of all sibling prefixes have at least one of the prefixes with a valid ROV status in RPKI. Furthermore, we identify sibling prefixes in 24 hypergiant and CDN networks. Finally, we plan to regularly publish a list of sibling prefixes to be used by network operators and fellow researchers in dual-stack studies.",http://arxiv.org/abs/2505.14199v1,IT,Mixed
Are requirements really all you need? A case study of LLM-driven configuration code generation for automotive simulations,"Large Language Models (LLMs) are taking many industries by storm. They possess impressive reasoning capabilities and are capable of handling complex problems, as shown by their steadily improving scores on coding and mathematical benchmarks. However, are the models currently available truly capable of addressing real-world challenges, such as those found in the automotive industry? How well can they understand high-level, abstract instructions? Can they translate these instructions directly into functional code, or do they still need help and supervision? In this work, we put one of the current state-of-the-art models to the test. We evaluate its performance in the task of translating abstract requirements, extracted from automotive standards and documents, into configuration code for CARLA simulations.",http://arxiv.org/abs/2505.13263v1,IT,Quantitative
Optimizing Retrieval Augmented Generation for Object Constraint Language,"The Object Constraint Language (OCL) is essential for defining precise constraints within Model-Based Systems Engineering (MBSE). However, manually writing OCL rules is complex and time-consuming. This study explores the optimization of Retrieval-Augmented Generation (RAG) for automating OCL rule generation, focusing on the impact of different retrieval strategies. We evaluate three retrieval approaches $\unicode{x2013}$ BM25 (lexical-based), BERT-based (semantic retrieval), and SPLADE (sparse-vector retrieval) $\unicode{x2013}$ analyzing their effectiveness in providing relevant context for a large language model. To further assess our approach, we compare and benchmark our retrieval-optimized generation results against PathOCL, a state-of-the-art graph-based method. We directly compare BM25, BERT, and SPLADE retrieval methods with PathOCL to understand how different retrieval methods perform for a unified evaluation framework. Our experimental results, focusing on retrieval-augmented generation, indicate that while retrieval can enhance generation accuracy, its effectiveness depends on the retrieval method and the number of retrieved chunks (k). BM25 underperforms the baseline, whereas semantic approaches (BERT and SPLADE) achieve better results, with SPLADE performing best at lower k values. However, excessive retrieval with high k parameter can lead to retrieving irrelevant chunks which degrades model performance. Our findings highlight the importance of optimizing retrieval configurations to balance context relevance and output consistency. This research provides insights into improving OCL rule generation using RAG and underscores the need for tailoring retrieval.",http://arxiv.org/abs/2505.13129v1,IT,Quantitative
Minos: Exploiting Cloud Performance Variation with Function-as-a-Service Instance Selection,"Serverless Function-as-a-Service (FaaS) is a popular cloud paradigm to quickly and cheaply implement complex applications. Because the function instances cloud providers start to execute user code run on shared infrastructure, their performance can vary. From a user perspective, slower instances not only take longer to complete, but also increase cost due to the pay-per-use model of FaaS services where execution duration is billed with microsecond accuracy. In this paper, we present Minos, a system to take advantage of this performance variation by intentionally terminating instances that are slow. Fast instances are not terminated, so that they can be re-used for subsequent invocations. One use case for this are data processing and machine learning workflows, which often download files as a first step, during which Minos can run a short benchmark. Only if the benchmark passes, the main part of the function is actually executed. Otherwise, the request is re-queued and the instance crashes itself, so that the platform has to assign the request to another (potentially faster) instance. In our experiments, this leads to a speedup of up to 13% in the resource intensive part of a data processing workflow, resulting in up to 4% faster overall performance (and consequently 4% cheaper prices). Longer and complex workflows lead to increased savings, as the pool of fast instances is re-used more often. For platforms exhibiting this behavior, users get better performance and save money by wasting more of the platforms resources.",http://arxiv.org/abs/2505.12928v1,IT,Quantitative
Exploring Requirements Elicitation from App Store User Reviews Using Large Language Models,"Mobile applications have become indispensable companions in our daily lives. Spanning over the categories from communication and entertainment to healthcare and finance, these applications have been influential in every aspect. Despite their omnipresence, developing apps that meet user needs and expectations still remains a challenge. Traditional requirements elicitation methods like user interviews can be time-consuming and suffer from limited scope and subjectivity. This research introduces an approach leveraging the power of Large Language Models (LLMs) to analyze user reviews for automated requirements elicitation. We fine-tuned three well-established LLMs BERT, DistilBERT, and GEMMA, on a dataset of app reviews labeled for usefulness. Our evaluation revealed BERT's superior performance, achieving an accuracy of 92.40% and an F1-score of 92.39%, demonstrating its effectiveness in accurately classifying useful reviews. While GEMMA displayed a lower overall performance, it excelled in recall (93.39%), indicating its potential for capturing a comprehensive set of valuable user insights. These findings suggest that LLMs offer a promising avenue for streamlining requirements elicitation in mobile app development, leading to the creation of more user-centric and successful applications.",http://arxiv.org/abs/2409.15473v1,IT,Mixed
Centralization potential of automotive E/E architectures,"Current automotive E/E architectures are subject to significant transformations: Computing-power-intensive advanced driver-assistance systems, bandwidth-hungry infotainment systems, the connection of the vehicle with the internet and the consequential need for cyber-security drives the centralization of E/E architectures. A centralized architecture is often seen as a key enabler to master those challenges. Available research focuses mostly on the different types of E/E architectures and contrasts their advantages and disadvantages. There is a research gap on guidelines for system designers and function developers to analyze the potential of their systems for centralization. The present paper aims to quantify centralization potential reviewing relevant literature and conducting qualitative interviews with industry practitioners. In literature, we identified seven key automotive system properties reaching limitations in current automotive architectures: busload, functional safety, computing power, feature dependencies, development and maintenance costs, error rate, modularity and flexibility. These properties serve as quantitative evaluation criteria to estimate whether centralization would enhance overall system performance. In the interviews, we have validated centralization and its fundament - the conceptual systems engineering - as capabilities to mitigate these limitations. By focusing on practical insights and lessons learned, this research provides system designers with actionable guidance to optimize their systems, addressing the outlined challenges while avoiding monolithic architecture. This paper bridges the gap between theoretical research and practical application, offering valuable takeaways for practitioners.",http://arxiv.org/abs/2409.10690v1,IT,Mixed
A Deep Dive Into How Open-Source Project Maintainers Review and Resolve Bug Bounty Reports,"Researchers have investigated the bug bounty ecosystem from the lens of platforms, programs, and bug hunters. Understanding the perspectives of bug bounty report reviewers, especially those who historically lack a security background and little to no funding for bug hunters, is currently understudied. In this paper, we primarily investigate the perspective of open-source software (OSS) maintainers who have used \texttt{huntr}, a bug bounty platform that pays bounties to bug hunters who find security bugs in GitHub projects and have had valid vulnerabilities patched as a result. We address this area by conducting three studies: identifying characteristics through a listing survey ($n_1=51$), their ranked importance with Likert-scale survey data ($n_2=90$), and conducting semi-structured interviews to dive deeper into real-world experiences ($n_3=17$). As a result, we categorize 40 identified characteristics into benefits, challenges, helpful features, and wanted features. We find that private disclosure and project visibility are the most important benefits, while hunters focused on money or CVEs and pressure to review are the most challenging to overcome. Surprisingly, lack of communication with bug hunters is the least challenging, and CVE creation support is the second-least helpful feature for OSS maintainers when reviewing bug bounty reports. We present recommendations to make the bug bounty review process more accommodating to open-source maintainers and identify areas for future work.",http://arxiv.org/abs/2409.07670v2,IT,Mixed
Paradoxes of Openness and Trans Experiences in Open Source Software,"In recent years, concerns have increased over the lack of contributor diversity in open source software (OSS), despite its status as a paragon of open collaboration. OSS is an important form of digital infrastructure and part of a career path for many developers. While there exists a growing body of literature on cisgender women's under-representation in OSS, the experiences of contributors from other marginalized groups are comparatively absent from the literature. Such is the case for trans contributors, a historically influential group in OSS. In this study, we interviewed 21 trans participants to understand and represent their experiences in the OSS literature. From their experiences, we theorize two related paradoxes of openness in OSS: the paradox of openness and display and the paradox of openness and governance. In an increasingly violent world for trans people, we draw on our theorizing to build recommendations for more inclusive and safer OSS projects for contributors.",http://arxiv.org/abs/2409.04511v1,IT,Qualitative
Universal Workers: A Vision for Eliminating Cold Starts in Serverless Computing,"Serverless computing enables developers to deploy code without managing infrastructure, but suffers from cold start overhead when initializing new function instances. Existing solutions such as ""keep-alive"" or ""pre-warming"" are costly and unreliable under bursty workloads. We propose universal workers, which are computational units capable of executing any function with minimal initialization overhead. Based on an analysis of production workload traces, our key insight is that requests in Function-as-a-Service (FaaS) platforms show a highly skewed distribution, with most requests invoking a small subset of functions. We exploit this observation to approximate universal workers through locality groups and three-tier caching (handler, install, import). With this work, we aim to enable more efficient and scalable FaaS platforms capable of handling diverse workloads with minimal initialization overhead.",http://arxiv.org/abs/2505.19880v1,IT,Qualitative
Effect of noise and topologies on multi-photon quantum protocols,"Quantum-augmented networks aim to use quantum phenomena to improve detection and protection against malicious actors in a classical communication network. This may include multiplexing quantum signals into classical fiber optical channels and incorporating purely quantum links alongside classical links in the network. In such hybrid networks, quantum protocols based on single photons become a bottleneck for transmission distances and data speeds, thereby reducing entire network performance. Furthermore, many of the security assumptions of the single-photon protocols do not hold up in practice because of the impossibility of manufacturing single-photon emitters. Multi-photon quantum protocols, on the other hand, are designed to operate under practical assumptions and do not require single photon emitters. As a result, they provide higher levels of security guarantees and longer transmission distances. However, the effect of channel and device noise on multiphoton protocols in terms of security, transmission distances, and bit rates has not been investigated. In this paper, we focus on channel noise and present our observations on the effect of various types of noise on multi-photon protocols. We also investigate the effect of topologies such as ring, star, and torus on the noise characteristics of the multi-photon protocols. Our results show the possible advantages of switching to multi-photon protocols and give insights into the repeater placement and topology choice for quantum-augmented networks.",http://arxiv.org/abs/2505.19270v1,IT,Qualitative
Learning to Focus: Context Extraction for Efficient Code Vulnerability Detection with Language Models,"Language models (LMs) show promise for vulnerability detection but struggle with long, real-world code due to sparse and uncertain vulnerability locations. These issues, exacerbated by token limits, often cause models to miss vulnerability-related signals, thereby impairing effective learning. A key intuition is to enhance LMs with concise, information-rich context. Commit-based annotations offer precise, CWE-agnostic supervision, but are unavailable during inference, as they depend on historical code changes. Moreover, their extreme sparsity, often covering only a few lines, makes it difficult for LMs to process directly. In this paper, we propose FocusVul, a model-agnostic framework that improves LM-based vulnerability detection by learning to select sensitive context. FocusVul learns commit-based annotation patterns through hierarchical semantic modeling and generalizes them to identify line-level vulnerability-relevant regions during inference. It then extracts LM-oriented context via both dependency and execution flows surrounding selected regions, yielding semantically rich inputs for effective vulnerability detection. Experiments on real-world benchmarks show that FocusVul consistently outperforms heuristic-based and full-function fine-tuning approaches, improving classification performance by 164.04% and reducing FLOPs by 19.12% on average.",http://arxiv.org/abs/2505.17460v1,IT,Quantitative
LogStamping: A blockchain-based log auditing approach for large-scale systems,"Log management is crucial for ensuring the security, integrity, and compliance of modern information systems. Traditional log management solutions face challenges in achieving tamper-proofing, scalability, and real-time processing in distributed environments. This paper presents a blockchain-based log management framework that addresses these limitations by leveraging blockchain's decentralized, immutable, and transparent features. The framework integrates a hybrid on-chain and off-chain storage model, combining blockchain's integrity guarantees with the scalability of distributed storage solutions like IPFS. Smart contracts automate log validation and access control, while cryptographic techniques ensure privacy and confidentiality. With a focus on real-time log processing, the framework is designed to handle the high-volume log generation typical in large-scale systems, such as data centers and network infrastructure. Performance evaluations demonstrate the framework's scalability, low latency, and ability to manage millions of log entries while maintaining strong security guarantees. Additionally, the paper discusses challenges like blockchain storage overhead and energy consumption, offering insights for enhancing future systems.",http://arxiv.org/abs/2505.17236v1,IT,Quantitative
The Power of Alternatives in Network Embedding,"In the virtual network embedding problem, the goal is to map embed a set of virtual network instances to a given physical network substrate at minimal cost, while respecting the capacity constraints of the physical network. This NP-hard problem is fundamental to network virtualization, embodying essential properties of resource allocation problems faced by service providers in the edge-to-cloud spectrum. Due to its centrality, this problem and its variants have been extensively studied and remain in the focus of the research community. In this paper, we present a new variant, the virtual network embedding with alternatives problem (VNEAP). This new problem captures the power of a common network virtualization practice, in which virtual network topologies are malleable - embedding of a given virtual network instance can be performed using any of the alternatives from a given set of topology alternatives. We provide two efficient heuristics for VNEAP and show that having multiple virtual network alternatives for the same application is superior to the best results known for the classic formulation. We conclude that capturing the problem domain via VNEAP can facilitate more efficient network virtualization solutions.",http://arxiv.org/abs/2505.09753v1,IT,Mixed
Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models,"Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.",http://arxiv.org/abs/2505.09062v1,IT,Quantitative
Benchmarking and Revisiting Code Generation Assessment: A Mutation-Based Approach,"Code Large Language Models (CLLMs) have exhibited outstanding performance in program synthesis, attracting the focus of the research community. The evaluation of CLLM's program synthesis capability has generally relied on manually curated benchmarks. However, there is a substantial gap between real-world scenarios and benchmark settings. Existing benchmarks typically provide only a single input prompt for the evaluation of each synthesis problem. However, in practice, a problem can be described in various ways, including with typos, where developers may struggle to understand certain descriptions and seek clarification to find more suitable wording. Such various descriptions may lead to variations in the performance of CLLMs on the same question, resulting in a biased evaluation when using existing benchmarks. In this paper, we aim to explore these pitfalls with the goal of revisiting and enhancing future benchmark designs. To simulate real-world variations in problem descriptions, we propose 10 mutation strategies and introduce three new metrics to evaluate their impact on code generation. We then assess five popular CLLMs using 12,834 generated prompt variants, and found a significant performance discrepancy between the results from existing benchmarks and those from mutated benchmarks containing perturbations and variations. This finding underscores the need for more robust evaluation methods and benchmarks.",http://arxiv.org/abs/2505.06880v1,IT,Quantitative
OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution,"The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.",http://arxiv.org/abs/2505.04606v1,IT,Quantitative
"Running a Data Integration Lab in the Context of the EHRI Project: Challenges, Lessons Learnt and Future Directions","Historical study of the Holocaust is commonly hampered by the dispersed and fragmented nature of important archival sources relating to this event. The EHRI project set out to mitigate this problem by building a trans-national network of archives, researchers, and digital practitioners, and one of its main outcomes was the creation of the EHRI Portal, a ""virtual observatory"" that gathers in one centralised platform descriptions of Holocaust-related archival sources from around the world. In order to build the Portal a strong data identification and integration effort was required, culminating in the project's third phase with the creation of the EHRI-3 data integration lab. The focus of the lab was to lower the bar to participation in the EHRI Portal by providing support to institutions in conforming their archival metadata with that required for integration, ultimately opening the process up to smaller institutions (and even so-called ""micro-archives"") without the necessary resources to undertake this process themselves. In this paper we present our experiences from running the data integration lab and discuss some of the challenges (both of a technical and social nature), how we tried to overcome them, and the overall lessons learnt. We envisage this work as an archetype upon which other practitioners seeking to pursue similar data integration activities can build their own efforts.",http://arxiv.org/abs/2505.02455v1,IT,Mixed
DBSCAN-based Vehicle Clustering and UAV Placement for NOMA-based Resource Management in Cellular V2X Communications,"In the future wireless networks, terrestrial, aerial, space, and maritime wireless networks are integrated into a unified network to meet the needs of a fully connected global network. Nowadays, vehicular communication has become one of the challenging applications of wireless networks. In this article, we aim to address the radio resource management in Cellular V2X (C-V2X) networks using Unmanned Aerial Vehicles (UAV) and Non-orthogonal multiple access (NOMA). The goal of this problem is to maximize the spectral efficiency of vehicular users in Cellular Vehicle-to-Everything (C-V2X) networks under a fronthaul constraint. To solve this problem, a two-stage approach is utilized. In the first stage, vehicles in dense area are clustered based on their geographical locations, predicted location of vehicles, and speeds. Then UAVs are deployed to serve the clusters. In the second stage, NOMA groups are formed within each cluster and radio resources are allocated to vehicles based on NOMA groups. An optimization problem is formulated and a suboptimal method is used to solve it. The performance of the proposed method is evaluated through simulations where results demonstrate superiority of proposed method in spectral efficiency, min point, and distance.",http://arxiv.org/abs/2504.21656v1,IT,Quantitative
A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense,"With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.",http://arxiv.org/abs/2504.21480v1,IT,Quantitative
Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications,"Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/",http://arxiv.org/abs/2504.20801v2,IT,Mixed
Formal and Empirical Study of Metadata-Based Profiling for Resource Management in the Computing Continuum,"We present and formalize a general approach for profiling workload by leveraging only a priori available static metadata to supply appropriate resource needs. Understanding the requirements and characteristics of a workload's runtime is essential. Profiles are essential for the platform (or infrastructure) provider because they want to ensure that Service Level Agreements and their objectives (SLOs) are fulfilled and, at the same time, avoid allocating too many resources to the workload. When the infrastructure to manage is the computing continuum (i.e., from IoT to Edge to Cloud nodes), there is a big problem of placement and tradeoff or distribution and performance. Still, existing techniques either rely on static predictions or runtime profiling, which are proven to deliver poor performance in runtime environments or require laborious mechanisms to produce fast and reliable evaluations. We want to propose a new approach for it. Our profile combines the information from past execution traces with the related workload metadata, equipping an infrastructure orchestrator with a fast and precise association of newly submitted workloads. We differentiate from previous works because we extract the profile group metadata saliency from the groups generated by grouping similar runtime behavior. We first formalize its functioning and its main components. Subsequently, we implement and empirically analyze our proposed technique on two public data sources: Alibaba cloud machine learning workloads and Google cluster data. Despite relying on partially anonymized or obscured information, the approach provides accurate estimates of workload runtime behavior in real-time.",http://arxiv.org/abs/2504.20740v1,IT,Quantitative
Do Automatic Comment Generation Techniques Fall Short? Exploring the Influence of Method Dependencies on Code Understanding,"Method-level comments are critical for improving code comprehension and supporting software maintenance. With advancements in large language models (LLMs), automated comment generation has become a major research focus. However, existing approaches often overlook method dependencies, where one method relies on or calls others, affecting comment quality and code understandability. This study investigates the prevalence and impact of dependent methods in software projects and introduces a dependency-aware approach for method-level comment generation. Analyzing a dataset of 10 popular Java GitHub projects, we found that dependent methods account for 69.25% of all methods and exhibit higher engagement and change proneness compared to independent methods. Across 448K dependent and 199K independent methods, we observed that state-of-the-art fine-tuned models (e.g., CodeT5+, CodeBERT) struggle to generate comprehensive comments for dependent methods, a trend also reflected in LLM-based approaches like ASAP. To address this, we propose HelpCOM, a novel dependency-aware technique that incorporates helper method information to improve comment clarity, comprehensiveness, and relevance. Experiments show that HelpCOM outperforms baseline methods by 5.6% to 50.4% across syntactic (e.g., BLEU), semantic (e.g., SentenceBERT), and LLM-based evaluation metrics. A survey of 156 software practitioners further confirms that HelpCOM significantly improves the comprehensibility of code involving dependent methods, highlighting its potential to enhance documentation, maintainability, and developer productivity in large-scale systems.",http://arxiv.org/abs/2504.19459v1,IT,Quantitative
Technical Challenges in Maintaining Tax Prep Software with Large Language Models,"As the US tax law evolves to adapt to ever-changing politico-economic realities, tax preparation software plays a significant role in helping taxpayers navigate these complexities. The dynamic nature of tax regulations poses a significant challenge to accurately and timely maintaining tax software artifacts. The state-of-the-art in maintaining tax prep software is time-consuming and error-prone as it involves manual code analysis combined with an expert interpretation of tax law amendments. We posit that the rigor and formality of tax amendment language, as expressed in IRS publications, makes it amenable to automatic translation to executable specifications (code). Our research efforts focus on identifying, understanding, and tackling technical challenges in leveraging Large Language Models (LLMs), such as ChatGPT and Llama, to faithfully extract code differentials from IRS publications and automatically integrate them with the prior version of the code to automate tax prep software maintenance.",http://arxiv.org/abs/2504.18693v1,IT,Quantitative
Direct Feature Access -- Scaling Network Traffic Feature Collection to Terabit Speed,"Real-time traffic monitoring is critical for network operators to ensure performance, security, and visibility, especially as encryption becomes the norm. AI and ML have emerged as powerful tools to create deeper insights from network traffic, but collecting the fine-grained features needed at terabit speeds remains a major bottleneck. We introduce Direct Feature Access (DFA): a high-speed telemetry system that extracts flow features at line rate using P4-programmable data planes, and delivers them directly to GPUs via RDMA and GPUDirect, completely bypassing the ML server's CPU. DFA enables feature enrichment and immediate inference on GPUs, eliminating traditional control plane bottlenecks and dramatically reducing latency. We implement DFA on Intel Tofino switches and NVIDIA A100 GPUs, achieving extraction and delivery of over 31 million feature vectors per second, supporting 524,000 flows within sub-20 ms monitoring periods, on a single port. DFA unlocks scalable, real-time, ML-driven traffic analysis at terabit speeds, pushing the frontier of what is possible for next-generation network monitoring.",http://arxiv.org/abs/2505.17573v1,IT,Quantitative
Multimodal Online Federated Learning with Modality Missing in Internet of Things,"The Internet of Things (IoT) ecosystem generates vast amounts of multimodal data from heterogeneous sources such as sensors, cameras, and microphones. As edge intelligence continues to evolve, IoT devices have progressed from simple data collection units to nodes capable of executing complex computational tasks. This evolution necessitates the adoption of distributed learning strategies to effectively handle multimodal data in an IoT environment. Furthermore, the real-time nature of data collection and limited local storage on edge devices in IoT call for an online learning paradigm. To address these challenges, we introduce the concept of Multimodal Online Federated Learning (MMO-FL), a novel framework designed for dynamic and decentralized multimodal learning in IoT environments. Building on this framework, we further account for the inherent instability of edge devices, which frequently results in missing modalities during the learning process. We conduct a comprehensive theoretical analysis under both complete and missing modality scenarios, providing insights into the performance degradation caused by missing modalities. To mitigate the impact of modality missing, we propose the Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype learning to effectively compensate for missing modalities. Experimental results on two multimodal datasets further demonstrate the superior performance of PMM compared to benchmarks.",http://arxiv.org/abs/2505.16138v1,IT,Quantitative
"Automated, Cross-Layer Root Cause Analysis of 5G Video-Conferencing Quality Degradation","5G wireless networks are complex, leveraging layers of scheduling, retransmission, and adaptation mechanisms to maximize their efficiency. But these mechanisms interact to produce significant fluctuations in uplink and downlink capacity and latency. This markedly impacts the performance of real-time applications, such as video-conferencing, which are particularly sensitive to such fluctuations, resulting in lag, stuttering, distorted audio, and low video quality. This paper presents a cross-layer view of 5G networks and their impact on and interaction with video-conferencing applications. We conduct novel, detailed measurements of both Private CBRS and commercial carrier cellular network dynamics, capturing physical- and link-layer events and correlating them with their effects at the network and transport layers, and the video-conferencing application itself. Our two datasets comprise days of low-rate campus-wide Zoom telemetry data, and hours of high-rate, correlated WebRTC-network-5G telemetry data. Based on these data, we trace performance anomalies back to root causes, identifying 24 previously unknown causal event chains that degrade 5G video conferencing. Armed with this knowledge, we build Domino, a tool that automates this process and is user-extensible to future wireless networks and interactive applications.",http://arxiv.org/abs/2505.14540v1,IT,Quantitative
ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs,"Serverless computing has grown rapidly for serving Large Language Model (LLM) inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid scaling. However, our analysis reveals that current serverless can effectively serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to three key limitations: 1) massive parameter redundancy among functions where 99% of weights are unnecessarily duplicated, 2) costly artifact loading latency beyond LLM loading, and 3) magnified resource contention when serving multiple LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased Time-To-First-Token (TTFT), and high monetary costs. We propose ServerlessLoRA, a novel serverless inference system designed for faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM sharing across isolated LoRA functions to reduce redundancy. We design a pre-loading method that pre-loads comprehensive LoRA artifacts to minimize cold-start latency. Furthermore, ServerlessLoRA employs contention aware batching and offloading to mitigate GPU resource conflicts during bursty workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to state-of-the-art LLM inference solutions.",http://arxiv.org/abs/2505.14468v1,IT,Quantitative
Paradigm Shift in Infrastructure Inspection Technology: Leveraging High-performance Imaging and Advanced AI Analytics to Inspect Road Infrastructure,"Effective road infrastructure management is crucial for modern society. Traditional manual inspection techniques remain constrained by cost, efficiency, and scalability, while camera and laser imaging methods fail to capture subsurface defects critical for long-term structural integrity. This paper introduces ROVAI, an end-to-end framework that integrates high-resolution X-ray computed tomography imaging and advanced AI-driven analytics, aiming to transform road infrastructure inspection technologies. By leveraging the computational power of world-leading supercomputers, Fugaku and Frontier, and SoTA synchrotron facility (Spring-8), ROVAI enables scalable and high-throughput processing of massive 3D tomographic datasets. Our approach overcomes key challenges, such as the high memory requirements of vision models, the lack of labeled training data, and storage I/O bottlenecks. This seamless integration of imaging and AI analytics facilitates automated defect detection, material composition analysis, and lifespan prediction. Experimental results demonstrate the effectiveness of ROVAI in real-world scenarios, setting a new standard for intelligent, data-driven infrastructure management.",http://arxiv.org/abs/2505.13955v1,IT,Quantitative
Learning Driven Elastic Task Multi-Connectivity Immersive Computing Systems,"In virtual reality (VR) environments, computational tasks exhibit an elastic nature, meaning they can dynamically adjust based on various user and system constraints. This elasticity is essential for maintaining immersive experiences; however, it also introduces challenges for communication and computing in VR systems. In this paper, we investigate elastic task offloading for multi-user edge-computing-enabled VR systems with multi-connectivity, aiming to maximize the computational energy-efficiency (computational throughput per unit of energy consumed). To balance the induced communication, computation, energy consumption, and quality of experience trade-offs due to the elasticity of VR tasks, we formulate a constrained stochastic computational energy-efficiency optimization problem that integrates the multi-connectivity/multi-user action space and the elastic nature of VR computational tasks. We formulate a centralized phasic policy gradient (CPPG) framework to solve the problem of interest online, using only prior elastic task offloading statistics (energy consumption, response time, and transmission time), and task information (i.e., task size and computational intensity), while observing the induced system performance (energy consumption and latency). We further extend our approach to decentralized learning by formulating an independent phasic policy gradient (IPPG) method and a decentralized shared multi-armed bandit (DSMAB) method. We train our methods with real-world 4G, 5G, and WiGig network traces and 360 video datasets to evaluate their performance in terms of response time, energy efficiency, scalability, and delivered quality of experience. We also provide a comprehensive analysis of task size and its effect on offloading policy and system performance. In particular, we show that CPPG reduces latency by 28% and energy consumption by 78% compared to IPPG.",http://arxiv.org/abs/2505.13331v1,IT,Quantitative
Effects of the Auto-Correlation of Delays on the Age of Information: A Gaussian Process Framework,"The age of information (AoI) has been studied actively in recent years as a performance measure for systems that require real-time performance, such as remote monitoring systems via communication networks. The theoretical analysis of the AoI is usually formulated based on explicit system modeling, such as a single-server queueing model. However, in general, the behavior of large-scale systems such as communication networks is complex, and it is usually difficult to express the delay using simple queueing models. In this paper, we consider a framework in which the sequence of delays is composed from a non-negative continuous-time stochastic process, called a virtual delay process, as a new modeling approach for the theoretical analysis of the AoI. Under such a framework, we derive an expression for the transient probability distribution of the AoI and further apply the theory of stochastic orders to prove that the high dependence of the sequence of delays leads to the degradation of AoI performance. We further consider a special case in which the sequence of delays is generated from a stationary Gaussian process, and we discuss the sensitivity of the AoI to second-order statistics of the delay process through numerical experiments.",http://arxiv.org/abs/2505.12885v1,IT,Quantitative
"Digital Twins in the Cloud: A Modular, Scalable and Interoperable Framework for Accelerating Verification and Validation of Autonomous Driving Solutions","Verification and validation (V&V) of autonomous vehicles (AVs) typically requires exhaustive testing across a variety of operating environments and driving scenarios including rare, extreme, or hazardous situations that might be difficult or impossible to capture in reality. Additionally, physical V&V methods such as track-based evaluations or public-road testing are often constrained by time, cost, and safety, which motivates the need for virtual proving grounds. However, the fidelity and scalability of simulation-based V&V methods can quickly turn into a bottleneck. In such a milieu, this work proposes a virtual proving ground that flexibly scales digital twins within high-performance computing clusters (HPCCs) and automates the V&V process. Here, digital twins enable high-fidelity virtual representation of the AV and its operating environments, allowing extensive scenario-based testing. Meanwhile, HPCC infrastructure brings substantial advantages in terms of computational power and scalability, enabling rapid iterations of simulations, processing and storage of massive amounts of data, and deployment of large-scale test campaigns, thereby reducing the time and cost associated with the V&V process. We demonstrate the efficacy of this approach through a case study that focuses on the variability analysis of a candidate autonomy algorithm to identify potential vulnerabilities in its perception, planning, and control sub-systems. The modularity, scalability, and interoperability of the proposed framework are demonstrated by deploying a test campaign comprising 256 test cases on two different HPCC architectures to ensure continuous operation in a publicly shared resource setting. The findings highlight the ability of the proposed framework to accelerate and streamline the V&V process, thereby significantly compressing (~30x) the timeline.",http://arxiv.org/abs/2505.12661v1,IT,Mixed
Modeling and Performance Analysis of IoT-over-LEO Satellite Systems under Realistic Operational Constraints: A Stochastic Geometry Approach,"Current theoretical studies on IoT-over-LEO satellite systems often rely on unrealistic assumptions, such as infinite terrestrial areas and omnidirectional satellite coverage, leaving significant gaps in theoretical analysis for more realistic operational constraints. These constraints involve finite terrestrial area, limited satellite coverage, Earth curvature effect, integral uplink and downlink analysis, and link-dependent interference. To address these gaps, this paper proposes a novel stochastic geometry based model to rigorously analyze the performance of IoT-over-LEO satellite systems. By adopting a binomial point process (BPP) instead of the conventional Poisson point process (PPP), our model accurately characterizes the geographical distribution of a fixed number of IoT devices in a finite terrestrial region. This modeling framework enables the derivation of distance distribution functions for both the links from the terrestrial IoT devices to the satellites (T-S) and from the satellites to the Earth station (S-ES), while also accounting for limited satellite coverage and Earth curvature effects. To realistically represent channel conditions, the Nakagami fading model is employed for the T-S links to characterize diverse small-scale fading environments, while the shadowed-Rician fading model is used for the S-ES links to capture the combined effects of shadowing and dominant line-of-sight paths. Furthermore, the analysis incorporates uplink and downlink interference, ensuring a comprehensive evaluation of system performance. The accuracy and effectiveness of our theoretical framework are validated through extensive Monte Carlo simulations. These results provide insights into key performance metrics, such as coverage probability and average ergodic rate, for both individual links and the overall system.",http://arxiv.org/abs/2505.12336v1,IT,Quantitative
Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA Networks via MARL,"Visible Light Communication (VLC) combined with Non-Orthogonal Multiple Access (NOMA) offers a promising solution for dense indoor wireless networks. Yet, managing resources effectively is challenged by VLC network dynamic conditions involving user mobility and light dimming. In addition to satisfying Quality of Service (QoS) and network stability requirements. Traditional resource allocation methods and simpler RL approaches struggle to jointly optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA networks. This paper presents MARL frameworks tailored to perform complex joint optimization of resource allocation (NOMA power, user scheduling) and network stability (interference, handovers), considering heterogeneous QoS, user mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic channel conditions and diverse user QoS , enabling effective joint optimization. In these frameworks, VLC access points (APs) act as intelligent agents, learning to allocate power and schedule users to satisfy diverse requirements while maintaining network stability by managing interference and minimizing disruptive handovers. We conduct a comparative analysis of two key MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and 2) Centralized Training with Centralized Execution (CTCE). Comprehensive simulations validate the effectiveness of both tailored MARL frameworks and demonstrate an ability to handle complex optimization. The results show key trade-offs, as the CTDE approach achieved approximately 16\% higher for High priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7 dB higher average SINR and 12\% lower ping-pong handover ratio, offering valuable insights into the performance differences between these paradigms in complex VLC-NOMA network scenarios.",http://arxiv.org/abs/2505.15841v1,IT,Quantitative
Enhancing Code Quality with Generative AI: Boosting Developer Warning Compliance,"Programmers have long ignored warnings, especially those generated by static analysis tools, due to the potential for false-positives. In some cases, warnings may be indicative of larger issues, but programmers may not understand how a seemingly unimportant warning can grow into a vulnerability. Because these messages tend to be long and confusing, programmers tend to ignore them if they do not cause readily identifiable issues. Large language models can simplify these warnings, explain the gravity of important warnings, and suggest potential fixes to increase developer compliance with fixing warnings.",http://arxiv.org/abs/2505.11677v1,IT,Quantitative
"MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems","The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.",http://arxiv.org/abs/2505.11415v2,IT,Quantitative
Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability,"The proliferation of AI agents requires robust mechanisms for secure discovery. This paper introduces the Agent Name Service (ANS), a novel architecture based on DNS addressing the lack of a public agent discovery framework. ANS provides a protocol-agnostic registry infrastructure that leverages Public Key Infrastructure (PKI) certificates for verifiable agent identity and trust. The architecture features several key innovations: a formalized agent registration and renewal mechanism for lifecycle management; DNS-inspired naming conventions with capability-aware resolution; a modular Protocol Adapter Layer supporting diverse communication standards (A2A, MCP, ACP etc.); and precisely defined algorithms for secure resolution. We implement structured communication using JSON Schema and conduct a comprehensive threat analysis of our proposal. The result is a foundational directory service addressing the core challenges of secured discovery and interaction in multi-agent systems, paving the way for future interoperable, trustworthy, and scalable agent ecosystems.",http://arxiv.org/abs/2505.10609v1,IT,Quantitative
UICopilot: Automating UI Synthesis via Hierarchical Code Generation from Webpage Designs,"Automating the synthesis of User Interfaces (UIs) plays a crucial role in enhancing productivity and accelerating the development lifecycle, reducing both development time and manual effort. Recently, the rapid development of Multimodal Large Language Models (MLLMs) has made it possible to generate front-end Hypertext Markup Language (HTML) code directly from webpage designs. However, real-world webpages encompass not only a diverse array of HTML tags but also complex stylesheets, resulting in significantly lengthy code. The lengthy code poses challenges for the performance and efficiency of MLLMs, especially in capturing the structural information of UI designs. To address these challenges, this paper proposes UICopilot, a novel approach to automating UI synthesis via hierarchical code generation from webpage designs. The core idea of UICopilot is to decompose the generation process into two stages: first, generating the coarse-grained HTML hierarchical structure, followed by the generation of fine-grained code. To validate the effectiveness of UICopilot, we conduct experiments on a real-world dataset, i.e., WebCode2M. Experimental results demonstrate that UICopilot significantly outperforms existing baselines in both automatic evaluation metrics and human evaluations. Specifically, statistical analysis reveals that the majority of human annotators prefer the webpages generated by UICopilot over those produced by GPT-4V.",http://arxiv.org/abs/2505.09904v2,IT,Quantitative
IoT-Enabled Hemodynamic Surveillance System: AD8232 Bioelectric Signal Processing with ESP32,"This dissertation proposes an electrocardiogram (ECG) tracking device that diagnoses cardiopulmonary problems using the Internet of Things (IoT) desired results. The initiative is built on the internet observing an electrocardiogram with the AD8232 heart rhythm sensor and the ESP32 expansion kit, using an on-premise connected device platform to transform sensing input into meaningful data. That subsequently supervises an ECG signal and delivers it to an intelligent phone via Wi-Fi for data analysis. That is the pace of the circulating. Assessing body temperature, pulse rate, and coronary arteries are vital measures to defend your health. The heartbeat rate may be measured in two ways: there are by palpating the pulse at the wrist or neck directly or other alternative by utilizing a cardiac sensor. Monitoring alcohol levels in cardiac patients is critical for measuring the influence of liquor on their health and the efficacy of therapy. It assists in recognizing the association between alcohol consumption and cardiac issues, rather than rhythm recorded in beats per minute (bpm). An IR transmitter/receiver pair (OLED) needs to stay compatible up near the sensor's knuckle current or voltage pulse. The detector's electrical output is evaluated by suitable electronic circuits to produce a visual clue (digital display). We must design a cost-effective, user-friendly, and efficient ECG monitoring system with contemporary technology for both persons imprisoned by disease or aging, as well as healthcare professionals. Microcontroller combined with software. A smartphone application is created to monitor the cardiovascular health of distant patients in real-time",http://arxiv.org/abs/2505.18173v1,IT,Quantitative
LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols,"Integrating large AI models (LAMs) into 6G mobile networks promises to redefine protocol design and control-plane intelligence by enabling autonomous, cognitive network operations. While industry concepts, such as ETSI's Experiential Networked Intelligence (ENI), envision LAM-driven agents for adaptive network slicing and intent-based management, practical implementations still face challenges in protocol literacy and real-world deployment. This paper presents an end-to-end demonstration of a LAM that generates standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as part of control-plane procedures inside a gNB. We treat RRC messaging as a domain-specific language and fine-tune a decoder-only transformer model (LLaMA class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages linearized to retain their ASN.1 syntactic structure before standard byte-pair encoding tokenization. This enables combinatorial generalization over RRC protocol states while minimizing training overhead. On 30k field-test request-response pairs, our 8 B model achieves a median cosine similarity of 0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural and semantic RRC fidelity. Overall, our results show that LAMs, when augmented with Radio Access Network (RAN)-specific reasoning, can directly orchestrate control-plane procedures, representing a stepping stone toward the AI-native air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for future AI-native wireless standards.",http://arxiv.org/abs/2505.16821v2,IT,Mixed
Graph Attention Network for Optimal User Association in Wireless Networks,"With increased 5G deployments, network densification is higher than ever to support the exponentially high throughput requirements. However, this has meant a significant increase in energy consumption, leading to higher operational expenditure (OpEx) for network operators creating an acute need for improvements in network energy savings (NES). A key determinant of operational efficacy in cellular networks is the user association (UA) policy, as it affects critical aspects like spectral efficiency, load balancing etc. and therefore impacts the overall energy consumption of the network directly. Furthermore, with cellular network topologies lending themselves well to graphical abstractions, use of graphs in network optimization has gained significant prominence. In this work, we propose and analyze a graphical abstraction based optimization for UA in cellular networks to improve NES by determining when energy saving features like cell switch off can be activated. A comparison with legacy approaches establishes the superiority of the proposed approach.",http://arxiv.org/abs/2505.16347v1,IT,Quantitative
LLM-guided DRL for Multi-tier LEO Satellite Networks with Hybrid FSO/RF Links,"Despite significant advancements in terrestrial networks, inherent limitations persist in providing reliable coverage to remote areas and maintaining resilience during natural disasters. Multi-tier networks with low Earth orbit (LEO) satellites and high-altitude platforms (HAPs) offer promising solutions, but face challenges from high mobility and dynamic channel conditions that cause unstable connections and frequent handovers. In this paper, we design a three-tier network architecture that integrates LEO satellites, HAPs, and ground terminals with hybrid free-space optical (FSO) and radio frequency (RF) links to maximize coverage while maintaining connectivity reliability. This hybrid approach leverages the high bandwidth of FSO for satellite-to-HAP links and the weather resilience of RF for HAP-to-ground links. We formulate a joint optimization problem to simultaneously balance downlink transmission rate and handover frequency by optimizing network configuration and satellite handover decisions. The problem is highly dynamic and non-convex with time-coupled constraints. To address these challenges, we propose a novel large language model (LLM)-guided truncated quantile critics algorithm with dynamic action masking (LTQC-DAM) that utilizes dynamic action masking to eliminate unnecessary exploration and employs LLMs to adaptively tune hyperparameters. Simulation results demonstrate that the proposed LTQC-DAM algorithm outperforms baseline algorithms in terms of convergence, downlink transmission rate, and handover frequency. We also reveal that compared to other state-of-the-art LLMs, DeepSeek delivers the best performance through gradual, contextually-aware parameter adjustments.",http://arxiv.org/abs/2505.11978v1,IT,Quantitative
Interplay Between AI and Space-Air-Ground Integrated Network: The Road Ahead,"Space-air-ground integrated network (SAGIN) is envisioned as a key network architecture for achieving ubiquitous coverage in the next-generation communication system. Concurrently, artificial intelligence (AI) plays a pivotal role in managing the complex control of SAGIN, thereby enhancing its automation and flexibility. Despite this, there remains a significant research gap concerning the interaction between AI and SAGIN. In this context, we first present a promising approach for developing a generalized AI model capable of executing multiple tasks simultaneously in SAGIN. Subsequently, we propose a framework that leverages software-defined networking (SDN) and AI technologies to manage the resources and services across the entire SAGIN. Particularly, we demonstrate the real-world applicability of our proposed framework through a comprehensive case study. These works pave the way for the deep integration of SAGIN and AI in future wireless networks.",http://arxiv.org/abs/2505.09259v1,IT,Qualitative
Multi-Layer Hierarchical Federated Learning with Quantization,"Almost all existing hierarchical federated learning (FL) models are limited to two aggregation layers, restricting scalability and flexibility in complex, large-scale networks. In this work, we propose a Multi-Layer Hierarchical Federated Learning framework (QMLHFL), which appears to be the first study that generalizes hierarchical FL to arbitrary numbers of layers and network architectures through nested aggregation, while employing a layer-specific quantization scheme to meet communication constraints. We develop a comprehensive convergence analysis for QMLHFL and derive a general convergence condition and rate that reveal the effects of key factors, including quantization parameters, hierarchical architecture, and intra-layer iteration counts. Furthermore, we determine the optimal number of intra-layer iterations to maximize the convergence rate while meeting a deadline constraint that accounts for both communication and computation times. Our results show that QMLHFL consistently achieves high learning accuracy, even under high data heterogeneity, and delivers notably improved performance when optimized, compared to using randomly selected values.",http://arxiv.org/abs/2505.08145v1,IT,Quantitative
Computing in Integrated Terrestrial and Non-Terrestrial Networks: A Comprehensive Survey,"The rapid growth of Internet-of-things (IoT) devices, smart vehicles, and other connected objects is driving demand for ubiquitous connectivity and intensive computing capacity. 5G and upcoming 6G networks are crucial to meeting these demands and the fast-evolving services and applications. However, traditional terrestrial networks face limitations in coverage and capacity. Integrated Terrestrial and Non-Terrestrial Networks (ITNTN) are emerging to address these challenges. In essence, ITNTN combines ground-based infrastructure with aerial, space, and water surface networks to provide seamless connectivity and computing resources anytime, anywhere. Given the stringent quality-of-service (QoS) of future services, edge computing will be an inseparable component of ITNTN. Consequently, we dive in this survey into current efforts of integrating cloud/fog/edge computing into ITNTN layers to facilitate stringent QoS services and address the data processing needs of modern applications. Since there have been only limited and partial efforts in integrating computing functionalities within ITNTN, we aim to extend the discussion to the full integration of computing and identifying the challenges and future research directions to achieve it.",http://arxiv.org/abs/2505.03016v1,IT,Quantitative
Antifragility of RIS-assisted Communication Systems under Jamming Attacks,"Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.",http://arxiv.org/abs/2505.02565v1,IT,Quantitative
RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search,"Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash. Prior works use random testing like fuzzing, but these are not effective in the three-dimensional search space of PID parameters. The expensive dynamic execution of UAV tests further hinders random testing performance. We address PID parameter misconfiguration by combining the Routh-Hurwitz stability criterion with coordinate search, introducing RouthSearch. Instead of ad-hoc identification, RouthSearch principledly determines valid ranges for three-dimensional PID parameters. We first leverage the Routh-Hurwitz Criterion to identify a theoretical PID parameter boundary, then refine it using efficient coordinate search. The determined valid range can filter misconfigured PID parameters from users during flight and help discover logical bugs in flight control programs. We evaluated RouthSearch across eight flight modes in PX4 and Ardupilot. Results show RouthSearch determines valid ranges with 92.0% accuracy compared to ground truth. RouthSearch discovers 3,853 PID misconfigurations within 48 hours, while the STOA work PGFuzz discovers only 449 sets, significantly outperforming prior works by 8.58 times. Our method also helped detect three bugs in ArduPilot and PX4.",http://arxiv.org/abs/2505.02357v1,IT,Quantitative
Semantics-Aware Unified Terrestrial Non-Terrestrial 6G Networks,"The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which was introduced in 5G, is progressing toward a unified and seamless network of networks in Sixth-Generation (6G). This evolution leads to a significant increase in the volume of generated and communicated data, imposing technical and operational requirements accompanied by a higher cost and energy consumption. Efficiently managing the generation and transmission of data in these highly complex unified networks has become essential. In this article, we investigate the semantics-aware information handling problem within unified TN-NTNs, where data communication between the distant TN nodes is enabled via an NTN. To this end, an Internet of Things (IoT) monitoring system is employed, where status updates from a remote IoT device are communicated to a destination monitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage semantic metrics that capture the timeliness, relevance, and utility of information to provide the most informative data for timely and informed decision-making and eventually reduce the volume of transmitted and processed data. The outcome is significantly lower energy consumption, memory, control, and processing requirements (up to 73% lower energy charging demands compared to the state-of-the-art), all without compromising the conveyed information.",http://arxiv.org/abs/2505.01796v1,IT,Mixed
Q Cells in Wireless Networks,"For a given set of transmitters such as cellular base stations or WiFi access points, is it possible to analytically characterize the set of locations that are ""covered"" in the sense that users at these locations experience a certain minimum quality of service? In this paper, we affirmatively answer this question, by providing explicit simple outer bounds and estimates for the coverage manifold. The key geometric elements of our analytical method are the Q cells, defined as the intersections of a small number of disks. The Q cell of a transmitter is an outer bound to the service region of the transmitter, and, in turn, the union of Q cells is an outer bound to the coverage manifold. In infinite networks, connections to the meta distribution of the signal-to-interference ratio allow for a scaling of the Q cells to obtain accurate estimates of the coverage manifold.",http://arxiv.org/abs/2505.00138v2,IT,Mixed
On Queueing Theory for Large-Scale CI/CD Pipelines Optimization,"Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software development. In large organizations, the high volume of builds and tests creates bottlenecks, especially under shared infrastructure. This article proposes a modeling framework based on queueing theory to optimize large-scale CI/CD workflows. We formalize the system using classical $M/M/c$ queueing models and discuss strategies to minimize delays and infrastructure costs. Our approach integrates theoretical results with practical techniques, including dynamic scaling and prioritization of CI/CD tasks.",http://arxiv.org/abs/2504.18705v1,IT,Mixed
Spatiotemporal Analysis of Parallelized Computing at the Extreme Edge,"Extreme Edge Computing (EEC) pushes computing even closer to end users than traditional Multi-access Edge Computing (MEC), harnessing the idle resources of Extreme Edge Devices (EEDs) to enable low-latency, distributed processing. However, EEC faces key challenges, including spatial randomness in device distribution, limited EED computational power necessitating parallel task execution, vulnerability to failure, and temporal randomness due to variability in wireless communication and execution times. These challenges highlight the need for a rigorous analytical framework to evaluate EEC performance. We present the first spatiotemporal mathematical model for EEC over large-scale millimeter-wave networks. Utilizing stochastic geometry and an Absorbing Continuous-Time Markov Chain (ACTMC), the framework captures the complex interaction between communication and computation performance, including their temporal overlap during parallel execution. We evaluate two key metrics: average task response delay and task completion probability. Together, they provide a holistic view of latency and reliability. The analysis considers fundamental offloading strategies, including randomized and location-aware schemes, while accounting for EED failures. Results show that there exists an optimal task segmentation that minimizes delay. Under limited EED availability, we investigate a bias-based EEC and MEC collaboration that offloads excess demand to MEC resources, effectively reducing congestion and improving system responsiveness.",http://arxiv.org/abs/2504.18047v1,IT,Quantitative
Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach,"Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints.",http://arxiv.org/abs/2504.18031v1,IT,Mixed
Balancing Costs and Utilities in Future Networks via Market Equilibrium with Externalities,"We study the problem of market equilibrium (ME) in future wireless networks, with multiple actors competing and negotiating for a pool of heterogeneous resources (communication and computing) while meeting constraints in terms of global cost. The latter is defined in a general way but is associated with energy and/or carbon emissions. In this direction, service providers competing for network resources do not acquire the latter, but rather the right to consume, given externally defined policies and regulations. We propose to apply the Fisher market model, and prove its convergence towards an equilibrium between utilities, regulatory constraints, and individual budgets. The model is then applied to an exemplary use case of access network, edge computing, and cloud resources, and numerical results assess the theoretical findings of convergence, under different assumptions on the utility function and more or less stringent constraints.",http://arxiv.org/abs/2504.16480v1,IT,Mixed
Performance Analysis of IEEE 802.11bn Non-Primary Channel Access,"This paper presents a performance analysis of the Non-Primary Channel Access (NPCA) mechanism, a new feature introduced in IEEE 802.11bn to enhance spectrum utilization in Wi-Fi networks. NPCA enables devices to contend for and transmit on the secondary channel when the primary channel is occupied by transmissions from an Overlapping Basic Service Set (OBSS). We develop a Continuous-Time Markov Chain (CTMC) model that captures the interactions among OBSSs in dense WLAN environments when NPCA is enabled, incorporating new NPCA-specific states and transitions. In addition to the analytical insights offered by the model, we conduct numerical evaluations and simulations to quantify NPCA's impact on throughput and channel access delay across various scenarios. Our results show that NPCA can significantly improve throughput and reduce access delays in favorable conditions for BSSs that support the mechanism. Moreover, NPCA helps mitigate the OBSS performance anomaly, where low-rate OBSS transmissions degrade network performance for all nearby devices. However, we also observe trade-offs: NPCA may increase contention on secondary channels, potentially reducing transmission opportunities for BSSs operating there.",http://arxiv.org/abs/2504.15774v1,IT,Quantitative
Multiscale Parallel Simulation of Malignant Pleural Mesothelioma via Adaptive Domain Partitioning -- an Efficiency Analysis Study,"A novel parallel efficiency analysis on a framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumours is presented. Proliferation of MPM tumours in the pleural space is simulated using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). Using segmented lung data from CT scans, an environment is set up with artificial tumour data in the pleural space, representing the simulation domain, onto which a dynamic bounding box is applied to restrict computations to the region of interest, dramatically reducing memory and CPU overhead. This adaptive partitioning of the domain enables efficient use of computational resources by reducing the three-dimensional (3D) domain over which the PDEs are to be solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is realized using the public Python library mpi4py in combination with LinearGMRESSolver and PETSc for efficient convergence. Performance analyses have shown that parallelization achieves a reduced solving time compared to serial computation. Also, optimizations enable efficient use of available memory and improved load balancing amongst the cores.",http://arxiv.org/abs/2505.03067v1,IT,Quantitative
Towards Polyglot Data Processing in Social Networks using the Hadoop-Spark ecosystem,"This article explores the use of the Hadoop-Spark ecosystem for social media data processing, adopting a polyglot approach with the integration of various computation and storage technologies, such as Hive, HBase and GraphX. We discuss specific tasks involved in processing social network data, such as calculating user influence, counting the most frequent terms in messages and identifying social relationships among users and groups. We conducted a series of empirical performance assessments, focusing on executing selected tasks and measuring their execution time within the Hadoop-Spark cluster. These insights offer a detailed quantitative analysis of the performance efficiency of the ecosystem tools. We conclude by highlighting the potential of the Hadoop-Spark ecosystem tools for advancing research in social networks and related fields.",http://arxiv.org/abs/2504.14314v1,IT,Quantitative
Towards End-to-End Network Intent Management with Large Language Models,"Large Language Models (LLMs) are likely to play a key role in Intent-Based Networking (IBN) as they show remarkable performance in interpreting human language as well as code generation, enabling the translation of high-level intents expressed by humans into low-level network configurations. In this paper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro, ChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their capacity to generate E2E network configurations for radio access networks (RANs) and core networks in 5G/6G mobile networks. We introduce a novel performance metrics, known as FEACI, to quantitatively assess the format (F), explainability (E), accuracy (A), cost (C), and inference time (I) of the generated answer; existing general metrics are unable to capture these features. The results of our study demonstrate that open-source models can achieve comparable or even superior translation performance compared with the closed-source models requiring costly hardware setup and not accessible to all users.",http://arxiv.org/abs/2504.13589v1,IT,Quantitative
Integrating Large Language Models for Automated Structural Analysis,"Automated analysis for engineering structures offers considerable potential for boosting efficiency by minimizing repetitive tasks. Although AI-driven methods are increasingly common, no systematic framework yet leverages Large Language Models (LLMs) for automatic structural analysis. To address this gap, we propose a novel framework that integrates LLMs with structural analysis software. LLMs serve as the core engine: they parse structural descriptions from text and translate them into executable Python scripts. Moreover, the framework integrates the generative capabilities of LLMs with code-based finite element (FE) tools like OpenSeesPy. It employs domain-specific prompt design and in-context learning strategies to enhance the LLM's problem-solving capabilities and generative stability, enabling fully automated structural analysis from descriptive text to model outputs. In our experiments, we introduce a well-curated small-scale benchmark dataset of 20 structural analysis word problems (SAWPs) with ground-truth solutions and evaluate the performance of different LLMs within our framework in solving these SAWPs. The role of system instructions, crafted by structural engineers, is also investigated to understand their impact on LLM-driven structural analysis. Additionally, the generative stability of our framework is examined. Through multiple validation experiments on the benchmark, our results demonstrate that the proposed framework can substantially increase the level of automation in solving SAWPs compared to traditional methods. Quantitatively, the framework, built on GPT-4o, achieved 100% accuracy, surpassing GPT-4 (85%), Gemini 1.5 Pro (80%), and Llama-3.3 (30%) on the test examples. Furthermore, integrating domain-specific instructions enhanced performance by 30% on problems with asymmetrical structural configurations.",http://arxiv.org/abs/2504.09754v1,IT,Quantitative
Potential gains of communication-compute-control co-design based performance optimization methods in cyber-physical systems,"In this paper we propose and quantitatively evaluate three performance optimization methods that exploit the concept of communication-compute-control co-design by introducing awareness of communication and compute characteristics into the application logic in different ways to improve overall system performance. We have implemented a closed-loop control of a robotic arm over a wireless network where the controller is deployed into an edge cloud environment. When implementing an industrial system that leverages network and cloud technologies, the level of determinism of the control application can be decreased by nature. This means that some imperfections may be introduced into the control system, and the closed-loop control in substance changes to open-loop during disturbances. We aim to improve the performance of these open-loop control periods by applying methods that can compensate for the imperfections statistically or in a guaranteed way. We demonstrate that co-design-based application improvements with minimal dependencies on the underlying technologies can already yield an order of magnitude gain when it comes to the accurate execution of the robot trajectories during the openloop control periods. Furthermore, by combining the proposed methods, the performance improvements add up and can produce up to 45% shorter trajectory executions compared to individual evaluations.",http://arxiv.org/abs/2503.03521v1,IT,Quantitative
Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification,"The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. Our study presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 63% improvement in inference speed with minimal accuracy degradation. Our findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.",http://arxiv.org/abs/2502.16627v4,IT,Quantitative
Improving SDN Performance Using Network Coding: A Quantitative Analysis,Software Defined Networking or SDN is an architectural approach to managing the network where the control and forwarding are different planes that are controlled through an application interface.,http://arxiv.org/abs/2502.00789v1,IT,Mixed
Unveiling Code Clone Patterns in Open Source VR Software: An Empirical Study,"Code cloning is frequently observed in software development, often leading to a variety of maintenance and security issues. While substantial research has been conducted on code cloning in traditional software, to the best of my knowledge, there is a lack of studies on cloning in VR software that consider its unique nature, particularly the presence of numerous serialized files in conjunction with the source code. In this paper, we conduct the first large-scale quantitative empirical analysis of software clones in 345 open-source VR projects, using the NiCad detector for source code clone detection and large language models (LLMs) for identifying serialized file clones. Our study leads to a number of insights into cloning phenomena in VR software, guided by seven carefully formulated research questions. These findings, along with their implications, are anticipated to provide useful guidance for both researchers and software developers within the VR field.",http://arxiv.org/abs/2501.07165v1,IT,Quantitative
A Survey on Open-Source Edge Computing Simulators and Emulators: The Computing and Networking Convergence Perspective,"Edge computing, with its low latency, dynamic scalability, and location awareness, along with the convergence of computing and communication paradigms, has been successfully applied in critical domains such as industrial IoT, smart healthcare, smart homes, and public safety. This paper provides a comprehensive survey of open-source edge computing simulators and emulators, presented in our GitHub repository (https://github.com/qijianpeng/awesome-edge-computing), emphasizing the convergence of computing and networking paradigms. By examining more than 40 tools, including CloudSim, NS-3, and others, we identify the strengths and limitations in simulating and emulating edge environments. This survey classifies these tools into three categories: packet-level, application-level, and emulators. Furthermore, we evaluate them across five dimensions, ranging from resource representation to resource utilization. The survey highlights the integration of different computing paradigms, packet processing capabilities, support for edge environments, user-defined metric interfaces, and scenario visualization. The findings aim to guide researchers in selecting appropriate tools for developing and validating advanced computing and networking technologies.",http://arxiv.org/abs/2505.09995v1,IT,Quantitative
Federated Learning for Cyber Physical Systems: A Comprehensive Survey,"The integration of machine learning (ML) in cyber physical systems (CPS) is a complex task due to the challenges that arise in terms of real-time decision making, safety, reliability, device heterogeneity, and data privacy. There are also open research questions that must be addressed in order to fully realize the potential of ML in CPS. Federated learning (FL), a distributed approach to ML, has become increasingly popular in recent years. It allows models to be trained using data from decentralized sources. This approach has been gaining popularity in the CPS field, as it integrates computer, communication, and physical processes. Therefore, the purpose of this work is to provide a comprehensive analysis of the most recent developments of FL-CPS, including the numerous application areas, system topologies, and algorithms developed in recent years. The paper starts by discussing recent advances in both FL and CPS, followed by their integration. Then, the paper compares the application of FL in CPS with its applications in the internet of things (IoT) in further depth to show their connections and distinctions. Furthermore, the article scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent transportation systems, cybersecurity services, smart cities, and smart healthcare solutions. The study also includes critical insights and lessons learned from various FL-CPS implementations. The paper's concluding section delves into significant concerns and suggests avenues for further research in this fast-paced and dynamic era.",http://arxiv.org/abs/2505.04873v1,IT,Quantitative
Taming the Titans: A Survey of Efficient LLM Inference Serving,"Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",http://arxiv.org/abs/2504.19720v1,IT,Quantitative
Performance Analysis of OpenVPN on a Consumer Grade Router,"Virtual Private Networks (VPNs) offer an alternative solution using Internet Protocol (IP) tunnels to create secure, encrypted communication between geographically distant networks using a common shared medium such as the Internet. They use tunneling to establish end-to-end connectivity. OpenVPN is a cross-platform, secure, highly configurable VPN solution. Security in OpenVPN is handled by the OpenSSL cryptographic library which provides strong security over a Secure Socket Layer (SSL) using standard algorithms such as Advanced Encryption Standard (AES), Blowfish, or Triple DES (3DES). The Linksys WRT54GL router is a consumer-grade router made by Linksys, a division of Cisco Systems, capable of running under Linux. The Linux-based DD-WRT open-source router firmware can run OpenVPN on the Linksys WRT54GL router. For this case study, the performance of OpenVPN is measured and analyzed using a $2^{k-p}$ fractional factorial design for 5 minus 1 factors where $k=5$ and $p=1$. The results show that the throughput is mainly limited by the encryption cipher used, and that the round-trip time (RTT) is mostly dependent on the transport protocol selected.",http://arxiv.org/abs/2504.19069v1,IT,Qualitative
The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges,"Based on recent trends in artificial intelligence and IoT research. The cooperative potential of integrating the Internet of Things (IoT) and Multimodal Language Models (MLLMs) is presented in this survey paper for future 6G systems. It focuses on the applications of this integration in different fields, such as healthcare, agriculture, and smart cities, and investigates the four pillars of IoT integration, such as sensors, communication, processing, and security. The paper provides a comprehensive description of IoT and MLLM technologies and applications, addresses the role of multimodality in each pillar, and concludes with an overview of the most significant challenges and directions for future research. The general survey is a roadmap for researchers interested in tracing the application areas of MLLMs and IoT, highlighting the potential and challenges in this rapidly growing field. The survey recognizes the need to deal with data availability, computational expense, privacy, and real-time processing to harness the complete potential of IoT, MLLM, and 6G technology",http://arxiv.org/abs/2504.13971v1,IT,Quantitative
Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant,"The integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) has the potential to enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a significant challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves and integrates course content and curriculum from Canvas LMS into the LLM-powered assistant, Ask ME. By employing prompt engineering to structure retrieved content within the LLM's context window, DCCI ensures accuracy, relevance, and contextual alignment, mitigating hallucination. To evaluate DCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI in education, a mixed-methods approach was employed, incorporating user satisfaction ratings and a structured survey. Results from a pilot study indicate high user satisfaction (4.614/5), with students recognizing Ask ME's ability to provide timely and contextually relevant responses for both administrative and course-related inquiries. Additionally, a majority of students agreed that Ask ME's integration with course content in Canvas LMS reduced platform-switching, improving usability, engagement, and comprehension. AI's role in reducing classroom hesitation and fostering self-directed learning and intellectual curiosity was also highlighted. Despite these benefits and positive perception of AI tools, concerns emerged regarding over-reliance on AI, accuracy limitations, and ethical issues such as plagiarism and reduced student-teacher interaction. These findings emphasize the need for strategic AI implementation, ethical safeguards, and a pedagogical framework that prioritizes human-AI collaboration over substitution.",http://arxiv.org/abs/2504.03966v1,IT,Quantitative
Towards Resilient Federated Learning in CyberEdge Networks: Recent Advances and Future Trends,"In this survey, we investigate the most recent techniques of resilient federated learning (ResFL) in CyberEdge networks, focusing on joint training with agglomerative deduction and feature-oriented security mechanisms. We explore adaptive hierarchical learning strategies to tackle non-IID data challenges, improving scalability and reducing communication overhead. Fault tolerance techniques and agglomerative deduction mechanisms are studied to detect unreliable devices, refine model updates, and enhance convergence stability. Unlike existing FL security research, we comprehensively analyze feature-oriented threats, such as poisoning, inference, and reconstruction attacks that exploit model features. Moreover, we examine resilient aggregation techniques, anomaly detection, and cryptographic defenses, including differential privacy and secure multi-party computation, to strengthen FL security. In addition, we discuss the integration of 6G, large language models (LLMs), and interoperable learning frameworks to enhance privacy-preserving and decentralized cross-domain training. These advancements offer ultra-low latency, artificial intelligence (AI)-driven network management, and improved resilience against adversarial attacks, fostering the deployment of secure ResFL in CyberEdge networks.",http://arxiv.org/abs/2504.01240v1,IT,Quantitative
"Reliability and Availability in Virtualized Networks: A Survey on Standards, Modeling Approaches, and Research Challenges","The rise of Network Function Virtualization (NFV) has transformed network infrastructures by replacing fixed hardware with software-based Virtualized Network Functions (VNFs), enabling greater agility, scalability, and cost efficiency. Virtualization increases the distribution of system components and introduces stronger interdependencies. As a result, failures become harder to predict, monitor, and manage compared to traditional monolithic networks. Reliability, i.e. the ability of a system to perform regularly under specified conditions, and availability, i.e. the probability of a system of being ready to use, are critical requirements that must be guaranteed to maintain seamless network operations. Accurate modeling of these aspects is crucial for designing robust, fault-tolerant virtualized systems that can withstand service disruptions. This survey focuses on reliability and availability attributes of virtualized networks from a modeling perspective. After introducing the NFV architecture and basic definitions, we discuss the standardization efforts of the European Telecommunications Standards Institute (ETSI), which provides guidelines and recommendations through a series of standard documents focusing on reliability and availability. Next, we explore several formalisms proposed in the literature for characterizing reliability and availability, with a focus on their application to modeling the failure and repair behavior of virtualized networks through practical examples. Then, we overview numerous references demonstrating how different authors adopt specific methods to characterize reliability and/or availability of virtualized systems. Moreover, we present a selection of the most valuable software tools that support modeling of reliable virtualized networks. Finally, we discuss a set of open problems with the aim to encourage readers to explore further advances in this field.",http://arxiv.org/abs/2503.22034v1,IT,Quantitative
Automatic High-Level Test Case Generation using Large Language Models,"We explored the challenges practitioners face in software testing and proposed automated solutions to address these obstacles. We began with a survey of local software companies and 26 practitioners, revealing that the primary challenge is not writing test scripts but aligning testing efforts with business requirements. Based on these insights, we constructed a use-case $\rightarrow$ (high-level) test-cases dataset to train/fine-tune models for generating high-level test cases. High-level test cases specify what aspects of the software's functionality need to be tested, along with the expected outcomes. We evaluated large language models, such as GPT-4o, Gemini, LLaMA 3.1 8B, and Mistral 7B, where fine-tuning (the latter two) yields improved performance. A final (human evaluation) survey confirmed the effectiveness of these generated test cases. Our proactive approach strengthens requirement-testing alignment and facilitates early test case generation to streamline development.",http://arxiv.org/abs/2503.17998v1,IT,Quantitative
"A Survey on the Landscape of Self-adaptive Cloud Design and Operations Patterns: Goals, Strategies, Tooling, Evaluation and Dataset Perspectives","Cloud-native applications have significantly advanced the development and scalability of online services through the use of microservices and modular architectures. However, achieving adaptability, resilience, and efficient performance management within cloud environments remains a key challenge. This survey provides an overview of self-adaptive cloud design and operations patterns published over the last seven years, focusing on a taxonomy of their objectives, scope of control, decision-making mechanisms approach, automation level and validation methodologies. Overall, 96 papers have been taken under consideration, indicating a significant increase in the years since 2023 in the produced output. The analysis highlights the prevalence of feedback loop structures, with both reactive and proactive implementations, and underscores the increasing role of machine learning techniques in predictive management, especially when it comes to resource provisioning and management of the executed applications. On the other hand, adaptive application architectures through direct application-level pattern-based management seem significantly underrepresented in the current field of research, thus serving as an uninvestigated area for future research. Furthermore, the current work highlights practical aspects such as validation datasets per category (application, resource, network, etc.), tools, technologies and frameworks usage during the experimentation, in order to guide researchers in the validation process for comparative and robust experimentation.",http://arxiv.org/abs/2503.06705v2,IT,Quantitative
Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models,"The rapid advancement of artificial intelligence (AI) technologies has led to an increasing deployment of AI models on edge and terminal devices, driven by the proliferation of the Internet of Things (IoT) and the need for real-time data processing. This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models. We define on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy. The survey is structured around key themes, including the fundamental concepts of AI models, application scenarios across various domains, and the technical challenges faced in edge environments. We also discuss optimization and implementation strategies, such as data preprocessing, model compression, and hardware acceleration, which are essential for effective deployment. Furthermore, we examine the impact of emerging technologies, including edge computing and foundation models, on the evolution of on-device AI models. By providing a structured overview of the challenges, solutions, and future directions, this survey aims to facilitate further research and application of on-device AI, ultimately contributing to the advancement of intelligent systems in everyday life.",http://arxiv.org/abs/2503.06027v2,IT,Quantitative
SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA,"Federated fine-tuning of large language models (LLMs) is critical for improving their performance in handling domain-specific tasks. However, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks. Existing privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities. In this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (HE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environment. Heterogeneous clients adaptively select partial model parameters for homomorphic encryption based on parameter sensitivity assessment, with the encryption subset obtained via negotiation. To ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients. Extensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to the state-of-the-art attacks, and significantly reduces communication overhead by 94.901\% and encryption computation overhead by 99.829\%, compared to baseline. Our code is accessible at https://anonymous.4open.science/r/SHE-LoRA-8D84.",http://arxiv.org/abs/2505.21051v1,IT,Quantitative
Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking,"Integrating multiple (sub-)systems is essential to create advanced Information Systems. Difficulties mainly arise when integrating dynamic environments, e.g., the integration at design time of not yet existing services. This has been traditionally addressed using a registry that provides the API documentation of the endpoints. Large Language Models have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input oken limitations, especially regarding comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. In the present work, we (i) analyze the usage of Retrieval Augmented Generation for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input oken length while preserving the most relevant information. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints nd retrieves specification details on demand. We evaluate RAG for endpoint discovery using (iii) a proposed novel service discovery benchmark SOCBench-D representing a general setting across numerous domains and the real-world RestBench enchmark, first, for the different chunking possibilities and parameters measuring the endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same test data set. The prototype shows how to successfully employ RAG for endpoint discovery to reduce the token count. Our experiments show that endpoint-based approaches outperform naive chunking methods for preprocessing. Relying on an agent significantly improves precision while being prone to decrease recall, disclosing the need for further reasoning capabilities.",http://arxiv.org/abs/2505.19310v1,IT,Quantitative
SEW: Self-Evolving Agentic Workflows for Automated Code Generation,"Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks. To enable LLMs to address more complex coding challenges, existing research has focused on crafting multi-agent systems with agentic workflows, where complex coding tasks are decomposed into sub-tasks, assigned to specialized agents. Despite their effectiveness, current approaches heavily rely on hand-crafted agentic workflows, with both agent topologies and prompts manually designed, which limits their ability to automatically adapt to different types of coding problems. To address these limitations and enable automated workflow design, we propose \textbf{S}elf-\textbf{E}volving \textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that automatically generates and optimises multi-agent workflows. Extensive experiments on three coding benchmark datasets, including the challenging LiveCodeBench, demonstrate that our SEW can automatically design agentic workflows and optimise them through self-evolution, bringing up to 33\% improvement on LiveCodeBench compared to using the backbone LLM only. Furthermore, by investigating different representation schemes of workflow, we provide insights into the optimal way to encode workflow information with text.",http://arxiv.org/abs/2505.18646v1,IT,Quantitative
Recursive Offloading for LLM Serving in Multi-tier Networks,"Heterogeneous device-edge-cloud computing infrastructures have become widely adopted in telecommunication operators and Wide Area Networks (WANs), offering multi-tier computational support for emerging intelligent services. With the rapid proliferation of Large Language Model (LLM) services, efficiently coordinating inference tasks and reducing communication overhead within these multi-tier network architectures becomes a critical deployment challenge. Existing LLM serving paradigms exhibit significant limitations: on-device deployment supports only lightweight LLMs due to hardware constraints, while cloud-centric deployment suffers from resource congestion and considerable prompt communication overhead caused by frequent service requests during peak periods. Although the model-cascading-based inference strategy adapts better to multi-tier networks, its reliance on fine-grained, manually adjusted thresholds makes it less responsive to dynamic network conditions and varying task complexities. To address these challenges, we propose RecServe, a recursive offloading framework tailored for LLM serving in multi-tier networks. RecServe integrates a task-specific hierarchical confidence evaluation mechanism that guides offloading decisions based on inferred task complexity in progressively scaled LLMs across device, edge, and cloud tiers. To further enable intelligent task routing across tiers, RecServe employs a sliding-window-based dynamic offloading strategy with quantile interpolation, enabling real-time tracking of historical confidence distributions and adaptive offloading threshold adjustments. Experiments on eight datasets demonstrate that RecServe outperforms CasServe in both service quality and communication efficiency, and reduces the communication burden by over 50\% compared to centralized cloud-based serving.",http://arxiv.org/abs/2505.16502v2,IT,Quantitative
Performance of Confidential Computing GPUs,"This work examines latency, throughput, and other metrics when performing inference on confidential GPUs. We explore different traffic patterns and scheduling strategies using a single Virtual Machine with one NVIDIA H100 GPU, to perform relaxed batch inferences on multiple Large Language Models (LLMs), operating under the constraint of swapping models in and out of memory, which necessitates efficient control. The experiments simulate diverse real-world scenarios by varying parameters such as traffic load, traffic distribution patterns, scheduling strategies, and Service Level Agreement (SLA) requirements. The findings provide insights into the differences between confidential and non-confidential settings when performing inference in scenarios requiring active model swapping. Results indicate that in No-CC mode, relaxed batch inference with model swapping latency is 20-30% lower than in confidential mode. Additionally, SLA attainment is 15-20% higher in No-CC settings. Throughput in No-CC scenarios surpasses that of confidential mode by 45-70%, and GPU utilization is approximately 50% higher in No-CC environments. Overall, performance in the confidential setting is inferior to that in the No-CC scenario, primarily due to the additional encryption and decryption overhead required for loading models onto the GPU in confidential environments.",http://arxiv.org/abs/2505.16501v1,IT,Quantitative
LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach,"Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling machines to create content across modalities. As GenAI evolves into autonomous agents capable of reasoning, collaboration, and interaction, they are increasingly deployed on network infrastructures to serve humans automatically. This emerging paradigm, known as the agentic network, presents new optimization challenges due to the demand to incorporate subjective intents of human users expressed in natural language. Traditional generic Deep Reinforcement Learning (DRL) struggles to capture intent semantics and adjust policies dynamically, thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI Model (LAM)-empowered Two-stage Approach for intent-aware agentic network optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD), which efficiently distills intent-understanding capabilities from resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users. Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs with a policy-based DRL framework. In SRL, E-LAMs translate natural language user intents into structured preference vectors that guide both state representation and reward design. The DRL, in turn, optimizes the generative service function chain composition and E-LAM selection based on real-time network conditions, thus optimizing the subjective Quality-of-Experience (QoE). Extensive experiments conducted in an agentic network with 81 agents demonstrate that IoKD reduces mean squared error in intent prediction by up to 22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in maximizing intent-aware QoE.",http://arxiv.org/abs/2505.12247v1,IT,Quantitative
Scalable Time-Tagged Data Acquisition for Entanglement Distribution in Quantum Networks,"In distributed quantum applications such as entanglement distribution, precise time synchronization and efficient time-tagged data handling are essential. Traditional systems often suffer from overflow, synchronization drift, and storage inefficiencies. We propose a modular Time Tagging (TT) agent that uses a 1 pulse per second (PPS) signal from White Rabbit (WR) devices to achieve network-wide synchronization, while applying real-time calibration, overflow mitigation, and compression. A live two-lab entanglement distribution experiment validated the system's performance, achieving synchronized coincidence detection at 25,000 counts/sec.",http://arxiv.org/abs/2505.12102v1,IT,Quantitative
CGReplay: Capture and Replay of Cloud Gaming Traffic for QoE/QoS Assessment,"Cloud Gaming (CG) research faces challenges due to the unpredictability of game engines and restricted access to commercial platforms and their logs. This creates major obstacles to conducting fair experimentation and evaluation. CGReplay captures and replays player commands and the corresponding video frames in an ordered and synchronized action-reaction loop, ensuring reproducibility. It enables Quality of Experience/Service (QoE/QoS) assessment under varying network conditions and serves as a foundation for broader CG research. The code is publicly available for further development.",http://arxiv.org/abs/2505.11973v1,IT,Quantitative
Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents,"Detecting vulnerabilities in source code remains a critical yet challenging task, especially when benign and vulnerable functions share significant similarities. In this work, we introduce VulTrial, a courtroom-inspired multi-agent framework designed to enhance automated vulnerability detection. It employs four role-specific agents, which are security researcher, code author, moderator, and review board. Through extensive experiments using GPT-3.5 and GPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent baselines. Using GPT-4o, VulTrial improves the performance by 102.39% and 84.17% over its respective baseline. Additionally, we show that role-specific instruction tuning in multi-agent with small data (50 pair samples) improves the performance of VulTrial further by 139.89% and 118.30%. Furthermore, we analyze the impact of increasing the number of agent interactions on VulTrial's overall performance. While multi-agent setups inherently incur higher costs due to increased token usage, our findings reveal that applying VulTrial to a cost-effective model like GPT-3.5 can improve its performance by 69.89% compared to GPT-4o in a single-agent setting, at a lower overall cost.",http://arxiv.org/abs/2505.10961v1,IT,Quantitative
The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware),"Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping the software industry by enabling FMware, systems that integrate these FMs as core components. In this KDD 2025 tutorial, we present a comprehensive exploration of FMware that combines a curated catalogue of challenges with real-world production concerns. We first discuss the state of research and practice in building FMware. We further examine the difficulties in selecting suitable models, aligning high-quality domain-specific data, engineering robust prompts, and orchestrating autonomous agents. We then address the complex journey from impressive demos to production-ready systems by outlining issues in system testing, optimization, deployment, and integration with legacy software. Drawing on our industrial experience and recent research in the area, we provide actionable insights and a technology roadmap for overcoming these challenges. Attendees will gain practical strategies to enable the creation of trustworthy FMware in the evolving technology landscape.",http://arxiv.org/abs/2505.10640v1,IT,Mixed
Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using Edge AI,"This research addresses the growing need for privacy-preserving and accessible language translation by developing a fully offline Neural Machine Translation (NMT) system for Vietnamese-English translation on iOS devices. Given increasing concerns about data privacy and unreliable network connectivity, on-device translation offers critical advantages. This project confronts challenges in deploying complex NMT models on resource-limited mobile devices, prioritizing efficiency, accuracy, and a seamless user experience. Leveraging advances such as MobileBERT and, specifically, the lightweight \textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} quantized Transformer-based model is implemented and optimized. The application is realized as a real-time iOS prototype, tightly integrating modern iOS frameworks and privacy-by-design principles. Comprehensive documentation covers model selection, technical architecture, challenges, and final implementation, including functional Swift code for deployment.",http://arxiv.org/abs/2505.07583v1,IT,Mixed
"LA-IMR: Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling for Tail-Latency-Sensitive Cloud Robotics","Hybrid cloud-edge infrastructures now support latency-critical workloads ranging from autonomous vehicles and surgical robotics to immersive AR/VR. However, they continue to experience crippling long-tail latency spikes whenever bursty request streams exceed the capacity of heterogeneous edge and cloud tiers. To address these long-tail latency issues, we present Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling (LA-IMR). This control layer integrates a closed-form, utilization-driven latency model with event-driven scheduling, replica autoscaling, and edge-to-cloud offloading to mitigate 99th-percentile (P99) delays. Our analytic model decomposes end-to-end latency into processing, network, and queuing components, expressing inference latency as an affine power-law function of instance utilization. Once calibrated, it produces two complementary functions that drive: (i) millisecond-scale routing decisions for traffic offloading, and (ii) capacity planning that jointly determines replica pool sizes. LA-IMR enacts these decisions through a quality-differentiated, multi-queue scheduler and a custom-metric Kubernetes autoscaler that scales replicas proactively -- before queues build up -- rather than reactively based on lagging CPU metrics. Across representative vision workloads (YOLOv5m and EfficientDet) and bursty arrival traces, LA-IMR reduces P99 latency by up to 20.7 percent compared to traditional latency-only autoscaling, laying a principled foundation for next-generation, tail-tolerant cloud-edge inference services.",http://arxiv.org/abs/2505.07417v1,IT,Quantitative
QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration,"The deployment of mixture-of-experts (MoE) large language models (LLMs) presents significant challenges due to their high memory demands. These challenges become even more pronounced in multi-tenant environments, where shared resources must accommodate multiple models, limiting the effectiveness of conventional virtualization techniques. This paper addresses the problem of efficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a serving system that employs \textit{similarity-based expert consolidation} to reduce the overall memory footprint by sharing similar experts across models. To ensure output quality, we introduce \textit{runtime partial reconfiguration}, dynamically replacing non-expert layers when processing requests from different models. As a result, our approach achieves a competitive output quality while maintaining throughput comparable to serving a single model while incurring a negligible increase in time-to-first-token (TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using Mixtral-8x7B models demonstrate an 85\% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on Google's Switch Transformer Base-8 model with up to four variants demonstrate the scalability and resilience of our approach in maintaining output quality compared to other model merging baselines, highlighting its effectiveness.",http://arxiv.org/abs/2505.06481v1,IT,Quantitative
P4Kube: In-Network Load Balancer for Kubernetes,"Kubernetes Services such as LoadBalancer and NodePort expose applications running on pods within a Kubernetes cluster to external users. While the LoadBalancer Service requires an external load-balancing middleware, its alternative, NodePort Service, adds additional hops on the path between clients and the worker nodes. In this paper, we propose P4Kube, a framework consisting of a P4 data plane program and a Kubernetes plugin. Our solution effectively performs load balancing of requests to the worker nodes of a cluster based on the number of running replicas. In P4Kube, the data packets completely bypass the system's control plane. Unlike the previous work, to update its state, the P4Kube data plane works directly with the Kubernetes control plane without any involvement of the network control plane. Our experiments show up to 50% improvement in the average request time to the cluster compared to conventional approaches.",http://arxiv.org/abs/2505.05996v1,IT,Quantitative
CacheFL: Privacy-Preserving and Efficient Federated Cache Model Fine-Tuning for Vision-Language Models,"Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation.",http://arxiv.org/abs/2505.05130v2,IT,Quantitative
Impact of Weather on Satellite Communication: Evaluating Starlink Resilience,"Satellite communications have emerged as one of the most feasible solutions to provide global wireless coverage and connect the unconnected. Starlink dominates the market with over 7,000 operational satellites in low Earth orbit (LEO) and offers global high-speed and low-latency Internet service for stationary and mobile use cases, including in-motion connectivity for vehicles, vessels, and aircraft. Starlink terminals are designed to handle extreme weather conditions. Starlink recommends a flat high performance (FHP) terminal for users living in areas with extreme weather conditions. The earlier studies evaluated Starlink's FHP throughput for stationary and in-motion users without providing a detailed analysis of how weather affects its performance. There remains a need to investigate the impact of weather on FHP's throughput. In this paper, we address this shortcoming by analyzing the impact of weather on Starlink's performance in Oulu, Finland, a city located in Northern Europe near the Arctic Circle. Our measurements reveal that rain degrades median uplink and downlink throughput by 52.27% and 37.84%, respectively. On the contrary, there was no noticeable impact on the round-trip time. Additionally, we also examine the impact of cloud cover on the Starlink throughput. The linear regression analysis reveals the negative relationship between throughput and cloud cover. The cloud cover of up to 12.5% has around 20% greater throughput than the cloud cover of 87.5%",http://arxiv.org/abs/2505.04772v1,IT,Quantitative
RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation,"When using LLMs to address Non-Functional Requirements (NFRs), developers may behave differently (e.g., expressing the same NFR in different words). Robust LLMs should output consistent results across these variations; however, this aspect remains underexplored. We propose RobuNFR for evaluating the robustness of LLMs in NFR-aware code generation across four NFR dimensions: design, readability, reliability, and performance, using three methodologies: prompt variation, regression testing, and diverse workflows. Our experiments show that RobuNFR reveals robustness issues in the tested LLMs when considering NFRs in code generation. Specifically, under prompt variation, including NFRs leads to a decrease in Pass@1 by up to 39 percent and an increase in the standard deviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e., Function-Only). While incorporating NFRs generally improves overall NFR metrics, it also results in higher prompt sensitivity. In regression settings, some LLMs exhibit differences across versions, with improvements in one aspect (e.g., reduced code smells) often accompanied by regressions in another (e.g., decreased correctness), revealing inconsistencies that challenge their robustness. When varying workflows, the tested LLMs show significantly different NFR-aware code generation capabilities between two workflows: (1) integrating NFRs and functional requirements into the initial prompt and (2) enhancing Function-Only-generated code with the same NFR.",http://arxiv.org/abs/2503.22851v2,IT,Quantitative
Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM Inference,"Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models.",http://arxiv.org/abs/2503.08311v1,IT,Quantitative
Umbilical Choir: Automated Live Testing for Edge-To-Cloud FaaS Applications,"Application users react negatively to performance regressions or availability issues across software releases. To address this, modern cloud-based applications with their multiple daily releases rely on live testing techniques such as A/B testing or canary releases. In edge-to-cloud applications, however, which have similar problems, developers currently still have to hard-code custom live testing tooling as there is no general framework for edge-to-cloud live testing. With Umbilical Choir, we partially close this gap for serverless edge-to-cloud applications. Umbilical Choir is compatible with all Function-as-a-Service platforms and (extensively) supports various live testing techniques, including canary releases with various geo-aware strategies, A/B testing, and gradual roll-outs. We evaluate Umbilical Choir through a complex release scenario showcasing various live testing techniques in a mixed edge-cloud deployments and discuss different geo-aware strategies.",http://arxiv.org/abs/2503.05495v1,IT,Quantitative
Optimizing Decentralized Online Learning for Supervised Regression and Classification Problems,"Decentralized learning networks aim to synthesize a single network inference from a set of raw inferences provided by multiple participants. To determine the combined inference, these networks must adopt a mapping from historical participant performance to weights, and to appropriately incentivize contributions they must adopt a mapping from performance to fair rewards. Despite the increased prevalence of decentralized learning networks, there exists no systematic study that performs a calibration of the associated free parameters. Here we present an optimization framework for key parameters governing decentralized online learning in supervised regression and classification problems. These parameters include the slope of the mapping between historical performance and participant weight, the timeframe for performance evaluation, and the slope of the mapping between performance and rewards. These parameters are optimized using a suite of numerical experiments that mimic the design of the Allora Network, but have been extended to handle classification tasks in addition to regression tasks. This setup enables a comparative analysis of parameter tuning and network performance optimization (loss minimization) across both problem types. We demonstrate how the optimal performance-weight mapping, performance timeframe, and performance-reward mapping vary with network composition and problem type. Our findings provide valuable insights for the optimization of decentralized learning protocols, and we discuss how these results can be generalized to optimize any inference synthesis-based, decentralized AI network.",http://arxiv.org/abs/2501.16519v1,IT,Quantitative
Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions,"In today's Function-as-a-Service offerings, a programmer is usually responsible for configuring function memory for its successful execution, which allocates proportional function resources such as CPU and network. However, right-sizing the function memory force developers to speculate performance and make ad-hoc configuration decisions. Recent research has highlighted that a function's input characteristics, such as input size, type and number of inputs, significantly impact its resource demand, run-time performance and costs with fluctuating workloads. This correlation further makes memory configuration a non-trivial task. On that account, an input-aware function memory allocator not only improves developer productivity by completely hiding resource-related decisions but also drives an opportunity to reduce resource wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we present MemFigLess, a serverless solution that estimates the memory requirement of a serverless function with input-awareness. The framework executes function profiling in an offline stage and trains a multi-output Random Forest Regression model on the collected metrics to invoke input-aware optimal configurations. We evaluate our work with the state-of-the-art approaches on AWS Lambda service to find that MemFigLess is able to capture the input-aware resource relationships and allocate upto 82% less resources and save up to 87% run-time costs.",http://arxiv.org/abs/2411.07444v1,IT,Quantitative
ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation,"Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose ROCODE, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.",http://arxiv.org/abs/2411.07112v2,IT,Quantitative
Moving Faster and Reducing Risk: Using LLMs in Release Deployment,"Release engineering has traditionally focused on continuously delivering features and bug fixes to users, but at a certain scale, it becomes impossible for a release engineering team to determine what should be released. At Meta's scale, the responsibility appropriately and necessarily falls back on the engineer writing and reviewing the code. To address this challenge, we developed models of diff risk scores (DRS) to determine how likely a diff is to cause a SEV, i.e., a severe fault that impacts end-users. Assuming that SEVs are only caused by diffs, a naive model could randomly gate X% of diffs from landing, which would automatically catch X% of SEVs on average. However, we aimed to build a model that can capture Y% of SEVs by gating X% of diffs, where Y >> X. By training the model on historical data on diffs that have caused SEVs in the past, we can predict the riskiness of an outgoing diff to cause a SEV. Diffs that are beyond a particular threshold of risk can then be gated. We have four types of gating: no gating (green), weekend gating (weekend), medium impact on end-users (yellow), and high impact on end-users (red). The input parameter for our models is the level of gating, and the outcome measure is the number of captured SEVs. Our research approaches include a logistic regression model, a BERT-based model, and generative LLMs. Our baseline regression model captures 18.7%, 27.9%, and 84.6% of SEVs while respectively gating the top 5% (weekend), 10% (yellow), and 50% (red) of risky diffs. The BERT-based model, StarBERT, only captures 0.61x, 0.85x, and 0.81x as many SEVs as the logistic regression for the weekend, yellow, and red gating zones, respectively. The generative LLMs, iCodeLlama-34B and iDiffLlama-13B, when risk-aligned, capture more SEVs than the logistic regression model in production: 1.40x, 1.52x, 1.05x, respectively.",http://arxiv.org/abs/2410.06351v1,IT,Quantitative
Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems,"Edge computing has emerged as a pivotal technology, offering significant advantages such as low latency, enhanced data security, and reduced reliance on centralized cloud infrastructure. These benefits are crucial for applications requiring real-time data processing or strict security measures. Despite these advantages, edge devices operating within edge clusters are often underutilized. This inefficiency is mainly due to the absence of a holistic performance profiling mechanism which can help dynamically adjust the desired system configuration for a given workload. Since edge computing environments involve a complex interplay between CPU frequency, power consumption, and application performance, a deeper understanding of these correlations is essential. By uncovering these relationships, it becomes possible to make informed decisions that enhance both computational efficiency and energy savings. To address this gap, this paper evaluates the power consumption and performance characteristics of a single processing node within an edge cluster using a synthetic microbenchmark by varying the workload size and CPU frequency. The results show how an optimal measure can lead to optimized usage of edge resources, given both performance and power consumption.",http://arxiv.org/abs/2505.07755v1,IT,Quantitative
Efficient Function Orchestration for Large Language Models,"Function calling is a fundamental capability of today's large language models, but sequential function calling posed efficiency problems. Recent studies have proposed to request function calls with parallelism support in order to alleviate this issue. However, they either delegate the concurrent function calls to users for execution which are conversely executed sequentially, or overlook the relations among various function calls, rending limited efficiency. This paper introduces LLMOrch, an advanced framework for automated, parallel function calling in large language models. The key principle behind LLMOrch is to identify an available processor to execute a function call while preventing any single processor from becoming overburdened. To this end, LLMOrch models the data relations (i.e., def-use) among different function calls and coordinates their executions by their control relations (i.e., mutual-exclusion) as well as the working status of the underlying processors. When comparing with state-of-the-art techniques, LLMOrch demonstrated comparable efficiency improvements in orchestrating I/O-intensive functions, while significantly outperforming (2$\times$) them with compute-intensive functions. LLMOrch's performance even showed a linear correlation to the number of allocated processors. We believe that these results highlight the potential of LLMOrch as an efficient solution for parallel function orchestration in the context of large language models.",http://arxiv.org/abs/2504.14872v1,IT,Quantitative
A Datagram Extension to DNS over QUIC: Proven Resource Conservation in the Internet of Things,"In this paper, we investigate the Domain Name System (DNS) over QUIC (DoQ) and propose a non-disruptive extension, which can greatly reduce DoQ's resource consumption. This extension can benefit all DNS clients - especially Internet of Things (IoT) devices. This is important because even resource-constrained IoT devices can generate dozens of DNS requests every hour. DNS is a crucial service that correlates IP addresses and domain names. It is traditionally sent as plain-text, favoring low-latency results over security and privacy. The repercussion of this can be eavesdropping and information leakage about IoT devices. To address these concerns, the newest and most promising solution is DoQ. QUIC offers features similar to TCP and TLS while also supporting early data delivery and stream multiplexing. DoQ's specification requires that DNS exchanges occur over independent streams in a long-lived QUIC connection. Our hypothesis is that due to DNS's typically high transaction volume, managing QUIC streams may be overly resource intensive for IoT devices. Therefore, we have designed and implemented a data delivery mode for DoQ using QUIC datagrams, which we believe to be more preferable than stream-based delivery. To test our theory, we analyzed the memory, CPU, signaling, power, and time of each DoQ delivery mode in a setup generating real queries and network traffic. Our novel datagram-based delivery mode proved to be decisively more resource-friendly with little compromise in terms of functionality or performance. Furthermore, our paper is the first to investigate multiple queries over DoQ, to our knowledge.",http://arxiv.org/abs/2504.09200v1,IT,Quantitative
Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models,"LLMs have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups. These associations perpetuate and even amplify harmful social biases, raising significant fairness concerns. To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference. However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases. Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations. To this end, we propose Fairness Mediator (FairMed), a bias mitigation framework that neutralizes stereotype associations. Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer. The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts to detect the emission probabilities for social groups. Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups. Extensive experiments across nine protected attributes show that FairMed significantly outperforms SOTA methods in effectiveness. Compared to the most effective baseline, FairMed presents competitive efficiency by cutting mitigation overhead by hundreds of minutes. FairMed also maintains the LLM's language understanding capabilities without compromising overall performance.",http://arxiv.org/abs/2504.07787v1,IT,Mixed
A Case for Network-wide Orchestration of Host-based Intrusion Detection and Response,"Recent cyber incidents and the push for zero trust security underscore the necessity of monitoring host-level events. However, current host-level intrusion detection systems (IDS) lack the ability to correlate alerts and coordinate a network-wide response in real time. Motivated by advances in system-level extensions free of rebooting and network-wide orchestration of host actions, we propose using a central IDS orchestrator to remotely program the logic of each host IDS and collect the alerts generated in real time. In this paper, we make arguments for such a system concept and provide a high level design of the main system components. Furthermore, we have developed a system prototype and evaluated it using two experimental scenarios rooted from real-world attacks. The evaluation results show that the host-based IDS orchestration system is able to defend against the attacks effectively.",http://arxiv.org/abs/2504.06241v1,IT,Quantitative
Efficient Resource Allocation in 5G Massive MIMO-NOMA Networks: Comparative Analysis of SINR-Aware Power Allocation and Spatial Correlation-Based Clustering,"With the evolution of 5G networks, optimizing resource allocation has become crucial to meeting the increasing demand for massive connectivity and high throughput. Combining Non-Orthogonal Multiple Access (NOMA) and massive Multi-Input Multi-Output (MIMO) enhances spectral efficiency, power efficiency, and device connectivity. However, deploying MIMO-NOMA in dense networks poses challenges in managing interference and optimizing power allocation while ensuring that the Signal-to-Interference-plus-Noise Ratio (SINR) meets required thresholds. Unlike previous studies that analyze user clustering and power allocation techniques under simplified assumptions, this work provides a comparative evaluation of multiple clustering and allocation strategies under identical spatially correlated network conditions. We focus on maximizing the number of served users under a given Quality of Service (QoS) constraint rather than the conventional sum-rate maximization approach. Additionally, we consider spatial correlation in user grouping, a factor often overlooked despite its importance in mitigating intra-cluster interference. We evaluate clustering algorithms, including user pairing, random clustering, Correlation Iterative Clustering Algorithm (CIA), K-means++-based User Clustering (KUC), and Grey Wolf Optimizer-based clustering (GWO), in a downlink spatially correlated MIMO-NOMA environment. Numerical results demonstrate that the GWO-based clustering algorithm achieves superior energy efficiency while maintaining scalability, whereas CIA effectively maximizes the number of served users. These findings provide valuable insights for designing MIMO-NOMA systems that optimize resource allocation in next-generation wireless networks.",http://arxiv.org/abs/2503.08466v1,IT,Quantitative
Data-driven Modality Fusion: An AI-enabled Framework for Large-Scale Sensor Network Management,"The development and operation of smart cities relyheavily on large-scale Internet-of-Things (IoT) networks and sensor infrastructures that continuously monitor various aspects of urban environments. These networks generate vast amounts of data, posing challenges related to bandwidth usage, energy consumption, and system scalability. This paper introduces a novel sensing paradigm called Data-driven Modality Fusion (DMF), designed to enhance the efficiency of smart city IoT network management. By leveraging correlations between timeseries data from different sensing modalities, the proposed DMF approach reduces the number of physical sensors required for monitoring, thereby minimizing energy expenditure, communication bandwidth, and overall deployment costs. The framework relocates computational complexity from the edge devices to the core, ensuring that resource-constrained IoT devices are not burdened with intensive processing tasks. DMF is validated using data from a real-world IoT deployment in Madrid, demonstrating the effectiveness of the proposed system in accurately estimating traffic, environmental, and pollution metrics from a reduced set of sensors. The proposed solution offers a scalable, efficient mechanism for managing urban IoT networks, while addressing issues of sensor failure and privacy concerns.",http://arxiv.org/abs/2502.04937v1,IT,Mixed
Using Causality for Enhanced Prediction of Web Traffic Time Series,"Predicting web service traffic has significant social value, as it can be applied to various practical scenarios, including but not limited to dynamic resource scaling, load balancing, system anomaly detection, service-level agreement compliance, and fraud detection. Web service traffic is characterized by frequent and drastic fluctuations over time and are influenced by heterogeneous web user behaviors, making accurate prediction a challenging task. Previous research has extensively explored statistical approaches, and neural networks to mine features from preceding service traffic time series for prediction. However, these methods have largely overlooked the causal relationships between services. Drawing inspiration from causality in ecological systems, we empirically recognize the causal relationships between web services. To leverage these relationships for improved web service traffic prediction, we propose an effective neural network module, CCMPlus, designed to extract causal relationship features across services. This module can be seamlessly integrated with existing time series models to consistently enhance the performance of web service traffic predictions. We theoretically justify that the causal correlation matrix generated by the CCMPlus module captures causal relationships among services. Empirical results on real-world datasets from Microsoft Azure, Alibaba Group, and Ant Group confirm that our method surpasses state-of-the-art approaches in Mean Squared Error (MSE) and Mean Absolute Error (MAE) for predicting service traffic time series. These findings highlight the efficacy of leveraging causal relationships for improved predictions.",http://arxiv.org/abs/2502.00612v1,IT,Quantitative
Implementing LoRa MIMO System for Internet of Things,"Bandwidth constraints limit LoRa implementations. Contemporary IoT applications require higher throughput than that provided by LoRa. This work introduces a LoRa Multiple Input Multiple Output (MIMO) system and a spatial multiplexing algorithm to address LoRa's bandwidth limitation. The transceivers in the proposed approach modulate the signals on distinct frequencies of the same LoRa band. A Frequency Division Multiplexing (FDM) method is used at the transmitters to provide a wider MIMO channel. Unlike conventional Orthogonal Frequency Division Multiplexing (OFDM) techniques, this work exploits the orthogonality of the LoRa signals facilitated by its proprietary Chirp Spread Spectrum (CSS) modulation to perform an OFDM in the proposed LoRa MIMO system. By varying the Spreading Factor (SF) and bandwidth of LoRa signals, orthogonal signals can transmit on the same frequency irrespective of the FDM. Even though the channel correlation is minimal for different spreading factors and bandwidths, different Carrier Frequencies (CF) ensure the signals do not overlap and provide additional degrees of freedom. This work assesses the proposed model's performance and conducts an extensive analysis to provide an overview of resources consumed by the proposed system. Finally, this work provides the detailed results of a thorough evaluation of the model on test hardware.",http://arxiv.org/abs/2501.07148v1,IT,Quantitative
A Correlated Data-Driven Collaborative Beamforming Approach for Energy-efficient IoT Data Transmission,"An expansion of Internet of Things (IoTs) has led to significant challenges in wireless data harvesting, dissemination, and energy management due to the massive volumes of data generated by IoT devices. These challenges are exacerbated by data redundancy arising from spatial and temporal correlations. To address these issues, this paper proposes a novel data-driven collaborative beamforming (CB)-based communication framework for IoT networks. Specifically, the framework integrates CB with an overlap-based multi-hop routing protocol (OMRP) to enhance data transmission efficiency while mitigating energy consumption and addressing hot spot issues in remotely deployed IoT networks. Based on the data aggregation to a specific node by OMRP, we formulate a node selection problem for the CB stage, with the objective of optimizing uplink transmission energy consumption. Given the complexity of the problem, we introduce a softmax-based proximal policy optimization with long short-term memory (SoftPPO-LSTM) algorithm to intelligently select CB nodes for improving transmission efficiency. Simulation results validate the effectiveness of the proposed OMRP and SoftPPO-LSTM methods, demonstrating significant improvements over existing routing protocols and node selection strategies. The results also reveal that the combined OMRP with the SoftPPO-LSTM method effectively mitigates hot spot problems and offers superior performance compared to traditional strategies.",http://arxiv.org/abs/2501.06464v2,IT,Quantitative
Dynamic Scaling of Unit Tests for Code Reward Modeling,"Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",http://arxiv.org/abs/2501.01054v1,IT,Mixed
Efficient Serverless Cold Start: Reducing Library Loading Overhead by Profile-guided Optimization,"Serverless computing abstracts away server management, enabling automatic scaling, efficient resource utilization, and cost-effective pricing models. However, despite these advantages, it faces the significant challenge of cold-start latency, adversely impacting end-to-end performance. Our study shows that many serverless functions initialize libraries that are rarely or never used under typical workloads, thus introducing unnecessary overhead. Although existing static analysis techniques can identify unreachable libraries, they fail to address workload-dependent inefficiencies, resulting in limited performance improvements. To overcome these limitations, we present SLIMSTART, a profile-guided optimization tool designed to identify and mitigate inefficient library usage patterns in serverless applications. By leveraging statistical sampling and call-path profiling, SLIMSTART collects runtime library usage data, generates detailed optimization reports, and applies automated code transformations to reduce cold-start overhead. Furthermore, SLIMSTART integrates seamlessly into CI/CD pipelines, enabling adaptive monitoring and continuous optimizations tailored to evolving workloads. Through extensive evaluation across three benchmark suites and four real-world serverless applications, SLIMSTART achieves up to a 2.30X speedup in initialization latency, a 2.26X improvement in end-to-end latency, and a 1.51X reduction in memory usage, demonstrating its effectiveness in addressing cold-start inefficiencies and optimizing resource utilization.",http://arxiv.org/abs/2504.19283v1,IT,Quantitative
STGen: A Novel Lightweight IoT Testbed for Generating Sensor Traffic for the Experimentation of IoT Protocol and its Application in Hybrid Network,"A Wireless Sensor Network (WSN) is a network that does not rely on a fixed infrastructure and consists of numerous sensors, such as temperature, humidity, GPS, and cameras, equipped with onboard processors that manage and monitor the environment in a specific area. As a result, building a real sensor network testbed for verifying, validating, or experimenting with a newly designed protocol presents considerable challenges in adapting a laboratory scenario due to the significant financial and logistical barriers, such as the need for specialized hardware and large-scale deployments. Additionally, WSN suffers from severe constraints such as restricted power supply, short communication range, limited bandwidth availability, and restricted memory storage. Addressing these challenges, this work presents a flexible testbed solution named STGen that enables researchers to experiment with IoT protocols in a hybrid environment that emulates WSN implementations with the physical Internet through a dedicated physical server named STGen core, which receives sensor traffic and processes it for further actions. The STGen testbed is lightweight in memory usage and easy to deploy. Most importantly, STGen supports large-scale distributed systems, facilitates experimentation with IoT protocols, and enables integration with back-end services for big data analytics and statistical insights. The key feature of STGen is the integration of real-world IoT protocols and their applications with WSN. Its modular and lightweight design makes STGen efficient and enables it to outperform other popular testbeds, such as Gotham and GothX, reducing memory usage by 89\%. While GothX takes approximately 26 minutes to establish a large topology with four VM nodes and 498 Docker nodes, STGen requires only 1.645 seconds to initialize the platform with 500 sensor nodes.",http://arxiv.org/abs/2504.17725v1,IT,Quantitative
Statistical Analysis and End-to-End Performance Evaluation of Traffic Models for Automotive Data,"Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models.",http://arxiv.org/abs/2504.14017v1,IT,Quantitative
Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption,"In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.",http://arxiv.org/abs/2504.09634v1,IT,Quantitative
A Dataset For Computational Reproducibility,"Ensuring the reproducibility of scientific work is crucial as it allows the consistent verification of scientific claims and facilitates the advancement of knowledge by providing a reliable foundation for future research. However, scientific work based on computational artifacts, such as scripts for statistical analysis or software prototypes, faces significant challenges in achieving reproducibility. These challenges are based on the variability of computational environments, rapid software evolution, and inadequate documentation of procedures. As a consequence, such artifacts often are not (easily) reproducible, undermining the credibility of scientific findings. The evaluation of reproducibility approaches, in particular of tools, is challenging in many aspects, one being the need to test them with the correct inputs, in this case computational experiments. Thus, this article introduces a curated dataset of computational experiments covering a broad spectrum of scientific fields, incorporating details about software dependencies, execution steps, and configurations necessary for accurate reproduction. The dataset is structured to reflect diverse computational requirements and methodologies, ranging from simple scripts to complex, multi-language workflows, ensuring it presents the wide range of challenges researchers face in reproducing computational studies. It provides a universal benchmark by establishing a standardized dataset for objectively evaluating and comparing the effectiveness of reproducibility tools. Each experiment included in the dataset is carefully documented to ensure ease of use. We added clear instructions following a standard, so each experiment has the same kind of instructions, making it easier for researchers to run each of them with their own reproducibility tool.",http://arxiv.org/abs/2504.08684v1,IT,Quantitative
Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL,"Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms.",http://arxiv.org/abs/2504.02688v1,IT,Quantitative
Asynchronous Personalized Federated Learning through Global Memorization,"The proliferation of Internet of Things devices and advances in communication technology have unleashed an explosion of personal data, amplifying privacy concerns amid stringent regulations like GDPR and CCPA. Federated Learning offers a privacy preserving solution by enabling collaborative model training across decentralized devices without centralizing sensitive data. However, statistical heterogeneity from non-independent and identically distributed datasets and system heterogeneity due to client dropouts particularly those with monopolistic classes severely degrade the global model's performance. To address these challenges, we propose the Asynchronous Personalized Federated Learning framework, which empowers clients to develop personalized models using a server side semantic generator. This generator, trained via data free knowledge transfer under global model supervision, enhances client data diversity by producing both seen and unseen samples, the latter enabled by Zero-Shot Learning to mitigate dropout-induced data loss. To counter the risks of synthetic data impairing training, we introduce a decoupled model interpolation method, ensuring robust personalization. Extensive experiments demonstrate that AP FL significantly outperforms state of the art FL methods in tackling non-IID distributions and client dropouts, achieving superior accuracy and resilience across diverse real-world scenarios.",http://arxiv.org/abs/2503.00407v1,IT,Quantitative
Prediction of the Received Power of Low-Power Networks Using Inertial Sensors,"Low-power and cost-effective IoT sensing nodes enable scalable monitoring of different environments. Some of these environments impose rough and extreme operating conditions, requiring continuous adaptation and reconfiguration of physical and link layer parameters. In this paper, we closely investigate the stability of the wireless links established between nodes deployed on the surface of different water bodies and propose a model to predict the received power. Our model is based on Minimum Mean Square Estimation (MMSE) and relies on the statistics of received power and the motion the nodes experience during communication. One of the drawbacks of MMSE is its reliance on matrix inversion, which is at once computationally expensive and difficult to implement with resource constrained devices. We forgo this stage by estimating model parameters using the gradient-descent approach, which is much simpler to implement. The model achieves a prediction accuracy of 91% even with a small number of iterations.",http://arxiv.org/abs/2502.14107v2,IT,Mixed
Instant AoI Optimization through Relay Location Selection in Disaster Multi-hop Communication,"Meteorological disasters such as typhoons, forest fires, and floods can damage the communication infrastructures, which will further disable the communication capabilities of cellular networks. The multi-hop wireless communication based on IoT devices (e.g., rescue robots, UAVs, and mobile devices) becomes an available and rapidly deployable communication approach for search and rescue operations. However, Age of Information (AoI), an emerging network performance metric, has not been comprehensively investigated in this multi-hop model. In this paper, we first construct a UAV-relayed wireless network model and formulate the end-to-end instant AoI. Then we derive the optimal location of the relay UAV to achieve the minimum instant AoI by mathematical analysis. Simulations show that the derived relay location can always guarantee the optimal AoI and outperform other schemes.",http://arxiv.org/abs/2505.09386v1,IT,Quantitative
DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral,"Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.",http://arxiv.org/abs/2505.03214v1,IT,Quantitative
Tracking the Moving Target: A Framework for Continuous Evaluation of LLM Test Generation in Industry,"Large Language Models (LLMs) have shown great potential in automating software testing tasks, including test generation. However, their rapid evolution poses a critical challenge for companies implementing DevSecOps - evaluations of their effectiveness quickly become outdated, making it difficult to assess their reliability for production use. While academic research has extensively studied LLM-based test generation, evaluations typically provide point-in-time analyses using academic benchmarks. Such evaluations do not address the practical needs of companies who must continuously assess tool reliability and integration with existing development practices. This work presents a measurement framework for the continuous evaluation of commercial LLM test generators in industrial environments. We demonstrate its effectiveness through a longitudinal study at LKS Next. The framework integrates with industry-standard tools like SonarQube and provides metrics that evaluate both technical adequacy (e.g., test coverage) and practical considerations (e.g., maintainability or expert assessment). Our methodology incorporates strategies for test case selection, prompt engineering, and measurement infrastructure, addressing challenges such as data leakage and reproducibility. Results highlight both the rapid evolution of LLM capabilities and critical factors for successful industrial adoption, offering practical guidance for companies seeking to integrate these technologies into their development pipelines.",http://arxiv.org/abs/2504.18985v1,IT,Quantitative
Enhancing Cloud Task Scheduling Using a Hybrid Particle Swarm and Grey Wolf Optimization Approach,"Assigning tasks efficiently in cloud computing is a challenging problem and is considered an NP-hard problem. Many researchers have used metaheuristic algorithms to solve it, but these often struggle to handle dynamic workloads and explore all possible options effectively. Therefore, this paper presents a new hybrid method that combines two popular algorithms, Grey Wolf Optimizer (GWO) and Particle Swarm Optimization (PSO). GWO offers strong global search capabilities (exploration), while PSO enhances local refinement (exploitation). The hybrid approach, called HybridPSOGWO, is compared with other existing methods like MPSOSA, RL-GWO, CCGP, and HybridPSOMinMin, using key performance indicators such as makespan, throughput, and load balancing. We tested our approach using both a simulation tool (CloudSim Plus) and real-world data. The results show that HybridPSOGWO outperforms other methods, with up to 15\% improvement in makespan and 10\% better throughput, while also distributing tasks more evenly across virtual machines. Our implementation achieves consistent convergence within a few iterations, highlighting its potential for efficient and adaptive cloud scheduling.",http://arxiv.org/abs/2505.15171v1,IT,Quantitative
Balanced and Elastic End-to-end Training of Dynamic LLMs,"To reduce computational and memory costs in Large Language Models (LLMs), dynamic workload reduction schemes like Mixture of Experts (MoEs), parameter pruning, layer freezing, sparse attention, early token exit, and Mixture of Depths (MoDs) have emerged. However, these methods introduce severe workload imbalances, limiting their practicality for large-scale distributed training. We propose DynMo, an autonomous dynamic load balancing solution that ensures optimal compute distribution when using pipeline parallelism in training dynamic models. DynMo adaptively balances workloads, dynamically packs tasks into fewer workers to free idle resources, and supports both multi-GPU single-node and multi-node systems. Compared to static training methods (Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs), 3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early exit), and 1.17x (MoDs). DynMo is available at https://anonymous.4open.science/r/DynMo-4D04/.",http://arxiv.org/abs/2505.14864v1,IT,Mixed
Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy,"Background: Deep learning has potential to improve the efficiency and consistency of radiation therapy planning, but clinical adoption is hindered by the limited model generalizability due to data scarcity and heterogeneity among institutions. Although aggregating data from different institutions could alleviate this problem, data sharing is a practical challenge due to concerns about patient data privacy and other technical obstacles. Purpose: This work aims to address this dilemma by developing FedKBP+, a comprehensive federated learning (FL) platform for predictive tasks in real-world applications in radiotherapy treatment planning. Methods: We implemented a unified communication stack based on Google Remote Procedure Call (gRPC) to support communication between participants whether located on the same workstation or distributed across multiple workstations. In addition to supporting the centralized FL strategies commonly available in existing open-source frameworks, FedKBP+ also provides a fully decentralized FL model where participants directly exchange model weights to each other through Peer-to-Peer communication. We evaluated FedKBP+ on three predictive tasks using scale-attention network (SA-Net) as the predictive model. Conclusions: Our results demonstrate that FedKBP+ is highly effective, efficient and robust, showing great potential as a federated learning platform for radiation therapy.",http://arxiv.org/abs/2505.14507v1,IT,Mixed
Graph Neural Networks Based Anomalous RSSI Detection,"In today's world, modern infrastructures are being equipped with information and communication technologies to create large IoT networks. It is essential to monitor these networks to ensure smooth operations by detecting and correcting link failures or abnormal network behaviour proactively, which can otherwise cause interruptions in business operations. This paper presents a novel method for detecting anomalies in wireless links using graph neural networks. The proposed approach involves converting time series data into graphs and training a new graph neural network architecture based on graph attention networks that successfully detects anomalies at the level of individual measurements of the time series data. The model provides competitive results compared to the state of the art while being computationally more efficient with ~171 times fewer trainable parameters.",http://arxiv.org/abs/2505.15847v1,IT,Quantitative
Choreographies as Macros,"Concurrent programming often entails meticulous pairing of sends and receives between participants to avoid deadlock. Choreographic programming alleviates this burden by specifying the system as a single program. However, there are more applications than implementations of choreographies, and developing new implementations takes a lot of time and effort. Our work uses Racket to expedite building a new choreographic language called Choret. Racket has a powerful macro system which allows Choret to reuse much of its infrastructure for greater functionality and correctness.",http://arxiv.org/abs/2505.20845v1,IT,Mixed
InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling,"Generative image editing using diffusion models has become a prevalent application in today's AI cloud services. In production environments, image editing typically involves a mask that specifies the regions of an image template to be edited. The use of masks provides direct control over the editing process and introduces sparsity in the model inference. In this paper, we present InstGenIE, a system that efficiently serves image editing requests. The key insight behind InstGenIE is that image editing only modifies the masked regions of image templates while preserving the original content in the unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant computations associated with the unmasked areas by reusing cached intermediate activations from previous inferences. To mitigate the high cache loading overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps computation with cache loading. Additionally, to reduce queuing latency in online serving while improving the GPU utilization, InstGenIE proposes a novel continuous batching strategy for diffusion model serving, allowing newly arrived requests to join the running batch in just one step of denoising computation, without waiting for the entire batch to complete. As heterogeneous masks induce imbalanced loads, InstGenIE also develops a load balancing strategy that takes into account the loads of both computation and cache loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving systems for image editing, achieving up to 3x higher throughput and reducing average request latency by up to 14.7x while ensuring image quality.",http://arxiv.org/abs/2505.20600v1,IT,Quantitative
Large Language Models for IT Automation Tasks: Are We There Yet?,"LLMs show promise in code generation, yet their effectiveness for IT automation tasks, particularly for tools like Ansible, remains understudied. Existing benchmarks rely primarily on synthetic tasks that fail to capture the needs of practitioners who use IT automation tools, such as Ansible. We present ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g., configuring servers, managing files) where each task accounts for state reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs' ability to generate functional Ansible automation scripts via dynamic execution in controlled environments. We evaluate 14 open-source LLMs, none of which accomplish pass@10 at a rate beyond 12%. To explain these low scores, we analyze 1,411 execution failures across the evaluated LLMs and identify two main categories of prevalent semantic errors: failures in state reconciliation related reasoning (44.87% combined from variable (11.43%), host (11.84%), path(11.63%), and template (9.97%) issues) and deficiencies in module-specific execution knowledge (24.37% combined from Attribute and parameter (14.44%) and module (9.93%) errors). Our findings reveal key limitations in open-source LLMs' ability to track state changes and apply specialized module knowledge, indicating that reliable IT automation will require major advances in state reasoning and domain-specific execution understanding.",http://arxiv.org/abs/2505.20505v1,IT,Quantitative
Justin: Hybrid CPU/Memory Elastic Scaling for Distributed Stream Processing,"Distributed Stream Processing (DSP) engines analyze continuous data via queries expressed as a graph of operators. Auto-scalers adjust the number of parallel instances of these operators to support a target rate. Current auto-scalers couple CPU and memory scaling, allocating resources as one-size-fits-all packages. This contrasts with operators' high diversity of requirements. We present Justin, an auto-scaler that enables hybrid CPU and memory scaling of DSP operators. Justin monitors both CPU usage and the performance of operators' storage operations. Its mechanisms enable finegrain memory allocation for tasks upon a query reconfiguration. The Justin policy identifies individual operators' memory pressure and decides between adjusting parallelism and/or memory assignment. We implement Justin in Apache Flink, extending the Flink Kubernetes Operator and the DS2 CPU-only auto-scaler. Using the Nexmark benchmark, our evaluation shows that Justin identifies suitable resource allocation in as many or fewer reconfiguration steps as DS2 and supports a target rate with significantly fewer CPU and memory resources.",http://arxiv.org/abs/2505.19739v1,IT,Quantitative
An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection,"Large Language Models (LLMs) are being used more and more for various coding tasks, including to help coders identify bugs and are a promising avenue to support coders in various tasks including vulnerability detection -- particularly given the flexibility of such generative AI models and tools. Yet for many tasks it may not be suitable to use LLMs, for which it may be more suitable to use smaller language models that can fit and easily execute and train on a developer's computer. In this paper we explore and evaluate whether smaller language models can be fine-tuned to achieve reasonable results for a niche area: vulnerability detection -- specifically focusing on detecting the reentrancy bug in Solidity smart contracts.",http://arxiv.org/abs/2505.19059v1,IT,Mixed
GitHub Proxy Server: A tool for supporting massive data collection on GitHub,"GitHub is the most popular social coding platform and widely used by developers and organizations to host their open-source projects around the world. Besides that, the platform has a web API that allow developers collect information from public repositories hosted on it. However, collecting massive amount of data from GitHub can be very challenging due to existing restrictions and abuse detection mechanisms. In this work, we present a tool, called GitHub Proxy Server, which abstracts such complexities into a tool that is independent on operational system and programming language. We show that, using the proposed tool, it is possible to improve the performance of GitHub mining tasks without any additional complexities.",http://arxiv.org/abs/2505.18305v1,IT,Quantitative
From What to How: A Taxonomy of Formalized Security Properties,"Confidentiality, integrity, availability, authenticity, authorization, and accountability are known as security properties that secure systems should preserve. They are usually considered as security final goals that are achieved by system development activities, either in a direct or an indirect manner. However, these security properties are mainly elicited in the high-level requirement phase during the System Development Life Cycle (SDLC) and are not refined throughout the latter phases as other artifacts such as attacks, defenses, and system assets. To align security properties refinement with attacks, defenses, and system assets refinements, we propose an SDLC taxonomy of security properties that may be used in a self-adaptive context and present the methodology for defining it. To verify and check the correctness of the resulting taxonomy, we use the Event-B formal language.",http://arxiv.org/abs/2505.14514v1,IT,Mixed
An Integrated UVM-TLM Co-Simulation Framework for RISC-V Functional Verification and Performance Evaluation,"The burgeoning RISC-V ecosystem necessitates efficient verification methodologies for complex processors. Traditional approaches often struggle to concurrently evaluate functional correctness and performance, or balance simulation speed with modeling accuracy. This paper introduces an integrated co-simulation framework leveraging Universal Verification Methodology (UVM) and Transaction-Level Modeling (TLM) for RISC-V processor validation. We present a configurable UVM-TLM model (vmodel) of a superscalar, out-of-order RISC-V core, featuring key microarchitectural modeling techniques such as credit-based pipeline flow control. This environment facilitates unified functional verification via co-simulation against the Spike ISA simulator and enables early-stage performance assessment using benchmarks like CoreMark, orchestrated within UVM. The methodology prioritizes integration, simulation efficiency, and acceptable fidelity for architectural exploration over cycle-level precision. Experimental results validate functional correctness and significant simulation speedup over RTL approaches, accelerating design iterations and enhancing verification coverage.",http://arxiv.org/abs/2505.10145v1,IT,Quantitative
CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation,"We introduce CRPE (Code Reasoning Process Enhancer), an innovative three-stage framework for data synthesis and model training that advances the development of sophisticated code reasoning capabilities in large language models (LLMs). Building upon existing system-1 models, CRPE addresses the fundamental challenge of enhancing LLMs' analytical and logical processing in code generation tasks. Our framework presents a methodologically rigorous yet implementable approach to cultivating advanced code reasoning abilities in language models. Through the implementation of CRPE, we successfully develop an enhanced COT-Coder that demonstrates marked improvements in code generation tasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that our COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1 accuracy of 21.88, exceeds all models with similar or even larger sizes. Furthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base, exhibits superior performance with a pass@1 accuracy of 35.08, outperforming GPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source method that encompasses the complete pipeline from instruction data acquisition through expert code reasoning data synthesis, culminating in an autonomous reasoning enhancement mechanism.",http://arxiv.org/abs/2505.10594v1,IT,Quantitative
"Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability","While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.",http://arxiv.org/abs/2505.01328v1,IT,Mixed
Enhancing the Cloud Security through Topic Modelling,"Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline.",http://arxiv.org/abs/2505.01463v1,IT,Quantitative
Online Experimental Design for Network Tomography,"How to efficiently perform network tomography is a fundamental problem in network management and monitoring. A network tomography task usually consists of applying multiple probing experiments, e.g., across different paths or via different casts (including unicast and multicast). We study how to optimize the network tomography process through online sequential decision-making. From the methodology perspective, we introduce an online probe allocation algorithm that dynamically performs network tomography based on the principles of optimal experimental design and the maximum likelihood estimation. We rigorously analyze the regret of the algorithm under the conditions that i) the optimal allocation is Lipschitz continuous in the parameters being estimated and ii) the parameter estimators satisfy a concentration property. From the application perspective, we present two case studies: a) the classical lossy packet-switched network and b) the quantum bit-flip network. We show that both cases fulfill the two theoretical conditions and provide their corresponding regrets when deploying our proposed online probe allocation algorithm. Besides these two case studies with theoretical guarantees, we also conduct simulations to compare our proposed algorithm with existing methods and demonstrate our algorithm's effectiveness in a broader range of scenarios.",http://arxiv.org/abs/2504.21549v1,IT,Quantitative
Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3,"Determining the most effective Large Language Model for code smell detection presents a complex challenge. This study introduces a structured methodology and evaluation matrix to tackle this issue, leveraging a curated dataset of code samples consistently annotated with known smells. The dataset spans four prominent programming languages Java, Python, JavaScript, and C++; allowing for cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT 4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation metrics. Our analysis covers three levels of detail: overall performance, category level performance, and individual code smell type performance. Additionally, we explore cost effectiveness by comparing the token based detection approach of GPT 4.0 with the pattern-matching techniques employed by DeepSeek V3. The study also includes a cost analysis relative to traditional static analysis tools such as SonarQube. The findings offer valuable guidance for practitioners in selecting an efficient, cost effective solution for automated code smell detection",http://arxiv.org/abs/2504.16027v1,IT,Quantitative
SkyNetPredictor: Network Performance Prediction in Avionic Communication using AI,"Satellite-based communication systems are integral to delivering high-speed data services in aviation, particularly for business aviation operations requiring global connectivity. These systems, however, are challenged by a multitude of interdependent factors such as satellite handovers, congestion, flight maneuvers and seasonal trends, making network performance prediction a complex task. No established methodologies currently exist for network performance prediction in avionic communication systems. This paper addresses the gap by proposing machine learning (ML)-based approaches for pre-flight network performance predictions. The proposed models predict performance along a given flight path, taking as input positional and network-related information and outputting the predicted performance for each position. In business aviation, flight crews typically have multiple flight plans to choose from for each city pair, allowing them to select the most optimal option. This approach enables proactive decision-making, such as selecting optimal flight paths prior to departure.",http://arxiv.org/abs/2504.14443v1,IT,Quantitative
Comparative Analysis of POX and RYU SDN Controllers in Scalable Networks,"This paper explores the Quality of Service (QoS) performance of two widely used Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet for network simulation. SDN, a transformative approach to network architecture, separates the control and data planes, enabling centralized management, improved agility, and cost-effective solutions. The study evaluates key QoS parameters, including throughput, delay, and jitter, to understand the capabilities and limitations of the POX and Ryu controllers in handling traffic under diverse network topologies. The research employs a systematic methodology involving the design of custom network topologies, implementation of OpenFlow rules, and analysis of controller behavior under simulated conditions. Results reveal that while POX offers simplicity and ease of use, making it suitable for smaller-scale applications and experimentation, Ryu provides superior scalability and adaptability for more complex network environments. The findings highlight the strengths and challenges of each controller, providing valuable insights for organizations seeking to optimize SDN deployment. This study contributes to the growing body of knowledge on SDN technologies and their role in building scalable, efficient, and resilient network infrastructures.",http://arxiv.org/abs/2504.12770v1,IT,Quantitative
Qualitative Research Methods for Large Language Models: Conducting Semi-Structured Interviews with ChatGPT and BARD on Computer Science Education,"In the current era of artificial intelligence, large language models such as ChatGPT and BARD are being increasingly used for various applications, such as language translation, text generation, and human-like conversation. The fact that these models consist of large amounts of data, including many different opinions and perspectives, could introduce the possibility of a new qualitative research approach: Due to the probabilistic character of their answers, “interviewing” these large language models could give insights into public opinions in a way that otherwise only interviews with large groups of subjects could deliver. However, it is not yet clear if qualitative content analysis research methods can be applied to interviews with these models. Evaluating the applicability of qualitative research methods to interviews with large language models could foster our understanding of their abilities and limitations. In this paper, we examine the applicability of qualitative content analysis research methods to interviews with ChatGPT in English, ChatGPT in German, and BARD in English on the relevance of computer science in K-12 education, which was used as an exemplary topic. We found that the answers produced by these models strongly depended on the provided context, and the same model could produce heavily differing results for the same questions. From these results and the insights throughout the process, we formulated guidelines for conducting and analyzing interviews with large language models. Our findings suggest that qualitative content analysis research methods can indeed be applied to interviews with large language models, but with careful consideration of contextual factors that may affect the responses produced by these models. The guidelines we provide can aid researchers and practitioners in conducting more nuanced and insightful interviews with large language models. From an overall view of our results, we generally do not recommend using interviews with large language models for research purposes, due to their highly unpredictable results. However, we suggest using these models as exploration tools for gaining different perspectives on research topics and for testing interview guidelines before conducting real-world interviews.",https://www.semanticscholar.org/paper/5f7dc95099fa3bb778c4a95233549ea8f4008a4a,CS,Qualitative
Perceptions and Aspirations of Undergraduate Computer Science Students Towards Generative AI: A Qualitative Inquiry,"This article presents a comprehensive study conducted during the spring semester of 2024, aimed at exploring undergraduate computer science students’ perceptions, awareness, and understanding of generative artificial intelligence (GAI) tools within the context of their Artificial Intelligence (AI) courses. The research methodology employed qualitative techniques, including human-subject research and focus groups, to delve into students’ insights on the evolution of AI as delineated in the seminal textbook by Russell and Norvig. The study-initiated discussions on the historical development of AI, prompting students to reflect on the aspects that intrigued them the most, and to identify which historical concepts and methodologies, perhaps even those not directly covered in their curriculum, piqued their interest. Furthermore, the dialogue encompassed the learning methodologies highlighted in the textbook, seeking students’ feedback on the strategies that have been most effective for mastering complex AI theories and their practical applications. Interdisciplinary applications of AI were also discussed, encouraging students to contemplate AI’s role beyond the realm of computer science and its potential to foster innovative solutions across various fields. Finally, the conversation shifted towards students’ personal goals and aspirations in AI, urging them to consider how their perspectives have evolved in light of technological advancements, societal needs, and ethical considerations. The findings underscore a notable gap in students’ awareness of AI’s history and its current capabilities, indicating a need for educational strategies that not only deepen understanding but also foster a broader appreciation of AI’s potential. This study contributes valuable insights into enhancing AI education and encouraging interdisciplinary innovation among the next generation of computer scientists.",https://www.semanticscholar.org/paper/14bb2ca9995f7c8606660cdf282968056e7e2b3c,CS,Qualitative
What do faculty members know about universal design and digital accessibility? A qualitative study in computer science and engineering disciplines,"Purpose Students in higher education are a diverse group comprising people with different backgrounds and abilities. Regulations require that digital learning materials and platforms employed in higher education accommodate this diversity. Furthermore, they require faculty members to have an understanding of universal design and digital accessibility, as well as practical knowledge of how to make learning materials and courses accessible for more students. The goal of this research is to gain insight into the status of such knowledge among faculty members. Methods The research presented in this paper involved a qualitative study. Semi-structured interviews were conducted with 35 faculty members employed in higher education institutions (HEIs) in Norway and Poland. The participants worked in the computer science and engineering disciplines. The data was analysed using thematic analysis, and two main themes and six sub-themes were identified. Results We found that most participants lack sufficient understanding of digital barriers and assistive technologies. Very few were aware of legislation and guidelines related to universal design. Most importantly, the majority lack practical knowledge on how to make digital learning materials and courses accessible. Furthermore, the solutions they propose for addressing the barriers are intuitive and only encompass barriers that are easy to recognise and identify. Conclusion The findings indicate that there is a gap between legislation and implementation in practice when it comes to making digital learning materials accessible in higher education. The lack of knowledge among faculty members shows that training is necessary to increase understanding and practical knowledge, and HEIs should prioritise this in strategies and action plans going forward.",https://www.semanticscholar.org/paper/c2218d037c1b7cd82a95e4817d09f76d28e9ac99,CS,Qualitative
H is for Human and How (Not) To Evaluate Qualitative Research in HCI,"Concern has recently been expressed by HCI researchers as to the inappropriate treatment of qualitative studies through a positivistic mode of evaluation that places emphasis on metrics and measurement. This contrasts with the nature of qualitative research, which privileges interpretation and understanding over quantification. This paper explains the difference between positivism and interpretivism, the limits of quantification in human science, the distinctive contribution of qualitative research, and how quality assurance might be provided for in the absence of numbers via five basic criteria that reviewers may use to evaluate qualitative studies on their own terms.",https://www.semanticscholar.org/paper/8891c6784e5f3d7deaf57d9dc20c2bc979ef5565,IS,Qualitative
State-Level Perspectives of High School Computer Science Education Policy: A Complex Systems Analysis,"Policymakers and intermediary organizations have focused on secondary computer science (CS) as a policy goal with 30 states requiring high schools to offer CS, 37 states counting CS as math or science course, and 8 states mandating CS as a graduation requirement in 2023. This qualitative study unveils preliminary findings from focus groups with Computer Science Education State Supervisors (CSEdSS) (n=11). CSEdSS were asked about the impact, implementation, and perspectives of secondary CS policies. Using a complex systems lens, we identify accelerators and policy tensions related to secondary CS education implementation from the perspectives of CSEdSS and discuss ways in which researchers and policymakers can collaborate to advance equitable CS education.",https://www.semanticscholar.org/paper/f477b13109325a259d6704b13149c6e7c11b09cd,IS,Qualitative
"Enhancing digital literacy skills among teachers for effective integration of computer science and design education: a case study at Astana International School, Kazakhstan","This study explores the development and impact of digital literacy skills among teachers at Astana International School, Kazakhstan, and examines how these skills influence the teaching of Computer Science and Design to middle school students. Employing a mixed-methods approach, the research combined quantitative assessments of students’ proficiency with qualitative evaluations of teacher and student experiences, involving 71 teachers and 382 students from grades 7 to 10. The findings indicate that students taught by digitally literate teachers demonstrated significant improvements in designing and utilizing virtual reality tools, mobile applications, and other digital resources, with teachers facilitating more interactive and engaging learning environments that enhanced students’ technical skills and creative capacities. This research contributes new insights into the dynamics of digital literacy in education, emphasizing the critical role of teacher training in digital tools for enhancing educational practices and uniquely demonstrating how systematic application of digital literacy can transform educational outcomes, supporting the integration of technology in teaching, aligned with the needs and competencies of Generation Z students.",https://www.semanticscholar.org/paper/96fc7ec4c2b8bc01b4699864ad5a05a6dbf1c36b,CS,Qualitative
The Perceptions and Experiences of In-Service Teachers in a Computer Science Professional Development Program,"This research aimed to investigate the perceptions and experiences of in-service teachers participating in a professional development (PD) program focused on computer science (CS). The main research question explored the teachers’ perceptions of their experience in the CSPD program, while sub-research questions examined the challenges encountered and the program’s impact on their capacities to teach CS in elementary schools. The study adopted an interpretivist paradigm and employed a qualitative research approach to understand the subjective meanings and hidden factors underlying teachers’ experiences. Data collection involved observations, reflection essays, and a semi-structured focus group interview. The data analysis was guided by the community of practice elements. The findings revealed prerequisite challenges faced by the teachers, such as the need to develop self-directed learning and research skills. Additionally, the PD program was found to enhance teachers’ knowledge, skills, and confidence in teaching CS. It also fostered changes in their beliefs and self-efficacy. Challenges in the pre-implementation and implementation stages were also revealed, including conflicting perspectives, limited supervisor support, and passive learning and teaching. These findings provide valuable insights that can contribute to the design of effective PD initiatives in CS education and promote sustainable education practices.",https://www.semanticscholar.org/paper/69329ade25edd409f8b992bfc1a122c347681f9f,IS,Qualitative
"EVALUATING THE INFLUENCE OF INCORPORATING COMPUTER SCIENCE, MATERIALS SCIENCE, AND DESIGN IN MECHANICAL AND ELECTRICAL ELECTRONICS EDUCATION THROUGH INTERDISCIPLINARY APPROACHES","The field of engineering education needs to adapt to the rapid advancements in technology and the increasing complexity of modern systems. This necessitates a shift towards more interdisciplinary approaches. In this study, we assess the impact of integrating elements from computer science, materials science, and design into traditional mechanical and electrical electronics curricula. We employ a mixed-methods approach that combines quantitative assessment of student performance with qualitative analysis of student experiences. Our goal is to examine the potential benefits and challenges of interdisciplinary learning in this particular domain. The results of our study indicate that interdisciplinary approaches can have a positive impact on students' problem-solving abilities. Furthermore, these approaches can foster creativity and better prepare students for the multifaceted demands of the industry. However, we also identified several challenges that need to be addressed in order to effectively implement interdisciplinary education. These challenges include curriculum design, resource allocation, and faculty expertise. Overall, our research contributes to the ongoing discourse on interdisciplinary education. It provides valuable insights for educators, policymakers, and industry stakeholders who are interested in cultivating a versatile and adaptable workforce. Such a workforce would be capable of addressing the complex technological challenges that arise in today's world. In conclusion, as technology continues to advance at a rapid pace, it is crucial for engineering education to embrace interdisciplinary approaches. By integrating elements from various disciplines, such as computer science, materials science, and design, we can enhance students' problem-solving abilities and prepare them for the demands of the industry. However, it is important to address the challenges associated with interdisciplinary education in order to ensure its effective implementation and maximize its benefits.",https://www.semanticscholar.org/paper/a3eb85a1883a25c379666356013bb1c9d1c2119c,CS,Qualitative
"Qualitative Research in Digital Era: Innovations, Methodologies and Collaborations","The differentiation of contemporary approaches to qualitative data analysis can seem daunting even for experienced social science researchers. Especially when they move forward in the data analysis process from general analytical strategies used in qualitative research to more specific approaches for different types of qualitative data, including interviews, text, audio, images, videos, and so-called virtual data, by discovering the domain ontology of the qualitative research field, we see that there are more than twice as many different classes of data analysis methods as qualitative research methods. This article critically reflects on qualitative research and the qualitative computer data analysis process, emphasising its significance in harnessing digital opportunities and shaping collaborative work. Using our extensive analytical and research project experience, the last research results, and a literature review, we try to show the impact of new technologies and digital possibilities on our thinking. We also try to do the qualitative data analysis. The essence of this procedure is a dialectical interplay between the new world of digital technology and the classic methodology. The use of digital possibilities in qualitative research practices shapes the researcher’s identity and their analytical and research workshop. Moreover, it teaches collaborative thinking and teamwork and fosters the development of new analytical, digital, and Information Technology (IT) skills. Imagining contemporary qualitative research and data analysis in the humanities and social sciences is difficult. Opening to modern technologies in computer-based qualitative data analysis shapes our interpretation frameworks and changes the optics and perception of research problems.",https://www.semanticscholar.org/paper/4c4d2c62bc8f8edd3aadb0f3e0d26cd88473e8d5,IS,Qualitative
History and Legacy of Alan Turing for Computer Science,"Throughout history we encounter some personalities who think ahead of their time and, in many cases, are misunderstood or even wronged. However, they all have one thing in common: they are geniuses who find impressive solutions to complex problems. This article aims to carry out, through bibliographical research using qualitative data analysis methodology, a historical review of one of the important personalities who wrote his name in history and has great relevance in the development of modern computing: Alan Mathison Turing or how it is known: Alan Turing. Considered the father of theoretical computer science and artificial intelligence, he made his great contribution to humanity by developing, in 1936, through an academic article, an automatic machine (""a-machine"", which was later called the Turing), who could read and write different numbers and symbols. This article covers since his birth, covering his academic trajectory, understanding a little of his mentality and vision around machines and technological equipment, his partnerships and some interesting facts about his life. Thus, it can be understood that, even at that time, it was already possible to visualize the signs of the first computational logics and realize that machines have dynamics beyond human understanding, as they can also learn from humans and other machines and, best of all, Above all, transmit this knowledge with quality. Thus realizing the legacy and historical milestone that this British mathematician, computer scientist, philosopher and biologist left for the modern and contemporary world in which we live today.",https://www.semanticscholar.org/paper/025ab8cc818c245ca791ca59f7852340c096f8a4,CS,Qualitative
Voices of Elementary Computer Science Teachers: Computer Science Integration Rationales and Practices,"Objectives. Computer Science (CS) education has become increasingly prevalent in elementary schools because of multiple rationales, such as the importance of computational literacy and the growing demand for CS-related workforce preparation. As elementary CS standards continue to be adopted by many states, more examples and voices from the field may help educators understand what effective computational thinking (CT)/CS integration looks like in practice. Method. We employed a descriptive qualitative approach to study eight award-winning K-8 CS teachers and aimed to answer three research questions: (1) What were elementary CS teachers’ rationales for CT/CS integration? (2) How do elementary CS teachers integrate CT/CS into their classroom practices? (3) What are the needs of CS teachers for meaningful CT/CS integration in elementary schools? Data were collected through interviews, a questionnaire, and artifacts and analyzed using thematic analysis. Findings. Our findings of elementary CS teachers’ rationales for CT/CS integration encompassed its essential nature as a literacy skill, its potential to promote equity in education, and its alignment with standards across various disciplines. To support CT/CS integration at the elementary level, CS teachers described important aspects with examples from their instructional practices: (a) Real-world applications; (b) hands-on activities; (c) strategies to scaffold and guide student learning; (d) collaboration with classroom teachers; and (e) support from professional development (PD). Conclusion. The study results shared CS teachers’ voices from practice and shed light on the urgent need for more support and PD opportunities for both homeroom teachers and CS teachers in elementary schools, as well as the crucial need for multiple levels of support within K-12 school systems. Continuous efforts in creating systematic and sustainable PD plans and supporting collaborative professional communities for teachers within the instructional context are a must to help prepare our elementary students with the essential CS knowledge and skills they need to thrive in this technology-rich society.",https://www.semanticscholar.org/paper/b2549c848c289f6d6a65d93c984b12d68e16e851,CS,Qualitative
‘Couldn't You Have Got a Computer Program to Do that for You?’ Reflections on the Impact that Machines Have on the Ways We Think About and Undertake Qualitative Research in the Socio‐Legal Community,"This article addresses the role that computer software programs play in the sort of textual analysis that has typically been the preserve of the qualitative researcher. Drawing on two distinct research projects conducted separately by the authors, it considers the transformation of social science software from a competent assistant that can help to sort and retrieve data, to an intelligent assistant capable of independently finding trends and counter‐arguments, to a co‐investigator capable of doing things that human researchers cannot. In addition to challenging some of the claims of ‘siliconistas’, this article considers the impact of new technology on the aesthetics of research and the professional identity of qualitative researchers. In doing so, it raises some important questions about how well we are training early‐career academics for the challenges that they are likely to face in the future world of socio‐legal empirical research.",https://www.semanticscholar.org/paper/53383d50d2b887c6bcabd375da5121338e86779a,CS,Qualitative
"AI in Computer Science Education: Tool, Subdomain, and Wildcard","The future of AI will be determined in part by how its developers are educated. Thus, how computer science (CS) education incorporates instruction in various aspects of AI will have a substantial impact on AI’s evolution. Understanding how and what CS educators think about AI education is, therefore, an important piece of the landscape in anticipating – and shaping – the future of AI.However, little is known about how educators perceive the role of AI education in CS education, and there is no consensus yet regarding what AI topics should be taught to all students. This paper helps to fill that gap by presenting a qualitative analysis of data collected from high school CS instructors, higher education CS faculty, and those working in the tech industry as they reflected on their priorities for high school CS instruction and on anticipated changes in high school, college, and workplace CS. We conclude with recommendations for the CS education research community around AI in K-12, particularly at the high school level.",https://www.semanticscholar.org/paper/112e3d312192f6dfabcf8995f594f6bbc295ab07,IS,Qualitative
Computer science for secondary multilingual learners: how school policies and practices enable and constrain equitable access,"Abstract Computer Science affords a unique context for English Learners (who we refer to as Multilingual Learners or MLs) to engage in disciplinary practices that support computational thinking and language development. Yet MLs at the secondary level tend to be systematically excluded from CS courses. Through a collaborative research process involving teachers, counselors, and administrators in seven high schools, this study considered: How do school policies and practices enable or constrain CS course access for MLs? Findings from qualitative analyses revealed how school structures, norms, and educators’ mindsets represented both enablers of and barriers to CS access.",https://www.semanticscholar.org/paper/fbbe027950daf5883aecb78043eeb84f4a964c9d,CS,Qualitative
A Longitudinal Case Study of Middle School Teachers' Concept Development of Culturally Responsive Pedagogy in Teaching Computer Science,"This work-in-progress paper reports a case study that investigates middle school teachers' conceptual development of culturally responsive pedagogy (CRP) in teaching computer science (CS). The study includes four teachers who are participating in a five-year ongoing professional learning (PL) program named CS Pathways. The project aims to build the capacity of middle school teachers in teaching its computer science and digital literacy (CSDL) curriculum and eventually engaging culturally diverse students in creating apps for community and social good. Through the lens of conceptual change, the study will explore how teachers build their concept of CRP throughout the project by using both qualitative and quantitative data. We are seeking the answers to two research questions: 1) What are teachers' understandings of CRP before and after attending the project's PL and teaching computer science? 2) How do teachers develop their concepts of CRP for teaching CS? Findings of this study will provide a deeper understanding of different CRP conceptual development pathways that teachers experience. Findings of this study can provide insights for the design of PL programs to help teachers update and reorganize their understandings and eventually improve their CRP classroom practices in teaching CS.",https://www.semanticscholar.org/paper/224fcdd54867502bcae953063bf6770330a47c5b,IS,Qualitative
Effects of Project-based Learning on Students' Learning Performance and Engagement in Computer Science,"Attention has been drawn to the benefits of integrating project-based learning (PjBL) into educational curricula to enhance student engagement. This study investigated how well project-based learning fosters student engagement and achievements in computer science courses. Qualitative and quantitative research methods were used for this experiment. A total of 60 first year undergraduate students included in the study were divided into the two following groups: experimental group (n=30) and control group (n= 30). National Survey of Student Engagement’ (NSSE) based on Coates’ five-dimensional framework theory of student engagement was used to collect the data. The results showed that there was no significant difference in the pretest scores between the experimental group and the control group (p >0.05) before the intervention. However, after the experiment, the post test scores of the experimental group were significantly higher than those of the control group (p <0.01), indicating an improvement in engagement. The mean grades and mean engagement of the experimental group were also higher compared to the control group. The difference in engagement between the experimental group before and after the intervention was statistically significant (Z=-3.522, p <0.05), with the median data after intervention being higher than before. Specifically, students engaged in project-based learning (PjBL) were more actively involved in self-perception and self-worth compared to students in traditional learning settings. These findings suggest that highly contextualized project-based learning has a positive impact on student engagement and achievements.",https://www.semanticscholar.org/paper/f7f9d5841a9492f02f7b6c3cb6383092092cd3cb,IS,Qualitative
"Understanding of Linear Algebra Concepts through the TAPPS (Thinking Aloud Pair Problem Solving) Learning Model at the Faculty of Computer Science, Dharma AUB University of Surakarta","Education aims to improve the quality of human life which is technically through lectures. A good lecture produces a positive influence on students' ability. The interaction between lecturers and students in the class is an important role in achieving the goals. Linear Algebra is an important subject in the Mathematic. Many students think that Linear Algebra are difficult. Linear Algebra at the Faculty of Computer Science, Dharma AUB University of Surakarta have not been able to improve students' understanding of Linear Algebra concepts. Because of class that are centered on lecturers. Based on the description above, the author applies the TAPPS (Thinking Aloud Pair Problem Solving) cooperative learning model to increase concept understanding of Linear Algebra. Formulation of the problem in this research: ""Is there an increase concept understanding of Linear Algebra after using the TAPPS learning model?"". This research is a qualitative descriptive group with the type of Classroom Action Research. The data used are field notes and documentation. Based on the research results, it can be concluded that there is an increase in understanding of the concept of Linear Algebra through the TAPPS (Thinking Aloud Pair Problem Solving) model.",https://www.semanticscholar.org/paper/83a5f83ef6e2fd3026033022c646462e10592619,IS,Qualitative
"Belonging, Engagement, and Wellbeing: Latine Youth Experiences in Informal, Resource Pedagogy Based Computer Science Education Programs","This critical qualitative study explores the experiences of Latine youth with informal, resource pedagogy (RP) based computer science education (CSed) programs in which youth learn to code while making music with the goal of informing more inclusive, future policy for CSed. The research question is What are the experiences of Latine youth in a program guided by RPs? Methodologically, the inquiry utilizes an embedded multiple-case approach. Data consist of focus group interviews and observations. Analysis, guided by critical, humanizing theory, included coding, analyzing, and interpreting data through an iterative process. Key project concepts and a lens of RPs framed code creation and the development of the findings. Findings illustrate how students experienced a sense of belonging that connected into their cultural identities, inspired engagement, and nourished their psychological/emotional wellbeing; and how actual and potential outcomes are related to persistence in computer science (CS), increase in CS content knowledge, and developing a CS identity.",https://www.semanticscholar.org/paper/909d81e437e1be9d2d6c6d5107a69a8dde9db81d,IS,Qualitative
Evaluating computer science students reading comprehension of educational multimedia-enhanced text using scalable eye-tracking methodology,"In this research, a mixed-method approach was employed to conduct large-scale eye-tracking measurements, traditionally associated with high costs and extensive time commitments. Utilizing consumer-grade webcams in conjunction with open-source software, data was collected from an expansive cohort of students, thereby demonstrating the scalability and cost-effectiveness of this innovative methodology. The primary objective of this research was to discern the disparities in reading behaviour when students were presented with standard text accompanied by illustrations, compared to the same text with highlighted key terms. The participants, comprised of first-year university students, completed a questionnaire and an introductory test to ascertain their knowledge level. Subsequently, they were segregated into two groups and participated in two reading sessions, during which their ocular movements were recorded. The amassed data underwent both qualitative analyses, facilitated by visualizations, and quantitative analysis, employing statistical measures on the data and test results. Notably, no significant difference was observed in the gaze patterns or test results between the experimental and control groups. However, a significant divergence in gaze patterns was identified between high-achieving students and those experiencing difficulties, as evidenced by the averaged composite heatmaps generated from the data. The findings underscore two pivotal points. Firstly, the feasibility of conducting large-scale eye-tracking experiments is demonstrated. Traditional studies in this field often employ small population samples due to the time and financial constraints associated with methods that utilize specialized eye-tracking hardware. In contrast, our methodology is scalable, relying on low-end hardware and enabling students to record data on their personal devices. Secondly, while eye-tracking may not provide substantial benefits for fine-tuning text already optimized for readability, it could serve as a valuable tool for identifying and assisting learners who are struggling. This mixed-method approach holds significant potential to revolutionize the conduct and interpretation of eye-tracking studies within educational settings.",https://www.semanticscholar.org/paper/b53fc7965aff219a3aa97b3c17f1e11e494f482d,CS,Qualitative
Booting the system: Leadership practices for initiating and infrastructuring district-wide computer science instructional programs,"While a small number of school districts across the United States are well into the process of implementing system-wide computer science education (CSed), most districts are only just getting started. But what does it look like to “get started” on CSed for a whole district? This manuscript presents a single case study of a district’s process of initiating their CS instructional initiative, highlighting a distinct set of instructional leadership practices and the institutional conditions they were responding to. Early implementation research around CSed shows that in some districts, leadership practices are less often the focus of early activities. This study sheds light on what such leadership practices can look like in the early stages of a district’s CSed initiative. Our analysis, based on qualitative data collected longitudinally over 18 months of the district’s work, identified eight intertwined leadership practices that aimed to support instructional coherence, and in our findings, we share a narrative of the district’s initiation of its CS initiative around them. The case begins with the (1) initial leadership team formation and details how that team engaged in (2) content-specific instructional capacity building for its members and (3) sensemaking of ideas around CS with their relationship to existing district activities. It moves on to the team’s (4) development of an instructional vision and an (5) associated implementation strategy, which fed into processes of (6) sensegiving to foster buy-in among teachers, and providing encouragement to engage in (7) instructional piloting. Finally, leaders engaged in (8) landscape analysis activities in order to understand existing district resources and teacher perceptions related to CS. Throughout the case, we highlight the motivations behind these practices, what resources they drew on, intersections, and dependencies among them. We close our analysis exploring a number of tensions and unintended consequences associated with these leadership activities.",https://www.semanticscholar.org/paper/517b22ce5656273eeb98d6e16a992aedfccaa327,IS,Qualitative
Adapting to online education: insights from computer science teachers,"This paper explored the attitudes and roles of computer science (CS) teachers in elementary and high schools in Croatia in the period from completely online to “normal” schooling. The research was conducted in two phases to gain insight into the adaption of online education in schools. The first part of the research was a quantitative study conducted during online education in 2019/2020 among 538 elementary and high school informatics teachers across Croatia. Data were collected from an online survey. The second part of the research was a qualitative study conducted in 2022, employing interviews for data collection. The interview questions were designed based on the survey results mentioned above. The results showed that CS teachers had a significant role in implementing online schools, acting as implementers of tools for online teaching and as helpdesk IT support to other teachers. Still, formal education nowadays is affected by online schooling, and some online practices are now adopted into regular teaching. Therefore, the findings can serve as a guideline for future education related to the digital competencies of teachers and students.",https://www.semanticscholar.org/paper/7c1708e18c99491d143433f1d2304ef443154c15,IS,Qualitative
"Evaluation of Nature-Based Production Activities with Computer Science for Middle School Students: Example of the ""Inspired by Nature"" Project","This research reflects the outputs of the ""I'm Inspired by Nature"" project supported by the TÜBİTAK Nature and Science Schools program. The aim of the research is the informatics production workshops that take place together with the nature workshops; The aim of this study is to examine the effects of participants on their scientific attitudes, their beliefs in science and their attitudes towards STEM. Sequential explanatory design, which is one of the mixed research methods in which quantitative and qualitative approaches are used together, was used in the research. Secondary school students participating in the research went through a 70-hour training process. At the end of this process, it was seen that there was no significant change in the scientific attitudes and scientific views of the participants as a result of the measurements made with the measurement tools, but there was a positive increase in their attitudes towards STEM.",https://www.semanticscholar.org/paper/30ea801e2719b3f0566a52ff30856c5c9b42fcb3,IS,Qualitative
Power of Robots: A Dedication of Computer Science for Human Capitals Management,"Robotics technology is becoming an increasingly challenging necessity for the next time zone. Humans have even become one with the technology, in various increasingly concrete applications. This study aims to describe and investigate the roles of organizational robots to redesign concepts and theories of human resources management into the new designation of human resources management. This research is a literature review with a qualitative approach that has a descriptive character. Data is taken from various sources that are compatible with this study. The data taken from different books on the related topics, public websites and other statistics, various journals, newspapers and which are collected from them. Considerable information has been gathered and collected from these sources thus allowing for accurate analysis, comparing, accumulation, explanation. The main finding is the organizational robotics have an effect on the most important concepts of theories of the human resources management especially human resources planning, employment, training and performance appraisal system for organizational regulations, finally they have ability to effect of costs, productivity, profitability of company and organizational performance.",https://www.semanticscholar.org/paper/296b78f9470081f119b1fbd307a11c9940ca9138,IS,Qualitative
"Evaluating the role of professional development on elementary teachers’ knowledge, comfort, and beliefs related to teaching computer science to students with high-incidence disabilities","Abstract This article reports results from the implementation of a model of professional development (PD) to help K-5 teachers develop the knowledge and skills to teach Computer Science (CS) in classrooms of diverse students, including students with high-incidence disabilities. This article describes our Inclusive CS model of PD, how we made the PD model available to teachers during a pandemic and presents quantitative and qualitative results about the impact of the PD on teachers’ knowledge, comfort, and beliefs related to teaching computer science to students. Results indicate that the teachers’ knowledge, comfort, beliefs and perceptions about teaching CS to students with disabilities significantly improved. Teachers’ knowledge and understanding of Universal Design for Learning for supporting students in learning about CS also improved.",https://www.semanticscholar.org/paper/0a28c3b722bc1aabb6406a2953d586c51f6aa114,CS,Qualitative
Collaborative Problem-Solving Workshops for Improving Confidence and Paving Pathways for Research Careers in Computer Science,"A workshop was designed and run to motivate and inspire female undergraduate students studying computer science (CS) to explore careers in research. Key activities include peer mentoring, alumni mentoring, graduate student mentoring, faculty mentoring, problem solving skill building (through collaborative competitions), hands-on work, and session topics about graduate school and research careers. The evaluation criteria for the workshop include building community, improving skills, instilling confidence, and motivation and inspiration. The workshop was attended by a total of 74 participants, all of whom were undergraduate women pursuing degrees in computer science (and related fields). This poster will present the design of the workshop, which was unique in that it included problem solving activities in addition to exploration of careers in CS research. Quantitative and qualitative data collected before and after the workshop will be presented in the poster, but are not described in the extended abstract due to length restrictions.",https://www.semanticscholar.org/paper/e72bfd30fe01cf2a336f20d0144a445cce74aa3a,IS,Qualitative
"Gender, Self-Assessment, and Persistence in Computing: How gender differences in self-assessed ability reduce women’s persistence in computer science","Are women less likely to persist in computer science because of gender differences in self-assessed computing ability? And why do gender differences exist in self-assessments among women and men who earn the same grades? We use a mixed-method research design to answer these questions, utilizing both quantitative survey data (n = 764) and qualitative interview data (n = 59) from students in introductory computing courses at a large U.S. state university. Quantitatively, we find that women self-assess their computing ability significantly lower than men who earn the same grades, and that these lower self-assessments reduce the likelihood that women enroll in future CS courses (relative to men who earn equivalent grades). Qualitatively, we explore how women and men perceive their own computing ability to understand why women self-assess their ability lower than men. Our interviews revealed that women were much less likely than men to make favorable comparative judgements about their ability relative to their classmates. Women also had higher personal performance standards than men. Lastly, women were more likely than men to experience disrespectful treatment, with an undertone of presumed incompetence, from their TAs and classmates. In sum, this research furthers our understanding of why gender differences exist in self-assessments of computing ability and how these differences can contribute to gender disparities in computing persistence. It also draws attention to the importance of feedback in computing courses and suggests that improving course feedback may reduce gender disparities in computing.",https://www.semanticscholar.org/paper/6f2d0ba9921ef86bf06cf18e65013295f19d0a70,IS,Qualitative
A Virtual Professional Development Program for Computer Science Education During COVID-19,"The need to expand computer science learning for all students has led to an increase in professional development (PD) opportunities for teachers. The Covid-19 pandemic, however, necessitated changes in well-established PD programs and a shift to virtual delivery. In this work, we describe our transition to a virtual PD institute, including the topics and design principles guiding the institute. We also examine how participation in the virtual PD institute influenced teacher outcomes. Data were collected from two cohorts of teachers. Data sources included surveys (N=30), lesson plans (N=22), and interviews (N=17) from a purposeful sample of participants. Findings gleaned from quantitative and qualitative analysis suggest an increase in teachers’ knowledge and self-efficacy while highlighting the affordances of virtual PD most valued by teachers. Findings have implications for research and practice.",https://www.semanticscholar.org/paper/a2de9988e31fec3883da6806c120175ee671d508,IT,Qualitative
IT Curriculum for Boot Camp: An Iterative Development In Applying OBE In Computer Science Education for Non-Formal Institution,"With the development of technology in software design, computer programming is now prevalent in almost every aspect of modern business and daily life. A renewed focus on programming education is attracting workers’ attention and converting skills. In previous research, researchers found difficulties encountered while learning programming languages. In this study, we conduct a case study on Teknokasi startup to develop a curriculum based on customer satisfaction. In this study, data was collected through surveys, which is a qualitative method. The methods used to collect data are interviews and questionnaires. The results of this study indicated three iterations, and further research is required to improve curriculum development. In this iteration, we will use the scrum framework and OBE as a theoretical base. Using input from mentors and students, this study was able to develop the next curriculum and improve programming skills, especially targeted Javascript expertise in a short time frame. Further studies need to be performed to assess this curriculum's effectiveness.",https://www.semanticscholar.org/paper/648c81058cbeea1c2c83a2908fee417d9a49cac3,CS,Qualitative
Teaching students programming with the help of educational games in the conditions of additional education in computer science,"The purpose of this research is to get students’ opinions on teaching programming to students with the help of educational games in the conditions of additional education in computer science. In order to carry out the study in accordance with the main purpose, the phenomenological approach, one of the qualitative research methods, was used. The participant group of the research consists of 40 students studying in computer science departments at various universities in Kazakhstan. Research data were collected with a semi-structured interview form developed by the researchers. As a result of the research, it has been determined that university students have half the experience of learning programming with the help of educational games. University students benefit from learning programming with the help of educational games. They categorised it as ease of learning, fun learning environment, permanence of learning, increasing motivation and developing problem-solving ability. Students categorised the disadvantages of learning to programme with the help of educational games as inadequacy in educational games, deficiencies in technical infrastructure and deficiencies in information infrastructure. Some of the students stated that learning programming with the help of educational games is not a disadvantage. Finally, the majority of university students participating in the research stated that they wanted to learn programming with the help of educational games. The results have led to the necessity of giving more space to programming courses with the help of educational games in the curriculum of the departments that provide programming-based education in universities. Keywords: Computer science, educational games, programming, student opinions;",https://www.semanticscholar.org/paper/0d40710645b5e756d6d4f6621b55304ebc2aabc2,CS,Qualitative
Research on Scientific Research Performance Evaluation Based on the Innovation Ability by Computer Science,"Scientific and technological innovation is an important force in the construction of innovative national strategy. The promotion of scientific research and innovation ability is related to the development of economy and society. Scientific research performance evaluation is based on the improvement of scientific research, as well as the development of scientific research activities. By computer science, we can assure its proficiency. It aims to actively promote the initiative of scientific research, and improve the efficiency of scientific research management. Scientific research and innovation ability are different to an extent. Traditionally, scientific research evaluation is based on the level of project leaders, personnel composition, the number of projects obtained or the level of achievements, which are not that appropriate nor scientific. Therefore, based on the qualitative and quantitative analysis of the scientific research by computer, this paper focuses on exploring scientific research performance evaluation by computer science. With the premise of clarifying the scientific research innovation ability, innovation composition, innovation goal and innovation significance, this paper will explore the strategies and methods by computer, to enhance scientific research innovation ability of colleges and universities, so as to give a reference to colleges and universities.",https://www.semanticscholar.org/paper/aa26f180314f0d548cfe19743382de21d48f9edc,IS,Qualitative
Imagine Yourself as a Media and Computer Science Teacher,"In order to investigate pre-service primary teachers’ mental images and beliefs about Media and Computer Science teaching, the Draw-A-Science-Teacher-Test Checklist (DASTT-C) was adapted for the field of Media and Computer Science. For this explorative study, 78 student teachers were asked to imagine themselves as Media and Computer Science teacher before and after a methods seminar. Using a qualitative research approach and building upon the ideas of the Draw-A-Science-Teacher-Test Checklist, the drawings before and after the course were coded. A coding scheme was developed, resulting in the Draw-A-Media-And-Computer-Science-Teacher Repository (DMECS-R). Subsequently, the results of the coding were analysed and evaluated with a mixed-method approach. Quantitative comparison of the number of recategorizations of the drawings after the methods course, comparison of ‘average’ images, visualization with multidimensional scaling and qualitative observations of minimum and maximum individual changes have led the authors to three conclusions: (1) After the method course it is less likely that student teachers draw individual work of students. (2) After the course, student teachers were less likely to draw children working on closely guided assignments. Post-course, more student teachers draw pupils working on own projects and tasks. (3) After the method course, it is less likely that student teachers draw themselves in a conventional, classically furnished classroom with only chalkboard and neatly arranged tables and chairs. Taking a dialogic and constructivist approach of learning into account, this research shows that the methods course expanded the student teachers’ repertoire of teaching methods for Media and Computer Science lessons. For following studies in computer science education, the results should be verified by accompanying interviews and subsequently find their way into pedagogical training.",https://www.semanticscholar.org/paper/c0f651d6083ca24045fa7866332cddb6f9dd5043,IS,Qualitative
Student Agency in a High School Computer Science Course,"This study explores student agency in the context of a culturally authentic computer science (CS) curriculum implemented in an introductory CS course in two high schools. Drawing on focus group and interview data, the study utilizes qualitative research methods to examine how students exercise critical agency as they engage in the course and how the curriculum supports student agency. Findings suggest three ways in which the curriculum served as a context for student agency: (1) gaining CS knowledge and skills that students then apply to address real-world needs and problems, (2) creating opportunities to “try-on” or improvise new identities and/or envision “future selves” in CS, and (3) engaging in personally relevant project work that leverages assets students brought to their experience with the curriculum. Implications for CS education research and practice are discussed.",https://www.semanticscholar.org/paper/e10b06150308ff91771d39aec2afe4b1c51da2b4,IS,Qualitative
Can Math Be a Bottleneck? Exploring the Mathematics Perceptions of Computer Science Students,"Software Engineering and Computer Science (CS) programs often contain several mathematical courses or courses with large mathematics components or dependencies. Study success in those subjects can be influenced by students’ attitudes towards mathematics and their perceptions about their own relation with mathematics. To gain insights into how the mathematics perceptions of computer science students affect their studies, we collected qualitative data from fourteen CS students from two universities in the context of an elective mathematics course. In this qualitative research, we used a triangulation strategy by collecting data from three different sources (interviews, questionnaires, and math-history assignments) to develop a comprehensive understanding of CS students’ mathematics perceptions. The thematic analysis revealed the factors that affect students’ perceptions about mathematics, including various types of prior experiences, their self-efficacy, math anxiety, and motivation sources. The analysis also highlighted that students find mathematics important for skill transfer to specific CS topics and for supporting continuous learning.",https://www.semanticscholar.org/paper/c1f54750de141d9952db976b31fed8d347fd4efe,CS,Qualitative
Youth Experiences with Authentically Embedded Computer Science in Sport,"Research on bridging sports and computer science education tends to center computing as primary, and sports as secondary. In contrast, this project aims to center sports as primary while introducing technology and computing as avenues to engage with sports more deeply. We collaborated with basketball coaches to design and implement campamento:bit, a summer program with a basketball team of Latinx students in Puerto Rico. This program integrated computing artifacts in a traditional basketball camp. We present insights from our experience testing this program with 11 participants over 5 days. We present select case studies of participant experiences and developing understandings using a qualitative-methods approach. These analyses present different shifts in athletes’ perceptions of computing and instantiated students’ interests in future possibilities involving computing. Furthermore, the analyses suggest strategies for organizing such technology-integrated experiences for youth in sports contexts.",https://www.semanticscholar.org/paper/64b561727811276f48eb8a630069feffdb1cc60a,IS,Qualitative
Identification of pupils’ preferences of patterners and dramatists in secondary school computer science education,"With the increasing integration of computer science into school curricula, a growing number of pupils are coming into contact with this subject. To get as many pupils as possible interested in computer science, the teaching strategies and methods must meet the heterogeneous prerequisites of the pupils. The purpose of this paper is to find out how to identify two different student behavioral preferences mentioned in the computer science education literature. Considering these preferences, the focus was set on the preferences for socially enriched learning on the one hand and task-oriented learning on the other hand. Even though related research has been found on the preferences of preschoolers and adults, research is missing in the field of computer science education at the secondary school level. Thus, this study focuses on the learning preferences of pupils in secondary school education regarding the two behavioral preferences. As a result, an instrument was developed and piloted that, validated by qualitative methods, measures these different preferences. Preliminary exploratory results from analyses with school classes show that preference clusters can be found in secondary education. Further research aims to use these findings to improve computer science education.",https://www.semanticscholar.org/paper/2021d9ae673d5217d155534ca7689a579e685e88,CS,Qualitative
Teaching Qualitative Research Methodology at Undergraduate Courses in Computing: An Evidence-Based Proposal,"The teaching of scientific methodology is a mandatory component for the Brazilian undergraduate courses in Computing. However, this component typically does not address qualitative research despite its increasing dissemination and relevance in Computer Science research. In this paper, we introduce the proposal of a qualitative research methodology course tailored to undergraduate courses in Computing. Among others, the didactic proposal of the course includes resources for stimulating students to think qualitatively, the linked teaching of data collection techniques, the discussion and analysis of real studies, and the conduction of group dynamics. We applied the course of qualitative research methodology as an elective course in the context of the Bachelor's Degree in Computer Science offered by Cefet/RJ, Brazil. Despite the challenges posed by exceptional remote learning, we achieved good levels of retention and approval. Through an opinion survey, we found a considerably positive perception about the learning experience and the didactic resources applied.",https://www.semanticscholar.org/paper/9cf43076e2dd631760e68cc3571fa1c7a60c4476,IS,Qualitative
Exploring the Humanistic Role of Computer Science Teaching Assistants across Diverse Institutions,"Recently, there has been a growing interest in the role of teaching assistants (TAs) in computer science (CS). This interest is due to the vital role CS TAs play in supporting student learning and their expanding responsibilities driven by growing enrollments in CS programs worldwide. While much of this research focuses on the technical and pedagogical aspects of CS TAs' duties, researchers recognize the need to further explore the unique value human CS TAs provide, particularly with the rise of AI tools and assistants. In this paper, we use qualitative methods to analyze 109 survey responses collected across two different institutions in the United States as part of a larger design-based research project to make two contributions. First, we illustrate how CS TAs adopt humanistic stances and demonstrate care in their roles, thereby expanding prevailing understandings of CS TAs. Second, we detail similarities and differences across CS TAs' experiences at each institution that underscore the importance of understanding CS TAs as they are situated in different institutional contexts. We conclude by discussing implications of this work for computing instruction and TA training, emphasizing the importance of foregrounding the roles and values brought by TAs.",https://www.semanticscholar.org/paper/6bd6361e0c1310b28d18c2e39a6fe60952a24cb8,IS,Qualitative
Computer Science Students' Experiences of Learning Technical Writing,"Computer science students regularly produce technical writing, whether it's commit messages; issues, feature requests or bug reports; user- or developer-facing documentation; research papers; reports; or other forms of writing. Post-secondary educators have published papers about curriculum and course designs that teach technical writing and other communicative skills but little work has investigated computer science students' perspectives on what technical writing skills they are learning, how they are learning them, and how they are experiencing this learning. In this lightning talk, we will provide an overview of a semester-long qualitative interview and reflective journaling study that we are conducting with upper-year computer science students to learn about their experiences, attitudes, and beliefs related to learning technical writing. We aim to answer two research questions 1) How do University of British Columbia computer science undergraduate students describe their technical writing experiences? What are their reflections on these experiences? and 2) What are University of British Columbia computer science undergraduate students' attitudes and beliefs about learning how to write technical documents in computer science courses? We will analyze the data and construct themes via Braun and Clarke's method of reflexive thematic analysis and hope that what we generate will impact the teaching and learning of technical writing in computer science. Through this lightning talk we will share our preliminary findings and aim to connect with collaborators who share interests in studying computer science students' learning of technical writing.",https://www.semanticscholar.org/paper/46318cd4f95be8ab3f7d1c088fc2b0518dbc67c1,IS,Qualitative
Exploring Cooperation and Competition in Computer Science Education: An Investigation Based on Game Theory,": The study investigates the use of the game Overcooked as a pedagogical tool in primary education, focusing on cooperation and competition. The research, conducted throughout 2022, analyzed one semester as a baseline and another with pedagogical interventions. Four groups were studied: control (no interventions), game without instructions, game with teamwork guidance, and game without verbal communication. Questionnaires and qualitative/quantitative data assessed engagement, motivation, and cooperation. The interventions significantly improved performance, motivation, and skills such as teamwork and problem-solving, validating gamification as an educational resource.",https://www.semanticscholar.org/paper/68cd1ff15b3f9115afe548ca3912adf4188f718b,IS,Qualitative
Qualitative Content Analysis – A Research Method in Social Science,"This paper discusses qualitative content analysis (QCA) as one of the qualitative research methods and its prospective implementation in social research. The article presents the genesis of the content analysis method, with emphasis on the QCA technique. It scrutinises the scope, characteristics, basic principles and techniques of QCA. It also tackles the issue of methodological rigour criteria and quality assurance measures of this research method, aimed at ensuring the credibility of the research. This article also demonstrates the general run of the QCA model proposed by Mayring. Finally, this paper evaluates computer-aided research as part of QCA.",https://www.semanticscholar.org/paper/4530e47e08191e78554ed967ab5f0e605b7a345f,CS,Qualitative
The use of ethnography in computer science research: a systematic literature review,"Ethnography is a qualitative research method frequently used in social science. We performed a Systematic Literature Review to know better its use in Computer Science research. We selected 273 papers published at ACM Digital Library in 2015 and 2016, and extracted information about their research goals, characteristics of the samples, investigation period, data collection and data analysis procedures. The main contribution of this work is to provide researchers with a pragmatic understanding of the method, presenting references for specific situations such as small samples, studies that applied specific types of ethnography or used research instruments different from observation and interview.",https://www.semanticscholar.org/paper/8f5aae822da951f6b9016ebd95315cc9ec7290e9,CS,Qualitative
Ecological Design-Based Research for Computer Science Education: Affordances and Effectivities for Elementary School Students,"Abstract This article integrates an ecological approach and design-based research in computer science education research by following the simultaneous development of a computer programming environment and curriculum for elementary school age children over 2-1/2 years. We studied the alignment of the affordances provided by the programming environment and curriculum with the effectivities of students in 4th through 6th grade (9-12 years old). We used the computer science concept of initializing as a tracer idea and both qualitative and quantitative data to identify mismatches between the affordances provided by our programming environment and the learners’ effectivities. These included requisite mathematical skills, confusion between resetting and setting up, and incorrectly assuming that features of the programming environment conveyed information. We then describe how we addressed the mismatches by removing or adding functionality to the programming environment, adding signifiers, adapting the curriculum to include scaffolding related to the effectivities, or removing activities.",https://www.semanticscholar.org/paper/885f94003984a7d979e33269da519245c7556ddc,CS,Qualitative
Students’ Perceptions of Social Networks Platforms use in Higher Education: A Qualitative Research,"The use of social networks platforms for interactions, cooperative learning, and knowledge sharing for sharing information to improve students’ educational achievement seems to be one of the more widely examined topics in the Information Systems (IS) domain compared to the adoption of other technologies. However, as social networks platforms use distracts from studies and affects study habits, using social networks platforms can result in academic difficulties. Therefore, this research seeks to identify the interaction elements such as interaction with peers, cooperative learning and engagement for sharing information and perceptual elements such as perceived usefulness and perceived ease of use social networks platforms to improve educational achievement among students. This study is designed in accordance with the theory of constructivism. Qualitative research was applied to interviews conducted with a sample of 37 students. Data were analyzed using SPSS Statistics 20, and NVivo 11 was used for qualitative coding to investigate relationships between variables. The study found that interactions among students and interactions with lecturers enhance learning significantly. Additionally, the perceived overall benefits of using media platforms for learning and knowledge sharing that enhances satisfaction and affects educational achievement were high. Furthermore, the impact of social networks platforms use for education and knowledge sharing was also significant. Finally, the results indicate that students are satisfied with the use of media platforms as a means of learning and knowledge sharing. Findings show that using social networks platforms for learning and knowledge sharing should positively affected educational achievement of students.",https://www.semanticscholar.org/paper/61be7cfba4da3113ece1b38d80b4e4167cfff2df,IS,Qualitative
Bridging the Gap Between Qualitative and Quantitative Assessment in Science Education Research with Machine Learning — A Case for Pretrained Language Models-Based Clustering,"Science education researchers typically face a trade-off between more quantitatively oriented confirmatory testing of hypotheses, or more qualitatively oriented exploration of novel hypotheses. More recently, open-ended, constructed response items were used to combine both approaches and advance assessment of complex science-related skills and competencies. For example, research in assessing science teachers’ noticing and attention to classroom events benefitted from more open-ended response formats because teachers can present their own accounts. Then, open-ended responses are typically analyzed with some form of content analysis. However, language is noisy, ambiguous, and unsegmented and thus open-ended, constructed responses are complex to analyze. Uncovering patterns in these responses would benefit from more principled and systematic analysis tools. Consequently, computer-based methods with the help of machine learning and natural language processing were argued to be promising means to enhance assessment of noticing skills with constructed response formats. In particular, pretrained language models recently advanced the study of linguistic phenomena and thus could well advance assessment of complex constructs through constructed response items. This study examines potentials and challenges of a pretrained language model-based clustering approach to assess preservice physics teachers’ attention to classroom events as elicited through open-ended written descriptions. It was examined to what extent the clustering approach could identify meaningful patterns in the constructed responses, and in what ways textual organization of the responses could be analyzed with the clusters. Preservice physics teachers (N = 75) were instructed to describe a standardized, video-recorded teaching situation in physics. The clustering approach was used to group related sentences. Results indicate that the pretrained language model-based clustering approach yields well-interpretable, specific, and robust clusters, which could be mapped to physics-specific and more general contents. Furthermore, the clusters facilitate advanced analysis of the textual organization of the constructed responses. Hence, we argue that machine learning and natural language processing provide science education researchers means to combine exploratory capabilities of qualitative research methods with the systematicity of quantitative methods.",https://www.semanticscholar.org/paper/59ba5be9b4d84d2ca77d7871c5a0d43cb73f2e4c,CS,Qualitative
Scopus-based Comparative Analysis of Computer Science Research in India and USA,"Recently many Indian authors have paid attention towards research impact analysis of Indian Institutes in several domains. This paper focuses on analysing the research impact of some potential Indian Institutes in contrast to some potential USA Institutes while considering the Computer Science domain. To achieve this goal, the data is collected from Scopus till May 05, 2019 and analysed based on several quantitative and qualitative citation metrics using Python",https://www.semanticscholar.org/paper/20d8f26731e089112094c2b8d0f9eb5197a4ac74,IS,Qualitative
Content Analyses of User Comments in Journalism: A Systematic Literature Review Spanning Communication Studies and Computer Science,"Abstract Different disciplines have studied the content of online user comments in various contexts, using manual qualitative/quantitative or (semi-)automated approaches. The broad spectrum and disciplinary divides make it difficult to grasp an overview of those aspects which have already been examined, e.g. to identify findings related to one’s own research, recommendable methodological approaches, and under-researched topics. We introduce a systematic literature review concerning content analyses of user comments in a journalistic context. Our review covers 192 papers identified through a systematic search focussing on communication studies and computer science. We find that research predominantly concentrates on the comment sections of Anglo-American newspaper brands and on aspects like hate speech, general incivility, or users’ opinions on specific issues, while disregarding media from other parts of the world, comments in social media, propaganda, and constructive comments. From our results we derive a research agenda that addresses research gaps and also highlights potentials for automating analyses as well as for cooperation across disciplines.",https://www.semanticscholar.org/paper/f0377b5a4b2c8aaf16943ef013bd3f13af9ec2ea,IS,Qualitative
Measuring the Research Performance of UK Computer Science departments via Network DEA,"In this study we evaluate the research performance of the Computer Science departments in United Kingdom. We consider the research activity as a series process with two components, where the first component portrays the research productivity and the second component portrays the impact of the research outcomes of each department. The analysis is based on the data drawn from the Research Excellence Framework 2014 (REF 2014), which is the system for assessing the quality of research in the higher education institutions of United Kingdom. We carry out the assessment by employing the composition approach of Network Data Envelopment Analysis (Network DEA). Also, we encompass a qualitative aspect into the exercise based on the categorization of the publications provided by REF 2014.",https://www.semanticscholar.org/paper/67468c3b48232d0ac483bc634f7001de22c09ad1,IT,Qualitative
"'It's a Bit Weird, but it's OK'? How Female Computer Science Students Navigate being a Minority","Within Computer Science (CS) education, women have long been underrepresented. In the UK, women make up less than 20% of CS students at A-Level and undergraduate level. The lack of diversity within CS has become well-studied, often through quantitative surveys of female CS students or evaluations of different pedagogical or cultural interventions aimed to increase diversity and inclusivity. However, there have been far fewer studies that explore the experiences of female CS students at a more in-depth, qualitative level. This paper reports on the results of 15 in-depth semi-structured interviews with first year female CS undergraduate students at a UK university. Several of our findings are consistent with much existing research, such as the prevalence of gendered CS stereotypes and a lack of confidence. However, we also find two key strategies by which female Computer Science students navigate their minority experience that have not been given much prior attention. Firstly, we find that female CS students find it hard to articulate their minority experience without utilising the linguistic device of hedging. Asked about their experiences as a minority, participants would often mention a negative feeling in relation to this situation, followed quickly by a diminishing clause- 'it's a bit weird, but it's ok'. Secondly, participants tended to individualise the problem, stressing the importance of their own individual responsibility to fit into CS and succeed, despite having sometimes experienced discrimination or sexism. We conclude by considering the implications of these findings for educators.",https://www.semanticscholar.org/paper/ee620f1a91acc4ddf2f038b52e98a447dc46d6a4,IS,Qualitative
Research on Metonymy of Cognitive Linguistics from Corpus Approach of Computer Science,"The rapid development of computer technology and corpus linguistics has realized the large-scale corpus’s collection, arrangement, annotation and processing. The corpus linguistics combines description with empirical analysis, and qualitative research with quantitative research that the various language phenomenons can be fully explained. The paper makes use of corpus-based approach to study Metonymy of Cognitive Linguistics and draws real data from Center for Chinese Linguistics Peking University of China (CCL), which makes the research more convincing.",https://www.semanticscholar.org/paper/84bfe3040ed2b219c2e8726ec5b36b638c152356,CS,Qualitative
Forming Strategic Partnerships: New Results from the Revolutionizing Engineering and Computer Science Departments Participatory Action Research,"This research paper investigates the formation of strategic partnerships, as experienced by teams of change agents in academic engineering and computer science. In this qualitative study of twelve teams making cultural, structural, and curricular change at their respective institutions, we examine the process of forming strategic partnerships through three initial stages: identifying potential partners, making an intentional approach, and establishing governance. We find teams have utilized a variety of strategies within each of these stages, such as establishing alignment of goals across the project team, the partner organization, and the home institution. These results delineate practices for initiating strategic partnerships within higher education and encourage faculty to build mutually beneficial strategic partnerships.",https://www.semanticscholar.org/paper/b1a390ae1b6eb442733f53e07943aae460fdd63f,IS,Qualitative
Team Mensa: A case study of supporting middle school girls' interest in computer science through an informal learning program,"Abstract This qualitative case study describes how participation in an outside-of-school program sustained middle school girls’ interest in computer science and positively influenced their computational perspectives. Data consists of interviews, observations, and videos analyzed from ten girls participating in a nine-month program during 2017–2018. Connected learning and computational participation are the study’s theoretical frameworks and were incorporated into its research questions, data collection methods, and analysis strategies. Findings illustrate 1) girls’ sustained interest in and positive attitudes toward computer science; 2) girls’ evolving confidence and awareness of computational perspectives; and 3) the importance of group work in nurturing girls’ computational participation. This study contributes to the research on strategies for addressing the gender gap in computing through providing informal learning opportunities for young girls.",https://www.semanticscholar.org/paper/fc2108a0936c8ba9aa917e3a9151de4aceb87d1b,IS,Qualitative
Emotional support from the perspective of extrinsic emotion regulation: insights of computer science doctoral supervisors,"ABSTRACT While higher education (HE) research has long recognized the importance of providing emotional support to doctoral students, empirical inquiries have barely investigated doctoral supervisors’ practice from the perspective of extrinsic emotion regulation. The current qualitative study used the framework of extrinsic emotion regulation (Gross 2014, 2015) to investigate 17 Chinese computer science doctoral supervisors’ strategies to improve their students’ emotions. Semi-structured interviews revealed that participants reduced students’ negative emotions and enhanced positive ones through antecedent-oriented approach, especially situation modulation strategy and cognitive change strategy. Response-oriented approach was also utilized albeit less commonly. The findings suggest that Chinese doctoral supervisors are concerned about students’ psychological wellbeing, but demonstrate remarkable variations in their emotion regulation strategy repertoire and flexibility in selecting and combining different approaches or strategies according to specific contexts. These variations further highlight the need for higher education institutions to provide professional training that fosters supervisors’ emotion regulation abilities.",https://www.semanticscholar.org/paper/5c81fed9f1762fc73228fc53a65d8389a5489e81,IS,Qualitative
"Investigating the Intersection of Science Fiction, Human-Computer Interaction and Computer Science Research","This paper outlines ongoing dissertation research located in the intersection of science fiction, human-computer interaction and computer science. Through an interdisciplinary perspective, drawing from fields such as human-computer interaction, film theory and studies of science and technology, qualitative and quantitative content analysis techniques are used to contextually analyze expressions of science fiction in peer-reviewed computer science research repositories, such as the ACM or IEEE Xplore Digital Libraries. This paper concisely summarizes and introduces the relationship of science fiction and computer science research and presents the research questions, aims and implications in addition to prior work and study methodology. In the latter part of this work-in-progress report, preliminary results, current limitations, future work and a post-dissertation trajectory are outlined.",https://www.semanticscholar.org/paper/665843b523e475e8cac4c5965adea6e9cda35c9c,CS,Qualitative
Learning Environment - What Matters for the High Ability Computer Science Students?,"The aim of this exploratory research effort was to examine the triggers of motivation to learn in highly capable Computer Science students and to identify which types of environments they perceive as being likely to ensure their academic success. The study participants, gifted students and graduates from Innopolis University, Russia, were categorized as gifted based on winning and participating in the higher stages of school subject and project Olympiads. Mixed approach was used to study this population. First, qualitative data were collected by way of offline interviews of four focus groups. Based on the results of these interviews, and to collect quantitative data, an online questionnaire of forty questions was designed and distributed among the University students online; 106 of the University students responded, each of those also qualified as gifted based on their self-identification as Olympiads participants or winners. The results of both qualitative and quantitative data analysis revealed the importance of appealing to interest and challenge in the learning process of gifted IT students, as well as the need for greater independence and flexibility in the choice of material they focus on; these conclusions imply such students are likely to prefer educational experiences that are similar to the process of preparation to the said Olympiads.",https://www.semanticscholar.org/paper/cbfdb59bac59822b52ed973603aa3268191b7575,IS,Qualitative
Computer Science Student-Centered Instructional Continuum,"The Computer-Science Student-Centered Instructional Continuum (CS-SCIC) is a new framework to support PreK-12 instructors in their lesson design. Educators are faced with choices when building lessons; there is a tension between direct instruction, constructivism and constructionism and difficulty in providing differentiated instruction. Theoretically aligned to Vygotsky's zone of proximal development, CS-SCIC places research-based instructional strategies on a simple learning continuum. Teachers use the continuum to discuss, review and design learning events. Used internationally, initial qualitative feedback from teachers who attended pilot CS-SIC workshops was emphatically positive. Future work includes more feedback from academia and formal research, including pre and post-professional development workshop surveys.",https://www.semanticscholar.org/paper/ac70d7c7b7f9945c4d2c80b5542a4a2981be6984,IS,Qualitative
Impact of UX Internships on Human-computer Interaction Graduate Students: A Qualitative Analysis of Internship Reports,"Objectives. Internships can bring a host of professional and academic benefits to students. Then, how do User Experience (UX) internships influence Human-Computer Interaction (HCI) graduate students’ professional and academic growth? What are the challenges experienced by HCI graduate students during internships? We explored these two research questions. Participants. Our study participants were 42 HCI graduate students who completed UX internships. They came from computing and related disciplines, including computer science, information technology, psychology, and design. Some of the participants’ internship titles were Interaction Designer, Design Researcher, UX Programmer, and Business Intelligence Analyst. Study Method. We conducted a thematic analysis on 42 graduate students’ UX internship reports that were collected over 6 years to uncover themes in relation to our two research questions. Findings. As for UX internship benefits, we found that students learned about the workplace culture (e.g., academia vs. industry/government on research design processes) and core UX technical (e.g., research, design, programming) and people skills (e.g., teamwork, empathy toward end-users); they also realized what they wanted in future careers after completing their internships. We also found internship challenges that were related to the internship program (e.g., the availability of internship opportunities), the host organizations (e.g., the quality of mentorship received), and remote working (e.g., difficulty over conducting remote usability testing). Conclusions. We make practical recommendations for HCI educators, UX practitioners, and HCI graduate students on how they can work collaboratively to create a meaningful UX internship experience. These recommendations include researching the host organization prior to internships, providing comprehensive onboarding, and being transparent with internship constraints.",https://www.semanticscholar.org/paper/0068bbfe7de335ed285ffeba51496a6566625c96,IS,Qualitative
Personalized Feedback Emails: A Case Study on Online Introductory Computer Science Courses,"The absence of face-to-face interaction between instructors and students in online courses has been the focus of discussion in many research papers. To compensate for this defect, the concept of Personalized Feedback Email (PFE) was introduced in two undergraduate online courses at the University of Georgia. A distinct component of PFE is a grade forecast for each individual student projected visually in graphs. The quantitative and qualitative data collected from students made it possible to claim that PFE contributes to students' engagement in online courses and encourages the majority of them to do better in class. Given that the rate of contribution of each student in course activities is correlated with student's performance, we were able to show that students who find PFE motivating make higher contributions in class activities. PFE is especially capable of targeting students who stand in the middle of the grade-range and improves their contribution and performance. In this respect, PFE also has a considerable short-term effect. The extensive applications of this effect should be limited by the optimization of the number of PFEs. All this machinery is expected to enable the complex of decision-makers associated with students to adopt the most effective learning strategies. This study shows a drastic and positive change in the performance of students who alter their learning strategy after being exposed to their forecasted grades, which enhances the potential of supervised improvement. The accuracy of forecasting model will be crucial when forecast grades are expected early in the semester to identify at-risk students. Applying machine learning methods, particularly the Greedy Linear Regression, satisfies this expectation and increases the correlation coefficient of the forecasts to 0.98.",https://www.semanticscholar.org/paper/619ec4653af5e8a2d51fdc707288b16f35cceed7,CS,Qualitative
"Minoritized Students' Engagement, Identity, and Agency in Computer Science: Listening to the Students Themselves","This panel focuses on the voices of computer science (CS) high school students who come from communities historically underrepresented in CS. Our UCLA team worked in research-practice partnership with teachers and students from the Los Angeles Unified School District (LAUSD) to understand how youth engagement, agency, and identity are being impacted, if at all, by efforts to broaden participation in computing. Going beyond a ""numbers"" approach, we define equity as attending to the cultural wealth, funds of knowledge, and perspectives youth bring to their CS learning experiences, amplifying minoritized youth's visions of what CS education should be in the CS for All movement. This panel begins by sharing findings from our year-long qualitative study in four CS classrooms, followed by hearing from the students themselves about what impacts their motivation to learn CS. Student panelists will explain how and why they choose to engage in CS learning toward empowering themselves and their communities, as well as how they are developing CS identities. Together, we will explore what it takes for youth to acquire a sense of ""rightful presence"" [1] in a field dominated by people who do not look like them or come from their communities.",https://www.semanticscholar.org/paper/20407e169a66c5d926ce9586034750c00c089e69,IS,Qualitative
Teacher Perceptions of Equity in High School Computer Science Classrooms,"Effective and equitable CS teaching is contingent on teachers’ robust understanding of equity issues in CS classrooms. To this end, this study examined high school teachers’ perceptions of equity during their participation in a CS teacher certificate program over two years. The participants are from various disciplines and from schools that serve under-represented students. Using a qualitative approach, we conducted content analysis of the teachers’ written reflections and responses to semi-structured interviews. Based on the justice-centered framework, we analyzed the major themes that emerged from the content analysis. The findings provide insights into high school CS teachers’ understanding of equity, the strategies that teachers use to address equity issues, and how teachers interpret the causes of inequities in CS classrooms. This research presents frameworks for examining teachers’ conceptualizations of equity and can inform the implementation of future professional development programs for CS teachers.",https://www.semanticscholar.org/paper/849c4fa167c88ab4d57a3e8398b58e474b4a9071,IS,Qualitative
Drama in Enhancing Motivation of Non-English Department Students: Computer Science Students,"Teaching English for the non-English Department has a huge challenge. Most of the students are in low motivation to learn a foreign language including English because they are more interested in their main subject than the supported subject. They assumed that English is difficult and complicated so that they have a low desire to learn the subject. It affects their competences and performances, therefore another creative method is crucial to be implemented in the classroom. One of the solutions is creating a fun learning of English final project. This research aims to identify whether drama can build their motivation and interest to speak and learn English. Drama can be performed by the students in groups and create stories based on their creativity. The research method used is qualitative by interviewing some of the students and the quantitative research methods by distributing a questionnaire. The research results show that most of the students stated that learning English through drama has a positive effect on their motivation, it is more than 45% of the students agree that drama encourages them to learn English. It is expected that drama can be implemented in the classroom to enhance students’ motivation in the first semester of the year.",https://www.semanticscholar.org/paper/c5c95f8e7bfbfdb4c3e0962673c4708da92307ff,CS,Qualitative
Design and Evaluation of a Teaching-Related Knowledge Sharing System to Meet the Needs of Computer Science Instructors,"This full research paper focuses on the design of a Teaching Practices Management System (TPMS) which supports the sharing of teaching practices (TPs) amongst computer science instructors. Many years of valuable TPs can be lost due to academic retirement when no competent knowledge management system is available for recording these TPs. Consequently, novice teachers are currently facing critical challenges when delivering subject knowledge that relates to algorithms, programming and the development of computational thinking skills without the benefit of others’ experience. After a study of the relevant literature, it could be seen that far too little attention has been paid to the capturing and sharing of TPs which are not easily expressed or communicated in visual or verbal terms. Thus, we design and then demonstrate a Teaching Practices Management System (TPMS) which supports the capturing of TPs. A quantitative and a qualitative evaluation of users’ experiences of employing the system shows that instructors are satisfied with it and are mostly positive about its features. The findings of this study hold considerable promise in relation to developing engaging and effective knowledge sharing systems for use by academic instructors.",https://www.semanticscholar.org/paper/f34f6a3c2177cdc0132fe48c291925ba83a112ba,IS,Qualitative
Two Case Studies of Online Discussion Use in Computer Science Education,"In this chapter, two cases that include computer science (CS) instructors' integration of an online discussion platform (Piazza) into their courses were examined. More specifically, the instructors' perspectives and role in these cases were explored to gain insight that might enable further improvements. Employing a mixed methods research design, these cases were investigated with text mining and qualitative data analysis techniques with regard to instructors' integration strategies and students' reactions to them. The results of the study showed that among these cases, one entailed a deep integration (Case 1) and the other a shallow one (Case 2). Instructors' presence and guidance through their posting behaviors had a bigger effect than the nature of the course content. Additionally, TA support in online discussions helped address the limitations of the asynchronous discussion when the TAs had the maturity to only respond to questions for which they were adequately prepared.",https://www.semanticscholar.org/paper/7c5777ce4026ad1fccbe938e420d168e47916c6d,CS,Qualitative
Defining and Designing Computer Science Education in a K12 Public School District,"Computer science is poised to become a core discipline in K12 education, however there are unresolved tensions between the definitions and purposes of computer science and public education. This study's goal is to explore how logistical and conceptual challenges emerge while designing a comprehensive K12 computer science program in a public school district. While the policy infrastructure for K12 computer science education is rapidly developing, few districts have yet implemented computer science as a core discipline in their K12 programs and very little research has explored the challenges involved in putting ideas into practice. This study reports on a committee designing a comprehensive K12 computer science education program at a small public school district in California. Through a grounded-theory qualitative interpretation of committee-member interviews and board meeting transcripts, we surfaced three themes which were the primary points of tension: how computer science is defined, how it ought to be taught, and what process ought to be used to answer these questions. Grounding these tensions in the academic discourse on K12 computer science education, this study offers recommendations to other districts designing comprehensive computer science education and suggests future directions of computer science education research that will be most useful to stakeholders of these processes.",https://www.semanticscholar.org/paper/8557b8d13c6bcbbd48eb6875047cb43e8fde3356,IS,Qualitative
Translanguaging for epistemic access to Computer Science concepts: A call for change,"Multilingual education has seemingly been relegated to English and academic literacy lecturers in South African universities. This paper reports on the use of translanguaging in a multilingual second-year computer science class. Using descriptive statistical analysis as well as qualitative analysis, results showed that students perceived translanguaging to be a helpful pedagogical strategy that could be used to assist them to gain deeper meaning and understanding of difficult concepts in their field of study. The research also showed and proved that, for translanguaging to become relevantly useful to the students, all members of the faculty should be involved in using this pedagogical strategy in the classroom. Therefore, we argue that, through collaborative efforts by using translanguaging among lecturers from different faculties, students will be able to acquire deeper meaning and understanding of subject material, especially in content-based courses or modules.",https://www.semanticscholar.org/paper/77c22bab3a453149f5fb4310e9e867d2bc37b4be,IS,Qualitative
Gender Disparity in Computer Science Education in Bangladesh: A Study of Women’s Participation in Computer Science,"Bangladesh is yet to produce adequate number of engineering professionals in the field of Computer Science.29 public, 54 private and 2 international universities aren’t enough to allow more students in engineering and technical education. On the other hand, Bangladesh is still economically struggling and eastern patriarchal social barriers dominate the socio-cultural condition. Females of Bangladesh has been facing many visible and inherent obstacles in the way of earning higher education in technological discipline. The gender ratio of computer science education in undergraduate level in Bangladesh is 1:4, while in the IT sector the percentage of females goes down to less than 10%. In this paper, we have carried out an explorative study to investigate the complexities of these phenomena through qualitative research of Bangladeshi female students enrolled in computer science undergraduate program. We studied on a mixed sample of students who are deeply involved with computer science related activities and competitive programming. The study reveals that social barriers and economic issues quite hinder the smooth progress of an enthusiastic and potential female student of Computer science, while strong determination and family support take students to achieve their desired learning curve in computer science.",https://www.semanticscholar.org/paper/00e5b204cbc481ed445f297424ce4a7a308ed4ea,CS,Qualitative
Greek Computer Science Teachers’ Training Needs Assessment,"The present study, following the results of a qualitative study that investigated the explicitly expressed, the felt (but not explicitly expressed) and the latent training needs of the computer science teachers of the Heraklion region of Greece, has questioned all computer science teachers in Greece (5865) to confirm or not the qualitative study results. This has resulted in the largest survey that has taken place for Greek computer science teachers training needs and the only one that is competency-based as far as teaching and pedagogical training needs are concerned. The aforementioned properties of the study enable the design of a three modules (Subject knowledge - Teaching methodology - Pedagogy) training program with explicit training goals; a training program that leaves little mismatch between training needed and training provided.",https://www.semanticscholar.org/paper/cc6d6278fb81bfb09e37f4072de34e7e7d9802f8,CS,Qualitative
Computer Science Educators Stack Exchange: Perceptions of Equity and Gender Diversity in Computer Science,"The representation of women in computer science (CS) is low in the United States and has been declining over the past few decades. Prior research has demonstrated that educators play an important role in increasing gender diversity. Not much, however, is known about their views regarding gender diversity in CS. Educators are often isolated as the only ones teaching CS in their schools, with limited opportunity to converse with each other about the underrepresentation of women in CS and other issues facing the field. Thus, to better understand educators rq questions and concerns about gender diversity, we conducted a qualitative exploratory study within an online discussion forum, Stack Exchange for CS educators. The conversations gave us insights into gender diversity issues that the forum members were interested in, such as enrollment, retention, and roles of teachers. We also analyzed the lenses through which the forum members spoke about gender representation. Results from this study suggest that researchers need to continue to examine educator perceptions so that we can design appropriate online teacher communities, teacher education courses, and professional development workshops to address equity and gender diversity issues in CS.",https://www.semanticscholar.org/paper/0c6ae1130f33cf9eff5772db8e1f616cd9967eff,IS,Qualitative
Digital Storytelling and Group Work: Integrating the Narrative Approach into a Higher Education Computer Science Course,"This study discusses the integration of digital storytelling and the narrative approach into a University level Computer Science course. The pedagogical intervention took place on a project basis. The plan involved student work in groups for the production of digital stories in three phases, including an abstract, a manuscript and a final story. The overall instructional design included workshops and lectures, online tutorials, and group work. The students were assigned to explore the topic of recursion. Face-to-face meetings for the coordination of group work were emphasized during lectures, workshops and project instructions. The study uses qualitative research methods and the findings indicate two main patterns of group work. The first pattern follows from loose coordination and division of tasks among group members at the initial stages of the project. This results in documentary-like and program-based video stories. The second pattern involves tighter collaboration with face-to-face meetings for common task completion, video recording and editing, and manuscript improvement. This mode of work results in short-film style stories where recursion is well-represented. In both patterns, however, the videos present external rather than internal examples of recursion. As a result, the digital stories represent what the code does instead of how it does it.",https://www.semanticscholar.org/paper/cbc60d0b43e2c95bc74e1e7874e2d906649daedc,IT,Qualitative
A Holistic Approach for Computer Science Education in Secondary Schools,"In this study, effectiveness of a computer science course at the secondary school level is investigated through a holistic approach addressing the dimensions of instructional content design, development, implementation and evaluation framed according to ADDIE instructional design model where evaluation part constituted the research process for the current study. The process has initiated when the computer science curriculum had major revisions in order to provide in-service teachers with necessary support and guidance. The study is carried through as a project, which lasted more than one year and both quantitative and qualitative measures were used through a sequential explanatory method approach. The intention was to investigate the whole process in detail in order to reveal the effectiveness of the process and the products. In this regard, not only teachers’ perceptions but also students’ developments in their perceptions of academic achievement and computational thinking, as well as correlations between the computational thinking sub-factors were investigated. The findings showed that the instructional materials and activities developed within the scope of the study, positively affected the computational thinking and academic achievement of students aged 10 and 12 years old. The teachers’ weekly feedbacks regarding application structures and implementation processes were also supported the findings and revealed some more details that will be useful both for instructional designers and teachers.",https://www.semanticscholar.org/paper/8c9741c891f44cf6eac3783ef0e735f79d2b9cdd,CS,Qualitative
Coding Code: Qualitative Methods for Investigating Data Science Skills,"Abstract Despite the elevated importance of Data Science in Statistics, there exists limited research investigating how students learn the computing concepts and skills necessary for carrying out data science tasks. Computer Science educators have investigated how students debug their own code and how students reason through foreign code. While these studies illuminate different aspects of students’ programming behavior or conceptual understanding, a method has yet to be employed that can shed light on students’ learning processes. This type of inquiry necessitates qualitative methods, which allow for a holistic description of the skills a student uses throughout the computing code they produce, the organization of these descriptions into themes, and a comparison of the emergent themes across students or across time. In this article we share how to conceptualize and carry out the qualitative coding process with students’ computing code. Drawing on the Block Model to frame our analysis, we explore two types of research questions which could be posed about students’ learning. Supplementary materials for this article are available online.",https://www.semanticscholar.org/paper/cca3ce8c2882fce5f4c1c193f8c70747c67b32d9,CS,Qualitative
Building Momentum: How Department Chairs Lead Initiatives to Broaden Participation in Computer Science,"Broadening participation in computer science (CS) for women and underrepresented minority (URM) students is an important national priority. This research is part of a larger, longitudinal, qualitative study that highlights the role of the department chair in leading diversity-related change initiatives at a set of institutions across the country. Findings from this study reveal important insights about how department chairs lead diversity-related change efforts. Further, findings also highlight the ways that department chairs grapple with dramatically increasing enrollments, representative diversity, and critical mass.",https://www.semanticscholar.org/paper/2c322dad8a4c98feebc652881509ce6b2d30604f,IS,Qualitative
The Impact of Integrating a MOOC Platform on the Teaching of Computer Science Course: A Case Study,"The present study reports an attempt to identify and evaluate the effectiveness of integrating teaching MOOC platform into traditional learning system to teach a face-to-face course. It investigated students’ opinions of Sharurah College of Science and Arts about the benefits and challenges based on their learning experiences in a hybrid learning environment. The researcher used both quantitative and qualitative methods to address the research questions. The study shows the following findings: a MOOC platform, as a new source for learning, supports teaching and learning in a hybrid learning environment; it can be used to support traditional learning; students like and are interested in a hybrid MOOC learning environment; and, finally, the most challenges affecting the hybrid MOOC environment are Internet drop/low speed connections and limited time for discussion in the MOOC.",https://www.semanticscholar.org/paper/a33363b4bd62b34ab5be4ebdea31fea073cd26b3,IS,Qualitative
A qualitative study of students’ perspectives on barriers to and challenges in health informatics research in Iran,"Background Health Informatics (HI) is a crucial field of study that combines computer science, medical science, and information science to develop and produce information systems and software that support healthcare providers. As a multidisciplinary area, it presents specific challenges for research. This study aimed to identify the barriers and challenges faced by HI researchers. Methods This qualitative study was conducted in 2023. The study population consisted of students from Health Information Management and Medical Informatics who were actively engaged in research. Data collection was carried out through interviews using a voice recorder, with the process continuing until data saturation was achieved. Data analysis was performed using the content validity method. Results Barriers and challenges were classified into four main themes and 13 main categories. The main categories included teaching, data, time, finances, relationships and cooperation, individual factors, organizational aspects, project management, instruments and infrastructure, cultural influences, methodological concerns, and laws and procedures. Conclusion Research in HI faces both global challenges and region-specific barriers, such as sanctions and ambiguous roles within healthcare systems, necessitating international collaboration and targeted policy frameworks. Strengthening problem-solving skills, fostering professional mentorship, and adopting interdisciplinary approaches can enhance student engagement and research quality. By emphasizing HI’s critical role in advancing healthcare and leveraging successful global case studies, opportunities for innovation and growth in this field can be unlocked. Supplementary Information The online version contains supplementary material available at 10.1186/s12913-025-12697-7.",https://www.semanticscholar.org/paper/7c112ead4b908c92568cc94c40f0851ce85c2ea3,IS,Qualitative
High School Students’ Motivation to Learn Climate Change Science through Educational Computer Games,"Background Teaching climate change is difficult. Its complexity spans many subjects, often taught disjointedly. Many climate change effects are not immediately observable, making it hard for students to connect to it personally. Aim This study investigates how we can spark high school students’ interest in learning about climate change using educational computer games. Method We adopted a qualitative case research design to understand how games boost students’ drive and their role in motivating them. We selected a high school teacher and her eight students as our subjects, interviewing them in person. We analyzed their responses were using Keller’s ARCS Theory of Motivation Model and blending deductive and inductive methods. Results The findings were encouraging: games positively impacted students’ interest in climate change. They transformed the learning atmosphere into a concentrated, captivating space where the content was seen as tough yet enjoyable. Moreover, the games helped students make real-world connections, enhancing their understanding and appreciation of the topic. Conclusion Educational games are a powerful tool in motivating students to learn about climate change science. Hence, educators should be ready to harness the games’ power to create immersive, fun, and stimulating learning environments.",https://www.semanticscholar.org/paper/26f7c1ee244abcef6a7f3653033292fe38a631a4,CS,Qualitative
Context-based teaching and learning of fundamental computer science concepts: exploring teachers' ideas,"In context-based education, authentic situations ('contexts') are used as starting points for learning content matter ('concepts'). In this way, contexts provide significance and meaning to the concepts taught. The context-based approach has been investigated extensively in the field of science education. Context-based education has the potential to be a useful strategy in computer science (CS), in particular for teaching and learning of fundamental concepts. Initiatives like Informatik im Kontext confirm that context-based teaching and learning is a promising approach. So far, however, little research has been done on particular aspects of context-based learning in CS, such as the effective selection of contexts, principles for connecting concepts and contexts, and mechanisms for fostering knowledge transfer. This work-in-progress paper reports on an ongoing qualitative study on context-based teaching of fundamental CS concepts connected to algorithmic thinking. The study focuses on experiences and ideas of teachers, as they play a key role in the adaptation of contexts stemming from a rapidly changing field. We conducted semi-structured interviews with CS teachers on the above aspects of context-based teaching. The results reveal various ideas that teachers have on the use and effects of context-based learning and raises questions about the selection of contexts.",https://www.semanticscholar.org/paper/b5afd695a2e6c96eee73ed2c41eb2d45041f3bf6,CS,Qualitative
Rising English Vocabulary Mastery: Crosswords Puzzle Games for Computer Science Students,"English is a Foreign Language as a subject that has been studying since elementary. However, most of the students; English is a difficult course to learn. Thus, it needs improvement both from the media system used and the appropriate strategy for the English course as a general basic course in particular in the Faculty of Computer Science into a fun learning. A way to make English fun and easy to learn is the use of the strategy of a game called Crossword Puzzle Games. The purpose of the research is to find out whether the use of Crossword Puzzle strategy can improve the value of English courses, whether by using the Crossword Puzzle Learning Strategy can improve students’ Vocabulary and find out whether using the learning strategy of Crosswords Puzzle can increase students’ interest in Computer science faculty in learning English. The method used in this research is descriptive qualitative, which the result of this research is the elucidation of data obtained from quantitative and qualitative. The results of this study obtained through the collecting and analyzing of quantitative and qualitative data. Quantitative data obtained through the oral test at Mid Semester Test and qualitative data acquisition of questionnaires, observation, and interviews with several students. The result obtained from the research is the use of strategy Crossword Puzzle can increase the value of English courses in the first-semester students’ faculty of Computer Science, by using Crossword Puzzle learning strategy can improve Vocabulary first-semester students’ faculty of the Computer Science, and by using Crosswords Puzzle learning strategy can increase student interest of the first-semester students’ faculty of Computer Science in learning English.",https://www.semanticscholar.org/paper/8385b4a46a08c9a864492683edcb3846843a3753,CS,Qualitative
Corpus linguistics is not just for linguists: Considering the potential of computer-based corpus methods for library and information science research,"The purpose of this paper is to generate awareness of and interest in the techniques used in computer-based corpus linguistics, focusing on their methodological implications for research in library and information science (LIS).,This methodology paper provides an overview of computer-based corpus linguistics, describes the main techniques used in this field, assesses its strengths and weaknesses, and presents examples to illustrate the value of corpus linguistics to LIS research.,Overall, corpus-based techniques are simple, yet powerful, and they support both quantitative and qualitative analyses. While corpus methods alone may not be sufficient for research in LIS, they can be used to complement and to help triangulate the findings of other methods. Corpus linguistics techniques also have the potential to be exploited more fully in LIS research that involves a higher degree of automation (e.g. recommender systems, knowledge discovery systems, and text mining).,Numerous LIS researchers have drawn attention to the lack of diversity in research methods used in this field, and suggested that approaches permitting mixed methods research are needed. If LIS researchers learn about the potential of computer-based corpus methods, they can diversify their approaches.,Over the past quarter century, corpus linguistics has established itself as one of the main methods used in the field of linguistics, but its potential has not yet been realized by researchers in LIS. Corpus linguistics tools are readily available and relatively straightforward to apply. By raising awareness about corpus linguistics, the author hopes to make these techniques available as additional tools in the LIS researcher’s methodological toolbox, thus broadening the range of methods applied in this field.",https://www.semanticscholar.org/paper/b71c813153d9b1f3e90567914c7794832eb20361,CS,Qualitative
KSAO Framework for Computer Science Project Student Teams,"None of the existing Knowledge, skills, abilities and other factors (KSAO) frameworks have included task work skills as KSAO factors; all of them only proposes teamwork KSAO factors. Moreover, all of these frameworks were proposed for professional teams only. The aim of this research is to propose a KSAO framework for computer science capstone project student teams; the framework not only have the teamwork skill component but the taskwork skill component as well. Research papers on technology students, capstone projects, self-managing teams and KSAO frameworks are analyzed comprehensively so as to give rise to a framework that is presented here. The methodology followed is qualitative thematic analysis. The framework is developed as part of a doctoral thesis for guiding the identification of team building criterion for building computer science capstone student project teams and for developing an automated intelligent software for assisting students in team building.",https://www.semanticscholar.org/paper/1a066c3fe4ffa38a608fdd188242e6e1e00a5057,CS,Qualitative
Exploring ChatGPT’s impact on post-secondary education: A qualitative study,"As Chat Generative Pre-trained Transformer (ChatGPT) gains traction, its impact on post-secondary education is increasingly being debated. This qualitative study explores the perception of students and faculty members at a research university in Canada regarding ChatGPT’s use in a post-secondary setting, focusing on how it could be incorporated and what ways instructors can respond to this technology. We present the summary of a discussion that took place in a two-hour focus group session with 40 participants from the computer science and engineering departments, and highlight issues surrounding plagiarism, assessment methods, and the appropriate use of ChatGPT. Findings suggest that students are likely to use ChatGPT, but there is a need for specific guidelines, more classroom assessments, and mandatory reporting of ChatGPT use. The study contributes to the emergent research on ChatGPT in higher education and emphasizes the importance of proactively addressing challenges and opportunities associated with ChatGPT adoption and use.",https://www.semanticscholar.org/paper/a6b235ca0a986f3558e47c4c8ec3b51c562745d9,IS,Qualitative
Factors that influence the adoption of rehabilitation technologies: a multi-disciplinary qualitative exploration,"Background Technological innovation is recognised as having the potential to enhance rehabilitation for people with disability. Yet, resistance to, and abandonment of, rehabilitation technology is prevalent and the successful translation of technology into rehabilitation settings remains limited. Therefore, the aim of this work was to develop an in-depth, multi-stakeholder perspective on what influences the adoption of rehabilitation technologies. Methods Semi-structured focus groups were conducted as part of a larger research project aiming to facilitate the co-design of a novel neurorestorative technology. Focus group data were analysed using a five-phase hybrid deductive-inductive approach to qualitative data analysis. Results Focus groups were attended by 43 stakeholders with expertise in one or more of the following fields: people with disability, allied health, human movement science, computer science, design, engineering, ethics, funding, marketing, business, product development, and research development. Six main themes influencing the adoption of technology in rehabilitation were identified: cost beyond the purchase price, benefits to all stakeholders, trust to be earned in technology, ease of technology operation, ability to access technology, and the ‘co’ in co-design. All six themes were found to be interrelated; in particular, the importance of direct stakeholder engagement in the development of rehabilitation technologies (the ‘co’ in co-design) was prevalent in all themes. Conclusions A range of complex and interrelated factors influence the adoption of rehabilitation technologies. Importantly, many of the issues that have the potential to negatively impact rehabilitation technology adoption may be addressed during development by utilising the experience and expertise of stakeholders who influence its supply and demand. Our findings state that a wider cohort of stakeholders needs to be actively engaged in the development of rehabilitation technologies to better address the factors that contribute to technology underutilisation and abandonment and facilitate better outcomes for people with disability.",https://www.semanticscholar.org/paper/a31d0c3ecd528ec43ab70b0841f099aa0d935930,IS,Qualitative
Natural Language Processing (NLP) in Qualitative Public Health Research: A Proof of Concept Study,"Qualitative data-analysis methods provide thick, rich descriptions of subjects’ thoughts, feelings, and lived experiences but may be time-consuming, labor-intensive, or prone to bias. Natural language processing (NLP) is a machine learning technique from computer science that uses algorithms to analyze textual data. NLP allows processing of large amounts of data almost instantaneously. As researchers become conversant with NLP, it is becoming more frequently employed outside of computer science and shows promise as a tool to analyze qualitative data in public health. This is a proof of concept paper to evaluate the potential of NLP to analyze qualitative data. Specifically, we ask if NLP can support conventional qualitative analysis, and if so, what its role is. We compared a qualitative method of open coding with two forms of NLP, Topic Modeling, and Word2Vec to analyze transcripts from interviews conducted in rural Belize querying men about their health needs. All three methods returned a series of terms that captured ideas and concepts in subjects’ responses to interview questions. Open coding returned 5–10 words or short phrases for each question. Topic Modeling returned a series of word-probability pairs that quantified how well a word captured the topic of a response. Word2Vec returned a list of words for each interview question ordered by which words were predicted to best capture the meaning of the passage. For most interview questions, all three methods returned conceptually similar results. NLP may be a useful adjunct to qualitative analysis. NLP may be performed after data have undergone open coding as a check on the accuracy of the codes. Alternatively, researchers can perform NLP prior to open coding and use the results to guide their creation of their codebook.",https://www.semanticscholar.org/paper/cbae17234b0da5136bd7d4445cc20de3eeb9614c,CS,Qualitative
Caregiving experiences of stroke caregivers: A systematic review and meta-synthesis of qualitative studies,"Background: Caregivers of stroke patients have demanding caregiving tasks and roles, and they face multiple challenges and a variety of needs in their caregiving process. This study aimed to systematically integrate and evaluate qualitative research data to understand the care experiences of caregivers of stroke patients and to provide them with targeted assistance. Methods: We critically assessed the study using the Joanna Briggs Institute Critical Assessment Checklist for Qualitative Research. Extraction, summarization and meta-synthesis of qualitative data. Qualitative studies related to this study were searched in PubMed, Web of Science, China National Knowledge Infrastructure, China Science and Technology Journal Database, China Biomedical Literature Database, and Wanfang Database by computer from the establishment of the database to February 2022. The method of tracing citations was used to find other articles that might be included to ensure the comprehensiveness of the search articles. Results: A total of 11 studies, including 167 participants, were included, and 40 research results were extracted, summarized into 8 new categories, and formed 2 meta-themes. Meta-theme 1: Caregivers had emotional struggles during caregiving but were still able to provide compassion and care for patients through positive coping. Meta-theme 2: caregivers faced many difficulties in the care process and were eager for support. Conclusions: Caregiving is a long and heavy process. Society, medical institutions, and families should pay full attention to the feelings and needs of stroke caregivers in the care process and provide appropriate support for them.",https://www.semanticscholar.org/paper/975e641c6a809cbd60286712ba2e0961bbce3aca,CS,Qualitative
Therapeutic experience of intravenous thrombolysis after acute ischaemic stroke experiencing workplace violence: meta-integration of qualitative studies,"Objective To systematically evaluate the therapeutic effect of intravenous thrombolysis after acute ischaemic stroke caused by workplace violence, and to provide reference for healthcare professionals to optimise the care model and implement precise intervention. Methods Computer searches of the Cochrane Library, PubMed, Embase, CINAHL, Web of Science and other databases were used to search for qualitative studies on intravenous thrombolysis treatment after acute ischaemic stroke experiencing workplace violence from the time of database construction to May 2023. The quality of the literature was assessed using the Australian JBI Centre for Evidence-Based Medicine's Quality Assessment Criteria for Qualitative Research, and the results were integrated using the pooled integration method. Results A total of 9 papers were included and 40 studies were extracted, which were aggregated to form 6 new categories and summarised into 2 synthesised studies. Conclusion Healthcare professionals should pay great attention to the treatment experience and information needs of patients with AIS intravenous thrombolysis, and build an information platform, medical support and humanistic care for AIS emergency nurses to help patients increase their confidence in coping with the disease.",https://www.semanticscholar.org/paper/51720c75527dbf28549d2b8d1f066355506a0279,CS,Qualitative
Application of Natural Science Methods in Social Science Research,"The introduction of natural science methodologies into social science research aims to enhance the scientific rigor and validity of studies through precise techniques. By employing quantitative analysis, data mining, and model construction, researchers are able to delve deeper into the complexities of social phenomena, injecting new vitality into traditional qualitative research. Quantitative analysis methods render research outcomes more objective and reproducible, while data mining techniques can extract underlying patterns and trends from vast amounts of data. Model construction offers a systematic understanding of social dynamic processes, supporting the prediction and intervention of future social developments. Computer simulation technologies, particularly with the backing of big data and high-performance computing, enable hypothesis testing and virtual experiments on social systems, allowing for multi-level and multi-perspective observations of social phenomena. This interdisciplinary integration not only provides new research tools and methodologies for the social sciences but also effectively fosters the interchange of knowledge between different disciplines. However, continuous reflection and adjustment in practical applications are necessary to address the potential pitfalls and limitations that may arise from the complexity of these methods.",https://www.semanticscholar.org/paper/a69cf722640a3be051b7639551b0b393709e6042,CS,Qualitative
Mathematics and Science Teacher’s Conception and Reflection on Computer Programming with Scratch: Technological and Pedagogical Standpoint,"The current study investigates teachers' conception and reflection of computer programming from scratch in terms of technological and pedagogical standpoint. Mixed research approaches typically achievement tests and interviews were used to collect quantitative and qualitative data respectively. A total of twenty-one Mathematics and Science teachers were randomly selected to participate in the study. A paired t-test was used to analyze quantitative data from pre and post-test, while descriptive and interpretive analyses were used to analyze qualitative data from the interview. The results of the study showed that Mathematic and Sciences teachers have a great conception of scratch programming after attending scratch programming. There was a significant difference in mean between pre and post-test (p ˂ 0.05). It was also found that scratch is an effective pedagogical tool for teaching and learning Mathematics and Sciences. In addition, teachers expressed positive views of using scratch in teaching and learning Mathematics and Sciences as it helps them to visualize abstract content, motivate students, increase students' interest, critical thinking, and problem-solving skills, act as an assessment tool and increase student's academic performance.",https://www.semanticscholar.org/paper/24e509baf9110f00821ba8607658705919a37a46,CS,Qualitative
"Reflections on qualitative data analysis software – Possibilities, limitations and challenges in qualitative educational research","The progressing digitalization is increasingly permeating all spheres of life: also science. It has reached not only natural sciences, but also the humanities, such as qualitative educational research. For example, for the analysis of qualitative data in educational science researchers are increasingly using computer-assisted tools. This so-called QDA (qualitative-data-analyses) software was developed in the early 1980s in the course of the spreading digitalization to support the category based analyzes of qualitative research. Providing an overview about the current literature situation regarding QDA software and the issues addressed a systematical bibliographic research in pertinent databases were conducted. As a result, five different types of literature could be identified. Even though all categories are demonstrated only one is presented in detail. The paper focus on the category in which methodological issues are negotiated and illustrates the therein addressed arguments. Possibilities and limitations of QDA software for educational research are illustrated and past and present discussions regarding this software are outlined. In particular, it is emphasized that so far limitations of QDA software are rarely addressed. In order to deal with this void in the future more comprehensively a set of questions is proposed, which allows to reflect the use of QDA software in a specific research project in an extensive manner. Concluding further questions are raised and it is discussed if also sequential analyzes can be carried out with current QDA software or if they need new digital tools.",https://www.semanticscholar.org/paper/0af5a56dc174ca5f3af8c91406c7d78948ea0b33,CS,Qualitative
The Impact of Blended Teaching in Computer Fundamentals Under the Online Learning Platform on College Students' Learning Outcomes,"Background and Aim: With the rapid development of technology, online learning platforms have become important tools in the field of education, having a profound impact on traditional teaching methods. This study aims to construct a blended teaching model for computer fundamentals based on the 5Y online learning platform, exploring its effects on enhancing students' learning outcomes (theoretical knowledge, practical skills, attendance, and interactive participation). Additionally, the study analyzes students' perceptions of blended learning in computer fundamentals under this platform. Materials and Methods: This study adopts a mixed-method research approach, combining quantitative research through quasi-experiments and qualitative research through face-to-face interviews. A total of 84 first-year students from Zhanjiang University of Science and Technology, Guangdong Province, China, were selected using purposive sampling during the 2023 academic year. The quasi-experiment divided the students into a control group (42 students) and an experimental group (42 students). The experimental group engaged in blended learning using the 5Y online learning platform, while the control group received traditional teaching. Data were collected through performance tests, attendance records, participation scores, and face-to-face interviews, and analyzed using descriptive statistics, independent sample t-tests, and thematic analysis. Results: The results show that blended learning in computer fundamentals using the 5Y online learning platform significantly outperforms traditional face-to-face teaching in improving students' learning outcomes. The experimental group demonstrated significant advantages in enhancing theoretical knowledge, practical skills, attendance, and interactive participation. Compared to the control group, all improvement dimensions (theoretical knowledge, practical skills, attendance, and interaction) had p-values less than 0.05, indicating statistically significant differences. Furthermore, students expressed positive attitudes toward the learning experience in this blended teaching model. Conclusion: Blended teaching based on the 5Y online learning platform shows a significant advantage in improving learning outcomes in computer fundamentals courses, providing students with a positive learning experience. These findings provide important insights for future digital teaching strategies and research.",https://www.semanticscholar.org/paper/e71675c9fe3cf6aee90034646da3fbad5b274ffc,CS,Qualitative
Research progress and trend analysis of computer vision based on cite space,"In recent years, with the rapid development of artificial intelligence and the proposal of efficient algorithms such as deep learning, the ability of computer vision to deal with complex problems has made a qualitative leap. Therefore, the study of computer vision is deepenly expanded from academia to industry, and the application of industry continues to be enriched. It involves many disciplines in natural science and social science. The main applications and technologies include image classification, image recognition, deep learning, convolutional neural network, etc. In this paper, the bibliometrics software CiteSpace was used to conduct visualization processing based on the literatures in the core database of Web of Science with the time span from 2010 to 2020 and the subject words as computer vision. The current situation of scientific research cooperation is analyzed from the perspectives of countries, institutions, individuals and cited literatures themselves. The involvement of computer vision discipline is analyzed from the perspectives of natural sciences and social sciences in front, and analyzed from the funding institutions. Funded institutions are analyzed from the perspective of computer vision discipline. The current research hotspots and development frontiers are analyzed by using maps. It is found that the future computer vision will focus on deep learning, transfer learning and semantic segmentation. Finally, it points out that there are some problems in the current research, such as the lack of close scientific research cooperation, the lack of comprehensive discipline involved, and the difficulty of technology implementation. Three suggestions are put forward to solve these problems: (1) Holding trans-regional academic exchange meetings to promote the sharing of academic achievements. (2) Raising the attention to the partial humanities and promote the depth of disciplines. (3) Considering the cost and efficiency in practical application to accelerate the realization of scenario application.",https://www.semanticscholar.org/paper/1b589004fc8db31e76803e6fff29dcefc83ccdc5,CS,Qualitative
Hybrid AI Systems Grounded on Qualitative Spatio-Temporal Reasoning,"Qualitative Spatial and Temporal Reasoning, or QSTR for short, is a major research area in AI that deals with the fundamental cognitive concepts of space and time in an abstract, human-like manner. For instance, in natural language one uses expressions such as “Region X is located inside or north of Region Y” or “Task A is scheduled after or during Task B” to spatially or temporally relate one object with another object or oneself, without resorting to providing quantitative information about these entities. In brief, QSTR simplifies complex mathematical theories that revolve around spatial and temporal entities to manageable qualitative constraint languages (calculi), which can in turn give rise to interpretable spatio-temporal representations. Thus, QSTR forms a concise and explainable paradigm for dealing with entities pertaining to space and time, with the potential to boost research in a plethora of domains that can range anywhere from theoretical computer science and logic to practical algorithms and applications. In this tutorial, we take a twofold approach to introducing our audience to the rich research area of Qualitative Spatial and Temporal Reasoning. First, we present the scientific background in detail, mentioning some terminology, key definitions, and problems associated with the field, and follow up with a presentation of the state-of-the-art frameworks that exist for handling QSTR data, focusing on native methods and Boolean satisfiability (SAT) and Answer Set Programming (ASP) approaches. Secondly, and most importantly, we address the gap that exists between QSTR—a symbolic paradigm—and Machine Learning, and bring forward some successful examples of neuro-symbolic integration in the context of spatio-temporal information from the recent literature; we argue for further pursuing this promising research direction and explain the current challenges that need to be overcome for obtaining hybrid AI systems that can be applied to highly active areas such as planning, data mining, and robotic applications.",https://www.semanticscholar.org/paper/39f6fac7d630d0b7ab241b0159e218cf360f004b,CS,Qualitative
Qualitative Study of Professional Virtue Development in Engineering,"This research-to-practice work-in-progress paper describes the approach and methods for a qualitative study of early engineering students’ development of professional values. The goal of the research program as a whole is to study students’ understanding of professional virtues and how various virtue formation activities affect how students establish positive ethical traits. This specific study was designed to understand the foundation upon which these values can develop and become integrated with the practice of engineering.This study focuses on individual interviews with engineering and computer science students, primarily freshmen and sophomores, at Franciscan University in Steubenville during the spring 2022 semester. Students were asked to describe their motivation to study engineering and their perception of virtues for professional engineering and computing practice. Responses to broad questions were further probed to gather relevant details. This paper describes the background, motivation, and methods for the initial study in this research program.",https://www.semanticscholar.org/paper/210c25ab78d64df69199e1b31399f4c6dbea57ae,IS,Qualitative
An HCI Research Agenda for Online Science Communication,"Social media, blogs, podcasts, and other computer-mediated communication technology have become an integral way for the public to access and engage with research. However, despite the evolving challenges researchers face navigating these platforms, and the high stakes of online science communication, relatively little HCI research has focused on understanding and supporting online science communication through these participatory platforms. Through a review of the literature and a set of interviews with HCI researchers (n = 24), we identify challenges currently facing researchers who try to engage with the public about their work, and establish a research agenda for HCI to study, design, and evaluate technology to support science communication. Specifically, we advocate for the design of tools to support audience analytics, automated summary and outreach workflows, and providing quantitative and qualitative feedback about online outreach efforts, as well as additional research to elucidate the impacts of self-directed science communication efforts and the evolving roles of scientists on the participatory web. With shifting online platforms placing researchers in the role of advocates and participants in science communication, understanding and supporting these interactions is now more important than ever.",https://www.semanticscholar.org/paper/71ca0fd821a4f1a0a4f3023405ce8602cf7924f1,IS,Qualitative
Computer's Place in Teaching and Learning for University Students in the Web of Science Database,"Among the tools used in teaching and learning, computers are very im-portant. Determining what kind of computer-related studies are carried out in the learning and teaching processes for university students will contribute to future research. In this context, the aim of the study is to examine university students’ documents related to computers in teaching and learning, scanned in the Web of Science database, thematically and methodologically. The study was designed by adopting the case study method from the qualitative research models. A sample was not determined for the articles to be included in the study but it was aimed to reach the whole population. In this context, all documents in the Web of Science database have been accessed through the library system of the university. In the document search, the keywords ‘learning and teaching’ and ‘computer’ were searched in all areas, and the studies were found by searching the keyword ‘university student’ within the title. All the studies were analysed by year, document type, Web of Science category, country and language of publication. The findings obtained from the research are explained in detail by content analysis method. A total of 305 documents were reached in the research findings. The findings obtained as a result of the research carried out are given in detail in the findings and results section.",https://www.semanticscholar.org/paper/107a6bf128c25c764688c724dfff0b844c837f27,CS,Qualitative
The Analysis of Students’ Difficulties in Using Computer Terms,"Computer is used for many purposes especially for improving and supporting in education system. Students can operate computer as well as possible, but most of them don’t know the meaning of some terms in computer science. The aims of this study are to investigate the Students’ difficulties in Using Computer Science Terms at STMIK Pelita Nusantara, then to explore the students’ difficulties in Using Computer Science Terms at STMIK Pelita Nusantara. This is a qualitative research. The participants of this study were students of second semester of STMIK Pelita Nusantara. The total of students were 35 students that consist of 17 males and 18 females. The result showed that computer terminology is the one problem that the students face. The lack of vocabulary become the main reason why the students cannot use the terms well. Students’ obstacles were not only in mastering vocabulary but also in mastering the translation in using computer terms. Sometimes the students know the usage of computer terms but they don’t know the meaning of them.",https://www.semanticscholar.org/paper/039110b964d8eb9b9e4be955023c3e4424103b55,CS,Qualitative
"Philosophy of future: analytical overview of interaction between education, science, and artificial intelligence in the context of contemporary challenges","Currently, artificial intelligence is changing the future landscape of technology and digitalization and transforming the way knowledge is generated and shared in education and science. The aim of the article is to interpret the future through interactions between education, science, and AI and to outline their potential challenges when digital transformations occur. While answering the research questions the mixed-methods approach was applied. For the study both quantitative and qualitative data was collected through descriptive and empirical survey. The empirical survey included computer-assisted web interview to assess the most relevant concepts according to the participants responses. The study involved 42 participants that were selected randomly. The selection criteria concerned educational background, previous experience, representation of diverse perspectives, theories, or approaches within the philosophy, active engagement in educational practice, and wiliness to participate. To analyse the data statistical software NVivo was applied. Approximately 70 recent scientific works were studied to present the problem through multifaceted approach. The findings show the participants’ anticipations are related to the cognitive load theory, constructivist theory, and socio-cultural theory. A number of challenges threatening innovative developments arise within the educational and scientific environments. They include ethical concerns, misinformation, digital divide, unequal infrastructure, lack of regulation, lack of digital skills, resistance to change, technology integration, and limited digital pedagogy. Ethics plays a crucial role in shaping digital transformations in education and science. The category of academic virtue is closely connected to ethical and responsible use of Ai. The paradigm of academic virtue includes ethical of knowledge, collaboration, responsibility, honesty, accuracy, adaptability, openness. AI brings automated content creation, language processing opportunities, real-time translation, and enhanced accessibility and, therefore, changes communication in the field of education and science. The research showed that the principles of effective implementation of AI in education and science include philosophical, educational, and ethical dimensions.",https://www.semanticscholar.org/paper/d9574049b72b34c6a91df81d1c07ab4e1ba59635,IS,Qualitative
AI literacy curriculum and its relation to children's perceptions of robots and attitudes towards engineering and science: An intervention study in early childhood education,"The number of artificial intelligence (AI) literacy studies in K‐12 education has recently increased, with most research focusing on primary and secondary education contexts. Little research focuses on AI literacy programs in early childhood education.The aim of this mixed‐methods study is to examine the feasibility of an AI literacy program called “AI4KG” and explore how it might affect kindergarteners' perceptions of robots and attitudes towards engineering and science.A total of 26 child–parent dyads recruited from a Hong Kong kindergarten were involved in this study, consisting of 26 children (Mage = 4 years, SD = 0.28) and their parents. Quantitative and qualitative data were collected through surveys and interviews designed to explore children's perceptions of robots and attitudes towards engineering and science, and parents' perceptions of the AI4KG intervention.It is found that children have increased their perceptions of robots after the AI literacy program, but the AI4KG curriculum had no significant effects on kindergarten children's engineering and science attitudes. Most parents (22 out of 26) agreed that their children's AI knowledge, AI skills, and AI attitudes have been enhanced after learning through the AI4KG curriculum.This study suggests that the AI4KG curriculum is potentially effective in promoting early AI literacy and favourable attitudes towards the technology, but further research is needed to develop age‐appropriate measures and assess its long‐term impact on children's education and career paths.",https://www.semanticscholar.org/paper/10e7b2df6c48980c72a480d8a8a0c04189072df9,CS,Qualitative
Augmenting Social Science Research with Multimodal Data Collection: The EZ-MMLA Toolkit,"While the majority of social scientists still rely on traditional research instruments (e.g., surveys, self-reports, qualitative observations), multimodal sensing is becoming an emerging methodology for capturing human behaviors. Sensing technology has the potential to complement and enrich traditional measures by providing high frequency data on people’s behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website and provides easy access to multimodal data collection algorithms. One can collect a variety of data modalities: data on users’ attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), gestures (from hand motion), emotions (from facial expressions and speech) and lower-level computer vision algorithms (e.g., fiducial/color tracking). This toolkit can run from any browser and does not require dedicated hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used by aspiring educational researchers in a classroom context. We conclude by discussing future work and other applications of this toolkit, potential limitations and implications.",https://www.semanticscholar.org/paper/eeb717032ee835b95a3fa875bd0e1f200e9e902c,CS,Qualitative
The Effect of the Computer-Based Analogy Used in Science Teaching on Learning Outcomes,"The aim of this study is to examine the effects of computer-based analogy (CBA) used in science teaching in terms of the unit of Structure and Properties of Matter on seventh-grade students’ academic success. The attitudes of students against science teaching lesson were also described in this study. This study was applied to 60 seventh-grade students from two different classes of the same teacher from a public school in Köprübaşı/Trabzon. In the study, pre-test – post-test control group quasi-experimental research model was used. The remarks of the students were described by content analysis within the context of qualitative research method. During the research period, computer-based analogy method was applied to the experimental group; on the other hand, traditional teaching method was applied to the control group. In the study, quantitative data as data collection tool was collected through academic success test prepared by the researcher. On the other hand, qualitative data was collected through an interview form, including open-ended questions formed by consulting experts. The results of the study show that the computer-based analogy method is more effective than the present program applications in terms of students’ academic success in science lessons and permanence of knowledge. The results of the qualitative data analysis show that the students had positive views related to the lesson made by the computer-based analogy method.",https://www.semanticscholar.org/paper/ff09cc2ecb2a2b932e130f366f15fbfad3f9a0e9,CS,Qualitative
Interactive Tablets: Catalyzing Engaged Science Learning in English Instruction,"Schools in Thailand are increasingly using tablet computers for active learning. As interactive technology is increasingly used in the classroom, teachers are becoming more concerned about it. This qualitative study explores using a tablet computer as an interactive tool for active learning in teaching science in English. The research study involved three classes with ninety-three Grade 8 students from a demonstration school in Southern Thailand. The study was carried out for three weeks in all three classes. The classroom implementation was recorded and analyzed. Ninety-five percent of the students used tablet computers for class participation, while five percent used phones. The results showed that students used their tablet computers to search for different body parts in English, words that described the body parts and the meaning of the words. They learned independently and at their own pace to complete the classroom tasks. The results suggested that tablet computers are an excellent interactive tool to support active learning in teaching science in English in Grade 8 classrooms.",https://www.semanticscholar.org/paper/cd27fa8fb34e7c88d396353e0622301ddfd99089,CS,Qualitative
Retention and Attrition in U.S. STEM Education with the Help of Computer Technology and Curriculum Development,"This article explores the pressing issue of retention and attrition in STEM (Science, Technology, Engineering, and Mathematics) education in the United States, focusing on the role of computer technology and curriculum development in improving retention rates. The study examines factors contributing to student persistence and attrition, including gender and racial disparities, and the impact of first-year experiences. It also delves into the evolution of educational technology, such as online learning platforms, virtual and augmented reality, and learning analytics, and their influence on student engagement and retention. The role of curriculum innovations, such as active learning, project-based learning, and interdisciplinary approaches, is analyzed to highlight effective strategies for retaining students in STEM fields. Through a mixed-methods research design, including quantitative and qualitative data collection, the study presents findings on the effectiveness of technological and curricular interventions, offering recommendations for policy and best practices to enhance STEM education in the U.S.",https://www.semanticscholar.org/paper/0becf47e581e15801cffe78233916ad3fe62ea0e,IS,Qualitative
Barriers to and Facilitators of Technology in Cardiac Rehabilitation and Self-Management: Systematic Qualitative Grounded Theory Review,"Background Dealing with cardiovascular disease is challenging, and people often struggle to follow rehabilitation and self-management programs. Several systematic reviews have explored quantitative evidence on the potential of digital interventions to support cardiac rehabilitation (CR) and self-management. However, although promising, evidence regarding the effectiveness and uptake of existing interventions is mixed. This paper takes a different but complementary approach, focusing on qualitative data related to people’s experiences of technology in this space. Objective Through a qualitative approach, this review aims to engage more directly with people’s experiences of technology that supports CR and self-management. The primary objective of this paper is to provide answers to the following research question: What are the primary barriers to and facilitators and trends of digital interventions to support CR and self-management? This question is addressed by synthesizing evidence from both medical and computer science literature. Given the strong evidence from the field of human-computer interaction that user-centered and iterative design methods increase the success of digital health interventions, we also assess the degree to which user-centered and iterative methods have been applied in previous work. Methods A grounded theory literature review of articles from the following major electronic databases was conducted: ACM Digital Library, PsycINFO, Scopus, and PubMed. Papers published in the last 10 years, 2009 to 2019, were considered, and a systematic search with predefined keywords was conducted. Papers were screened against predefined inclusion and exclusion criteria. Comparative and in-depth analysis of the extracted qualitative data was carried out through 3 levels of iterative coding and concept development. Results A total of 4282 articles were identified in the initial search. After screening, 61 articles remained, which were both qualitative and quantitative studies and met our inclusion criteria for technology use and health condition. Of the 61 articles, 16 qualitative articles were included in the final analysis. Key factors that acted as barriers and facilitators were background knowledge and in-the-moment understanding, personal responsibility and social connectedness, and the need to support engagement while avoiding overburdening people. Although some studies applied user-centered methods, only 6 involved users throughout the design process. There was limited evidence of studies applying iterative approaches. Conclusions The use of technology is acceptable to many people undergoing CR and self-management. Although background knowledge is an important facilitator, technology should also support greater ongoing and in-the-moment understanding. Connectedness is valuable, but to avoid becoming a barrier, technology must also respect and enable individual responsibility. Personalization and gamification can also act as facilitators of engagement, but care must be taken to avoid overburdening people. Further application of user-centered and iterative methods represents a significant opportunity in this space.",https://www.semanticscholar.org/paper/636c2a9dacac55174d2b6d8d8a6ea1db458ad2a9,CS,Qualitative
Students’ problem-solving strategies in qualitative physics questions in a simulation-based formative assessment,"Previous studies on quantitative physics problem solving have been concerned with students’ using equations simply as a numerical computational tool. The current study started from a research question: “How do students solve conceptual physics questions in simulation-based formative assessments?” In the study, three first-year college students’ interview data were analyzed to characterize their problem-solving strategies in qualitative physics questions. Prior to the interview, the participating students completed four formative assessment tasks in physics integrating computer simulations and questions. The formative assessment questions were either constructed-response or two-tiered questions related to the simulations. When interviewing students, they were given two or three questions from each task and asked to think aloud about the questions. The findings showed that students still used equations to answer the qualitative questions, but the ways of using equations differed between students. The study found that when students were able to connect variables to a physical process and to interpret relationships among variables in an equation, equations were used as explanatory or conceptual understanding tools, not just as computational tools.",https://www.semanticscholar.org/paper/bb27013d9adaa2bebb4b9291b37bea6bcf976db2,IS,Qualitative
Digital Transformation in Science Education: Teachers' Self-Efficacy of Distance Learning and Blended Learning Experiences,"Blended learning emerges as an indispensable tool for pioneering science education practices in the 21st century. Studies underscore the efficacy of blended learning applications in science education, particularly in enhancing learners' computer proficiency, fostering positive attitudes and motivation towards science courses, and surmounting learning hurdles. This study aims to assess the levels of self-efficacy among science teachers for blended education within the context of the digital transformation process. It also seeks to investigate the predictive capacity of various factors and analyze teachers' experiences with blended learning. The research adopts a mixed-method approach, integrating both qualitative and quantitative research methodologies. Quantitatively, the study utilizes the ""Self‐Efficacy Perception of Distance Education Scale"" to gauge science teachers' self-efficacy perceptions about distance education. The quantitative segment encompasses a sample of 175 science teachers. On the other hand, for the qualitative aspect, 26 teachers' experiences were examined through an “Open-Ended Question Form” to gain nuanced insights. The results of the quantitative analysis demonstrated that science educators displayed a heightened level of perceived self-efficacy in the domain of distance education. The influencing factors identified included gender, seniority, school type, class size, and usage of laboratory facilities. The qualitative findings indicated that science teachers' engagement with blended learning was characterized by both positive and negative sentiments. These experiences were further categorized into subgroups of positive and negative encounters.",https://www.semanticscholar.org/paper/74927564f8d3372a83e66492cb363c840b28cc1c,IS,Qualitative
Computational thinking with game design: An action research study with middle school students,"Middle school students often enter Computer Science (CS) classes without previous CS or Computational Thinking (CT) instruction. This study evaluated how Code.org’s block-based programming curriculum affects middle school students’ CT skills and attitudes toward CT and CS. Sixteen students participated in the study. This was a mixed methods action research study that used pre- and post-tests, surveys, artifacts, and interviews as data sources. Descriptive statistics, paired samples t-tests, and inductive thematic analysis were administered. Findings showed a statistically significant increase in participants’ algorithmic thinking, debugging, and pattern recognition skills but not in abstraction skills. Attitudes toward CT and CS improved but the difference was not statistically significant. Qualitative themes revealed benefits of game-based learning to promote CT skills, collaboration to promote successful error debugging, and enjoyment of programming resulting from a balance between structured guidance and creative freedom. Findings emphasize the importance of low-threshold and engaging strategies to introduce novice learners to CT and CS.",https://www.semanticscholar.org/paper/87b1b5ec9e31cb7290eb3895891bd1d4c5b1145a,CS,Qualitative
Gifted Students' Perceptions of Artificial Intelligence through Drawings: A Perspective from Science and Art Centers,"Artificial Intelligence (AI) emerges as the development of computer systems and software that imitate human abilities and perform human-like tasks. Understanding what gifted students think about this system that includes deep cognitive abilities is considered important. Based on this premise, this study examines the perceptions of gifted students towards the concept of AI. The research was conducted using phenomenological design, a qualitative research method. The data of the research were collected from 50 gifted students enrolled at a Science and Art Center and selected through a convenience sampling method. The “Draw-Write” form was used as the data collection tool. The data obtained were analyzed using content analysis, consistent with the nature of qualitative research methods. The study shows that participants may have three types of positive, neutral and negative roles related to AI. The research findings suggest that gifted students perceive AI as both a supportive tool and a potential competitor to human capabilities. The research showed the need for ethical considerations and awareness of the societal impacts of AI. Their futuristic vision shows that they are ready to explore the potential applications of AI in various fields. By recognising and addressing these perspectives, educators and policy makers can foster an environment that balances innovation with ethical responsibility and enables AI to serve as a tool for collective progress.",https://www.semanticscholar.org/paper/80eef32c063d1ab563d4c47bc46bcbd815fef9f8,CS,Qualitative
"Shell Noun Usage in Science Undergraduate Academic Writing: Insights from Mukuba University, Zambia","This study examined the use of shell nouns in the academic writing of science undergraduate students at Mukuba University, Zambia, focusing on their frequency, patterns, and functions across multiple disciplines. The topic was chosen due to the critical role of shell nouns in enhancing coherence, formality, and objectivity in academic writing, which are essential skills for scientific communication. Grounded in Halliday and Hasan’s theory of cohesion, the research aimed to (1) investigate the frequency of shell noun use across disciplines, (2) identify patterns in their usage across academic years, and (3) evaluate their functional contributions to coherence, formalization, and objectivity in student writing. A qualitative approach was employed, involving the analysis of written samples from disciplines including Biology, Chemistry, Public Health, Nutrition Science, Agricultural Science, Computer Science, Environmental and Climate Change, and Biomedical Science. The data set consisted of lab reports, research papers, and essays collected from first-year to final-year students. The data analysis involved frequency counts and thematic coding to identify trends and patterns in shell noun usage. The findings revealed notable disciplinary variations, with Chemistry and Biomedical Science demonstrating higher frequencies of shell noun use. Shell nouns played a vital role in maintaining textual coherence, formalizing tone, and enhancing objectivity in the students' writing. More advanced usage was evident among final-year students, suggesting a developmental progression in academic writing skills. However, challenges such as inconsistent use and difficulty applying shell nouns effectively in complex writing tasks were also identified. The study highlighted the importance of targeted pedagogical interventions, recommending the inclusion of explicit instruction on shell noun usage in writing curricula. Suggestions for future research included expanding the sample size, incorporating a broader range of genres, and adopting a longitudinal approach. By examining linguistic features in academic writing within a sub-Saharan African context, this research provided valuable insights for educators seeking to enhance writing instruction and support student success.",https://www.semanticscholar.org/paper/a6c5fadb8ae4d67eea7eb668fb77e7b367634335,IS,Qualitative
Digital Literacy Research in Education : Trends and Insights,": This study explores the latest trends and insights on digital literacy in education through bibliometric analysis. Using data from the Scopus database and VOSviewer software, the study analyzed 3,386 publications on digital literacy in educational contexts from 2015 to 2024. Qualitative content analysis methods were used to code and interpret text data. Findings show a significant growth in the number of publications, with the highest peak in 2023. The United States led the research contributions, followed by Spain, the United Kingdom, and Indonesia. The dominant field of study was social sciences, followed by computer science and engineering. Lead authors include Makhachashvili, R., with affiliations at Universidad Salamanca and Monash University. Significant trends identified include an increased focus on digital health, information accuracy, and online learning in response to the COVID-19 pandemic. The study emphasized the importance of digital literacy as a critical skill in a digital society, and identified gaps in the existing literature. This encourages further research to explore the impact of digital literacy on student learning outcomes and education policy. These findings can be a guide for developing a more relevant curriculum by integrating local cultural and technological elements, as well as improving the quality of education.",https://www.semanticscholar.org/paper/72ef23a783a74ba4caf4852efa2fd7e4b380c050,CS,Qualitative
Understanding interactions between scientists and elementary school students in a citizen science project,"Participation in citizen science enables students to gain authentic research experience through collaboration with expert scientists. The purpose of this exploratory study was to investigate how the interactions between expert scientists and student citizen scientists were mediated through the collaborative investigation and co-creation of knowledge artifacts within a computer supported collaborative learning environment. Approximately 38 elementary students participated in a public citizen science project at the end of a school year. Their data (posts, comments, and photos) were downloaded for a post-hoc analysis. A mixed methods design, which merged quantitative SNA analyses and contextualized qualitative descriptions, provided an understanding of the interactions on the site. This analysis found that discussions related to knowledge artifacts that were novel or unexpected engaged a higher number of participants, but that the quality of scientific discussion was not related to the level of engagement. Expert scientists fulfilled a crucial role in generating scientific discussions about the artifacts. Students appeared to play moderating roles by asking questions and making assertions. However, they also were sometimes sidetracked by non-scientific interactions. The use of citizen science projects shows promise in engaging students in authentic research and providing a platform for expert scientists to demonstrate science practices for students. Recommendations for future research are offered to further enhance scientific discussions between all participants.",https://www.semanticscholar.org/paper/15aeffc6008ed1a2f42323553e86563680dc6da9,IS,Qualitative
Computer Vision-based Analysis of Buildings and Built Environments: A Systematic Review of Current Approaches,"Analysing 88 sources published from 2011 to 2021, this article presents a first systematic review of the computer vision-based analysis of buildings and the built environment. Its aim is to assess the potential of this research for architectural studies and the implications of a shift to a cross-disciplinarity approach between architecture and computer science for research problems, aims, processes, and applications. To this end, the types of algorithms and data sources used in the reviewed studies are discussed in respect to architectural applications such as a building classification, detail classification, qualitative environmental analysis, building condition survey, and building value estimation. Based on this, current research gaps and trends are identified, with two main research aims emerging. First, studies that use or optimise computer vision methods to automate time-consuming, labour-intensive, or complex tasks when analysing architectural image data. Second, work that explores the methodological benefits of machine learning approaches to overcome limitations of conventional analysis to investigate new questions about the built environment by finding patterns and relationships among visual, statistical, and qualitative data. The growing body of research offers new methods to architectural and design studies, with the article identifying future challenges and directions of research.",https://www.semanticscholar.org/paper/14fd680e40272402964ff823c581282ac6dfe657,CS,Qualitative
"Foundations and Frameworks of ELT and Applied Linguistics Research: Principles, Processes and Practices","This article offers a comprehensive analysis of the core principles, processes, and practices that characterize research in English Language Teaching (ELT) and Applied Linguistics. The article highlights the key distinctions between scholarship, aimed at personal knowledge acquisition, and research, which generates socially useful knowledge. Research in applied linguistics is problem-oriented, focused on addressing practical language-related challenges such as language acquisition, teaching methodologies, and multilingual communication. The paper emphasizes the interdisciplinary nature of applied linguistics, integrating insights from fields like psychology, sociology, and computer science to provide comprehensive solutions. Methodological diversity is a hallmark of this research, encompassing both qualitative and quantitative approaches, including case studies, surveys, and experimental studies. The research cycle, from identifying a problem to collecting and analyzing data, is systematically outlined. Key areas of ELT research-such as second language acquisition, curriculum development, and language assessment-are discussed, underscoring the field's contributions to both theory and practice. The article concludes by emphasizing the importance of applying research findings to improve language education, develop policies, and enhance communication across diverse linguistic contexts Int. J. Soc. Sc. Manage. Vol. 11, Issue-4: 126-135.",https://www.semanticscholar.org/paper/422939475c92a9ad21c8258130d0dab6ce6e8082,CS,Qualitative
Trends in sociolinguistics research in the last decade: Bibliometric analysis,"Sociolinguistics, which bridges linguistics, sociology, and other disciplines, explores the complex relationship of language with society. This research was conducted to find trends in sociolinguistic studies from 2014 to 2023, and this bibliometric analysis was conducted to reveal significant scholarly activities and themes studied. The study provides an in-depth quantitative and qualitative overview of the vast sociolinguistic literature produced during this period. Using keywords such as ""sociolinguistics"" and ""bibliometric analysis"", the analysis identified and assessed relevant articles in the 2014-2023 timeframe. The bibliometric data, analyzed with VOSviewer and Biblioshiny, measured publication frequency and author interconnections and identified thematic clusters. In addition, it evaluated interdisciplinary contributions from fields such as Social Sciences, Arts and Humanities, Computer Science, Psychology, and Engineering to sociolinguistics. The main findings show a spike in research activity around 2016, reflecting the growing interest in sociolinguistics. Various disciplines contribute and provide multidimensionality to the field. Important keywords include linguistics, language, and humanity, highlighting research into the structure of language, language as a means of social communication, and its role in human development. The finding of the word technology illustrates the growing technological developments in sociolinguistics. The appearance of the word ""gender"" signifies ongoing research into the complex relationship between language and gender. This study offers a comprehensive understanding of the dynamics of sociolinguistic research, underscores its relevance in the changing social and technological landscape, and suggests directions for further research.",https://www.semanticscholar.org/paper/d042018217088107eb8c1b9ab68509e72bd455a5,CS,Qualitative
Role-Playing Computer Ethics: Designing and Evaluating the Privacy by Design (PbD) Simulation,"There is growing consensus that teaching computer ethics is important, but there is little consensus on how to do so. One unmet challenge is increasing the capacity of computing students to make decisions about the ethical challenges embedded in their technical work. This paper reports on the design, testing, and evaluation of an educational simulation to meet this challenge. The privacy by design simulation enables more relevant and effective computer ethics education by letting students experience and make decisions about common ethical challenges encountered in real-world work environments. This paper describes the process of incorporating empirical observations of ethical questions in computing into an online simulation and an in-person board game. We employed the Values at Play framework to transform empirical observations of design into a playable educational experience. First, we conducted qualitative research to discover when and how values levers—practices that encourage values discussions during technology development—occur during the design of new mobile applications. We then translated these findings into gameplay elements, including the goals, roles, and elements of surprise incorporated into a simulation. We ran the online simulation in five undergraduate computer and information science classes. Based on this experience, we created a more accessible board game, which we tested in two undergraduate classes and two professional workshops. We evaluated the effectiveness of both the online simulation and the board game using two methods: a pre/post-test of moral sensitivity based on the Defining Issues Test, and a questionnaire evaluating student experience. We found that converting real-world ethical challenges into a playable simulation increased student’s reported interest in ethical issues in technology, and that students identified the role-playing activity as relevant to their technical coursework. This demonstrates that roleplaying can emphasize ethical decision-making as a relevant component of technical work.",https://www.semanticscholar.org/paper/5cbe0733b7655117763032beb6c03e3973e0c4f1,IS,Qualitative
Evaluation of Harezmi Education Model Applications According to Student Opinions: An Exploratory Sequential Mixed Design Research,"Harezmi Education Model (HEM), one of the applications based on an interdisciplinary approach, is an education model that allows students to develop 21st century skills such as algorithmic thinking, problem solving, and working in a team (MEB, 2021). Since 2016, HEM is an education model that integrates computer and science with social sciences, uses programming and teaching tools effectively, produces by having fun with robotics and game design, and constantly updates itself. In this research, it was tried to determine the opinions of students in secondary schools that implement HEM regarding HEM practices. The research using Mixed Research Method and Exploratory Sequential Design was carried out in three stages. The opinions of 9 students were consulted through the semi-structured student interview form prepared in the first stage, and the qualitative dimension of the research was carried out. Then, using the qualitative findings obtained, a scale was developed to determine student opinions regarding HEM practices. In the last stage, quantitative findings were obtained by applying the scale to 308 secondary school students, and based on this finding, it was tried to determine whether the qualitative findings were generalizable. As a result of the research, it was determined that the students' opinions about PEC applications were quite positive, and while their opinions were similar in terms of gender, their opinions differed in terms of school type and grade level. Students stated that although HEM applications had some shortcomings, they benefited from this training and were satisfied with the training they received. In their suggestions regarding HEM practices, students mostly stated that their scope, duration and comprehensiveness should be increased. In addition to these suggestions, some remedial suggestions were presented in the research.",https://www.semanticscholar.org/paper/a13d2620ef7f571bf1f7669d4ded173be50f3ab6,CS,Qualitative
Need Analysis on English for Computer and Technique,"Practice is as important as the theory in learning a language. A recommended syllabus is needed to use in the institution according to the need of the learners as well. This esearch aims to analyze the necessities, the lacks and wants of the learners in the English for Computer and Technique course. The research method is descriptive with a qualitative model. The survey research using a series of questionnaires and was distributed to 32 respondents from the informatics study program. In collecting the data, the researcher applied questionnaire for the students as the instrument of the research. The results of the needs analysis concluded that students need English to improve their speaking, listening, reading, and writing. material that per the field of science, namely computer science, becomes more interesting and strongly supports the knowledge being studied. The implementation of this research is to prepare an Semester Learning Plan in accordance to the student’s needs and to develop lecturer competence as well as technology and information progress.",https://www.semanticscholar.org/paper/1bc181d6dd251425fb5a3013265c8d03893077ab,CS,Qualitative
Encountering #Feminism on Twitter: Reflections on a Research Collaboration between Social Scientists and Computer Scientists,"The growth of social media presents an unparalleled opportunity for the study of social change. However, the speed and scale of this growth presents challenges for social scientists, particularly those whose methodologies tend to rely on the qualitative analysis of data that are gathered firsthand. Alongside the growth of social media, companies have emerged which have developed tools for interrogating ‘big data’, although often unconnected from social scientists. It is self-evident that collaboration between social scientists and social media analysis companies offers the potential for developing methods for analysing social change on large scales, bringing together their respective expertise in technological innovations and knowledge of social science. What is less well known is how such a partnership might work in practice. This article presents an example of such a collaboration, highlighting the opportunities and challenges that arose in the context of an exploration of feminism on Twitter. As will be shown, machine-learning technologies allow the analysis of data on a scale that would be impossible for human analysts, yet such approaches also heighten challenges regarding the study of social change and communication.",https://www.semanticscholar.org/paper/4e21d61337161f828c285c1a2d04e65a08af9310,IS,Qualitative
A systematic review of trends and gaps in the production of scientific knowledge on the sociopolitical impacts of emojis in computer-mediated communication,"Abstract This systematic literature review analyses trends in original research on emoji use in computer-mediated communications (CMC) published between 2011 to 2021. In total, 823 articles were identified that met the search criteria. The mixed-method approach included qualitative coding of articles and frequency analysis by year, impact quartile, research topic and multidisciplinarity, as well as a cluster analysis to examine trends in sociopolitical research. The results show that Computer Science, Communications and Social Sciences disciplines accounted for largest proportion of original research on emojis and CMC in the time period analysed and that the degree of scientific impact increased significantly across the time series. In recent years, sociopolitical research has had higher than average growth and can be clustered into various groups based on two broad objects of study: “culture-identity” and “social exclusion”. The study also identified significant knowledge gaps, particularly in relation to emoji standardization and its sociopolitical implications. Overall, multidisciplinary approaches are epistemologically constrained, Spanish-language production is low, and there is an almost complete absence of context appropriate methodologies. The study concludes that there is a need to for more sociopolitical research on emoji use in CMC and multidisciplinary approaches, a shift away from the hegemony of Anglocentrism, and greater questioning of the structural influences of standardization process on questions of cultural, identity and social exclusion.",https://www.semanticscholar.org/paper/1233b4cc46bb2ab4b38e5f391b1b1a2037ed0433,CS,Qualitative
Hotspot and Development Trend of University Education Research in China: Bibliometrics and Big Data Analysis Based on University Pedagogy,"Using big data analysis has become an important method in scientific research today.In the past 30 years, China's higher education cause has developed vigorously.This paper uses the software computer space to conduct a measurement and qualitative analysis of changes in the number of published articles, publishing institutions, authors, high citations, research hotspots and hotspots in university educational science over the past 30 years.The results show that the number of articles published in the Journal of University Education Science is declining, with Hunan University, Hunan Normal University, Xiamen University and Central South University as the main contributing articles. The journal has formed a stable group of core authors.The journal serves as a reprint database of the People's Congress.Teaching reform, university, college students, teaching content, academic freedom, college, socialism, talent training, academic annual conference, university teachers, college entrance examination reform and so on are the main contents of ""university education science"".Through big data analysis, it is found that the influence and discourse power of university education science have been significantly enhanced in the academic circle, and it has become an important hub of academic and cultural exchanges in the field of higher education in China, and has made a significant contribution to the theoretical exploration of promoting the construction of a strong education country in China.",https://www.semanticscholar.org/paper/f9c788f4d2ecad95d77370f088bc4f3291836f51,CS,Qualitative
Experiencing the community of inquiry framework using asynchronous online role-playing in computer-aided instruction class,"This current study investigates the use of online role-playing, in an online discussion forum, in learning the community of inquiry framework – an area of learning covered in the Computer-Aided Instruction (CAI) course, an elective course for Computer Science undergraduate students at Universitas Indonesia. The participants were divided into different roles. Each group was triggered to discuss the implementation of online collaborative learning. A mixed-methods approach was utilised to analyse the qualitative and quantitative data. The result of content analysis exhibited students implementing all the components of the CoI framework. Teaching presence was the rarest, as students were focused on delivering their ideas. Social presence appeared in almost all messages since it is the easiest, and students can feel the impact immediately. The discussion moved to the integration phase but did not proceed to resolution. This study suggested some recommendations and future research topics.",https://www.semanticscholar.org/paper/e47368014fae5a26b58d3232e8bc329cb4389b1d,IS,Qualitative
Understanding the application of handwritten text recognition technology in heritage contexts: a systematic review of Transkribus in published research,"Handwritten Text Recognition (HTR) technology is now a mature machine learning tool, becoming integrated in the digitisation processes of libraries and archives, speeding up the transcription of primary sources and facilitating full text searching and analysis of historic texts at scale. However, research into how HTR is changing our information environment is scant. This paper presents a systematic literature review regarding how researchers are using one particular HTR platform, Transkribus, to indicate the domains where HTR is applied, the approach taken, and how the technology is understood. 381 papers from 2015 to 2020 were gathered from Google Scholar, Scopus, and Web of Science, then grouped and coded into categories using quantitative and qualitative approaches. Published research that mentions Transkribus is international and rapidly growing. Transkribus features primarily in archival and library science publications, while a long tail of broad and eclectic disciplines, including history, computer science, citizen science, law and education, demonstrate the wider applicability of the tool. The most common paper categories were humanities applications (67%), technological (25%), users (5%) and tutorials (3%). This paper presents the first overarching review of HTR as featured in published research, while also elucidating how HTR is affecting the information environment.",https://www.semanticscholar.org/paper/03107b5ae376c6cb382bd6d127415668980dfe9d,CS,Qualitative
Investigating the Synonyms of Conversational Agents to Aid Cross-Disciplinary CA Research,"With the advent of artificial intelligence, the CHI community has regained a large interest in Conversational Agents (CAs). Designing CAs involves cross-disciplinary efforts, such as that of computer science (e.g., language models), psychology (e.g., cognition and emotions), linguistics (e.g., conversation design), or communication (e.g., trust, communication theory). However, CAs are named differently depending on the discipline and purpose (e.g., chatbot, embodied avatar, virtual butler). This divergence of vocabulary on CA brings challenges to researchers and designers in researching the full landscape of CA literature and sharing cross-disciplinary knowledge. We performed bibliometric and qualitative analyses to systematically assess divergent terms used for CA nomenclatures. We present 54 CA-terms and how these terms are used differently depending on the discipline and design characteristics (e.g., voice vs. non-verbal vs. text-based). Our work contributes to helping CA researchers effectively review the CA literature and build on each other’s work for novel cross-disciplinary CA research.",https://www.semanticscholar.org/paper/1c3870376fe548bc513e8dc3ebe738f3a67c0d30,CS,Qualitative
Research on the application of artificial intelligence generated AI technology in new media art,"With the rapid development of information society and computer science, new media art has not made a qualitative leap in the past decade. The reason is that in big data and machine learning, the rapid development of artificial intelligence technology has brought computer science up to a higher level, while art is still stuck in aesthetics, acoustics, vision, psychology and other aspects, lack of logic, intelligence and integration. Therefore, the new media art needs the support of the computer technology, especially the new technology.In the design stage of new media art, technologies such as artificial intelligence, machine learning and big data mining are integrated to improve the molding speed of new media art through mathematical modeling.Through machine learning, quickly understand user experience and user needs, further optimize new media art works, use artificial intelligence AI technology to simulate new media art, and simulate the whole process of user experience and interaction is the main content of this paper.This makes new media art become a typical application in the field of AICG (artificial intelligence production content). Generative AI will greatly reduce the marginal cost of creation and knowledge work, greatly improve labor productivity and economic value, and realize the generation of new media art original content at one tenth of the cost and thousands times production speed.",https://www.semanticscholar.org/paper/caf0a152d417ac4633bce38a6ac152c33ed186e3,CS,Qualitative
Study Designs for Quantitative Social Science Research Using Social Media,"Social media provides a rich amount of data on the everyday lives, opinions, thoughts, beliefs, and behaviors of individuals and organizations in near real-time. Leveraging these data effectively and responsibly should therefore improve our ability to understand political, psychological, economic, and sociological behaviors and opinions across time. This article is the first in a series of white papers that will provide a summary of the discussions derived from meetings of social scientists and computer scientists with the goal of creating consensus for how social and computer science could converge to answer important questions about complex human behaviors and dynamics using social media data. We present three basic research designs that are commonly used in social science and are applicable to research using social media data: qualitative observation, experiments, and surveys. We also discuss a fourth design that is primarily informed by computer science, non-designed data, but that can inform social science research. After a brief discussion of the general approach of these designs and their applicability for use with social media data, we discuss the challenges associated with their use with social media data and potential solutions for “convergence” of these methods for future quantitative research in the social sciences.",https://www.semanticscholar.org/paper/acfa6c544c4aaa1e747780e09f947e8b369b155c,IS,Qualitative
Negotiating across languages: Metadiscourse in English and Spanish abstracts in Soil Science,"This study aimed to contrast metadiscourse use across languages in abstracts in the field of Soil Science. Three corpora were compared: abstracts published in Spanish by Spanish speakers; abstracts published in English by Spanish speakers; and abstracts published in English by English speakers. Metadiscourse occurrences were qualitatively coded using computer-assisted qualitative data analysis software and interpreted in relation to independent variables language of publication, writers’ dominant language, and abstract rhetorical structure. Findings suggest an overall preference for boosting and a tendency to rely heavily on interpersonal features when presenting and discussing research outcomes, which may be accounted for in terms of the promotional function of the genre. Contrastive corpus analysis indicates a shift from Spanish local patterns of interaction when publishing in English towards dominating patterns of negotiation in the additional language, which might be attributed to the external demands posed by differing socio-pragmatic contexts of publication. Few divergencies observed in the use of hedging features and in the setting up of research background might indicate coexisting communication patterns and deliberate participation strategies by Spanish speakers.",https://www.semanticscholar.org/paper/9ba4a0c26ee4dbd849719c917f9bb079f2615c00,CS,Qualitative
STEM Education and Problem-Solving in Space Science: A Case Study with CanSat,"Research has shown that hands-on projects promote stem education, namely, via problem-solving. CanSat, literally 'satellite in a can', is a stem educational project promoted by the European Space Agency. This paper addresses this issue by researching this STEM project, which demonstrates how problem-solving can be achieved in secondary-level students within the framework of the CanSat. We use qualitative techniques of data collection and analysis. The results showed that students use sophisticated thinking strategies to process information within this interdisciplinary project: (a) cognitive testing, cognitive organization, cognitive regulation, and monitoring, in addition to computer language and physical–mathematical calculations, are cognitive and metacognitive behavior strategies revealed in the CanSat; (b) problem-solving was suggested as a specific model, where students’ higher cognitive and metacognitive ordering processes deepen in project development; (c) computational, lateral, or divergent and convergent thinking were detected as thinking types of students associated with and mobilized in the course of problem-solving, The findings of this research have practical implications for STEM education in space science. Hands-on projects using problem-solving are an essential strategy to promote STEM education. Additionally, they are a starting point to promote meaningful learning and new thinking types.",https://www.semanticscholar.org/paper/1ce2d383ba9018df868a6a0eaf9a4250f82a51c7,IS,Qualitative
"Set-Analytic Approaches, Especially Qualitative Comparative Analysis (QCA)","This report focuses on set-analytic approaches that use algorithms and computer software for parts of their analysis, particularly Qualitative Comparative Analysis (QCA). We concentrate on transparency concerning the ""analytic moment"" that stretches from assigning membership scores of cases in the condition and outcome sets to the presentation and interpretation of the results obtained via the truth table's logical minimization. We do not address transparency issues related to the research processes prior to and after this analytic moment as they are not specific to set-analytic approaches and are covered by other QTD working groups, such as those on research ethics, text-based sources, and non-automated content analysis.",https://www.semanticscholar.org/paper/5ea95c2444bc63de528cdd5d507aaff378a26d64,CS,Qualitative
A Bibliometric Analysis: Development of Elementary School Research in 2019-2023:,"Abstrak: This study aims to explore research trends that are developing in elementary schools. This study uses Scopus search analysis and VOSviewer software. Based on 42,017 article documents from 2019-2023 taken from Scopus indexed journals on 3 July 2023, co-authorship, co-citation, co-occurrence, and content analysis were carried out. The results of the quantitative analysis show that there are very many publications at the elementary school level, namely social science, psychology, and computer science. The literature on research in elementary schools has explored several hot themes over the last five years, including: covid-19, mental health, anxiety, bullying, qualitative research, educational computing, motor skills, online learning, and virtual reality. The bibliometric study carried out provides a thorough and complete picture of the development of research in elementary schools which may be valuable for researchers who are interested in developing research in elementary schools in the future. So, the researchers suggest exploring this trending research topic.",https://www.semanticscholar.org/paper/febf96c744842cca27f38fa1c25129940181a165,CS,Qualitative
Research trends of blended language learning: A bibliometric synthesis of SSCI-indexed journal articles during 2000–2019,"Abstract This study aims to synthesize research trends of blended language learning studies over the past two decades, from 2000 to 2019. Data were collected from the Web of Science, and a total of 60 SSCI-indexed journal articles were retrieved for bibliometric synthesis. Drawing on the revised technology-based learning model, participants, learning strategies, research methods, research foci, adopted technologies, and application effectiveness, advantages, and challenges were addressed. The findings demonstrated that publications were increasing rapidly, and that most articles were published in computer-assisted language learning, educational technology, and applied linguistic journals. The most common target language was English as a foreign language, and the most common learners were college students. In most studies, technologies were mainly used for the purposes of practice or exercises. Mixed, quantitative, and qualitative methods were frequently adopted, with a particular eye on the experiment design, questionnaires, and other specific methods in the second decade. Productive language skills, along with autonomy, satisfaction, and motivation, were major research foci. Language management systems and computer and web-based applications were frequently adopted technologies. Findings of application effectiveness, advantages, and challenges were summarized.",https://www.semanticscholar.org/paper/2e50a5e2b042d47b8a63a65d79ceaafb3ef735a1,CS,Qualitative
Experiential Factors Supporting Pupils’ Perceived Competence In Coding - An Evaluative Qualitative Content Analysis,"This Full Paper in the research to practice category explores pupils’ motivation to learn programming by game development in the game development environment (GDE) UnityTM. After getting to know block-based programming through controlling robots, 66 students with minimal computer literacy between 14 and 16 years of age got in touch with Unity in six Informatics courses with group sizes between 8 and 14 students each for approximately 40 course hours. In particular, we are interested in experiential factors that underlie students’ perceived competence in coding in high school computer science. We conducted an evaluative qualitative content analysis of 21 semi-structured interviews among students in six high school Informatics courses. Self-determination theory (SDT) identifies competence, autonomy and relatedness as basic needs that underpin motivation. We evaluated in how far students expressed these needs in their reciprocal interviews. Further, we explored student statements to find out about factors influencing their perceived competence. We were specifically interested in students’ future expectancy to get in contact with coding - which we attributed to the need for autonomy. According to our findings, most students were rather motivated to code in Unity. Yet, many did not express a future interest in (game) development. The majority of students expressed perceived competence in the programming tasks. A few students also explicitly mentioned relatedness as important learning factor. In line with SDT, relatedness appears to be important for students’ ability to deal with frustration along the journey of learning to code. It is open to consideration in how far students’ social environment – including student peers awareness of technology use in their current personal surroundings and possible future opportunities for getting in contact with coding in their lives – is deeply relevant to students’ perceived competence in programming apart from coding tools, such as Scratch or Unity, and incentives, such as programming games.",https://www.semanticscholar.org/paper/daffedcaf83d275556ac7c8344526cc97772642f,CS,Qualitative
Circular Economy and Internet of Things: Mapping Science of Case Studies in Manufacturing Industry,"This study investigates the “Internet of things” (IoT) and “Circular Economy” (CE) relationship in the current scientific literature focused on case studies or use cases on manufacturing context. To the best of our knowledge, this study is the first to map the science centered on “case studies” with respect to the “IoT” and “CE” connection, contributing to fill the gap of the subject that is already relevant to the scientific community and practitioners. The research methodology consists of developing a bibliometric study, employing PRISMA process, whose data is obtained from the Web of Science database. The VOSviewer was the computer program selected for the bibliometric analysis. The Web of Science (WoS) analysis tool supports VOSviewer. The papers were analyzed according to network analysis principles. The qualitative content analysis complements these results. The results show the high-frequency keywords and topics associated with the theme “IoT and CE”; the most cited papers; the intellectual structure of “IoT and CE”; the new emerging themes in scientific research; and social networks among the researchers. The paper’s contribution is the results of the bibliometric analysis and a better understanding of the relationship of “IoT” and “CE” by the “case studies” addressed in the empirical investigations.",https://www.semanticscholar.org/paper/c18dc1a71daf9426356dde1a132db95a9b9a5a87,IT,Qualitative
BIBLIOMETRIC ANALYSIS OF BIG DATA RESEARCH IN FINANCE,"Big data consists of structured and unstructured data. In the finance area, the data is extensively applied for business development. Consequently, researches on this area are growth in a fast pace. Hence, this study is intended to portray trend of several previous studies concerning of big data in the financial sector. Data sources refer to articles searched in the Science Direct, Emerald Insight and Google Scholar. Based on the purposive sampling method, it is acquired 61 papers. Findings of this study indicate that those papers mostly were published by the Science Direct. The research method mostly applied in those studies was a qualitative approach. Most of the papers in this area were published during the 2017 and China was a country that was frequently used as an object of study. The University of Minho was an institution that actively doing research in this area. The Procedia Computer Science was the most active publishing journal, and the decision making was an area that was frequently researched.",https://www.semanticscholar.org/paper/58cc6fd47f7b2d66d51703d313a6547ef72fd43e,IS,Qualitative
Learning as Making: Using 3D computer-aided design to enhance the learning of shape and space in STEM-integrated ways,"The recent and popular ""Maker"" movement worldwide has revived conversations about creativity, hands‐on ""Making,"" arts and design, humans with tools and digital experiences beyond the flat‐screen. However, such conversation mainly revolved outside the realm of formal education. This article presents two learning tasks, one in upper primary (age 10–11) and one in lower secondary (age 12–13), which integrate ""Making"" and 3D computer‐aided design (CAD), thereby facilitating Science, Technology, Engineering, and Mathematics (STEM) learning in mathematics classrooms. By adopting a design‐based research methodology, we examine students' mathematics learning with respect to dissecting and forming 2D shapes and volume of composite solids with the use of 3D CAD and the kinds of integrated STEM learning practices they demonstrated in the activities. Qualitative data were collected in the form of videos of the students' communication and screenshots of their initial and final designs as they engaged with the 3D CAD environment, as well as students' written reflections and teachers' lesson analysis. Results showed that the students used 3D CAD to develop spatial skills and to achieve mathematics learning far beyond using formulae and performing procedures. The learning activities also enabled an integrated STEM learning experience in productive and unobtrusive ways. [ABSTRACT FROM AUTHOR]",https://www.semanticscholar.org/paper/4891d6f154b51f9594b3e4df2255a6d9314e279e,IS,Qualitative
Situating science in Africa: The dynamics of computing research in Nairobi and Kampala,"Since the turn of the century, both Kampala and Nairobi have experienced a dramatic growth of computer science research, challenging accepted views of science in Africa. We deploy qualitative methods to follow active computer science researchers, graduate students, policy makers, administrators and entrepreneurs, in order to understand how computer science is enacted in these two cities. Our analysis focuses on four interrelated areas of labor, institutions, identities and scale. We illustrate the dynamics and frictions of computer science research across these areas, revealing the interlacing of moral economies of science and the political economy of higher education, the management of precarious professional lives and desire to get research done, and the pluralistic imaginations and multiple scales of computer science. Urban centers in East Africa are increasingly active in supporting granular and connective research communities that are socially transformative in ways that challenge conventional views of Africa as technologically dry. In this way, the computer science communities of Nairobi and Kampala are instructive for thinking about new geographies of science and technology studies.",https://www.semanticscholar.org/paper/8c3bef19c11a4b9dcaee720ba2a7195f8ff963db,IS,Qualitative
The Opportunities and Limitations of Using Artificial Neural Networks in Social Science Research,"Artificial Neural Networks (ANNs) are being increasingly used in various disciplines outside computer science, such as bibliometrics, linguistics, and medicine. However, their uptake in the social science community has been relatively slow, because these highly non-linear models are difficult to interpret and cannot be used for hypothesis testing. Despite the existing limitations, this paper argues that the social science community can benefit from using ANNs in a number of ways, especially by outsourcing laborious data coding and pre-processing tasks to machines in the early stages of analysis. Using ANNs would enable small teams of researchers to process larger quantities of data and undertake more ambitious projects. In fact, the complexity of the pre-processing tasks that ANNs are able to perform mean that researchers could obtain rich and complex data typically associated with qualitative research at a large scale, allowing to combine the best from both qualitative and quantitative approaches.",https://www.semanticscholar.org/paper/e3f7d733ebb0c759acf1d28a13e692c445beb74e,CS,Qualitative
Performing an Inductive Thematic Analysis of Semi-Structured Interviews With a Large Language Model: An Exploration and Provocation on the Limits of the Approach,"Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions. This paper presents results and reflections of an experiment done with the LLM GPT 3.5-Turbo to perform an inductive Thematic Analysis (TA). Previous research has worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. The paper presents the motivations for attempting this analysis; it reflects on how the six phases to a TA proposed by Braun and Clarke can partially be reproduced with the LLM and it reflects on what are the model’s outputs. The paper uses two datasets of open access semi-structured interviews, previously analysed by other researchers. The first dataset contains interviews with videogame players, and the second is a dataset of interviews with lecturers teaching data science in a University. This paper used the analyses previously conducted on these datasets to compare with the results produced by the LLM. The results show that the model can infer most of the main themes from previous research. This shows that using LLMs to perform an inductive TA is viable and offers a good degree of validity. The discussion offers some recommendations for working with LLMs in qualitative analysis.",https://www.semanticscholar.org/paper/3a740d17dc662e70be0ca0e24758ee9802c1d34d,CS,Qualitative
How do we study misogyny in the digital age? A systematic literature review using a computational linguistic approach,"Nowadays, despite centuries of striving for equality, women still face higher levels of discrimination compared to men in nearly every aspect of life. Recently, this systemic inequality has manifested in cyberspace through the proliferation of abusive content that is even more aggressive than what one would expect in the 21st century. Various research disciplines are now attempting to characterise this new manifestation of misogyny. The endeavour to comprehend this phenomenon has resulted in a significant increase in publications from several fields, including Social Sciences, Arts and Humanities, Psychology, and Computer Science. This paper presents a systematic review of multidisciplinary research on misogyny from the years 1990 to 2022, encompassing a total of 2830 articles retrieved from the Scopus database as of December 31, 2022. The literature is thoroughly analysed using three approaches: bibliometric analysis, topic detection, and qualitative analysis of the documents. The findings suggest that the analysis of online misogyny has been the primary driver behind the exponential growth in publications in this field. Additionally, the results of the topic analysis and topic interaction reveal a limited connection between the areas of knowledge that are necessary to fully grasp this complex phenomenon.",https://www.semanticscholar.org/paper/aa211982ba661c19c4f362226b17f649f1ff05dd,CS,Qualitative
Unpacking the Expressed Consequences of AI Research in Broader Impact Statements,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",https://www.semanticscholar.org/paper/17f357f3c6fdd86f7e8a141d1e3b9acb2e59a89a,CS,Qualitative
Early Adoption of Generative Artificial Intelligence in Computing Education: Emergent Student Use Cases and Perspectives in 2023,"Because of the rapid development and increasing public availability of Generative Artificial Intelligence (GenAI) models and tools, educational institutions and educators must immediately reckon with the impact of students using GenAI. There is limited prior research on computing students' use and perceptions of GenAI. In anticipation of future advances and evolutions of GenAI, we capture a snapshot of student attitudes towards and uses of yet emerging GenAI, in a period of time before university policies had reacted to these technologies. We surveyed all computer science majors in a small engineering-focused R1 university in order to: (1) capture a baseline assessment of how GenAI has been immediately adopted by aspiring computer scientists; (2) describe computing students' GenAI-related needs and concerns for their education and careers; and (3) discuss GenAI influences on CS pedagogy, curriculum, culture, and policy. We present an exploratory qualitative analysis of this data and discuss the impact of our findings on the emerging conversation around GenAI and education.",https://www.semanticscholar.org/paper/3924eaecc7287ed2b49bb759033c69161321a039,CS,Qualitative
Surfacing Inequities and Their Broader Implications in the CS Education Research Community,"Problem. Diversity, equity, and inclusion (DEI) need to be embedded throughout the computer science education (CSEd) research community in order to achieve empirically-based strategies in CSEd that is responsive to the needs of all of its constituents. However, there are no comprehensive studies that investigate what the barriers and challenges to DEI are among CSEd researchers. Research Question. When considering DEI among the CSEd research community, what barriers and challenges do different CSEd researchers face when conducting research? Method. We conducted a systematic literature review, developed a survey from the literature, and analyzed the quantitative and qualitative data from participants (n=72). Findings. Beyond finding that over half of the participants reported the COVID-19 pandemic as a barrier to engaging in research, participants reported that working more than an average 40-hour work week each year was a challenge. The lack of computing education being recognized as a subdiscipline within CS departments also was a barrier. Participants also reported that a lack of 1) awareness and adoption of practices from other education research fields and 2) general educational research theory were significant challenges for the CSEd research field. With respect to DEI, participants noted that lack of diversity among CSEd research partners/collaborators, among CSEd researchers in the community and among CSEd research community leadership are challenges for the community. Implications. Employing cultural competence is integral to CSEd research as we, as a community, inherently navigate differences in identities among researchers, and between researchers, practitioners, and participants in the currently unrepresentative and inequitable state of our field. As we grow our attitude, awareness, knowledge, and skill in cultural competence, we produce better-equipped allies, and greater resilience and belonging among community members from historically marginalized groups. We urge the community and relevant stakeholders to understand how to remove the barriers and challenges identified in our study.",https://www.semanticscholar.org/paper/0497f16c5ff40a74eec4d9eae1817026922d95d9,IS,Qualitative
A Case Study of Qualitative Methods,"In research – and many parts of life – we only see the finished product, a snapshot of calm and certainty even when the reality is chaotic. When people meet me, they might learn that I am a computer science (CS) professor. I assume they would never guess that I nearly failed data structures in college and still struggled in my second attempt. They would never imagine how many interviews I bombed and graduate schools I did not get into. They don’t see the inevitable paper and grant rejections or poor teaching evaluations. Those things aren’t on my CV, but reflecting back, I see these as some of the most influential elements for my learning. This chapter is a narrative of the actual doing of a research study, what Roth (2006) calls a praxis narrative. I hope to give you a “feel for the game” (Bourdieu, 1992) of doing one type of qualitative research. Ideally, you will gain some insights into qualitative methods, or at least recognition that if it feels chaotic, it is not necessarily wrong. Textbooks about qualitative methods have a burden of providing clarity to the methods. This chapter instead seeks to show all of the mess and ambiguity. In our current context, where computing knowledge is often perceived as only available to the intellectual elite or people with a “geek gene,” it is our responsibility to challenge these notions and help others see our humanness. I will attempt to do that while telling the backstory of this paper. In this chapter, I will share some of what I learned through writing, revising, and now reflecting on a paper that traversed a particularly rocky path. My qualitative analysis was eventually published in a paper at the Association for Computing Machinery (ACM) Special Interest Group on Computer Science Education (SIGCSE) International Computing Education Research (ICER) conference (Lewis, 2012a), but the path there was a bit bumpy. The analysis came from my master’s thesis (submitted December 2009), abbreviated to submit to ICER in April of 2010. It was rejected from ICER in 2010 and again in 2011. Despite the suspicion that the manuscript was doomed, I decided to revise and resubmit it again in 2012. Only in this third submission to ICER was it accepted. I received incredibly thoughtful – and harsh – reviews of my first submission to ICER in 2010. At that time, my work was described as preliminary and that the contribution was fairly minimal.",https://www.semanticscholar.org/paper/9cff2f00897e85d0585633ba89f310cfd611743d,CS,Qualitative
Computational thinking in elementary classrooms: measuring teacher understanding of computational ideas for teaching science,"ABSTRACT A number of efforts have focused on preparing teachers to integrate CT within secondary disciplinary subject areas; however, there is little research on how CT ideas could be embedded within elementary subjects. We designed a professional development activity for elementary teachers to embed CT within science and examined how their understanding of CT emerged over the course of PD. This paper reports results from qualitative analysis of teacher responses to vignettes and open-ended questions, which presented teaching scenarios related to CT. We found that the vignettes allow us to see shift in teachersâ€™ thinking about CT from broad and generalized ideas to more elaborate versions of those ideas. We discuss that while vignettes provided a good method to portray changes in teacher views about CT, we need additional mechanisms to monitor how teachers conceptualize and come to integrate computational thinking into elementary schools.",https://www.semanticscholar.org/paper/dfc9ae84dba515d5245ae0d4a5f01d73dd595c47,IS,Qualitative
Simulasi Pembelajaran IPA Menggunakan Computer Based Instruction MI Ma’arif Darussalam Plaosan,"Learning Natural Sciences (IPA) which is often considered difficult in the learning process takes place, that is, just sitting and listening to the material from the teacher through the lecture method so that students are passive in the process of implementing the learning process. So the purpose of writing this article is to find out the implementation of science learning using Computer Based Instruction through a simulation model. This research uses qualitative research with descriptive analysis method. While in this study using data collection techniques, namely using interviews and documentation. The results showed that at MI Ma'arif Darussalam Plaosan had implemented science learning using Computer Based Instruction through a simulation model with visual media where in the learning process itself the teacher gave concrete or real examples by displaying pictures related to learning when learning. take place. The response and enthusiasm of students is more active. In the simulation model there are role models, namely: explaining, intermediary, mentoring and learning while the learning model is the simulation model itself in four stages which are orientation, training, simulation and debriefing stages",https://www.semanticscholar.org/paper/55abd180f4c1d71e25fec80d469579fc95f47eff,CS,Qualitative
The relationship between personal and professional goals and emotional state in academia: a study on unethical use of artificial intelligence,"Artificial Intelligence (AI) is a concept that has been a subfield of computer science since the 1950s. In recent years, with its growing development power, AI technologies have made significant progress and are now being used in many fields. Like in all areas, the use of AI technologies in academia has provided convenience to academics while also bringing ethical debates. In the literature part of the study, concepts such as AI, academia, academics and academic progress, ethics, ethical theories, academic ethics, and emotional states have been thoroughly examined and defined. In this study, starting from AI and scientific ethics, ethical issues arising from emotional states in academic research have been identified, and concrete solutions to these ethical issues have been proposed. The aim is to discuss the views of academics in order to determine what types of scientific ethical violations and prevention methods are involved. In this context, the semi-structured interview technique, which is one of the qualitative research methods, was preferred as the method. In the study, in-depth semi-structured interviews were conducted with 4 ethics experts and 4 psychology experts selected through snowball sampling technique. The data obtained through semi-structured in-depth interviews will be analyzed using content analysis. Within the context of the literature review and interviews: Ethics is based on the foundation of acting correctly. In this context, scientific ethics can be summarized as acting truthfully and honestly, not distorting data, and not trying to progress unfairly. The use of AI in academia is becoming increasingly widespread. From a positive perspective, this usage significantly contributes to making studies more practical. However, it can lead to problems such as unfair authorship, devaluation of human authorship, and incorrect data. The connection between academics’ professional advancement goals and emotional states becomes prominent in this context. The potential of AI to facilitate progression can lead to unethical use. To prevent such situations, it is recommended to organize training sessions to increase professional awareness, internalize ethics personally, establish ethical committees specific to the field of AI, conduct more effective audits by academic publication and promotion committees, and implement specific regulations for AI. Finally, for future academic studies, it is suggested that the usage of AI in academic research be measured and evaluated by ethics experts. For psychologists, conducting surveys with academics to explore how they use AI in the context of their emotional states and professional advancement goals is recommended.",https://www.semanticscholar.org/paper/e1606eef95929e45dfd561264004f736865d61d1,CS,Qualitative
Unpacking High School Students’ Motivational Influences in Project-Based Learning,"Purpose: The presented study was conducted to unpack high school students’ motivational influences in engineering/computer science project-based learning (PjBL), using the attention, relevance, confidence, and satisfaction (ARCS) model of motivation as a conceptual framework. Methods: A qualitative research approach was used with student focus groups as the data source. A total of six focus groups with 32 student participants was conducted. The students were enrolled in high schools located in four different states in the U.S. The qualitative analysis of transcripts was performed using first and second cycle coding methods. Findings: The findings show that student motivation is nuanced in regard with attention, relevance, confidence, and satisfaction. The findings identify research-based strategies for fostering student motivation, such as implementing learner-focused scaffolding in PjBL environments, improving the relevance of the classroom content with the real-world context that students have experiences in or are knowledgeable about, and focusing on stimulating intrinsic motivation in addition to extrinsic rewards. Conclusion: The findings provide support for the comprehensibility and utility of the ARCS model of motivation in high school engineering/CS education, and more importantly empirically unpacks what the model factors mean from students’ perspective. Practitioners may use these findings to inform the design, development, and implementation of PjBL in high school settings.",https://www.semanticscholar.org/paper/036d39b6dcf52bb58f554d7084fc06bd311beb6c,IT,Qualitative
A review of machine learning experiments in equity investment decision-making: why most published research findings do not live up to their promise in real life,"The numerical nature of financial markets makes market forecasting and portfolio construction a good use case for machine learning (ML), a branch of artificial intelligence (AI). Over the past two decades, a number of academics worldwide (mostly from the field of computer science) produced a sizeable body of experimental research. Many publications claim highly accurate forecasts or highly profitable investment strategies. At the same time, the picture of real-world AI-driven investments is ambiguous and conspicuously lacking in high-profile success cases (while it is not lacking in high-profile failures). We conducted a literature review of 27 academic experiments spanning over two decades and contrasted them with real-life examples of machine learning-driven funds to try to explain this apparent contradiction. The specific contributions our article will make are as follows: (1) A comprehensive, thematic review (quantitative and qualitative) of multiple academic experiments from the investment management perspective. (2) A critical evaluation of running multiple versions of the same models in parallel and disclosing the best-performing ones only (“cherry-picking”). (3) Recommendations on how to approach future experiments so that their outcomes are unambiguously measurable and useful for the investment industry. (4) An in-depth comparison of real-life cases of ML-driven funds versus academic experiments. We will discuss whether present-day ML algorithms could make feasible and profitable investments in the equity markets.",https://www.semanticscholar.org/paper/216ed3fd50613671618277b9b354acc978183854,CS,Qualitative
Teaching Digital Accessibility in Computing Education: Views of Educators in India,"In recent years, there has been rising interest from both governments and private industry in developing software that is accessible to all, including people with disabilities. However, the computer science (CS) courses that ought to prepare future professionals to develop such accessible software hardly cover topics related to accessibility. While there is growing literature on incorporating accessibility topics in computing education in the West, there is little work on this in the Global South, particularly in India, which has a large number of computing students and software professionals. In this replication report, we present (A) our findings from a replication of surveys used in the US and Switzerland on who teaches accessibility and barriers to teaching accessibility and (B) a qualitative analysis of perceptions of CS faculty in India about digital accessibility and teaching accessibility. Our study corroborates the findings of the earlier surveys: very few CS faculty teach accessibility, and the top barriers they perceive are the same. The qualitative analysis further reveals that the faculty in India need training on accessibility concepts and disabilities sensitization, and exposure to existing and ongoing CS education research and pedagogies. In light of these findings, we present recommendations aimed at addressing these challenges and enhancing the integration of accessibility into computing education.",https://www.semanticscholar.org/paper/685ee6822029ebd1f8731c2379706fc1e544bd33,IT,Qualitative
Management control systems for sustainable development: a bibliographic study,"Abstract The purpose of this study was to provide a comprehensive review of previous studies on management control systems (MCS). The study tries to evaluate the key aspects that research on MCS mentioned from 1970 to 2023. Using qualitative methods and VOSviewer software, the study collected and analysed data from the Gale Academic OneFile and OpenAlex databases. The paper examined 2279 articles, including 248 from Gale Academic OneFile and 2031 from OpenAlex. The publications were collected based on articles with the keywords ‘management accounting system’ and ‘sustainable development’. After collection, the articles are classified and arranged according to 6 perspectives. The results show that during the research period (1970–2023), articles on the topic ‘Control Management’ appeared the most with 1850 times. Next are articles with ‘Business’ appearing 1658 times and ‘Computer science’ appearing 1570 times. The topic ‘Management control system’ ranked fourth with 1273 appearances. Results also show that general studies on MCS are conducted with the most significant number of articles (37), with the greatest concentration from 2020 to 2023. The decreasing level includes research on administrative control, detailed control, strategic control, control methods and cultural control. Control tools such as planning and performance measurement also received much attention. The study also found the most influential authors in MCS research (by number of citations and articles) and the countries, universities and journals with the most publications on MCS. The current study provides scholars and researchers with insights from previous investigations into essential areas of MCS. Thereby, it provides a new contribution and comprehensive assessment by highlighting what has been done and what remains to be done in research on MCS. Furthermore, our study reveals future research opportunities and agendas in MCS.",https://www.semanticscholar.org/paper/968dac23465e1c8870a3c176aa6d8e450a737573,CS,Qualitative
Implementing Computational Thinking Skills in Socio Scientific Issue (SSI) of Force Material Around Us at Elementary School,"Computational thinking is a sophisticated problem-solving approach based on computer science. Implementing computational thinking in elementary schools remains a challenge for education, particularly for teachers and students. Teachers must construct learning experiences using computational thinking to make the learning process more engaging, while students must solve issues logically, systematically, and effectively. This study aims to present an overview of the application of computational thinking in natural and social sciences (IPAS) learning and identify its application in fifth-grade elementary school students. The research employed a qualitative research design with a single one-shot case study. The research subjects were 40 fifth-grade students and four teachers. This investigation was conducted during a single meeting that followed the lesson plan for the force material around us. The findings revealed that (a) analysis of student activity data yielded a percentage of 81.25%; (b) the data analysis of student responses to learning yielded favorable results, approaching 100%. According to interviews and observations of CT (Computational Thinking) in SSI (Socio-scientific Issue) in IPAS subjects, implementing CT learning on the forces around us might bring up aspects of the CT foundation, such as pattern recognition decomposition, abstraction, and algorithms. The learning scenario is that students are requested to assess many types of activities that occur in everyday life. Additionally, students will outline various actions that use force.",https://www.semanticscholar.org/paper/1399e28e99cf1e794a05a6ac325a626a2e3bbd01,CS,Qualitative
A Qualitative Analysis of Students' Understanding of Conditional Control Structures,"Conditional logic and control structures are typically considered an important part of introductory computer science education, yet novices often struggle to correctly write and navigate such program logic. Previous research has largely attended to student difficulties with parsing Boolean expressions, but has not had much focus on the control structures themselves. To investigate how students work through complicated logic, we conducted a qualitative analysis of four one-on-one interviews with undergraduate students in which we gave students a piece of code with a complicated conditional control structure and asked them to write test cases for all paths. We found that several students struggled to determine the output the function would provide for a given input, and we hypothesize this occurred because they incorrectly treated an if statement as an else-if statement. One student simply wrote an incorrect output, which we believe occurred because they made this particular mistake, while another student got partway through the problem before verbally seeming to correct themself and re-identify a statement as an else-if. Based on our results, we hypothesize that novices may sometimes misidentify a sequential if statement as an else-if, which may lead them to incorrectly interpret a conditional control structure.",https://www.semanticscholar.org/paper/175abecc40f7ecb21f135a390c52e5e349d1373c,IS,Qualitative
Why Example Fading Works: A Qualitative Analysis Using Cascade,"Why Example Fading Works: A Qualitative Analysis Using Cascade Eric S. Fleischman (esfleisc@colby.edu) Randolph M. Jones * (rjones@colby.edu) Department of Computer Science, Colby College, 5830 Mayflower Hill Drive Waterville, ME 04901 − 8858 USA Abstract “Faded” examples are example problems that provide a solution, but first require students to generate a portion of the solution themselves. Empirical studies have shown that such examples can be more effective teaching aids than completely worked examples that require no work from the student. Cascade is a model of problem-solving skill acquisition that was originally developed to explain other empirical regularities associated with human problem solving and learning, most notably the self-explanation effect. Past research demonstrated that Cascade might also explain the mechanisms underlying the effectiveness of example fading. This paper analyzes new protocol data, and finds that it is consistent with predictions derived from Cascade. Overview Renkl, Atkinson, and Maier (2000) empirically demonstrated the qualitative result that, when learning problem-solving skills, students studying a series of “faded” examples show improved post-test performance over students studying only completely worked examples. Jones and Fleischman (2001) argue that this result can be explained by Cascade (VanLehn, Jones, & Chi, 1991), a computational model of problem-solving skill acquisition. Cascade was originally developed to understand the mechanisms of the self-explanation effect (Chi, Bassok, Lewis, Reimann, & Glaser, 1989; Pirolli & Anderson, Jones and Fleischman demonstrated that the mechanisms underlying self-explanation might also explain the effectiveness of studying faded examples. Although they showed that Cascade is consistent with the fading result, the explanation involved assumptions that had not yet been tested empirically. Therefore Jones and Fleischman (2001) finished with a small set of predictions and suggestions for new experiments to confirm or dispute Cascade’s account. Since that time, Renkl, Atkinson, and their colleagues have run additional experiments, collecting detailed transcripts of subjects studying two types of faded sequences of problems. Although the experiments are not yet complete, we have been able to perform a qualitative analysis of the protocol data for eight of the subjects. Additionally, we have fine-tuned Cascade’s knowledge base (but not its underlying mechanisms) to more faithfully model the current data. This paper reports the result of using Cascade to develop a qualitative analysis of the eight subjects. The primary result is that the findings remain The second author is also affiliated with Soar Technology, Inc. consistent with Cascade’s account of example fading, as well as the predictions made by Jones and Fleischman Background Years of research have demonstrated effective techniques for teaching students problem-solving skills in a variety of task domains. In particular, a number of studies show that students benefit from being given a series of completely worked example problems, followed by a series of unworked practice problems (e.g., Chi et al., 1989; Pirolli & Anderson, 1985; Renkl, 1997, VanLehn, 1996). Other studies show that the effectiveness of such a curriculum depends in part on the willingness of the students to explain the worked examples to themselves in detail, rather than simply giving the examples a superficial read (Chi et al., 1989; Fergusson-Hessler & de Jong, 1990; Pirolli & Bielaczyc, 1989). VanLehn and Jones (1993a, 1993b; VanLehn et al., 1991) developed Cascade in order to determine the cognitive mechanisms behind this self- explanation effect. In essence, Cascade suggests that thorough study of worked examples help students consciously expose and patch gaps in their task knowledge. In addition, self-explanation provides contextual memories that can guide future problem solving by analogy to familiar examples. Subsequent experiments by Renkl et al. (2000) suggest that student learning can improve even further by fading a curriculum from fully worked examples to partially worked examples. The partially worked examples provide a complete solution to the problem (as with fully worked examples), but first require students to derive one or more steps on their own. This in turn requires the students to understand the rest of the example in at least enough detail to be able to attempt a solution. Jones and Fleischman (2001) argue that the reason faded examples improve learning is that they retain much of the guidance provided by the context of a solved example, but they force the students to work on particular parts of the problem, in turn possibly forcing them to expose and patch knowledge gaps. This is in contrast to studying completely worked examples, where it is basically up to the students to decide whether they are going to put any effort into understanding the examples (because the students are not required to produce any answers in that case). This argument came directly from the assumption that Cascade is",https://www.semanticscholar.org/paper/dd0b1f9eee57eed86b474035c50aee2c2b3ffefc,CS,Qualitative
Augmented Reality (AR) Generated Value-in-Use Experiences: Literature Review and Analysis,"Augmented Reality (AR) research has sparked both academia and practice interest. However, despite the increasing interest to study this emergent topic, till now the growing body of knowledge is mainly from engineering and computer science disciplines. The aim of this study is to conduct a systematic literature review to identify, evaluate and synthesize (AR) emergent themes from business management publications in the last years. Hence, contribute to the literature with evidence-based results about the existing research progress, and classify into clusters with multidimensional interrelationships to help researchers build up on these find - ings and expand (AR) research from business standpoint. The study is exploratory and qualitative in nature; a structured hybrid-narrative review method is used to extract unstructured data within articles. Thus, discover relevant patterns, uncover hidden thematic clusters, relationships, and identify multidimensionality of dominant concepts from the existing (AR) business literature. It was evident from the stream of research, that most of researchers’ have a clear interest in exploring the interactive nature of Augmented Reality (AR) as new digital technology and defining its experiential characteristics with a hierarchical set of dimensions that will affect customers’ perceptions of service operations. Hence, in line with these findings, and grounded on the service literature body of knowledge, Augmented Reality can be defined as: “smart service based technology that generate value-in-use experiences”.",https://www.semanticscholar.org/paper/19dbe1715756fd7dd22b201b0097ff3ce1e87ac4,CS,Qualitative
"Small Steps, Big Progress: Analyzing District Led Goals to Advance CS Education","The demand to provide high-quality computer science (CS) education to K-12 students across the United States continues to grow due to societal transformations driven by AI and cybersecurity. However, the impact of state initiatives and mandates on district leaders' decision making remains an under-explored area in the literature. In 2022, CSforALL began work in Tennessee, a state poised to enact CS education policy, as part of a Research Practice Partnership (RPP). This study investigates the first eight school districts who participated in theStrategic CSforALL Resource and Implementation Planning Tool (SCRIPT) workshops in 2022 and 2023, setting goals based on the SCRIPT rubric. The study takes a general qualitative approach underpinned by the Capacity, Access, Participation, and Experience (CAPE) Framework to develop a coding scheme analyzing the districts' related rubric scores and goals, and to investigate the impacts on equity indicators. The districts participated in three SCRIPT workshops held in 2022 and 2023, and this study dives deeply into the initial goals as well as analyzing the ways the SCRIPT rubric aligned to the CAPE Framework to investigate how district leaders make decisions which impact teacher and student outcomes which lead to equitable high-quality CS education.",https://www.semanticscholar.org/paper/5b44771642dd8a3011ae88c5f2b28aa3f3e54726,IS,Qualitative
"International journal of information security: a bibliometric study, 2007-2023","This study employs various bibliometric analysis techniques to examine the intellectual structure of the International Journal of Information Security from 2007 to 2023. The aim is to identify the most cited journals, underlying research themes within the article corpus, and gradual changes in the research themes over time. “Lecture Notes on Computer Science” is the most referenced knowledge source. In addition, articles on the theme of encryption techniques received the highest average citations, followed by key management and authentication protocols. Moreover, machine learning, blockchain, and the Internet of Things are emerging topics of interest among published authors in the International Journal of Information Security. Qualitative and quantitative comparisons between open-access and regular articles suggested a few notable differences in author keywords but no differences in the number of citations received. Furthermore, regression analysis found a negative correlation between citation counts with the length of the article abstract and article title and a positive correlation with page count, being published in a special issue, and if at least the a�liation of one of the authors is different from others. Finally, we also identi�ed prominent authors, articles, institutions, and countries published in this journal.",https://www.semanticscholar.org/paper/1bdfd84d4567a0b85734d0a3fbac0239da61b3f7,CS,Qualitative
"Open, distance education and self-directed learning of teachers during Covid19 pandemic crisis","The fields and motivations of self-directed learning are for adult educators a matter of their lifelong learning. Teachers openly and remotely train themselves in various fields and develop knowledge, skills and attitudes. These fields include education, computer science, arts, environment, special education, school psychology, etc. The teachers’ motivations mainly include their personal and professional development, their socialization, an offering to other people, the improvement of their life quality etc. The present research, with a qualitative approach, attempted to study the fields and motivations of primary school teachers in terms of their self-directed learning. The semi-structured interview technique was chosen for data collection. The results showed that the fields and motivations of the teachers serve their daily school needs. They are related to their personal and professional development and the critical thinking of both themselves and their students afterwards. Especially due to the Covid19 pandemic crisis the training needs of teachers are carried out openly and remotely. Their participation in various programs, but also their future intention to participate, enhance the need to develop self-directed learning skills, as well as e-learning programs in this direction. The present study can contribute to the increase of knowledge and practice in the development, promotion and planning of self-directed learning interventions in open and distance education, while its originality lies in the fact that no similar research has been implemented in the field of self-directed learning and its connection to lifelong teacher education.",https://www.semanticscholar.org/paper/98ab1c5e4657a2dfee84a9b79553ad175402773d,CS,Qualitative
Integrating Computational Thinking Into Mathematics Class: Curriculum Opportunities and the Use of the Bee-Bot,"The incorporation of Computer Science teaching in educational systems has increased in recent years. Given international interest, Chile has promoted projects to promote the development of students' digital skills. Focusing on this new educational context, this research reports the results regarding the identification of computational concepts and practices that can be articulated with the contents and skills of the curriculum. of Chilean mathematics. for first grade of primary school based on the use of the Bee-Bot robot. For this, the study followed a qualitative approach, developing a case study of the Chilean study program with the content analysis technique and using, as analysis categories, computational concepts and practices from the field of educational computing. In total, 30 learning objectives of the study program were analyzed. The results indicate that, although there is little articulation between computational concepts and first grade content proposed in the curriculum, there is greater articulation between computational practices and mathematical skills suggested in the Chilean curriculum. It is concluded that Computational Thinking can be developed from the earliest school levels using the Bee-Bot robot (or similar), and this is demonstrated by the structure of the Chilean primary mathematics curricular program.",https://www.semanticscholar.org/paper/365ab603837219301970bc3d55b020ef4110de56,IS,Qualitative
MILSDeM: Guiding immersive learning system development and taxonomy evaluation,"Developing immersive learning systems is challenging due to their multidisciplinary nature, involving game design, pedagogical modelling, computer science, and the application domain. The diversity of technologies, practices, and interventions makes it hard to explore solutions systematically. A new methodology called Multimodal Immersive Learning Systems Design Methodology (MILSDeM) is introduced to address these challenges. It includes a unified taxonomy, key performance indicators, and an iterative development process to foster innovation and creativity while enabling reusability and organisational learning. This article further reports on applying design-based research to design and develop MILSDeM. It also discusses the application of MILSDeM through its implementation in a real-life project conducted by the research team, which included four initiatives and eight prototypes. Moreover, the article introduces a unified taxonomy and reports on the qualitative analysis conducted to assess its components by experts from different domains.",https://www.semanticscholar.org/paper/5d50ef9bdce16aa3f41d385d957490a981903c7d,IS,Qualitative
Evaluation of Packet Tracer Application Effectiveness in Computer Design Networking Subject,"The dynamics of technological progress in various fields indirectly affect the world of education. One of the indications can be seen in the use of media as a learning tool. This study aims; 1) to investigate the effectiveness of the packet tracer learning media application, 2) to investigate the responses of the students to such applications, 3) to determine the appropriate media used in a learning process. The research is carried out in the subject of Computer Design Networking. The evaluation research is employed by using a mixed method of qualitative and quantitative approach. The quantitative analysis involved 58 students, while the qualitative analysis involved one productive teacher, one curriculum representative, and four students. Data are analysed by using quantitative and qualitative data analysis techniques. The results show that, the effectiveness of learning media with packet tracer is high i.e., 82.76%, which covers three aspects; software engineering, learning aspects, and display aspects. This means that this packet tracer learning media can be applied as a solution to the limitation of facilities and infrastructures of network practices by considering certain learning situations and conditions. The results indicate that this application is very useful to deal with the high cost of practising tools. It makes the students more enthusiastic and motivated during the learning process. Hence, by implementing this learning method, the interaction and the learning outcomes of the student can be increased.",https://www.semanticscholar.org/paper/ace5575acf5e12ffd2fa1f8eca158241df66b549,CS,Qualitative
SCIENCE WRITING IN ENGLISH: DIFFICULTIES FOR NOVICE RUSSIAN AUTHORS,"Statement of the problem. The paper discusses multiple deviations of science texts in the English language written by Russian novice writers from the norms and expectations of the international discourse community. The purpose of the article is to propose a concept of utilizing computer linguistics tools of automated text analysis for pedagogical purposes to identify real needs of students in science writing in English. The methodology is based on the contrastive discourse-analysis of two corpora: students’ research paper manuscripts and published research papers by international expert writers. Both corpora belong to one engineering discipline and were compiled specifically for the research purposes. The contrastive corpus analysis was conducted by means of a computer linguistics tool Gramulator. Research results. The quantitative and qualitative analysis of differential bigrams in Gramulator demonstrated that the writing by novice Russian authors significantly differs from the expert writing. The considered bigrams indicated lower lexical diversity; insufficient skills in using grammar that is absent in the Russian language; tendency to underuse predicative clauses and hedging devices; tendency to overuse words and phrases typical of Russian scientific style, as well as referencing and specifying words and phrases. Conclusions. The proposed concept is aimed to make the academic writing process more student-centered. The identification of common linguistic problems in the novice researchers’ writing may raise students’ awareness of the differences in the rhetorical choice in the two languages, improve strategic skills in making the proper choice when writing their own texts, and, thus, approach to the norms and expectations of the target discourse community.",https://www.semanticscholar.org/paper/b5e1d8ec3ac535f8770548428ebe43ea2de3a1f6,CS,Qualitative
Application of Computer Aided Technology in Wushu Routine Sports,"With the development of science and technology, computer technology has also made a qualitative leap, and it has been applied in many fields. Martial arts routines are a series of combination of actions that include attack and defense, and are characterized by introversion in our traditional culture. The training of martial arts routines is a long-term and periodic process, and the workload is huge. Therefore, this article proposes the application research of martial arts routines based on computer-assisted technology, using literature data method, observation method, expert interview method and logical analysis method to learn the characteristics of martial arts routines, competition rules and other related content, and design The electronic training plan system, expert consulting system and computer simulation system of Wushu routines based on computer-aided technology. Finally, the comparative training situation of the experimental group and the control group in Wushu routines is analyzed. The experimental group scored 1.35 points in martial arts routines with the aid of computer technology, while the control group only got 0.23 points. The score of the experimental group was significantly higher than that of the control group. This article starts from the characteristics of martial arts routines, and formulates training programs based on the needs of computer technology, hoping to provide more reference and constructive opinions for martial arts routines..",https://www.semanticscholar.org/paper/a5ab7728898f949fe4fc5e0058b4b076d039fc3c,CS,Qualitative
The Importance of Creating a Control Structure Programming Module to Improve Students' Understanding,"The landscape of the education system has changed and evolved in line with global needs, driven by the impact of the information technology revolution occurring worldwide. The Ministry of Education Malaysia (MOE), through the Digital Education Policy (2023), has established six core pillars related to digitally fluent students, digitally competent teachers, and digital-based learning materials. Therefore, this study aims to identify the necessity of developing a control structure programming module to enhance the understanding of Form Four students in the Computer Science subject through a qualitative approach. The use of a digital module that includes multimedia elements enhances learning by making it more interactive and accessible. This study involves direct observation of the interactions between three students and one teacher while using the digital module. Additionally, interviews will be conducted with both the students and the teacher to gather their perspectives and experiences regarding the necessity of learning control structures through this digital module. The findings indicate the necessity of using this control structure module to improve students' understanding and assist them in writing control structure programs effectively. The results of this research are expected to provide a framework for educators in the creation or adaptation of digital learning resources. This approach aims to maximize the use of existing technology while simultaneously fostering students' engagement and understanding of Computer Science, with a specific focus on programming related to control structures.",https://www.semanticscholar.org/paper/33b575a2442b6f88c6d606b96ea15055c7ac0ba0,IS,Qualitative
"Cheap, Quick, and Rigorous: Artificial Intelligence and the Systematic Literature Review","The systematic literature review (SLR) is the gold standard in providing research a firm evidence foundation to support decision-making. Researchers seeking to increase the rigour, transparency, and replicability of their SLRs are provided a range of guidelines towards these ends. Artificial Intelligence (AI) and Machine Learning Techniques (MLTs) developed with computer programming languages can provide methods to increase the speed, rigour, transparency, and repeatability of SLRs. Aimed towards researchers with coding experience, and who want to utilise AI and MLTs to synthesise and abstract data obtained through a SLR, this article sets out how computer languages can be used to facilitate unsupervised machine learning for synthesising and abstracting data sets extracted during a SLR. Utilising an already known qualitative method, Deductive Qualitative Analysis, this article illustrates the supportive role that AI and MLTs can play in the coding and categorisation of extracted SLR data, and in synthesising SLR data. Using a data set extracted during a SLR as a proof of concept, this article will include the coding used to create a well-established MLT, Topic Modelling using Latent Dirichlet allocation. This technique provides a working example of how researchers can use AI and MLTs to automate the data synthesis and abstraction stage of their SLR, and aide in increasing the speed, frugality, and rigour of research projects.",https://www.semanticscholar.org/paper/8dbea697e3bace3b87d0681e2aae3f7b9b8fe9e0,CS,Qualitative
Comprehensive overview of computer-based health information tailoring: a systematic scoping review,"Objectives To explore the scope of the published literature on computer-tailoring, considering both the development and the evaluation aspects, with the aim of identifying and categorising main approaches and detecting research gaps, tendencies and trends. Setting Original researches from any country and healthcare setting. Participants Patients or health consumers with any health condition regardless of their specific characteristics. Method A systematic scoping review was undertaken based on the York’s five-stage framework outlined by Arksey and O’Malley. Five leading databases were searched: PubMed, Scopus, Science Direct, EBSCO and IEEE for articles published between 1990 and 2017. Tailoring concept was investigated for three aspects: system design, information delivery and evaluation. Both quantitative (ie, frequencies) and qualitative (ie, theme analysis) methods have been used to synthesis the data. Results After reviewing 1320 studies, 360 articles were identified for inclusion. Two main routes were identified in tailoring literature including public health research (64%) and computer science research (17%). The most common facets used for tailoring were sociodemographic (73 %), target behaviour status (59%) and psycho-behavioural determinants (56%), respectively. The analysis showed that only 13% of the studies described the tailoring algorithm they used, from which two approaches revealed: information retrieval (12%) and natural language generation (1%). The systematic mapping of the delivery channel indicated that nearly half of the articles used the web (57%) to deliver the tailored information; printout (19%) and email (10%) came next. Analysis of the evaluation approaches showed that nearly half of the articles (53%) used an outcome-based approach, 44% used process evaluation and 3% assessed cost-effectiveness. Conclusions This scoping review can inform researchers to identify the methodological approaches of computer tailoring. Improvements in reporting and conduct are imperative. Further research on tailoring methodology is warranted, and in particular, there is a need for a guideline to standardise reporting.",https://www.semanticscholar.org/paper/6dacd40c1771bd84621527bb188a636f22b69fd5,CS,Qualitative
Computer-Aided Argument Mapping for Improving Critical Thinking: Think Better! Discuss Better! Write Better!,"The aim of this study is to investigate the impacts of the use of computer-aided argument maps as a tool to promote prospective teachers’ critical thinking skills and dispositions. In this regard, qualitative research method was used in the study. The data of the research were collected through semi-structured interviews. Study group consists of 30 senior prospective teachers from three different classes studying science teaching at a university in Turkey in 2017-2018 academic year. At these three classes mentioned, individual and collaborative argument maps were created in addition to the ABI (Argumentation-based inquiry) activities. The study group was formed on a volunteer basis with 10 students from each class who were selected from, their course performances into consideration. A computer software was used to create the argument maps. Data obtained from interviews were analyzed through NVIVO program. The results obtained indicated that critical thinking skill sub-dimensions such as explanation, analysis, interpretation, evaluation, Self-correction and critical thinking disposition sub-dimensions such as questioning the reliability of sources, being open and fair-minded, being respectful of differences were emphasized more in the group in which the prospective teachers performed computer-aided, individual and collaborative mapping rather than the group in which only ABI activities were performed.",https://www.semanticscholar.org/paper/bbe83512e01f586fb21a52a20efa3d055b7fe74d,CS,Qualitative
Construction and Realization of Graphical Thinking of Brand Design in Computer Vision Communication,"Design method is an important part of design, and it is an important product of academic and educational research practice. This paper discusses different fields of visual design and their methodological issues, based on the definition of visual communication design, WOS and a search of relevant literature on the Internet, with the aim of understanding the essential properties of visual communication activities from a methodological stratification perspective and clarifying the norms of different disciplines involved in visual communication. The objectives, content, processing, and output of two types of visual design activities with purpose and means are analyzed in terms of design process, conceptual framework of visual representation, and qualitative and quantitative aspects. As an interdisciplinary design discipline, visual design is essentially a comprehensive activity of information dissemination that occurs in a variety of scenarios, including business, education, and science. It is necessary to keep the boundary, clarify the ideographic and stylistic characteristics of visual design, and improve the ontological quality of visual design profession, but it is also necessary to cross the boundary on the basis of keeping the boundary, so as to better cultivate visual ability, promote the transformation of visual design methods in other fields, and improve the disciplinary growth of the profession in the new era.",https://www.semanticscholar.org/paper/06d84cc0f5abd1400a52943cc39242e411b685c3,CS,Qualitative
Exploring Barriers and Strategies to boost Scientific Output in Computing Education in Africa: Early Insights,"There is a worrying lack of published work in international venues about computer science education (CSE) in the Global South, especially in Africa. Despite the increasing number of African institutions offering various computer science degree programs to meet the growing demand for computing skills and practices, there is limited scholarly evidence in the CSE context. As a result, it is important to understand why there is low representation of the continent in computing education research (CER) space. In this paper, we provide insights into the factors hindering the growth of CER in Africa, including strategies that could be employed to increase participation. This study employed a qualitative approach through the use of interviews. This study gathered the thoughts of ten computing education researchers across different countries and regions of Africa. The findings uncovered African researchers' motivation for CER and challenges such as limited awareness, financial constraints, and inadequate institutional support. Proposed solutions include establishing collaborative networks, raising awareness, and providing funding opportunities to boost CER output in the region. This research provides valuable insights for CSE, contributing to inclusion, equity, and broadening participation in computing.",https://www.semanticscholar.org/paper/e1c97b16d5a911c4d5880fcf77cca5e072546c44,IT,Qualitative
The Rising Influence of AI in Higher Education: Trends and Insights from a Bibliometric Analysis,"The purpose of this research was to examine the evolution, scope, and orientation of the scientific production on artificial intelligence applications in university students. The methodology, with a non-experimental design and qualitative approach, involved a search in Scopus, identifying 643 documents between 1975-2024, analyzed through VOSviewer and Bibliometrix. The results show an emerging field, but with rapid growth (4.59% per year), with notoriety of Kong, Abdulrahman and Chai. Research is predominantly in computer science (61%), social sciences (33%) and engineering (23%) from China, USA, Spain and Taiwan. Current applications focus on the use of AI in education, machine learning, support for academic decisions and student mental health. However, it is necessary to expand the approach towards ethical and regulatory aspects and the evaluation of multifaceted effects on different student profiles. In conclusion, although production is growing rapidly, more comprehensive perspectives are required to responsibly enhance the impact of these technologies on the university educational experience. Received: 9 June 2024 / Accepted: 25 August 2024 / Published: 05 September 2024",https://www.semanticscholar.org/paper/faf8b4b8e6c145859ef0af2b6b95735f2698dd2e,CS,Qualitative
A Preliminary Analysis of Indiana Schools’ Implementation of Virtual Instruction and CS Education in the 2020-2021 School Year,"Two datasets, published by the Indiana Department of Education (IDOE) related to Indiana schools’ implementation of virtual education and computer science (CS) instruction during the 2020-2021 school year, were analyzed. This research aims to explore schools’ virtual instructional practices during the unprecedented educational disruptions and understand the preparedness and execution of CS curricula. With quantitative and qualitative approaches, we first evaluated the percentages of different instructional modes, school districts’ implementation of virtual instruction, their support for students, successes and barriers schools experienced, as well as their future plans regarding virtual learning. Findings indicate that hybrid and virtual instruction were prevalent, with disparities in technology access and instructional quality, and schools shared barriers related to technology resources, teacher preparedness, and student engagement. In addition, this study reveals the unequal implementation of CS education, particularly in lower grade levels, underscoring the need for more dedicated resources and teacher support. These insights may help inform future educational strategies for multi-modality instruction and enhance the integration of CS education in K-12 curricula. Keywords: virtual instruction, computer science education, Indiana schools, technology access, teacher support.",https://www.semanticscholar.org/paper/073b78b050d8a53170f0ee2f210fa5bfd1f55194,CS,Qualitative
Computer Coding and Choreography: Contrasting Experiences of Learning About Collaboration in Engineering and Creative Arts,"This article argues that how collaboration is taught can have a significant impact on the ways in which collaboration is experienced, understood and valued. In doing so, the study draws attention to performing arts studio-pedagogies, and their potential relevance to enhancing creativity within science, technology, engineering, and mathematics (STEM) education. Through a mixed-methods study of teachers’ and students’ experiences of group work, this article compares two disciplines that maintain distinct discourses on teaching collaboration: Software design and choreography. The quantitative data reveals that despite significant demographic differences, students from the two disciplines maintain a common enthusiasm for group learning. There are significant distinctions however, on student perceptions of the teaching and learning of collaboration, their learning achievements about group work, and the relevance of group work in their discipline. Qualitative commentaries from students and teachers extend the arguments across both the distinctions and the similarities, emphasizing the impact of particular teaching practices and establishing standpoints for further research into the pedagogy of collaboration in higher education.",https://www.semanticscholar.org/paper/89218a12ba3ff41ac8aaecc1e75e480a010ac8ee,CS,Qualitative
A seventeen-year Research Topic Dispersion and Methodological Choices among LIS Postgraduates in Tanzania,"The study explored research topics dispersal and research strategies of Library and Information Science (LIS) postgraduate students in Tanzania from 2000-2017. Data were collected from the East Africana research repository section of the UDSM Library and the Information Studies Programme (ISP) offices from June to November 2017. A total of 269 LIS dissertations were assessed using content analysis to classify research topics. Findings revealed topics extensively researched were information dissemination, information access and information seeking behaviour. Least researched topics were information privacy and ethics, reading habits and encouragement, copyright and Information policy. History of information science and librarianship, quantitative and qualitative research, multimedia, webometrics, human–computer interaction, systems analysis, digital security, internet crime and information licensing and fair use were not researched. In addition, mixed research approach was mostly preferred by students but experimental and action research were least used. The study contributes in LIS education mapping in Tanzania and likely the first study to show topics selection based on globally accepted LIS classification schemes. The study gives an insight on research topics and research strategies used in higher learning organizations (HLIs) in Tanzania and arguably most developing countries. Findings may improve quality of research offered in LIS schools by supporting LIS postgraduate training and establishing library technology hubs and laboratories to match the world LIS research trend and order.",https://www.semanticscholar.org/paper/7272c1aaa34b2afc748183526d15af0fa6e4cc59,IS,Qualitative
Aiming Towards Abstraction: Does Algorithmic-Pattern-Oriented Instruction Promote the Teaching of Abstraction?,"Abstraction is a pivotal concept in computer science (CS); it is extensively utilized for various purposes such as problem simplification and algorithm design. Often considered the most important mental tool for computer scientists, abstraction is a key element in CS education and curricula. However, teaching CS abstraction is known to be difficult. One of the instructional methods offered in the literature to promote abstraction is pattern-oriented instruction (POI), which relies on algorithmic patterns and uses them as organizing principles and a central problem-solving strategy. Research has shown that using pattern-oriented instruction supports students' abstraction abilities; however, we claim that POI alone may not be sufficient and therefore deeper exploration is required to understand the relations between employing POI, teaching abstraction and developing abstraction skills. In this qualitative study, we thoroughly analyzed the teaching approaches of eight high-school CS teachers, focusing on their use of POI and how they teach abstraction (both their perceived and actual teaching approaches). The results suggest that although POI may support the perception of the teachers as promoting abstraction, its actual implementation may not fully align with this purpose.",https://www.semanticscholar.org/paper/d313df2584603c97953e53937f181a1bd8d487c8,CS,Qualitative
Transforming TVET Curriculum: Leveraging Coding and Robotics as catalysts for Functional Design Innovation at IGCSE level in Zimbabwean Schools,"This qualitative study investigates the transformation of the TVET curriculum in Zimbabwean schools through the integration of coding and robotics into the IGCSE Design and Technology syllabus. It explores how this synergy enhances students’ ability to produce innovative, functional, and industry-relevant design solutions. A key focus is on bridging the gap between conceptual design and real-world application, emphasizing that designs often fall short when they lack programmed instructions provided by coding. Coding equips students with the ability to embed functionality into their designs, while robotics offers a platform for hands-on experimentation, enabling the creation of interactive, automated prototypes. The research underscores the necessity of harmonizing Design and Technology with Computer Science, fostering a multidisciplinary approach that enhances creativity, technical proficiency, and problem-solving skills. The study envisions a TVET curriculum that prepares students for the demands of Industry, empowering them to develop smart, user-centred products that align with global industrial trends and technological advancements.",https://www.semanticscholar.org/paper/bd10d8bd9e1842355661bf1e7d463c759e3c81a6,CS,Qualitative
Exploring Student Perspectives on Generative AI in Requirements Engineering Education,"The rapid development of generative AI (GenAI) technologies in recent years has enabled new opportunities as well as new challenges in higher education. While many studies in computer science have focused on GenAI in programming education, fewer have examined its possibilities and challenges in requirements engineering (RE). This study aims to explore the impact of GenAI on the pedagogical aspects of RE in higher education, focusing on the student perspective, to analyse how GenAI might influence learning experiences, knowledge acquisition, and skill development. The main research question to answer was: ""What are the students’ perspectives of the integration of GenAI in the educational practices of requirements engineering?"" An Action research strategy was employed, with one of the authors also serving as teacher in the investigated course. A mixed-methods approach was used to collect both qualitative and quantitative data from workshops and surveys. During the workshops, students used ChatGPT to generate and evaluate software requirements and compared these to manually crafted requirements. Thematic analysis of the qualitative data captured students’ perspectives, while survey data identified trends and preferences. Findings show that while students generally had a positive experience with GenAI, valuing its efficiency and the quality of generated requirements, they also recognized the need for human oversight to maintain accuracy. The study highlights both opportunities and challenges of using GenAI in RE education. While GenAI increased learning engagement and helped with brainstorming, students faced difficulties in creating effective prompts and found it time-consuming to refine AI-generated requirements. A hybrid approach, combining AI-generated and manually created requirements, proved most effective by balancing AI's advantages with human insights. Further research is needed on how GenAI could be effectively integrated into computer science education.",https://www.semanticscholar.org/paper/1e7291d1243368e4608e282f63f29716b127bc13,CS,Qualitative
On Students' Computational Thinking Skills for Solving SRAC and its Theoretical Framework on Multi-Step Time Series Forecasting on River Erosion using GNN under RBL-STEM Learning Stages,"Computational thinking involves the use of computer science principles to solve complex problems, extending beyond simple programming to various life applications. In today’s educational landscape, the promotion of these skills in the classroom is critical, yet students’ computational thinking skills remain underdeveloped due to inadequate learning models. Key indicators of computational thinking include problem decomposition, algorithmic thinking, pattern recognition, abstraction, and generalization. This study presents RBL-STEM learning activities aimed at enhancing students’ computational thinking through solving the Strong Rainbow Antimagic Coloring or SRAC problem and applying it to multi-step time series forecasting on river erosion using Graph Neural Networks or GNN. The research adopts a qualitative narrative method, beginning with the development of a prototype for multi-step time series forecasting on river erosion using SRAC and GNN, and progressing to the formulation of RBL-STEM learning steps. The results include a comprehensive RBL-STEM learning framework ready for implementation in future research. Learning framework offers student and educator a structured approach to integrating STEM on real life issues. By employing RBL-STEM, students are encouraging to solve river erosion problem systematically based on RBL stages. These finding suggest that the implementation of RBL-STEM with innovative mathematical problems such as SRAC can enhance students’ combinatorial skills, leading to practical solutions for everyday issues through education.",https://www.semanticscholar.org/paper/7bab848f8e4a1ac9de1b4358075dd9dff9e7afe9,CS,Qualitative
Artificial intelligence and learning environment: Human considerations,"Artificial intelligence (AI) has created new opportunities, challenges, and potentials in teaching; however, issues related to the philosophy of using AI technology in learners' learning have not been addressed and have caused some issues and concerns. This issue is due to the research gap in addressing issues related to ethical and human needs, and even values in AI in learning have become more obvious.This study investigates how human‐centered artificial intelligence (HAI) can help learners in a learning environment. In this regard, this article by developing key considerations of HAI in helping students tries to help implement or shift it in the future in learning environments.To better understand the key considerations of HAI, qualitative methods and interview techniques were applied in this study. In this regard, 18 samples were interviewed from two groups of experts and faculty members in the fields of technology and computer science and social and humanities sciences. The thematic content analysis method was used to analyse qualitative data.The results show that AI attempts to integrate ethical and human values in the process of design, development, and research in the fields of recognising and dealing with negative emotions, targeted emotional nature, and access to fairness and justice. It also shows significant promise in understanding feelings and emotions in a learning environment.Although AI has been studied in other contexts, HAI has not attracted much attention from researchers. Hence, this study has made worthwhile contributions to the literature as it has specifically focused on HAI in education. In addition, it can resolve some scientific community considerations regarding technological concerns in the field of AI. Furthermore, this article can increase social satisfaction with the use of AI by considering ethical considerations in the learning environment and can particularly benefit researchers, educators, and AI specialists who are involved in the study of HAI applications.",https://www.semanticscholar.org/paper/5d3617d41d9074b85dace7a12bae529c084d8ebd,CS,Qualitative
Investigating the impact of introducing pair programming to primary computing education on female pupils' attitudes towards computing,"Gender balance in computing education is a decades-old issue that has been the focus of much previous research. In K-12, the introduction of mandatory computing education goes some way to giving all learners the opportunity to engage with computing throughout school, but a gender imbalance still persists when computer science becomes an elective subject. The study described in this paper investigates whether introducing pair programming would make a difference to primary-aged girls’ attitudes to computing and intent to study the subject in the future. A randomised controlled trial (RCT) was designed and implemented around a 12-week intervention with 785 female pupils between the ages of 8 and 10 years, alongside a qualitative evaluation investigating teachers’ and pupils' experience of the interventions and the development of materials and teacher preparation resources. The results of the RCT showed no statistically significant changes in student attitudes towards computing or intent to study further, although the qualitative data indicated that both teachers and pupils found the interventions engaging and enjoyable. Themes emerging from the qualitative data point to the importance of collaboration in supporting a development in pupil confidence. Overall, these results emphasise the societal and systemic barriers around computer science and technology engagement across genders that persist despite many initiatives being implemented over many years.",https://www.semanticscholar.org/paper/a02c0e3fcf941863ab416416b13bd60d598274fd,CS,Qualitative
MATHEMATICAL METHODS IN CYBER SECURITY: CLUSTER ANALYSIS AND ITS APPLICATION IN INFORMATION AND CYBERNETIC SECURITY,"The huge number of information threats and their complexity prompts research and modeling of new methodologies and information protection systems. The development and improvement of information and cyber security systems includes the creation and processing of mathematical models using information technologies. This article is a follow-up study on the application of mathematical methods and technologies in cyber security, namely: methods of cluster analysis. The modern development of computer technology and the growth of their power have contributed to the wide implementation of Data Mining algorithms for processing large volumes of information in various fields of society and science, in particular in the field of cyber security. Cluster analysis allows the set to be divided into subsets, so that the elements of each subset are similar to each other, and the elements of different subsets are the most different. This provides an opportunity to eliminate the shortcomings of the qualitative approach in assessing information risks. The paper reviews scientific sources regarding the applied aspect of the application of clustering methods in security systems, because timely forecasting of possible incidents allows you to manage information risks and make effective decisions to ensure confidentiality, availability and integrity of information. The stages of the clustering procedure are characterized, the issues of choosing the distance measure and the similarity measure for the objects under study are highlighted. The comparative characteristics of the most popular methods of cluster analysis are presented: the “nearest neighbor” algorithm, “k-means”, “fuzzy c-means”, “cosine similarity”, their advantages and disadvantages are defined. This study can be useful and used in the educational process of students of the specialty 125 “Cyber security and information protection”.",https://www.semanticscholar.org/paper/b0cf791c5c3b0f28efddd307a440ce54d71d307c,CS,Qualitative
Exploring Group Dynamics in a Group-Structured Computing Undergraduate Research Experience,"While the computer science community has explored the importance of Undergraduate Research Experiences (UREs) and, separately, collaboration in computing (e.g. pair programming), little research has studied collaboration in the context of a URE. We performed a qualitative thematic analysis of how students collaborate within a group-structured, academic-year, inclusive computing URE catered towards second-year students at two large public research universities in the United States. We analyzed free-response and Likert-scale survey data collected early and late in the program from a total of 106 students who comprised three program cohorts. We studied their overall group function, what aspects of group work led to positive or negative group experiences, how their group affected their feelings of being supported, and how their group affected their sense of belonging in computing. We found that group experiences were overwhelmingly positive. Further, we found that students’ experiences in groups centered around three themes: group fit and belonging, emotional and academic support, and logistics. Within each theme, their experiences were rich and nuanced, and we observed variations by gender, and to a lesser degree by race. Our work suggests that group-structured UREs are both feasible and beneficial for students, and we give concrete suggestions for how to make these experiences successful.",https://www.semanticscholar.org/paper/a192b587b69e1e2f3ae36e510779f79d01ac7625,CS,Qualitative
Supporting Guided Inquiry with Cooperative Learning in Computer Organization,"The computer organization course must help students acquire difficult conceptual knowledge and design skills, and improve their teamwork skills for subsequent project courses. Prior research supports that cooperative learning, in which students work together to achieve common goals, may address these challenges. We studied whether increasing the amount of guided inquiry activities and the cooperative support for it (teams and reflection) in an intermediate computer science course would improve achievement and engagement. The intervention group had lower scores on one of two achievement measures, lower engagement, and lower task value of collaborative activities. Qualitative analysis showed that students valued hands-on learning yet resisted guided inquiry, suggesting that sharing the purpose of each type of activity is important. Furthermore, the results showed that students valued learning with peers but were frustrated by group dysfunction, suggesting that instructors must address teamwork comprehensively to realize the benefits of cooperative learning.",https://www.semanticscholar.org/paper/e53640a356ad9837eb5aa27faf89b667d6f78d6d,IS,Qualitative
Idealized Influence and Employee Performance in Water Works Development Agencies in Kenya,"This study sought to examine the effect of idealized influence on employee performance in water works development agencies (WWDA) in Kenya. It was guided by Transformational Leadership Theory. A descriptive research design and positivist research paradigm was adopted. The population of this study was the 7 Water Works Development Agencies (WWDAs) while the unit of observation was 450 staff in the organizations. Krejcie and Morgan were used to determine the study sample size of 207 respondents. This study employed stratified random sampling in selecting the sample. The study used primary data collected using a semi structured questionnaire. Data analysis was done using Statistical Package for Social Sciences (SPSS) computer software version 26. Qualitative data collected was analyzed using content analysis and presented in prose form. Quantitative data was analyzed using descriptive and inferential statistics. Descriptive statistics included mean, standard deviation, frequencies, and percentages. The study also computed correlation analysis to measure strength and the direction of linear relationship between variables. Multiple regression models were fitted to the data to determine how the predictor/independent variables affect the response/dependent variable. The study further used hierarchical multiple regression models to measure the moderating effect of legal framework. The findings were presented in tables and figures. The study found that idealized influence has a positive and significant relationship with employee performance in WWDA in Kenya. On moderator, the study found that legal framework has positive significant moderating effect on the relationship between idealized influence and the employee performance in WWDA in Kenya. Based on the findings of this study, it is recommended that WWDA should enhance idealized influence among their leaders to improve their performance. This can be achieved by developing a leadership style that exhibits strong ethical and moral values, and by consistently practicing what they preach. Leaders should also be seen as role models, which can inspire their subordinates to emulate their behavior. Additionally, there is a need to establish effective legal frameworks that ensure transparency, fairness, and accountability in the management of public infrastructure projects, particularly those related to water service delivery. Further studies should explore the impact of other factors such as technology, funding, and political will influence employee performance in WWDA in Kenya.",https://www.semanticscholar.org/paper/4e0dd62ddd454cb7ee59ce2fb0e386f6cfa08f99,CS,Qualitative
"Principals’ Human Resource Management Skills and Institutional Performance in Public Secondary Schools in South Rift Region, Kenya","The study sought to establish the relationship between the Principals’ human resource management skills and institutional performance in public secondary schools in the south Rift region, Kenya. It adopted descriptive cross sectional survey research design and correlational research design. Multistage sampling was used to select 166 Principals, 166 senior teachers and 4 County Quality Assurance and Standards Officers. Questionnaires and interview schedules were used to collect data from sampled respondents. Data was collected from 163 Principals, 164 Senior Teachers and 4 CQASOs giving the study a return rate of 98.51%. Qualitative data obtained was analyzed thematically while quantitative data was cleaned coded and analyzed descriptively and inferentially with the aid of the Statistical Packages for Social Sciences (SPSS) version 27.0 computer software. Frequency and percentages were used to describe the existing relationship between the variables while hypothesis was tested using regression analysis at 0.05 level of significance. The study established that there is a positive and significant relationship between Principals’ human resource management skills and institutional performance. It also established that some Principals are faced with a lot of challenges relating to human resources management culminating into poor institutional performance. It is therefore recommended that the Ministry of Education (MOE) in collaboration with Teachers Service Commission (TSC) should ensure that Principals are trained adequately in human resource management prior to their deployment and periodically in-serviced to enable them effectively perform their human resource management functions.",https://www.semanticscholar.org/paper/806f620898a6d3ddc44996dc9a8dbb21193ef326,CS,Qualitative
"PRINCIPALS’ INSTRUCTIONAL LEADERSHIP SKILLS AND INSTITUTIONAL PERFORMANCE IN PUBLIC SECONDARY SCHOOLS IN SOUTH RIFT REGION, KENYA","The study sought to establish the relationship between the Principals’ instructional leadership skills and institutional performance in public secondary schools in the South Rift region, Kenya. The study adopted descriptive cross sectional survey research design and correlational research design. Multistage sampling was used to select 166 Principals, 166 Senior teachers and 4 County Quality Assurance and Standards Officers. Questionnaires and interview guide were used to collect data from sampled respondents. Data was collected from 163 Principals, 164 Senior Teachers and 4 CQASOs, giving the study a return rate of 98.51%. Qualitative data obtained was analyzed thematically while quantitative data was cleaned, coded and analyzed descriptively and inferentially with the aid of the Statistical Packages for Social Sciences (SPSS) version 27.0 computer software. Frequencies and percentages were used to describe the existing relationship between the variables while hypothesis was tested using regression analysis at 0.05 level of significance. The study established that there is a positive and significant relationship between Principals’ instructional leadership skills and institutional performance. It also established that majority of Principals lacked requisite skills in monitoring and evaluation framework and practice. It therefore recommended that institutions such as Kenya Education Management Institute (KEMI) which is charged with the responsibility of capacity building education managers of learning institutions should restructure their training curriculum to ensure a more concerted effort on instructional leadership.",https://www.semanticscholar.org/paper/e29d6ce673c65ef51cbbedfb995816870085c80a,CS,Qualitative
Retia: Cloud-Based Network Management System with Automated DDOS Mitigation Based on Dynamic Thresholding Algorithm,"Organizations use a Network Management System (NMS) to maintain the reliability and integrity of data transmission in their infrastructure, but it lacks network automation features, requiring manual configuration of network devices. The growth of computer networks also increases the risk of DDoS attacks. Previous research has identified a dynamic thresholding algorithm as a potential solution to this issue. However, the algorithm is only a model and has not yet been implemented in practice. This study aims to develop a comprehensive system called Retia that combines network monitoring, device automation, and DDoS mitigation. Design-Science Research used as the research methodology. This research uses qualitative data analysis techniques to determine the level of usefulness and improvement on previous research, namely network automation with REST API using Django, DDoS detection system using dynamic thresholding algorithm, and DDoS mitigation system using dynamic access control list. The objective of this research is a prototype to offer network administrators a centralized and streamlined approach to network management. This research successfully implements the Retia System with monitoring, logging, alerting, device configuration, and integration with the DDoS mitigation system.",https://www.semanticscholar.org/paper/0334ee37e3303a937e001ff79037fab8aa67102b,IT,Qualitative
Internet: perspectives from the legal standpoint as a citizen's right in Ecuador,"The research focused on analyzing the right to internet access as a human right in the Republic of Ecuador. A mixed-methodological approach was employed, integrating both quantitative and qualitative sources of information. Different methods including inductive, analytical, and juridical descriptive approaches were used to address the legal issue at hand. This study is classified as descriptive and falls under pure, doctrinal, and juridical descriptive research, with a non-experimental design. The target population included constitutional judges, entrepreneurs, and educators specializing in informatics and computer science in Ecuador. A structured questionnaire consisting of seven closed-ended questions was administered to gather relevant data from this population. The processing and analysis of the collected information were conducted using mathematical, computational, and logical techniques, allowing for an examination of the current state of internet rights in the Ecuadorian context. The research findings highlighted the challenges and opportunities related to ensuring universal internet access as a fundamental right in the country",https://www.semanticscholar.org/paper/99c37e3a56ff2a4f9efae4de48f78f0e1a32d915,IS,Qualitative
Asynchronous Learning: Its Effects on Academic Performance and Students’ Motivation In Science,"The main purpose of the study is to determine the effect of asynchronous learning to academic performance and students’ motivation towards Science. It is necessary since motivation serves as a foundation for better understanding of science concepts in an asynchronous setting. The respondents are 201 grade 11 students of Iligan Computer Institute (ICI) who are enrolled during the Academic Year 2020-2021. The ADDIE (Analysis, Design, Development, Implementation, and Evaluation) model was used in the study for the development of asynchronous lesson on Genetic Engineering with Google Classroom as the online platform. The research utilized SMTSL (Students Motivation Towards Science Learning) questionnaire developed by Tuan, et al. (2005) composed of 35 questions encompassing six (6) domains of motivation namely, self-efficacy, active learning strategies, science learning value, performance goal, achievement goal and learning environment stimulation. Quantitative data together with qualitative analysis showed that students are moderately motivated towards science learning in an asynchronous environment. The highest identified domain is science learning value indicating that students find the relevance of science with daily life which makes them motivated to learn science even in a remote learning environment where there is no direct contact and supervision. In contrast, self-efficacy and performance goal are the least domains suggesting that students are least concerned with their own ability to perform well in science learning tasks and they do not compete with other students and get attention from the teacher. Using the researcher-made achievement test and performance task to measure their academic performance, 80% of the students belong to approaching proficiency, proficient and advanced groups showing that students did well in science in an asynchronous learning environment. Moreover, students’ motivation was significantly correlated with science academic performance so science academic performance can be used as indirect evidence of students’ motivation. Based on these findings, it is recommended that curriculum experts and educators consider factors of motivation in revisiting the science curriculum in an asynchronous environment.",https://www.semanticscholar.org/paper/fb33932819af4929dc972aa57a4d6c4a4678c4ee,IS,Qualitative
Exploring the use of virtual reality by pre-service elementary teachers for teaching science in the elementary classroom,"Abstract This research was conducted on two groups of pre-service elementary teachers (PSETs) (N = 38) in a pre/post within-subjects design. Participants were asked to critique, create, and evaluate the use of virtual reality (VR) classroom applications during a three-stage intervention. Pre/post questionnaires assessed the change in attitudes toward using VR and technology when teaching. Participants felt significantly more ready to teach science using computer-based technology (t = 7.23, p < .0001) following the intervention. Qualitative analysis of responses showed that PSETs were positive about their VR experience and had come to see VR apps as supplementary educational tools. The majority of the PSET teams successfully created original instructional material using VR, increasing their self-efficacy.",https://www.semanticscholar.org/paper/7eda588f0613daf16e513159853f0220f709ce29,CS,Qualitative
Text classification in tourism and hospitality – a deep learning perspective,"Purpose This study aims to investigate the current state of research using deep learning methods for text classification in the tourism and hospitality field and to propose specific guidelines for future research. Design/methodology/approach This study undertakes a qualitative and critical review of studies that use deep learning methods for text classification in research fields of tourism and hospitality and computer science. The data was collected from the Web of Science database and included studies published until February 2022. Findings Findings show that current research has mainly focused on text feature classification, text rating classification and text sentiment classification. Most of the deep learning methods used are relatively old, proposed in the 20th century, including feed-forward neural networks and artificial neural networks, among others. Deep learning algorithms proposed in recent years in the field of computer science with better classification performance have not been introduced to tourism and hospitality for large-scale dissemination and use. In addition, most of the data the studies used were from publicly available rating data sets; only two studies manually annotated data collected from online tourism websites. Practical implications The applications of deep learning algorithms and data in the tourism and hospitality field are discussed, laying the foundation for future text mining research. The findings also hold implications for managers regarding the use of deep learning in tourism and hospitality. Researchers and practitioners can use methodological frameworks and recommendations proposed in this study to perform more effective classifications such as for quality assessment or service feature extraction purposes. Originality/value The paper provides an integrative review of research in text classification using deep learning methods in the tourism and hospitality field, points out newer deep learning methods that are suitable for classification and identifies how to develop different annotated data sets applicable to the field. Furthermore, foundations and directions for future text classification research are set.",https://www.semanticscholar.org/paper/d51da7b552a342cf23b6e21058dcd6cd2f325172,CS,Qualitative
Secondary science teachers’ implementation of a curricular intervention when teaching with global climate models,"Abstract In the past decade, emphasis on promoting “climate literacy” in K-16 science classrooms has increased. Teachers play a critical role in cultivating these opportunities, especially in secondary science classrooms. However, most prior climate education research has focused on students and student learning; little is known about how teachers implement climate-focused curricular interventions. Here, we report findings from a concurrent mixed methods, multiple-case study of four secondary science teachers’ implementation of a new, NGSS-aligned, model-centric climate curriculum module grounded in the use of a data-driven, computer-based climate modeling tool—Easy Global Climate Model (EzGCM). We employ multiple data sources, including video-recorded classroom observations, interviews, and instructional artifacts, and both qualitative and quantitative analyses, to investigate how teachers implemented the curriculum. Findings show that, overall, teachers implemented the curriculum in ways that were less model-centric than designed, placing greater emphasis on EzGCM itself rather than using the model to investigate Earth’s changing climate. Additionally, we present detailed single-case studies of each participant teacher that highlight differences in teachers’ implementation of the curriculum module and their reasoning for making observed instructional decisions. This research sheds light on the design of secondary science learning environments by illustrating the varied ways teachers implement a climate-focused curriculum to support students’ developing climate literacy. This has important implications for the design of climate-focused curriculum and supports for teachers.",https://www.semanticscholar.org/paper/ecd0e38b618e2cced23431915041db65ba5212ad,IS,Qualitative
Building upon the CAPE Framework for Broader Understanding of Capacity in K-12 CS Education,"Research Problem. The CAPE Framework has been used in multiple studies to situate capacity-building efforts within schools to offer equitable student access to and participation in K-12 computer science (CS) education. CAPE defines four major components of capacity, access, participation and experience. However, to define what each of the CAPE components can entail, well-defined subcomponents are needed. Research Question. Our research questions for this study were: What are the possible subcomponents for Capacity in the CAPE framework? and How feasible is it to use the newly defined subcomponents in a gap analysis study? Methodology. We conducted a qualitative content analysis by creating a codebook from an existing data framework and literature review. We reframed earlier findings on factors that influence student learning and academic achievement into the CAPE. Findings. We vetted an expanded framework that includes eight Capacity subcomponents, a third (categories) layer and a fourth (subcategories) layer that can be used to disaggregate the many elements that comprise Capacity. For our trial analysis of 196 articles, we added several codes at the category and subcategory level, but found no gaps in the codes for our a priori defined subcomponents. Implications. The extended Capacity framework can be used by others to inform its usage and develop a consensus of what is included within each subcomponent for Capacity, develop instrumentation and protocols for exploring Capacity at a more granular level, conduct scoping and literature reviews, and understand how various variables play a part in the CS educational ecosystem.",https://www.semanticscholar.org/paper/a99ba89e8d1bf92e6bbd7d94ae0f996bbd306c91,CS,Qualitative
Navigating a male dominated domain: experiences of female STEM students in higher education in Ireland,"ABSTRACT It is well established that female participation in STEM diminishes at all stages of the education pipeline. National policy in Ireland is focussed on initiatives to address the STEM gender gap from early years to the end of secondary school (age 18/19) education. However, strategy in higher education is limited, with gender equality policy primarily aimed at staff and the broader institution. This qualitative research study, involving in-depth interviews with 21 female STEM students provides new insights into the experiences of female students who choose mathematics-intensive STEM fields (physics, computer science, engineering, and mathematics), where the gender gap is most pronounced. The aim of the study was to identify how a predominately male-dominated STEM course and environment influenced female students’ experiences. Prior to entering university, participants held high self-concept and interest in STEM. Using a situated expectancy-value theoretical lens to interpret the data, the research found that unconscious gender bias in university led female students to feel undervalued by their male peers. This negatively impacted their self-beliefs and interest, resulting in female students feeling more pressure to perform and less willing to participate in the learning environment. The implications for policy, practice, and future research are considered.",https://www.semanticscholar.org/paper/42a9e0b8b5c393ac8c0e6f81a1d4f94f1b42938d,IS,Qualitative
Informal digital learning of English (IDLE): a scoping review of what has been done and a look towards what is to come,"Abstract As technology has advanced, so have opportunities for language socialization and practice. This reciprocal relationship has resulted in the emergence of a subfield of Computer Assisted Language Learning (CALL): Informal Digital Learning of English (IDLE). IDLE has manifested in various forms, including the more notable extramural and extracurricular varieties. Given the recent attention given to IDLE by Applied Linguists and language educators, this scoping review provides a roadmap for future research and explores the potential of IDLE to support English language teaching and learning in informal digital contexts. A Web of Science core eight database search for relevant research published between 1980 and 2019 using 35 IDLE-related key terms resulted in 38 studies of which 30 aligned with the inclusion criteria. Results showed the studies were conducted mostly under a mixed-method and qualitative paradigm and were published between 2017 and 2019. Only two studies used longitudinal data collection methods. Topics investigated included the linguistic dimension of CALL, the affective and cultural dimension of CALL, and the agency and digital literacies dimension of CALL. The small, yet salient, body of emergent IDLE literature points towards three trends: a growing relevance of langua-technocultural competence, the importance of digital literacies to communicative competence, and the importance of non-professional translation and interpreting to digital language learning.",https://www.semanticscholar.org/paper/41428f7882151f7b6f3fb8af71bd96df46b1c3fd,IS,Qualitative
Co-Creation in Secure Software Development: Applied Ethnography and the Interface of Software and Development,"Long-term ethnographic research conducted at a software company examined how security concerns and practices became part of software development. Participant observation over a two-year period was done by researchers with cybersecurity backgrounds and training in both computer science and qualitative research, with ongoing analysis done by a larger interdisciplinary team. In situ researchers joined as software engineers and participated in daily work activities while observing development practices and analyzing software (in)security. The first year of research found that improving security during software development can be helped by a co-creation model, whereby security experts work directly with software developers to provide security tools applicable to the specific software within the workflow. Researchers-as-developers fostered conversations, concerns, and considerations of how to implement security within the process of development. The second year used a situated learning approach to understand the interface between software development, security, and the development team. Through an interactive learning process, software engineers gathered knowledge and applied it, helping to foster greater concerns for security as part of the overall “culture” of development within the company. This locally situated co-creation approach has resonances with participatory approaches in business anthropology and implications for how to promote the co-creation of knowledge and expertise more broadly.",https://www.semanticscholar.org/paper/41bc47caa2ec1c92ae29495f625b4c332db6bcd9,CS,Qualitative
Mapping urban tourism issues: analysis of research perspectives through the lens of network visualization,"Purpose The purpose of this study is to identify research perspectives/clusters in the field of urban tourism (city tourism) in narrow sense and tourism cities (cities and tourism) in the broader sense to examine the complex relationship through the optics of science mapping. This paper believes that the existing qualitative assessments of this field can be experimentally verified and visualized. Design/methodology/approach First, the key conceptual dilemmas of research perspectives in urban tourism are highlighted. Based on the Web of Science (WOS) Core Collection and the VOSviewer (computer program for visualizing bibliometric networks), the data will be analyzed. Clustering is used to evaluate information retrieval (inclusivity or selectivity of the search query), publication patterns (journal articles), author keywords, terminology and to identify the respective cities and author collaborations between countries. Findings Terminological specificities and their contextuality (authors’ preferences) are elaborated, as the topic is studied by authors from different disciplinary fields. Compared to other specific tourisms, urban tourism includes geographic terms (variations of city names) and terms with different connotations (travelers, visitors). Recent Spanish (also Portuguese) linguistic/geographic contexts are noticeable and a strong presence of WOS Emerging Sources Citation Index papers. Research perspectives are represented in the network of clusters of connected terms. If the search is based on a narrower sense of strict urban tourism, then tourism-business topics predominate. If tourism and cities are less closely linked, socio-cultural and environmental-spatial perspectives emerge, as does tourism/cities vulnerability (climate change and health issues). Research limitations/implications The construction of a search syntax for the purpose of retrieval is always marked by compromises, given different terminological usages. A narrow search query will miss many relevant documents. On the other hand, if the query is too general, it returns less relevant documents. To this end, this paper tested queries on three different levels of inclusivity or selectivity. More consistent use of terms would benefit authors in the field of urban tourism when searching for references (information retrieval) and, as a consequence, would allow better integration of the field. Practical implications This study provides a practical method for evaluating cities and tourism in combining the expertise of an information scientist and a sociologist. It points out numerous caveats in information retrieval. It offers an overview of publishing just prior to the outbreak of Covid-19, thus providing an opportunity for further comparative studies. Originality/value This study is the first to examine urban tourism using such a method and can serve as a complement to the existing systematization of qualitative approaches. The findings are consistent with numerous qualitative assessments of weak the research interconnection between the specifics of cities and tourism in terms of broader socio-spatial processes. However, the study suggests that such research linkage is increasing, which is noticeable in relation to issues of social sustainability (e.g. overtourism, Airbnb and touristification).",https://www.semanticscholar.org/paper/bf36d397123f0b85378c24ca93ea4c8012f5f04c,CS,Qualitative
Effects of a Programming Course Using the GAME Model on Undergraduates’ Self-Efficacy and Basic Programming Concepts,"In higher education, it is challenging to cultivate non-computer science majors’ programming concepts. This study used the GAME model (gamification, assessment, modeling, and enquiry) in a programming education course to enhance undergraduates’ self-efficacy and performance of basic programming concepts. There were 83 undergraduates taking part in this study, which adopted a quasi-experimental research design. Students in the experimental group (n = 43) experienced a course in which the GAME model was used to design the block-based programming course. The control group (n = 40) was given a general information education course covering similar learning concepts without the game-based learning strategy. The analysis of covariance (ANCOVA) was adopted to investigate the effect of the GAME model on students’ learning outcomes for the quantitative data. In the qualitative analysis, students’ responses to the course perception questionnaire were coded and analyzed. The results showed that students in the experimental group outperformed their counterparts regarding self-efficacy and basic programming concepts. The experimental treatment resulted in a small to medium effect size difference between the two groups. The results showed that incorporating the GAME model into block-based programming teaching helped improve undergraduates’ self-efficacy and performance of basic programming concepts. In addition, these experimental group undergraduates also perceived the pedagogic GAME model positively. Several research suggestions are proposed based on the findings of the present study.",https://www.semanticscholar.org/paper/4ee56282cf8879b3193d2a1afedc4fb56234bce7,IS,Qualitative
Distance Education Experiences of Teacher-Parents during the COVID-19,"This research aimed to determine the opinions of teacher parents about distance education process during COVID-19. The study was designed as a case study which is one of the methods in qualitative research. The sample of the study composed of 83 teacher parents from different branches in Turkey. Maximum variation and criterion sampling methods were used to select the participants. The data of the study were gathered though open ended questions developed by the researchers and were analyzed through descriptive and content analyses. According to the participants, distance education is perceived and accepted as a means of support rather than an alternative to face-to-face education. Participants mentioned the distance education carried out during the pandemic process as a beneficial practice in order to prevent students from breaking off from education but also they stated administrative issues, lack of computer science knowledge and internet-related problems. The inability to disseminate distance education to all students, especially disadvantaged students due to the lack of infrastructure, indifferent parents and the fact that distance education is insufficient in the education of young children/special education students show that distance education has not yet met the expectations to provide equal opportunities for everyone involved. Keywords: COVID-19, distance education, pandemic, teacher, parent",https://www.semanticscholar.org/paper/650552ed262ee8b4d90d79b6bee04a67e84d6443,IT,Qualitative
Ready Coder One: Collaborative Game Design-Based Learning on Gifted Fourth Graders’ 21st Century Skills,"The purpose of this research was to describe the impact of digital game building on fourth grade gifted and talented students’ problem-solving, creativity, and collaboration skills. Increasingly, there has been a call to involve students in real-world experiences through projects that explore authentic issues using technology. Game design-based learning with its unique set of affordances may offer a path to integrating technology, computer science education, creativity, and problem-solving. Increasingly, the ability to create rather than just consume technology has gained attention linking creativity and collaboration to using coding language. In this study, data collection included student reflection journals, classroom observations, classroom video recordings, a focus group interview, and students’ games. Participants came from two GT classes (n = 45). Qualitative analysis identified five themes: overcoming challenges of group work, developing a culture of collaboration, creating narrative, and connecting science, problem-solving in Scratch’s coding environment, and reflecting on learning. Findings indicated involving gifted students in game design-based learning in science had a positive impact on student perceptions of problem-solving, creativity, and collaboration.",https://www.semanticscholar.org/paper/989ba2e230050297f2c69a458e1915ac3b3adcd5,IT,Qualitative
The Phenomena of The Cryptocurrency Fall in The Sharia Economic View,"Digital money was first developed in the 1960s when Western Union introduced electronic funds transfers (EFT). In Indonesia, digital money only became popular around 2007. Electronic money regulations are contained in Bank Indonesia Regulation Number 11/12/PBI/2009. Cryptocurrencies are made from a combination of blockchain technology and cryptography. Blockchain is a technology for recording interconnected transactions using unique and immutable codes. On the other hand, Cryptography is a branch of computer science that studies how to hide information. This research aims to obtain answers regarding cryptocurrencies as currency/method of exchange/assets/commodities in Indonesia. The theory applied is the theory of legal and vanity business transactions in Islam. This research is a literature study and is qualitative in nature. The data analysis technique used is analytical descriptive with a normative juridical Islamic law approach. From this research, the results show that cryptocurrencies are haram, referring to the MUI Fatwa, the decisions of the Laznah Bahtsul Masail East Java Nahdathul Ulama Regional Management (PWNU), the Tarjih Council and the Tajdin Central Executive of Muhammadiyah, the opinion of Islamic economics academics. The factors that make cryptocurrencies illegal because they are considered to contain speculation, maysir and are vulnerable to being used for illegal activities. The weaknesses of cryptocurrency are marked by the fall of crypto throughout 2022 and are predicted to continue in 2023. Crypto is illegal lighairihi or haram due to external factors, so it should be avoided.",https://www.semanticscholar.org/paper/5e7ff4ad21342f597716292dfbc1e705ba5b826b,CS,Qualitative
The Role of Mentoring in a Dual-Mentored Scalable CS Research Program,"Despite the documented importance of mentoring in undergraduate research, few studies examine how students---especially early undergraduates in computing---perceive their relationships with their mentors. We present a qualitative thematic analysis of the mentoring practices used in an inclusive, structured computer science research program targeting second-year undergraduates across two large public research universities in the United States. Uniquely in this program, students had two mentoring sources: a technical mentor for each research group and a graduate student mentor common to all groups. We analyzed reflections on mentoring from 64 undergraduate researchers at two points in the program. We compared the roles of the two mentors, characterized students' perceptions of both successful and unsuccessful mentors, and examined how mentoring relationships evolved. Generally, students valued mentors who provided project guidance or technical support and who were perceived to be friendly. We found that the roles of the two mentors were complementary in sometimes surprising ways. Overall, our analysis confirms prior work on undergraduate research mentoring, and provides new insights into the unique benefits of a dual-mentoring approach and how to best support early undergraduate computing researchers.",https://www.semanticscholar.org/paper/deec49859dc9879d6d7007ad8bec661524388a51,IS,Qualitative
Designing Emerging Technology-Supported Learning Activities Based on the DT Approach for K–12 Users,"Design Thinking (DT) paired with Emerging Technologies (ET) holds exciting potential as a pedagogical innovation for creatively approaching STEAM subjects, especially Computer Science (CS). By leveraging these tools and techniques, educators can develop engaging and effective learning experiences that help foster 21st Century Skills, digital literacy in particular. To tackle the overall research statement, “How can we design DT-based K–12 learning activities for CS education that are supported by ET?”, a first Systematic Literature Review is conducted to gain a solid understanding of the current role of DT and ET in CS learning venues. Subsequently, Design-Based Research methodology with both qualitative and quantitative data gathering is proposed, along with the use of Learning Analytics and multi-modal data to trace knowledge experience and improve its personalization. The research process involves close collaboration with target users, K–12 students and teachers, as participants in the studies. A qualitative data collection effort involving semi-structured interviews with teachers is reported as a first step. On-site DT sessions engaging K–12 students, and teachers will follow. Such sessions employ a game-based approach to coding and encourage the building of educational activities. The resulting insights will be used to inform the further refinement of activities, based on the expectations and demands of stakeholders. The final aim of the PhD research is to deliver guidelines for developing ET-supported, future-proof solutions for CS learning activities. By leveraging these activities’ potential impact on interdisciplinary pedagogical reforms and 21st Century Skills knowledge, the research wants to promote innovation and advancement in the CCI field for young learners and their educators.",https://www.semanticscholar.org/paper/e9ca402fe498b2c2d3880bb728fa2e43d950a877,IS,Qualitative
CS=Me: Exploring Factors that Shape Black Women's CS Identity at the Intersections of Race and Gender,"Improving equity and inclusion for underrepresented groups in the field of Computer Science (CS) has garnered much attention. In particular, there is a long-standing need for diversity efforts that center on the experiences of Black women, and specific actions to increase their representation—especially given the biases that they often encounter in the field. There is limited research concerning Black women in CS, specifically their conceptions of the field and their overarching CS identity development. More research in this area is especially important given the marginalization that Black women often experience at the intersections of their race and gender. Guided by a combination of critical theoretical lenses, this qualitative study examines Black women's conceptions of what it means to be a computer scientist and the degree to which those conceptions map onto how they see themselves in the field. Moreover, we explore experiences that help to bolster Black women's CS identity. The findings highlight key aspects of what it means to be a computer scientist for the Black women in this study—notably the ability to use computing to make societal contributions. Also, the results accentuate key nuances in the participants’ personal CS identification, particularly as it relates to the resilience required to overcome unique barriers that many Black women encounter when engaging within the field. Moreover, the findings highlight the importance of social support systems to facilitate Black women's CS identity development. Implications for policy and practice within education and industry are discussed.",https://www.semanticscholar.org/paper/8155213ce2bc9c2a5d10908608b213012f481ab3,IS,Qualitative
New Approaches to Simulation-Based Science Instruction to Enhance Reasoning and Communication Skills in Lebanese Elementary Education,"Purpose: The aim of this research is to investigate the effect of a self-instruction intervention received by teachers on the reasoning and communication skills of sixth and fifth graders. It comprised training elementary science teachers to integrate the Five-E Inquiry-based-computer-simulations. Approach/Methodology/Design: A mixed-methods quasi-experimental design was employed in this study to investigate the effect of the intervention on students’ reasoning and communication skills from the results of tests, interviews, and observations. The sample included five private schools in the region of Beirut and Mount Lebanon. Participants included 434 students and 10science teachers. Qualitative data were collected from the interview replies of ten science teachers, and 22 pre-post observations. Quantitative data were collected from the pre-post-test results in reasoning and communication over a sample of 434 students from grades five and six. Findings: The integration of the Five-E Inquiry Model-Computer-Simulation-approach enhanced students’ reasoning skills particularly at the levels of “Analysis”, “Interpretation” and “Conclusion”. Post-test results indicated a significant improvement in students’ reasoning and communication skills. Teachers tackled written communication skills more through writing laboratory reports, and the post-test results for written communication showed significant improvement. Therefore, there was a statistically significant difference between post-test scores of the experimental schools and those of the control schools. Practical Implications: The study will contribute to the proper integration of computer simulations in an inquiry-based learning environment to enhance elementary students’ reasoning and communication skills in science. Originality/value: This study responded to students’ lack of reasoning and communication skills and teachers’ lack of technological pedagogical knowledge. The study provided instructional strategies for the proper integration of computer simulations based on the Five-E instructional model that enhanced students’ reasoning and communication skills.",https://www.semanticscholar.org/paper/392da811eb0b3c566ba449bd58ee2ecadb7c5179,IS,Qualitative
A systematic review of the literature in nature on human-computer interaction: Preliminary results,"For the past three decades, computers have been dominating the way many people create, manage, and use information. Subsequently, Human-Computer Interaction (HCI) became an essential area of Information Studies. Due to its interdisciplinary nature, HCI has been defined loosely by what its constituting fields, such as Information Studies, Computer Science and Psychology, perceive as HCI research. A broader view of HCI remains unclear. One way to formulate such a broad view is to examine how scientific journals that represent a wide range of disciplines portray HCI. One comprehensive and prestigious scientific community that mentions the term “human-computer-interaction” as a field of study in its published body of articles is the Nature publications and journals. Through multiple rounds of screening, we identified 53 relevant publications across the Nature database and analyzed these articles using the Qualitative Analysis of Content method. The preliminary results show an exponential increase in the use of the term “human-computerinteraction” over the past six years in Nature publications. Our results also suggest that the scientific community represented in Nature views HCI as an independent field of research.",https://www.semanticscholar.org/paper/1d08899c3ae0736d70d758bd96bafd35ad75f543,CS,Qualitative
Co-design of mini games for learning computational thinking in an online environment,"Understanding the principles of computational thinking (CT), e.g., problem abstraction, decomposition, and recursion, is vital for computer science (CS) students. Unfortunately, these concepts can be difficult for novice students to understand. One way students can develop CT skills is to involve them in the design of an application to teach CT. This study focuses on co-designing mini games to support teaching and learning CT principles and concepts in an online environment. Online co-design (OCD) of mini games enhances students’ understanding of problem-solving through a rigorous process of designing contextual educational games to aid their own learning. Given the current COVID-19 pandemic, where face-to-face co-designing between researchers and stakeholders could be difficult, OCD is a suitable option. CS students in a Nigerian higher education institution were recruited to co-design mini games with researchers. Mixed research methods comprising qualitative and quantitative strategies were employed in this study. Findings show that the participants gained relevant knowledge, for example, how to (i) create game scenarios and game elements related to CT, (ii) connect contextual storyline to mini games, (iii) collaborate in a group to create contextual low-fidelity mini game prototypes, and (iv) peer review each other’s mini game concepts. In addition, students were motivated toward designing educational mini games in their future studies. This study also demonstrates how to conduct OCD with students, presents lesson learned, and provides recommendations based on the authors’ experience.",https://www.semanticscholar.org/paper/4fd6d81dff186d5a57bdf05f5a24e85f53323d97,IS,Qualitative
The Implementation of Educational Games as a Digital Learning Culture in Elementary School Learning,"Learning with technology content, including computer science or informatics, is a very crucial topic at this time. Unfortunately, there are still many schools that have not adapted to this digitalization culture. There are many factors behind schools not starting a digitalization culture, such as facilities and infrastructure and human resources that are not ready yet. One school that has not implemented digitalization in learning is SDN Kedungdalem. This school still does not have digital-based learning media, most of the learning activities are carried out using books and worksheets. This situation makes students not excited about learning because it is not interesting for students. The aim of this research is to analyze the description of the results of students' academic responses to ICT learning and educational games that have been given. The research method used in this activity is a qualitative approach with rural operations. The subjects of this research were 5th grade students at SDN Kedungdalem. The research results showed that students really liked the educational games provided because the games were funny, exciting and fun.",https://www.semanticscholar.org/paper/a39598b73a4e23e0b3b2ed4ad604a318988235ea,IT,Qualitative
Utilization of Canva as a Learning Aid in Informatics for Middle School Students,"This research explores the utilization of the Canva application as an instructional aid in computer science education at the junior high school level with the aim of identifying how Canva can be effectively used in the learning process and its impact on students' understanding and engagement in computer science subjects. The research method employed is descriptive qualitative with data collected through observation and questionnaires. The findings indicate that the use of Canva enhances student engagement and understanding of computer science concepts. Students responded positively to Canva, feeling comfortable and enthusiastic about using the application as a learning tool. Challenges in Canva implementation include the availability of technological devices and teacher training. This study recommends providing adequate training for teachers, integrating Canva into the curriculum, and improving technology infrastructure in schools.",https://www.semanticscholar.org/paper/89b8e515c1c1ee83dc80f521000a7b5dcdbc3de7,CS,Qualitative
NEED ANALYSIS FOR ENGLISH FOR SPECIAL PURPOSES (ESP) IN INFORMATICS MANAGEMENT CLASS AT ABC UNIVERSITY.,"The process of learning English with ESP (English for Specific Purposes) in higher education often runs less than optimally. One of the contributing factors is the lack of lesson planning so that learning takes place irrelevant to the field of knowledge they teach. For this reason, it is necessary to have a learning plan which will begin by analyzing student needs for learning English with special objectives. Therefore, the research aims to find and describe students' needs for ESP English. This needs to be done so that the English language learning materials with special purposes (ESP) received by students are in accordance with the field of knowledge they are capable of and the field of work they will be engaged in. This research is based on a qualitative descriptive approach. The object of this research is Informatics Management students class of 2020/2021 University of Computer Science and Technology. Data collection techniques used were questionnaires and direct interviews. Then the data obtained will be classified so that the needs of Informatics Management students for learning English can be identified, then they will be described. The results of this study will be used as a reference for subsequent research, namely syllabus planning and development of teaching materials for ESP English. Keywords: need analysis, ESP, English teaching.",https://www.semanticscholar.org/paper/c7724ca308a5d7cd4196d8873212c29a33a935be,CS,Qualitative
Mathematics Online Baseline Assessment: Senior Phase First-year Student Teachers’ Views,"Problem-solving and enquiry-based learning are integral in the Mathematics and Science curriculum in South Africa through online-based assessment. Online assessment has rapidly gained recognition because of technology. In this paper, the authors explored the views of online baseline assessment among Senior Phase First-year Mathematics Student Teachers. The researchers adopted a qualitative research method. Data was collected from the participants using an online Google form developed into a questionnaire. The participants in this study were first-year students enrolled for the Bachelor of Education in Mathematics at a university in a rural province of South Africa. The first-year student teachers were exposed to the senior phase baseline assessments through the licensed online Computer Aided Mathematics Instruction (CAMI) tool. One hundred and sixteen (116) senior-phase student teachers completed the online questionnaire. This study used convenience sampling since it was the most appropriate method to conveniently invite the participant. The findings revealed that first-year student teachers were enthusiastic and motivated to write the baseline assessment on computers for the first time. The result also showed that several first-year student teachers experienced difficulties solving mathematics problems using computers. This study recommends more intensive research on student teachers' views on online assessment regarding gender and time.",https://www.semanticscholar.org/paper/cb89efde6893e127ce9664d843f594d50e0436a3,IS,Qualitative
New complementary alternatives in third molar autotransplantation: A systematic review,"Background Dental autotransplantation (DAT) is defined as the replacement or direct transfer of an impacted, semi-impacted or erupted tooth to a donor site, either to a post-extraction socket or to a surgically created socket within the same individual. The use of new technological advances, such as 3-D dental models based on computer-aided design, among others, have been reported to improve the success rate of DAT. Therefore, we aimed to perform a systematic review to explore the possible benefits that the use of these innovative techniques can provide when applied to DAT. Material and Methods The literature search was conducted in PubMed, Scopus, and Web of Science databases following the PRISMA guidelines. The research question was: ""Are computerized technological advancements a useful tool for improving the success of third molar autotransplantation technique? Results The initial literature search identified 195 articles, of which only 11 were included for qualitative analysis. All studies used 3D dental models based on computer-aided design data. Surgical guides and stereolithographic models were used by 4 and 1 study respectively. A total of 91 transplanted teeth were evaluated, out of which only 88 were considered within the parameters of clinical success (96.7%). Only 7 out of the 11 articles reported the specific autotransplanted tooth, being mandibular third molars the most prevalent autotransplanted teeth. Conclusions Although the application of new technologies for DAT increases the success rate of this technique, further primary studies are still needed to address long-term teeth survival rates and complications. The cost and availability to implement the integration of these techniques to DAT may be a variable to consider, as this can be a limitation for some patients or for low-income countries. Key words:Autotransplantation, third molars, digital planning.",https://www.semanticscholar.org/paper/cdfd84a8810cd8930b82b6ab3899c7b6c9e0fb94,IT,Qualitative
ENHANCING SPEAKING SKILL THROUGH AI-POWERED TECHNOLOGY,". Artificial Intelligent (AI) is a field of computer science devoted to solve cognitive problems, commonly associated with human intelligence, such as learning, problem solving and pattern recognition. From education point of views, it helps teachers to facilitate learning process especially for speaking skill. The research method runs with mixed method (qualitative. And quantitative) The aim of this research is investigating the students’ speaking performance with artificial intelligence. The data collection is taken from interviews, photo, and literature studies. The result of this study is the increasing student’s speaking performance with AI.",https://www.semanticscholar.org/paper/c349767a33476b635f326c2f0695a74038f974f4,CS,Qualitative
A Method of Error Mode Effect Analysis for a Human-Computer Interaction System in Aviation,"With the development of science and technology, the automation degree and reliability of machine are getting higher and higher. On the one hand, more stringent requirements are put forward for human beings. On the other hand, human factors have also become an important factor affecting the reliability of human-machine systems, especially in the aviation field. Although there are many classification methods for error, the analysis methods are mostly from a qualitative or quantitative point view. Moreover, the research objects are mostly partial and do not combine specific mission scenarios. In this paper, a new method of EMEA based on SHERPA, CRAM and HEART has been proposed. This method not only can effectively carry out retrospective analysis of error modes, but also integrate man, machine, media and mission to identify and calculate the probability of the error mode.",https://www.semanticscholar.org/paper/5eb72a73ef8c9c0099960d2dc5b9a8e555e30600,IS,Qualitative
INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT) IN EDUCATION: INDICATORS OF IMPROVEMENT IN ECUADOR,"A documentary review of the elaboration and production of studies related to Information and Communication Technologies and their Impact on Ecuadorian Education was carried out through a bibliometric study of the main characteristics of 1,576 publications registered in the Scopus database during the period 2018-2022. The results obtained from this database were organized in graphs and figures, categorizing the information by variables such as Year of Publication, Country of Origin and Area of Knowledge, which allowed to identify, through qualitative analysis, the position of different authors regarding the proposed topic. The main findings of this research were that Ecuador stood out for having the highest scientific production, leading the list with 1,576 publications. Likewise, the area of knowledge that made the greatest contribution to the construction of bibliographic material related to the study of variables was computer science, with 989 published documents.",https://www.semanticscholar.org/paper/5a7d16e31b7bb32ec5362f0976b9b4c2ff4ce843,CS,Qualitative
Learning to program as empirical inquiry: using a conversation perspective to explore student programming processes,"ABSTRACT Background and Context We explore the potential for understanding the processes involved in students’ programming based on studying their behaviour and dialogue with each other and “conversations” with their programs. Objective Our aim is to explore how a perspective of inquiry can be used as a point of departure for insights into how students learn to program. Method We completed a qualitative study situated in elective computer science classes in an upper secondary school in Norway. We collected data by video recording classroom interactions and used screen-recording software. Findings Our findings include how we consider programs as both means and ends and reconsider the “error” in trial-and-error strategies, the role of error messages, and how programs are bound to context and particular moments in time. Implications Our findings have implications for the ways we understand programs as mediating tools in research and apply them in the field of practice.",https://www.semanticscholar.org/paper/d290f472575702e502187f8b94919e602f248537,CS,Qualitative
Prospective in Additive Manufacturing Based on R-Meta-Analysis and Bibliometric Study,"This work aims to perform a scientific mapping using advanced tools and R libraries to evaluate from a quantitative and qualitative point of view the evolution of the field of Additive Manufacturing (AM). A descriptive analysis of the data will be applied for the creation of attributes that allow its normalization and the visualization of its temporal evolution, as well as the measurement of the impact of the sources and the frequency of publication using Lotka's law. With all this, the weight and importance of AM in the evolution of different fields (materials science, engineering, computer science, etc.) will be established and the trend in research work will be analyzed both quantitatively (evolution of the number of publications in each field, quantitative impact indexes of publications and qualitatively. All this will allow the reader to know, on the one hand, the temporal evolution of advanced in AM, and on the other hand, the reader will be able to have a ""still picture"" of the current situation that can be taken as references for the prospective analysis of the technique in a complementary way to that provided in business studies.",https://www.semanticscholar.org/paper/55a209180dac07c6b9b26868be1381b4dde03423,IS,Qualitative
Software engineering and human-computer interaction: Students' perception of a project-based approach in a postgraduate course,"software engineering, software design, distributed systems and computer-supported collaborative learning, and new strategies for computer science teaching. Abstract This work in progress presents how students perceive the incorporation of Human-Computer Interaction (HCI) as content in an advanced Software Engineering (SE) course. This innovation was carried out in a postgraduate course during one semester at a large private university in Chile. The investigation adopted some qualitative, interpretative research using semi-structured interviews. The data was obtained from end-of-term interviews, surveys, and academic records. We focused on academic and professional training, personal evaluation of learning outcomes, utility of the course content and methodology, self-management, desire for learning, self-control, and time spent on core project activities. For this presentation, we concentrated on the analysis of the personal perceptions of utility content and learning. Preliminary results indicate that the incorporation of HCI into the course was highly valued by the students. They all recognized it as a benefit for the software development process. It was also noticed that the activities proposed were good triggers for the self-regulated learning of the expected course content, both SE and HCI. The authors are interested in receiving feedback about the continuity of this work, particularly in the understanding of how the content of HCI interacts with a proposal based on project-based learning (PBL).",https://www.semanticscholar.org/paper/716094fcb97a694ba73bdc349863d39f63a27eb4,CS,Qualitative
AI Based Emotion Detection for Textual Big Data: Techniques and Contribution,"Online Social Media (OSM) like Facebook and Twitter has emerged as a powerful tool to express via text people’s opinions and feelings about the current surrounding events. Understanding the emotions at the fine-grained level of these expressed thoughts is important for system improvement. Such crucial insights cannot be completely obtained by doing AI-based big data sentiment analysis; hence, text-based emotion detection using AI in social media big data has become an upcoming area of Natural Language Processing research. It can be used in various fields such as understanding expressed emotions, human–computer interaction, data mining, online education, recommendation systems, and psychology. Even though the research work is ongoing in this domain, it still lacks a formal study that can give a qualitative (techniques used) and quantitative (contributions) literature overview. This study has considered 827 Scopus and 83 Web of Science research papers from the years 2005–2020 for the analysis. The qualitative review represents different emotion models, datasets, algorithms, and application domains of text-based emotion detection. The quantitative bibliometric review of contributions presents research details such as publications, volume, co-authorship networks, citation analysis, and demographic research distribution. In the end, challenges and probable solutions are showcased, which can provide future research directions in this area.",https://www.semanticscholar.org/paper/ae31bbf51413c832d0a02146a738fb4f25b9e780,CS,Qualitative
Human Factors in Cybersecurity: A Scoping Review,"Humans are often considered to be the weakest link in the cybersecurity chain. However, traditionally the Computer Science (CS) researchers have investigated the technical aspects of cybersecurity, focusing on the encryption and network security mechanisms. The human aspect although very important is often neglected. In this work we carry out a scoping review to investigate the take of the CS community on the human-centric cybersecurity paradigm by considering the top conferences on network and computer security for the past six years. Results show that broadly two types of users are considered: expert and non-expert users. Qualitative techniques dominate the research methodology employed, however, there is a lack of focus on the theoretical aspects. Moreover, the samples have a heavy bias towards the Western community, due to which the results cannot be generalized, and the effect of culture on cybersecurity is a lesser known aspect. Another issue is with respect to the unavailability of standardized security-specific scales that can measure the cybersecurity perception of the users. New insights are obtained and avenues for future research are presented.",https://www.semanticscholar.org/paper/819891228071a18ffbe2cd43cccd2df31f77f943,IT,Qualitative
"The Role of Information and Communication Technology (ICT) for Older Adults’ Decision-Making Related to Health, and Health and Social Care Services in Daily Life—A Scoping Review","Information and communication technology (ICT) can potentially support older adults in making decisions and increase their involvement in decision-making processes. Although the range of technical products has expanded in various areas of society, knowledge is lacking on the influence that ICT has on older adults’ decision-making in everyday situations. Based on the literature, we aimed to provide an overview of the role of ICT in home-dwelling older adults’ decision-making in relation to health, and health and social care services. A scoping review of articles published between 2010 and 2020 was undertaken by searching five electronic databases. Finally, 12 articles using qualitative, quantitative, and mixed-method designs were included. The articles were published in journals representing biology and medicine, nursing, informatics, and computer science. A majority of the articles were published in the last five years, and most articles came from European countries. The results are presented in three categories: (i) form and function of ICT for decision-making, (ii) perceived value and effect of ICT for decision-making, and (iii) factors influencing ICT use for decision-making. According to our findings, ICT for decision-making in relation to health, and health and social care services was more implicitly described than explicitly described, and we conclude that more research on this topic is needed. Future research should engage older adults and health professionals in developing technology based on their needs. Further, factors that influence older adults’ use of ICT should be evaluated to ensure that it is successfully integrated into their daily lives.",https://www.semanticscholar.org/paper/2b437aec1cce2ffbef85efb3dbd32e39840c9c5e,CS,Qualitative
Scientia: The use of speech recognition in educational games as an alternative in learning science,"As a result of the pandemic, people had to find new ways to learn. This created new problems, like how hard it is for teachers to keep an eye on students who are often distracted by video games when they are using computers. However, video games can also be used for educational purposes. The main objective of this study was to develop an educational computer game with speech recognition that promoted an immersive and interactive experience for Grade 5 students at Pax Catholic Academy Diocese of Bacolod Inc. This research is an applied qualitative-quantitative study that sought to investigate participants' attitudes regarding educational games with speech recognition. The researchers approached Grade 5 Science teachers at Pax Catholic Academy Diocese of Bacolod Inc. to obtain the information necessary to develop an educational game for fifth graders. Copies of the teacher's learning materials were obtained to serve as the basis for the in-game books and quizzes. Biology, Chemistry, and Physics are the science topics included in the game. The game had a series of multiple-choice quizzes with speech recognition and other quizzes had drag-and-drop mechanics. Participants were provided the game to play, and their respective parents/guardians were asked to review the game based on their children's experience with it. Based on the results, it was determined that the game was playable, educational, substantial, and engaging. It can be concluded that 'Scientia' is an effective educational computer game for fifth graders since the speech recognition feature enhances immersion and involvement while learning Science. Keywords: speech recognition, educational game, learning science, game-based learning, Grade 5, quiz game",https://www.semanticscholar.org/paper/163feb05f4dd504c6f168d154389dc1f9c43bb6e,IS,Qualitative
Sistem Absensi Online Berbasis Website Studi Kasus: Program Studi Ilmu Komputer Universitas Pat Petulai,"The Computer Science Study Program at Pat Petulai University is currently still adopting a conventional or manual attendance system, both in implementing and managing attendance data, so there is a need for a website that can manage attendance data and processes online. Based on these problems, this research aims to build a website-based online attendance system that can manage attendance data and processes online without having to do it manually or conventionally. By using qualitative descriptive research methods in this research, it is hoped that it can explain the results of the research that has been carried out. The use of information technology in this attendance was made with PHP, MySQL as a database and Laravel framework. System design using Unifed Modeling Language (UML) and waterfall tools as software development methods. The results of the software were tested in terms of functionality using black box testing. With the website that has been built, it is hoped that it can support the progress of the Pat Petulai University Computer Science Study Program in terms of managing student attendance data.",https://www.semanticscholar.org/paper/745a47653c6120b52e8d4b9a56e44c579fea870e,CS,Qualitative
Enhancing Cybersecurity: Smart Intrusion Detection in File Server SYSTEMS,"System security is a major challenge worldwide, which has led to the increasing implementation of security surveillance systems in the public and private sectors. Likewise, it is inevitable to secure server-based systems that store vast amounts of sensitive data that is accessed from time to time. Intrusion Detection Systems (IDS) use metrics to detect anomalous activity on computers and computer networks. Modern detection algorithms try to reach detection metrics by acting as an antivirus. This is not enough, the need to explore more controlled, porous, and more secure systems is inevitable, hence this research. Therefore, this study’s main objective is to develop smart intrusion detection systems for file servers and client machines that can be used within any networked environment. A qualitative research methodology was employed in the study. The sources of information included four databases: SpringerOpen, EBSCO, Google Scholar, and Direct Science. The key findings of the study are that cyber-attacks and threats are increasing, and new strategies are needed to handle them because the current intrusion detection systems experience challenges and are unable to detect malware. Intrusion detection systems are the next-generation protection, which offers the visibility to identify advanced threats within legitimate content, even authorized applications and trusted sources. Organizations are recommended to implement smart IDSs in front of file server systems and behind the firewall to ensure all malware is filtered.",https://www.semanticscholar.org/paper/bfcc596941e9a4a98c3dcdec697a6b7cbe52f1ee,CS,Qualitative
Exploring an Inquiry-based Approach to Enhance English Vocabulary Learning: Academy Students' Perspectives,"Objective: The objectives of this study are (1) to describe how the IBL approach can improve and motivate students to learn English vocabulary and (2) to explore how academy students believe the IBL approach has improved their English vocabulary. Method: This qualitative study investigates the application of the IBL approach and how students address its significance in learning activities at a Computer Science Academy in Ternate, North Maluku, Indonesia. This research sample randomly chose 15 academy students based on their IBL experience. Results: Using observation and interviews, the results show that students at the Computer Science Academy used the IBL approach toward learning English, vocabulary, planning activities, retrieving information, assignment processes, creativity skill development, and project sharing. Furthermore, students liked applying the IBL approach in the classroom, particularly in English lessons. The IBL approach also plays a crucial role in improving students' English vocabulary skills. Novelty: An essential aspect of using the IBL approach in vocabulary learning is something new that sheds light on the group collaboration process, creativity, and engagement.",https://www.semanticscholar.org/paper/13159cff91099b48f729577bafc2c77eff9ee305,IS,Qualitative
Graphical Retrieval and Analysis of Temporal Information Systems (GRATIS): An Integrative Mixed Methodology and Open-Access Software to Analyze the (Non-)Linear Chronological Evolution of Information Embedded in Textual/Qualitative Data,"Like a video that reveals much more than a single photo, the incorporation of time to the analysis of qualitative evidence promotes contextualized understandings and allows research participants and readers to interactively review the processes and rationale that researchers followed to craft their findings and conclusions. However, mixed methods and qualitative methodologies available today forfeit the nuances gained by analyzing the chronological/temporal evolution of processes. We contribute to mixed methods research by introducing graphical retrieval and analysis of temporal information systems (GRATIS), a methodology (and open-access software) designed to visualize and analyze the time-based richness embedded in all qualitative/textual data. GRATIS employs dynamic network visualizations and data science mining/retrieval tools to combat the assumption that longitudinal studies require large timespans. We showcase how all qualitatively- or machine-learning-coded textual data may be analyzed with no extra feature engineering (i.e., data cleaning or preparation), rendering fully integrative/interactive outputs that strengthen the transparency of our findings and conclusions and open the “analytic black box” that characterizes most of mixed methods and qualitative studies to date. GRATIS contributes to democratizing data science by removing financial and computer programming barriers to benefit from data science applications. All data and software to replicate the analyses are provided with this submission.",https://www.semanticscholar.org/paper/1c16793caa851fe04458b0b51c22c5c3f4d3c2b0,CS,Qualitative
Synthesizing Qualitative Evidence: A Roadmap for Information Systems Research,"Qualitative synthesis research is an approach that consolidates the output of different qualitative studies to create new subject knowledge. Such work can help reveal more powerful explanations than that seen in a single study, thereby generating increased levels of understanding of a given phenomenon and greater research finding generalizability. Based on a review of the literature and a survey of qualitative researchers, we found that the information systems (IS) domain lacks a clear understanding of qualitative synthesis methods and, as a result, has largely failed to take advantage of this powerful, high-potential methodological opportunity. To address this shortcoming, this paper is the first to provide a rigorous overview of the full suite of 35 qualitative synthesis methods, as well as guidelines that include a three-tiered selection framework. By using the guidelines and framework in tandem, IS researchers are able to select the qualitative synthesis method most appropriate for a given research study, particularly when the research objective involves knowledge integration/aggregation, interpretation/theory development, and/or informing IS practice.",https://www.semanticscholar.org/paper/21e54c995de8da7e1ffb08741d3397e95610c581,IS,Qualitative
Quantitative and Qualitative Analysis on the Integration of Geographic Information Systems and Building Information Modeling for the Generation and Management of 3D Models,"3D virtual management is a topic of growing interest. The AEC industry is undergoing a real revolution because of the technological changes that are taking place. Synchronized 3D visualization is one of the tools being deployed at an accelerated pace. This, together with collaborative work, contributes to optimal management for all stakeholders. The integration of geographic information systems and building information modeling and heritage BIM is one of the most innovative concepts; it enables the generation of collaborative, fluid systems. The objective of this research is to identify the most significant technological developments and potential applications of the aforementioned integration. For this purpose, after a bibliographic consultation (26,245 sources), two analyses are carried out (from the screening of 179 sources), one quantitative (bibliometric) and the other qualitative (focused on five key concepts). The results show that regarding the integration of GIS with BIM and HBIM, the highest concentration of contributions is in engineering with 30.66%, followed by computer science with 21.01%. The country with the highest number of citations is China with 717, followed by Australia and the USA with 549 and 513, respectively, but relativizing the number of citations based on various indices (human development index, gross national income per capita, and population-tertiary education level), Hong Kong (18.04), Australia (10.64), and Egypt (10.16) would take the top positions, respectively. Regarding universities, the entity that has generated the most references is Delft University of Technology (the Netherlands) with 15 papers, followed by University College London (UK) with 13. Finally, the results show that GIS and BIM and HBIM provide virtual 3D models with multiple applications for buildings and infrastructures.",https://www.semanticscholar.org/paper/731cb57b19b1c3c92657e907ac83484c5b8ebfb2,IS,Qualitative
Qualitative comparative analysis in the information systems discipline: a literature review and methodological recommendations,"PurposeQualitative Comparative Analysis (QCA) is a promising, powerful method that is increasingly used for IS research. However, the Information Systems (IS) discipline still lacks a shared understanding of how to conduct and report QCA. This paper introduces the fundamental concepts of QCA, summarizes the status quo, and derives recommendations for future research.Design/methodology/approachA descriptive literature review in major IS outlets summarizes how and why QCA has been used in the IS discipline, critically evaluates the status quo, and derives recommendations for future QCA studies.FindingsThe literature review reveals 32 empirical research articles in major IS journals that have used the QCA method. Articles applied QCA to a broad range of research topics at the individual and organizational levels, mainly as a standalone analysis for theory development, elaboration and testing. The authors also provide evidence that most published IS research articles do not take full advantage of the potential QCA, such as analyzing necessary causal conditions or testing the robustness of QCA results. The authors provide seven actionable recommendations for future IS research using QCA.Originality/valueThe literature review assesses the status quo of QCA’s application in the IS discipline and provides specific recommendations on how IS researchers can leverage the full potential of QCA.",https://www.semanticscholar.org/paper/d4977bfee4f513668f46181e37b9447a5c1e0b41,IS,Qualitative
"Ηow to Use Artificial Intelligence (AI) as a Resource, Methodological and Analysis Tool in Qualitative Research?","Artificial Intelligence (AI) has had far-reaching effects in research and the academic world. It has been used in many ways by the scientific community within the context of qualitative research, such as literature and systematic reviews, for conceptualization purposes, thematic and content analysis. It has however prompted concerns and questions about the potential for unreliable research, bias, and unethical behavior in the outcomes of AI-produced research. The purpose this paper is to examine the current use of AI in research, its strengths and limitations, dilemmas and ethical considerations from theoretical critical perspective principles, while delivering five key considerations for the appropriate, rigorous, and reliable use of AI in research practice. The first step is to become acquainted with the data generated by AI systems. The second is concerned with removing biased content and addressing ethical concerns when using AI, while the third is concerned with cross-referencing information generated by AI. The fourth step is to control the analysis process. The fifth and most important key consideration is the demonstration of cognitive input and skills by the researcher throughout the process of using AI in any qualitative research study and in reaching conclusions.",https://www.semanticscholar.org/paper/714aa919e0504ecf5eea80ece94944a2f481f34d,CS,Qualitative
The Development of a Design Theory for Web Based Information Systems,"There is a common assumption among individuals that the complexity involved in developing novel systems utilizing Web technologies implies that Information Systems (IS) that are Web-based must possess fundamental and significant distinctions from conventional IS. This study raises skepticism regarding the veracity of this claim. The literature pertaining to academic research, manuals, and sales material frequently espouses optimistic claims regarding the capabilities of e-commerce and e-business technologies and applications, often grounded in the assumption of novelty associated with the Internet. The objective of the research is to establish a systematic classification system for information systems theory based on its efficacy in addressing four fundamental objectives: analysis, explanation, prescription, and prediction. This study utilized both experimental and descriptive qualitative methodologies. Subsequent to the analysis phase in the system development cycle of information technology, the design phase ensues. The results indicate that the evolution of an information technology system can be delineated by its phases of requirement specification, design planning, and execution. The manifestation of this phenomenon is observed through the development of a strategic blueprint, the production of a visual representation or draft, or the organization of multiple components into a functional entirety. In conclusion, it is imperative for information systems to give priority to both the user and the integration of the system.",https://www.semanticscholar.org/paper/9d2758f7465b6bca2add485a3775104c082f9ed7,IS,Qualitative
Qualitative and Critical Research in Information Systems and Human Computer Interaction: Divergent and Convergent Paths,Qualitative and Critical Research in Information Systems and Human-Computer Interaction: Divergent and Convergent Paths,https://www.semanticscholar.org/paper/16e5d19577fe635171637f71127164e6e70f6323,IS,Qualitative
The Evolution of Digital Accounting and Accounting Information Systems in the Modern Business Landscape,"Purpose: The study aims to investigate the evolution of digital accounting and accounting information systems (AIS) in the Modern Business Landscape through a qualitative examination of the existing literature. Research Design and Methodology: Employing a systematic review approach, the research examines academic journals, books, and conference proceedings relevant to digital accounting and AIS. The selection criteria focus on publication credibility, relevance, and recency, as well as the contributions to theoretical and empirical knowledge. Data collection integrates searching, screening, and supplementary techniques such as content analysis, citation analysis, and bibliometric analysis. Qualitative methods like coding, categorization, and thematic analysis are used to dissect and identify prevalent themes and theoretical frameworks within the literature. Findings and Discussion: The findings reveal a rich tapestry of historical progression, technological advancements, and organizational implications of digital accounting and AIS. From the automation of routine tasks to the integration of advanced analytics and implications for workforce dynamics, digital accounting emerges as a transformative force shaping organizational practices and strategic decision-making. Implications: The study underscores the foundational role of technology in shaping the trajectory of digital accounting, with implications for efficiency, accuracy, transparency, and strategic planning within organizations.",https://www.semanticscholar.org/paper/40279842d366105b24f2a521e2b4e1630993a20c,IS,Qualitative
Cybersecurity Challenges In The Era Of Digital Transformation A Comprehensive Analysis Of Information Systems,"This research aims to optimize business processes through information system integration using a case study in Banda Aceh. This research uses a qualitative approach with a single case study research method. Data was collected through in-depth interviews, direct observation and documentation. Research respondents consist of managers and employees involved in the business processes to be integrated. Data analysis was carried out using content analysis. The research results show that information system integration significantly increases the efficiency and effectiveness of business processes in Banda Aceh. Information system integration enables faster data processing, better monitoring of business processes, and more accurate decision making. Additionally, information system integration also enables better collaboration between different departments and stakeholders. Based on these findings, it can be concluded that information system integration plays an important role in optimizing business processes in Banda Aceh. By integrating information systems, companies can increase operational efficiency, reduce costs, and increase customer satisfaction. However, the implementation of information systems integration also faces challenges, such as technical difficulties and changes in organizational culture. Therefore, strong commitment and support from company management is needed to achieve optimal results.",https://www.semanticscholar.org/paper/60a96932ae4b9162ac726e762a30215591f6c050,IS,Qualitative
Cybersecurity in Accounting Information Systems: Challenges and Solutions,"Purpose: This study explores the challenges and solutions in securing Accounting Information Systems (AIS) amidst increasing cybersecurity threats, emphasizing the integration of technological, human, and organizational factors. Research Design and Methodology: Utilizing a qualitative approach, the study employs case studies and expert interviews across various sectors. The research framework draws on the Technology-Organization-Environment (TOE) and Socio-Technical Systems (STS) theories to provide a comprehensive analysis of AIS security dynamics. Findings and Discussion: The research identifies the growing sophistication of cyber threats, necessitating advanced technological solutions such as encryption, multi-factor authentication (MFA), and artificial intelligence (AI) for real-time threat detection. Despite these advancements, human factors like insufficient cybersecurity awareness and organizational factors such as inadequate security policies and investment remain significant vulnerabilities. The study highlights the critical role of a strong cybersecurity culture, effective governance, and regulatory compliance in enhancing AIS security. Empirical evidence supports the necessity of integrating these elements to mitigate cyber risks effectively. Implications: The findings underscore the importance of a holistic approach to AIS cybersecurity, combining technological advancements with robust organizational practices and continuous training. Organizations are advised to foster a cybersecurity-conscious culture, develop clear policies, and ensure top management support to enhance their cybersecurity posture. These insights provide actionable recommendations for organizations aiming to protect their AIS against evolving cyber threats.",https://www.semanticscholar.org/paper/3837e9c224248a18813f7bdca77f85a01d3eca8f,IS,Qualitative
Cyber Security Risk Modeling in Distributed Information Systems,"This paper deals with problems of the development and security of distributed information systems. It explores the challenges of risk modeling in such systems and suggests a risk-modeling approach that is responsive to the requirements of complex, distributed, and large-scale systems. This article provides aggregate information on various risk assessment methodologies; such as quantitative, qualitative, and hybrid methods; a comparison of their advantages and disadvantages; as well as an analysis of the possibility of application in distributed information systems. It also presents research on a comprehensive, dynamic, and multilevel approach to cyber risk assessment and modeling in distributed information systems based on security metrics and techniques for their calculation, which provides sufficient accuracy and reliability of risk assessment and demonstrates an ability to solve problems of intelligent classification and risk assessment modeling for large arrays of distributed data. The paper considers the main issues and recommendations for using risk assessment techniques based on the suggested approach.",https://www.semanticscholar.org/paper/a6c62d915c27b5913f477282daf7993ad10379d4,IS,Qualitative
Enhancing Security and Reliability of Information Systems through Blockchain Technology: A Case Study on Impacts and Potential,"In the digital age, ensuring the security and reliability of information systems is a paramount concern for organizations. This research investigates the potential of blockchain technology to enhance information system security and reliability within the dynamic landscape of Indonesian start-up companies. By employing a mixed-methods approach, combining qualitative interviews and quantitative surveys, this study explores the current state of information system security practices, assesses the challenges faced by start-ups, and evaluates perceptions regarding the adoption of blockchain technology. The qualitative findings highlight the existing security measures, challenges, and potential benefits associated with blockchain. The quantitative results provide insights into security practices and willingness to adopt blockchain. Through the integration of these findings, the study offers practical recommendations for enhancing information system security and reliability in the context of start-ups, while considering the challenges of blockchain adoption. This research contributes to the understanding of the symbiotic relationship between technology adoption and information security, offering guidance for start-ups, policymakers, and researchers.",https://www.semanticscholar.org/paper/4c3c37081eae95febc65d17c043bef6ee3cc2338,IS,Qualitative
Research on Human-centered Design in College Music Education to Improve Student Experience of Artificial Intelligence-based Information Systems,"The integration of Artificial Intelligence (AI) technology with music instruction necessitates a delicate balance between technical advancement and the maintenance of humanistic teaching. This study examined how human-centered design concepts were used to optimize the integration of AI while also investigating the effects of AI technology on college-level music instruction in China. It aimed to identify potential, difficulties and make recommendations for ethical AI deployment in this particular environment. Semi-structured interviews with 20 music students and professors from Chinese higher education institutions were conducted using a qualitative study design. To condense significant themes and subthemes from the data, open coding, axial coding, and selective coding were used. The study revealed complex interactions between AI and Chinese music instruction. Themes included ""Enhanced Learning with AI"", emphasizing AI's role in motivating and personalizing music education; ""User-Centric Design"", emphasizing the importance of intuitive interfaces and aesthetic appeal; ""Collaboration and Peer Learning"", demonstrating AI's facilitation of collaborative projects; ""Technical Challenges and Ethical Concerns"", addressing technical obstacles and ethical concerns; and ""Educator Support and Curriculum Alignment"", emphasizing the importance of educator support and curriculum alignment. This study adds knowledge about how AI can be successfully incorporated into Chinese music teaching. It informs best practices for the adoption of AI, ensuring that technology enhances the learning experience for students while preserving cultural nuances. The study improves the conversation about innovative pedagogy and responsible technology integration. Implications include the potential for AI to change music education, cultural preservation, and global viewpoints. However, drawbacks such as sample bias and the dynamic nature of AI technology necessitate more study and development of educational techniques that use AI. Personalization and multimodal methods used in college music instruction in the future, to help increase student involvement. The importance of ethical issues, long-term effect analyses, and user-centered design will call for interdisciplinary cooperation. The future of AI-enhanced music education will also be shaped by assuring accessibility, diversity, and active engagement in policy and regulation discussions.",https://www.semanticscholar.org/paper/cbc5db815752065682c0676888992b3f697b7fb4,IS,Qualitative
"Exploring the Components of Management Information Systems: Software, Database, and Brainware - A Literature Review","A more in-depth information system is often referred to as a sub-system, which is a more specific component of a larger system. Information systems have close relationships and interdependencies with other sub-systems, making them inseparable from one another. An information system is just one component of various existing sub-systems, managed as property by a specific institution, organization, or company. Previous research or relevant studies aim to strengthen theories or phenomena related to the influence among variables. This article is structured using a qualitative method sourced from previous research literature by experts. The review's findings reveal components influencing Management Information Systems, namely: Software, Database, and Human Resources (Brainware). The results of this literature review indicate that 1) Software has an impact on Management Information Systems; 2) Databases also influence Management Information Systems; and 3) Human Resources also affect Management Information Systems.",https://www.semanticscholar.org/paper/e96741886c77199c151b68cd491e0fd679e3bffd,IS,Qualitative
Utilizing Blockchain Technology in Global Supply Chain Management: An Exploration of Scalable Information Systems,"INTRODUCTION: Global supply chain management is a critical component in the increasingly complex and connected world of modern business. In the era of globalization, companies face pressure to increase efficiency, transparency, and security in their supply chains. Blockchain technology has emerged as a potential solution to address some of these challenges by enabling more decentralized, transparent, and efficient supply chain management. However, the use of this technology in global supply chain management also raises several issues related to regulation, law, and collaboration with third parties. OBJECTIVE: This research then aims to explore the potential of blockchain technology in global supply chain management and understand the regulatory framework needed to support the implementation of this technology. METHOD: This research was carried out using a qualitative approach. The data used in this research comes from various research results and previous studies that are relevant to the discussion. RESULTS: The results of this research then found that the use of blockchain technology in global supply chain management promises to increase transparency, efficiency, and security. Smart contracts enable the automation of business processes, reducing costs and increasing visibility of operations. Collaboration with third parties is an important strategy in increasing supply chain efficiency. Regulation, data security, and international harmonization remain challenges. CONCLUSION: Defining the legal status of smart contracts and protecting data is key. Effective collaboration with third parties requires good communication and a mature strategy. With a deep understanding of blockchain technology and proper regulation, companies can maximize their benefits to create an efficient, transparent, and reliable supply chain.",https://www.semanticscholar.org/paper/e769560c3952eee4b858e5afaecff26ea90378cc,IS,Qualitative
ECG Standards and Formats for Interoperability between mHealth and Healthcare Information Systems: A Scoping Review,"Interoperability is defined as the ability of a system or device to communicate between different technologies and software applications. This allows the exchange and use of data in an efficient, precise, and robust way. The present article gives researchers and healthcare information systems developers a qualitative and quantitative synthesis of the state of knowledge related to data formats and data standards proposed for mHealth devices interoperability in healthcare information systems that retrieve and store ECG data. We carry out a scoping review to answer to following questions: (1) What digital data formats or data standards have been proposed for the interoperability of electrocardiograph data between traditional healthcare information systems and mobile healthcare information systems? (2) What are the advantages and disadvantages of these data formats or data standards? The scoping review was conducted in four databases in accordance with the JBI methodology for scoping reviews, and in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR). A total of 4018 studies were identified of which 30 studies met the inclusion criteria. Based on our findings, we identify four standards and nine formats for capturing and storing streaming ECG data in mobile health applications. The standards used were HL7, SCP-ECG, x73-PHD, and PDF/A. Formats include CSV, PDF-ECG, and seven XML-based formats. These are ECG-XML, HL7-XML, mPCG-XML, mECGML, JSON, SaECG, and CDA R2.",https://www.semanticscholar.org/paper/e02ad52e09dc676b0d430b2e116f2498868e7b73,CS,Qualitative
Learner engagement with written corrective feedback in ESL and EFL contexts: a qualitative research synthesis using a perception-based framework,"Abstract Although research on the efficacy of written corrective feedback has received considerable attention in recent years, there is a dearth of research on learner engagement with written corrective feedback. Understanding how language learners engage with written corrective feedback is high on the agenda of feedback research because it provides a broadened perspective that feedback uptake is only one form of engagement, and that engagement with written corrective feedback is influenced by myriad contextual and individual factors. To narrow the research gap, this qualitative research synthesis examines learner engagement with written corrective feedback in English writing contexts through the lens of ecological systems theory and a perception-based framework. Focusing on 14 articles, relevant information was extracted and synthesised following three iterative stages informed by grounded theory to identify common engagement patterns and clarify relationships between factors affecting how learners engage with written corrective feedback. The results reveal the dynamic, contextualised and individualised nature of learner engagement with written corrective feedback. Pedagogical implications for practitioners are discussed to address the lingering issues around learner engagement with written corrective feedback.",https://www.semanticscholar.org/paper/2a8fb7b7de9dfd724a6ffaa4f2d5c743bb2fa19c,IS,Qualitative
A Design Framework for Novice Using Grounded Theory Methodology and Coding in Qualitative Research: Organisational Absorptive Capacity and Knowledge Management,"Grounded theory methodology (GTM) is an extensive research methodology that is immensely active in numerous social science research fields. It is by far one of the most popular techniques applied in qualitative research. The challenge in using such methods might appear in their complexity. Several steps of coding and analysis in GTM can be fuzzy and multifaceted for novice researchers specialised in Information Systems (IS) fields, knowledge management, and broad applications of IS. The current study suggests a design framework for novices in qualitative research that presents GTM as a set of techniques characterised graphically, allowing the extraction of grounded results and a set of pragmatic analysed data classifications rather than only concentrating on implementing a grounded “theory”. Hence, the research stresses using the term “grounded techniques”, permitting the creation of grounded categories to strengthen qualitative research results' rigour. The proposed framework meticulously exemplifies how an organised set of phases in a research design can enlighten the novice researcher while conducting a study in knowledge absorptive capacity using a comprehensive GTM process to enforce the understanding of GTM techniques.",https://www.semanticscholar.org/paper/0aa439a6baf090b49e90de7ae62088335a3a4b70,IS,Qualitative
Challenges in Collecting Qualitative Data for Information Systems Studies,"In research, problem is not necessarily something that is broken, but phenomenon which require further or an in-depth investigation for a fresh perspective. Thus, every research necessitates a problem statement, goal and objectives, which determines the data collection methods. The data type can be either quantitative or qualitative. According to Seidman (2012), depending on the objectives of the study, either the qualitative or quantitative research methods are selected for data collection. However, both methods can be selected, which is referred to as a mixed method (Barbour 2013; Silverman 2013). The choice of research methods is critical in that they influences the way in which data is collected and analysed. According to Myers and Avison (2002,p70), qualitative research methods were developed in the social sciences to enable researchers to study social and cultural phenomena. The primary purpose of qualitative research is to understand a phenomenon as it is seen by respondents within a period and space. This is achieved by studying the respondents in their natural environments. Yin (2010) takes the argument further and states that the events and ideas emerging from qualitative research can represent the meanings given to real life events by the people who live them, not the values, perceptions or meanings held by researchers. However, the meaning which individuals and groups give or associate to events of Information systems and technologies (IS/IT) has never been easy for researchers to understand.",https://www.semanticscholar.org/paper/39a2f42d9514f133b46f1b0bc15a43517db38957,IS,Qualitative
The Effectiveness of Accounting Information Systems on Vehicle Sales Growth,"To get big profits, companies must have a good management system, namely how the business is managed professionally, one of which is having a sales accounting information system. This system helps companies record all transactions, thereby generating information that helps companies monitor business progress. This research was conducted on the basis of qualitative methods with descriptive analysis techniques. This research was conducted at PT. Istana Kebon Jeruk by analyzing data flow diagrams and internal control questionnaire techniques. The results show that the accounting system provides an actual picture of all business processes, thereby reducing various risks of loss. In addition, the accounting system provides information about sales and revenue trends, thereby helping operations managers make informed decisions.",https://www.semanticscholar.org/paper/e915c0087a4f103aac99b178e14c076caef22b50,IS,Qualitative
Protection Motivation Theory in Information Systems Security Research,"Protection motivation theory (PMT) is one of the most commonly used theories to examine information security behaviors. Our systematic review of the application of PMT in information systems (IS) security and the comparison with its application for decades in psychology identified five categories of important issues that have not yet been examined in IS security research. Discussing these issues in terms of why they are relevant and important for IS security, and to what extent IS research has not considered them, offers new research opportunities associated with the study of PMT and IS security threats. We suggest how future studies can approach each of the open issues to provide a new road map for quantitative and qualitative IS scholars.",https://www.semanticscholar.org/paper/c1b63fadd45e1fb5d6730f1127f332c2a0bb6fde,IS,Qualitative
"Mixed-Methods in Information Systems Research: Status Quo, Core Concepts, and Future Research Implications","Mixed-methods studies are increasing in information systems research, as they deliver robust and insightful inferences combining qualitative and quantitative research. However, there is considerable divergence in conducting such studies and reporting their findings. Therefore, we aim (1) to evaluate how mixed-methods studies have developed in information systems research under the existence of heavily used guidelines and (2) to reflect on those observations in terms of potential for future research. During our review, we identified 52 mixed-methods papers and quantitatively elaborated on the adherence to the three core concepts of mixed-methods in terms of purpose, meta-inferences, and validation. Findings discover that only eight adhere to all three of them. We discuss the significance of our results for current and upcoming mixed-methods research and derive specific suggestions for authors. With our study, we contribute to mixed-methods research by showing how to leverage the insights from existing guidelines to strengthen future research and by contributing to the discussion of the legislation associated with research guidelines, in general, presenting the status quo in current literature.",https://www.semanticscholar.org/paper/4d9f1ada29f2816463737a05c71c155a28999a87,IS,Qualitative
Benefits and Challenges of Technology and Information Systems on Performance,"In this modern era, technology and information systems are very important in business. explore the relationship between technology, information systems, and organizational performance. The aim of this paper is to provide a better understanding of the role of information technology and systems in improving organizational performance and consider the challenges that may occur. The right methodology for analyzing the topic of technology, information system, and performance is to use a qualitative approach. Information technology and systems have become important components of organizational performance. These systems improve efficiency, communication, decision-making, innovation and more. However, they also present some challenges, including security risks, technical issues, costs and more. The suggestions from this research will be very useful for companies that want to improve performance through the use of technology and information systems.",https://www.semanticscholar.org/paper/1f610bb6f1fa76064f39822cc9cb204ea7ed0dc9,IS,Qualitative
Adding experts’ perceptions to complement existing research on information systems backsourcing,"This paper extends the existing literature on information systems (IS) backsourcing by the perception of practitioners. For this purpose, we conducted a series of qualitative, semi-structured interviews with IS sourcing experts. The interview questions focused on the participants’ perceptions and experiences with the topic, on identifying reasons for and against IS backsourcing, and on revealing relevant trends pertinent to IS backsourcing. We then compared those findings with two previously conducted comprehensive literature reviews on academic and practitioner literature on IS backsourcing. By following this approach, we contribute to the existing research by verifying previous findings, for example, the most important reasons why companies decide in favor of IS backsourcing. Additionally, we were able to enhance previous contributions as we highlight the significance of differentiating between the scope of IS backsourcing by looking at the underlying services which are potentially backsourced. Further, we identified the importance of managers’ personal preferences as an additional reason for IS backsourcing, for example, based on personal experiences or a perceived need for change. Based on our findings, we created a comprehensive overview of all aspects connected to the IS backsourcing process and derived opportunities for further research to contribute to the IS backsourcing research agenda.",https://www.semanticscholar.org/paper/8a80084ca11ac11571eb90e27f28e7345883a700,IS,Qualitative
Transgender and nonbinary individuals and ICT-driven information practices in response to transexclusionary healthcare systems: a qualitative study,"OBJECTIVE This qualitative research examines how transgender and gender nonbinary (T/GNB) persons from South Carolina navigate informational barriers within healthcare systems. This navigation can be described through the lens of information practices, or how T/GNB participants create, seek, use, and share information to achieve desired healthcare outcomes. Special focus is given to the roles of Information and Communication Technologies (ICTs) in shaping these practices. MATERIALS AND METHODS The research utilizes participant data from semistructured interviews and focus groups conducted with 26 T/GNB individuals focusing on their health information practices. Data analysis utilized emic/etic coding and the constant comparative method to identify themes describing transexclusionary information barriers and respondent ICT-led information practices. RESULTS Findings note healthcare systems producing cisnormativity by design resulting in T/GNB individuals viewing healthcare spaces as exclusionary. Exclusionary barriers included over reliance on medical, expert authority ignoring T/GNB embodiment, and a lack of contextual perspective to identities. In response, T/GNB seek, create, use, and share information via ICTs to challenge exclusionary practices. DISCUSSION T/GNB ICT use addresses systemic barriers within healthcare systems suggesting a need to reframe healthcare systems through the lens of design justice, one that values T/GNB agency in understanding and producing health knowledge. CONCLUSION While many healthcare providers are not intentionally being transexclusionary, the design of healthcare information systems rely on cisnormative values, thus excluding many T/GNB from accessing healthcare in comfortable and safe ways. Shifting toward the values and practices of T/GNB as informed by ICT use will afford healthcare providers ways to undo barriers to care.",https://www.semanticscholar.org/paper/b47c1893ddbe0fa35f3de0cf7c68d14d32d04d41,IS,Qualitative
Reviewing qualitative research approaches in the context of critical infrastructure resilience,"Modern societies are increasingly dependent on the proper functioning of critical infrastructures (CIs). CIs produce and distribute essential goods or services, as for power transmission systems, water treatment and distribution infrastructures, transportation systems, communication networks, nuclear power plants, and information technologies. Being resilient becomes a key property for CIs, which are constantly exposed to threats that can undermine safety, security, and business continuity. Nowadays, a variety of approaches exist in the context of CIs’ resilience research. This paper provides a state-of-the-art review on the approaches that have a complete qualitative dimension, or that can be used as entry points for semi-quantitative analyses. The study aims to uncover the usage of qualitative research methods through a systematic review based on PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). The paper identifies four principal dimensions of resilience referred to CIs (i.e., techno-centric, organisational, community, and urban) and discusses the related qualitative methods. Besides many studies being focused on energy and transportation systems, the literature review allows to observe that interviews and questionnaires are most frequently used to gather qualitative data, besides a high percentage of mixed-method research. The article aims to provide a synthesis of literature on qualitative methods used for resilience research in the domain of CIs, detailing lessons learned from such approaches to shed lights on best practices and identify possible future research directions.",https://www.semanticscholar.org/paper/7d3f19f6eac5cce54f90bb91736b232fc27f99b2,IT,Qualitative
"Women entrepreneurs' digital social innovation: Linking gender, entrepreneurship, social innovation and information systems","This article responds to increasing discourses on digital social innovation (DSI) from the perspectives of women entrepreneurs. Using the individual differences theory of gender and information technology (IDTGIT), this research explores how digital technology is used by women entrepreneurs to create opportunities in response to the challenges associated with individual identity, individual influences, social influences and structural influences. We also extend the IDTGIT by exploring how technology is used by women entrepreneurs in their DSI ventures and how technology facilitates the social impact of such ventures. This paper draws on a qualitative study using interviews with 17 women entrepreneurs in Australia, and our findings indicate that individual identity, individual influences and social and structural influences play a significant role in inhibiting women entrepreneurs' business ventures but technology helps to create opportunities for women entrepreneurs to address these factors. We also found that technology plays a role in helping women entrepreneurs to pursue social innovation in two different ways: through social innovation that is embodied by technology and social innovation that is enabled by technology. Our findings further indicate the social impact of DSI in the areas of education, employment, environment and climate, community development and progress and healthcare. The theoretical and practical implications of DSI for women entrepreneurs are provided.",https://www.semanticscholar.org/paper/28547df055704ba3a737a09be2edc77ac376463b,IS,Qualitative
"Examining the Relationship between Information Systems, Sustainable SCM, and Competitive Advantage","Sustainability, information systems (IS), and sustainable supply chain management (SSCM) are the main research areas of this study. In an era where environmental and social responsibility is becoming increasingly more important, SSCM is very significant for the survivability and competitiveness of organizations. Information systems may facilitate sustainable practices, as they support supply chain processes, decrease costs, and enable the control and monitoring of operations. The aim of this research is to explore the relationship between information systems and SSCM and explain under what circumstances they could lead to competitive advantage or increase internal business performance. To analyze the above, the methodology comprises a literature review enabling the understanding and conceptualization of the main research constructs and revealing the gaps of previous research. It also comprises qualitative research based on a single-case study allowing an in-depth examination of the subject of study. The findings reveal that the use of information systems, combined with strong interorganizational relationships and collaboration, can support SSCM practices. They can bring important business advantages, such as improved business performance and increased customer loyalty, but not necessarily competitive advantage. The results of this research show that SSCM is a promising field of study, with prospects for future research.",https://www.semanticscholar.org/paper/b5efbe78e4065a8b8ab80360bdba33f3f27468ae,IS,Qualitative
Client information systems’ support for case-based social work: experiences of Finnish social workers,"ABSTRACT In recent years, Finland has seen extensive development in the area of social and healthcare information management aimed at harmonizing client information systems (CISs) and ways of documenting client information. The developments aim to support the utilization of recorded information at different levels of service systems, including the level of social work practice. This article investigates the relationship between social work practice and the use of CISs by answering the following question: To what extent do CISs support case-based social work in Finland? Social work is approached as a knowledge-intensive practice in which CISs have become increasingly important for managing case work. The research question is considered in relation to recent national developments in social and healthcare information management. The study uses survey data from a subpopulation of social workers (n = 309) working in municipal social services and combines quantitative and qualitative data. The findings reveal that current CISs do not offer sufficient support for case-based social work. Although CISs serve reasonably well for storing and documenting case-based information, they generally serve social workers poorly in terms of providing them with the tools they need to understand cases in their entirety and form comprehensive knowledge for their practice. To provide adequate CISs that support case-based social work, it is necessary to see social workers as active users of information and understand the role of CISs in the knowledge formation process in social work.",https://www.semanticscholar.org/paper/035dff08af1551139b3a5332aa0fcca23922ddfc,IS,Qualitative
Exploring information systems security implications posed by BYOD for a financial services firm,"This study explores information systems security implications posed by Bring Your Own Device concept in financial services firms. Thus, the findings and recommendations from this study will help financial services and other organisations to be cognisant of the importance of BYOD policy formulation. The use of BYOD has become prevalent in the workplace due to the increased dependence on the Internet and advancements in technologies. It is beneficial to the organisation in that employees buy, use and insure their own devices, thus, the organisation does not bear these costs. However, there is a huge cost to the company if the use and connection of BYODs to the company’s Information Technology infrastructure is not regulated and monitored. BYODs expose information and information systems assets to threat actors. Financial institutions handle very sensitive information, making them a target for data breach and the adoption of BYODs more hazardous. A qualitative research method was conducted with eight (8) purposefully selected participants working in the Risk, IT and Information Systems Security departments of the financial institution. Telephonic interviews were conducted in line with the national protocols of the global Corona Virus Disease-2019 (COVID-19) pandemic. The study revealed the absence of a BYOD policy and employees could use any number of personal devices without restrictions. Users were aware of information systems security policies and protocols because of the annual training and awareness programmes.",https://www.semanticscholar.org/paper/f64c282320f2b5b21347e7903a34c62364a5ad36,IS,Qualitative
Hacker Definitions in Information Systems Research,"ABSTRACT The IS research community has called for more research on hackers and their behavioral motivations. One of the reasons that research on hackers has been so limited is that there is no clear definition of what a hacker is, or who may or may not be considered a hacker. Researchers have attempted to define the term hacker, yet overall attempts to craft a definition have been inconsistent and partially complete. The purpose of this paper is to present results from an inductive, qualitative study of how the term hacker has been defined in IS literature. We reviewed the leading IS journals to identify previously-used hacker definitions. The review and analysis conducted in this paper yields a more comprehensive definition of the term hacker to help advance hacker-related IS research efforts.",https://www.semanticscholar.org/paper/3eafbdae1cefc9533e13185c11dd4edd2d29ef14,IS,Qualitative
Thematic Analysis in Qualitative Research,"The popularity of qualitative methods in social science research is a well-noted and most welcomed fact. Thematic analysis, the often-used methods of qualitative research, provides concise description and interpretation in terms of themes and patterns from a data set. The application of thematic analysis requires trained expertise and should not be used in a prescriptive, linear, and inflexible manner while analyzing data. It should rather be implemented in relation to research question and data availability. To ensure its proper usage, Braun and Clarke have propounded the simplest yet effective six-step method to conduct thematic analysis. In spite of its systematic step-driven process, thematic analysis provides core skills to conduct different other forms of qualitative analysis. Thematic analysis, through its theoretical freedom, flexibility, rich and detailed yet complex analytical account has emerged as the widely used and most effective qualitative research tool in social and organizational context.",https://www.semanticscholar.org/paper/7226031c9f74af03c20068e70675805ada28ffea,IS,Qualitative
A Roadmap for Applying the Contextual Integrity Framework in Qualitative Privacy Research,"Privacy is an important topic in HCI and social computing research, and the theory of contextual integrity (CI) is increasingly used to understand how sociotechnical systems-and the new kinds of information flows they introduce-can violate privacy. In empirical research, CI can serve as a conceptual framework for explaining the contextual nature of privacy as well as an analytical framework for evaluating privacy attitudes and behaviors. Analytical applications of CI in HCI primarily employ quantitative methods to identify appropriate information flows but rarely engage with the full CI framework to evaluate such flows. In this paper, we present a roadmap to guide HCI and social computing researchers on how to apply the full CI framework to qualitative projects. To help researchers envision what such an analysis can look like, each step includes an example analysis using interview data from projects on privacy and fitness tracking. We conclude by discussing how harnessing the full CI framework can address critiques of CI and identify opportunities for further theory development.",https://www.semanticscholar.org/paper/4fe953bc4ef5673d93381aca1f6f184f401c8c13,CS,Qualitative
Financial Management Information Systems and accounting policies retention in Brazil,"PurposeThe successful implementation of International Public Sector Accounting Standards (IPSAS) depends on the adoption and subsequent maintenance of accrual accounting policies. Moreover, Financial Management Information Systems (FMIS) are important drivers of reforms, and their replacement might disrupt the execution of accrual accounting policies. This paper aims to analyze the effects of FMIS replacement (or maintenance) on the retention of accrual accounting policies in Brazilian local governments.Design/methodology/approachThe research adopts a sequential mixed-methods approach, starting with a quantitative analysis of the presence of accrual accounting policies in local governments and the effects of FMIS replacement. Next, a qualitative analysis is conducted with a survey, documents and interviews to observe the FMIS replacement process. Our analysis focuses on local governments from one state in Brazil, but the context is highly transferable to other states, as the same procurement law and accounting regulations apply.FindingsFMIS replacement may reduce accounting policies retention; consequently, public procurement regulation may induce a public procurement context in which the IPSAS project would find more difficulties to prosper.Research limitations/implicationsThis research contributes to the IPSAS literature by examining the phenomenon of accounting policies retention or persistence, as one should not take it for granted that an adopted accounting procedure will be sustained over time. The analysis argues that FMIS replacement due to compulsory rebidding should be carefully considered.Practical implicationsPromoters of accounting reforms may consider the regulation of contracting out for FMIS a relevant issue to the institutionalization of accounting policies.Originality/valueThe analysis innovates by linking IPSAS accounting reform to the contracting out of FMIS.",https://www.semanticscholar.org/paper/d02d0b2b3680cc0c3394eb6f08a989902fd0c4ba,IS,Qualitative
"Acceptance, Barriers, and Facilitators to Implementing Artificial Intelligence–Based Decision Support Systems in Emergency Departments: Quantitative and Qualitative Evaluation","Background Despite the increasing availability of clinical decision support systems (CDSSs) and rising expectation for CDSSs based on artificial intelligence (AI), little is known about the acceptance of AI-based CDSS by physicians and its barriers and facilitators in emergency care settings. Objective We aimed to evaluate the acceptance, barriers, and facilitators to implementing AI-based CDSSs in the emergency care setting through the opinions of physicians on our newly developed, real-time AI-based CDSS, which alerts ED physicians by predicting aortic dissection based on numeric and text information from medical charts, by using the Unified Theory of Acceptance and Use of Technology (UTAUT; for quantitative evaluation) and the Consolidated Framework for Implementation Research (CFIR; for qualitative evaluation) frameworks. Methods This mixed methods study was performed from March to April 2021. Transitional year residents (n=6), emergency medicine residents (n=5), and emergency physicians (n=3) from two community, tertiary care hospitals in Japan were included. We first developed a real-time CDSS for predicting aortic dissection based on numeric and text information from medical charts (eg, chief complaints, medical history, vital signs) with natural language processing. This system was deployed on the internet, and the participants used the system with clinical vignettes of model cases. Participants were then involved in a mixed methods evaluation consisting of a UTAUT-based questionnaire with a 5-point Likert scale (quantitative) and a CFIR-based semistructured interview (qualitative). Cronbach α was calculated as a reliability estimate for UTAUT subconstructs. Interviews were sampled, transcribed, and analyzed using the MaxQDA software. The framework analysis approach was used during the study to determine the relevance of the CFIR constructs. Results All 14 participants completed the questionnaires and interviews. Quantitative analysis revealed generally positive responses for user acceptance with all scores above the neutral score of 3.0. In addition, the mixed methods analysis identified two significant barriers (System Performance, Compatibility) and two major facilitators (Evidence Strength, Design Quality) for implementation of AI-based CDSSs in emergency care settings. Conclusions Our mixed methods evaluation based on theoretically grounded frameworks revealed the acceptance, barriers, and facilitators of implementation of AI-based CDSS. Although the concern of system failure and overtrusting of the system could be barriers to implementation, the locality of the system and designing an intuitive user interface could likely facilitate the use of optimal AI-based CDSS. Alleviating and resolving these factors should be key to achieving good user acceptance of AI-based CDSS.",https://www.semanticscholar.org/paper/1b79e48d84c7bcb12800a090be79130406a475ae,CS,Qualitative
A Method for Interpretively Synthesizing Qualitative Research Findings,"In the qualitative research world, one can use a method called meta-synthesis to interpretively assess a compiled body of literature on a specific topic, though it has seen little application in business research let alone in management information systems scholarship. However, because methods for qualitative inquiry have gained more popularity in the information systems discipline, this method holds great promise in supporting efforts toward theoretical generalization for qualitative researchers. Accordingly, in this paper, we present a methodological tutorial on the nature and practice of analytically synthesizing a body of qualitative research for developing and explicating theory.",https://www.semanticscholar.org/paper/168c183c4753929b14be4c0da283191a4b054a28,IS,Qualitative
A learner perspective on the implementation of Geographic Information Systems in selected schools in KwaZulu-Natal province,"Contemporary literature reveals a plethora of studies on the implementation of Geographic Information System (GIS) in South Africa. However, these studies provide an educator’s perspective while neglecting the learner’s perspective, which is provided in this study. The researchers used a qualitative collective case study in the interpretivist paradigm. A purposive sample of five schools and 50 grade 12 learners was used in this study. The data were generated using observations, five focus group discussions and 10 interviews. This study established that the learners lacked fundamental knowledge of GIS and that GIS was not being properly taught.",https://www.semanticscholar.org/paper/b314bb70163f4b92ce9165af2ecf7ea08fbe0cee,IS,Qualitative
Iterative and Incremental Development Analysis Study of Vocational Career Information Systems,"Software development process presents various types of models with their corresponding phases required to be accordingly followed in delivery of quality products and projects. Despite the various expertise and skills of systems analysts, designers, and programmers, systems failure is inevitable when a suitable development process model is not followed. This paper focuses on the Iterative and Incremental Development (IID)model and justified its role in the analysis and design software systems. The paper adopted the qualitative research approach that justified and harnessed the relevance of IID in the context of systems analysis and design using the Vocational Career Information System (VCIS) as a case study. The paper viewed the IID as a change-driven software development process model. The results showed some system specification, functional specification of system and design specifications that can be used in implementing the VCIS using the IID model. Thus, the paper concluded that in systems analysis and design, it is imperative to consider a suitable development process that reflects the engineering mind-set, with heavy emphasis on good analysis and design for quality assurance.",https://www.semanticscholar.org/paper/67bbc1c7d6929b90718dd3b1a918affd38a36aba,IS,Qualitative
Accounting Information Systems Evaluation of Medicines Management,"Good management, supervision and control of drug supplies is very important in order to avoid loss and damage to drug supplies. The truth of accounting data and avoiding fraud can be detrimental to the hospital. This research is a type of research that is a case study at PT. Indofarma, Tbk for Drug Management at PT. Indofarma, Tbk. The purpose of this study was to determine the description and problem solving regarding information systems at PT. Indofarma, Tbk, especially the accounting information system for drug procurement and management. This study uses qualitative research with a case study method. Data obtained by conducting interviews, observations, documentation, and distributing questionnaires. The focus of this study is a drug inventory accounting system consisting of planning and determining procedures, procurement procedures, storage procedures, distribution procedures, elimination procedures and physical counting procedures. The results showed that the accounting information system for drug inventory at PT. Indofarma, Tbk was in accordance with the theoretical study. But there are still weaknesses, such as the trapping of the function of the warehouse which in addition to receiving and storing drugs. Suggested improvement is good control by related parties so that fraud does not occur which can harm at PT. Indofarma, Tbk.",https://www.semanticscholar.org/paper/09c6d158577f9538f544d661d98a430b4dd9f7b3,IS,Qualitative
"Strategic Planning Of Information Systems And Information Technology At Agricultural Research And Development Agency, Ministry Of Agriculture","The research was conducted at the Agency for Agricultural Research and Development (IAARD), Ministry of Agriculture. The purpose of this research is to do the strategic planning of IS / IT Balitbangtan to support Balitbangtan vision and mission, to align business strategy and IS / IT strategy, formulate IS / IT business strategy, IS / IT management strategy, and IT Strategy. This research uses qualitative approach using Ward & Peppard method. The result is five strategies that can be input for future Balitbangtan development. The five strategies are: (1) Utilizing the advantages of technological innovation to improve the competitiveness of Balitbangtan; (2) Utilizing the benefits of technological innovation to continuously manage-update database of agricultural research results;. Next are, (3) Increasing the number and skills of human resources to build a database on agricultural research results; (4) Exploring the utilization of VPNs to develop science and technology facilities from upstream to downstream; (5) Building infrastructure to improve e-office programs across the UK / UPT. There are finding sixteen IS/IT strategic planning for IAARD which should be applied to make proper governance process.",https://www.semanticscholar.org/paper/24512dc6b611d1aaa299032002a77cfb39d1959f,IS,Qualitative
Workaround behaviour in information systems research,"Purpose The purpose of this paper is to provide an analysis of the studies about workaround behavior in the Information Systems (IS) area, addressing its positive and negative aspects and raising the key related issues. Design/methodology/approach A systematic literature review was conducted to verify how workaround behavior has been approached in IS studies. A qualitative method was adopted and implemented in two stages: selection of articles from the time period 2007-2017 in the databases Web of Science, ScienceDirect and Scopus and analysis of the selected articles. Findings The results showed that many studies have been concerned with identifying the principal reasons for the manifestation of workaround behavior and the measures taken to reduce its impacts. Many studies from international journals examined the implementation of an ERP along with the workaround behavior. The need to expand the national studies on workaround behavior is emphasized, because the majority of the identified studies are international. Research limitations/implications The key limitation is related to the period of analysis, because only articles published since 2007 were selected. Practical implications This paper contributes to both theory and practice, bringing relevant concepts about workaround behavior, and corroborating the importance of the studies on workaround in the IS area. The literature review of the 20 articles analyzed reveals the main features in each article, such as theoretical and methodological aspects that support the research. Based on this analysis, a conceptual map was developed presenting the most relevant points about workaround behavior, where the causes, the negative and positive consequences, the types of solutions and the organizational and individual impacts are presented. Originality/value Research into workaround behavior has increased in recent years, however very few studies have been conducted in Brazil. To the best of the authors' knowledge, no articles regarding Brazil and this subject were published between 2007 and 2017. Thus, this paper seeks to redress this imbalance.",https://www.semanticscholar.org/paper/2c3350dfab30af94ee3cbff89a275ad9759170b8,IS,Qualitative
Information systems project teams: factors for high performance,"Purpose The purpose of this research is twofold: identify and gain a better insight on factors that can influence high performance of Information Systems (IS) project teams from the perspective of IS professionals (i.e. team members and leaders), and thus contribute to the general discussion on high-performance project teams; and offer both IS project team members and their project managers some feedback on how to build and manage teams more constructively and to enhance team performance in today’s demanding business environment. Design/methodology/approach The authors used an exploratory case study of a small-size holding company and a qualitative analysis of the data to address the research questions. Findings Results show a set of perceived factors that can influence high team performance in IS projects. Participants’ perceptions barely coincide. For instance, mutual trust was the only factor suggested as facilitating high team performance by 5 participants (out of 13). Differences may be because of participants’ characteristics (e.g. time on the job). All perceived factors are classified in the literature as nontechnical (i.e. having to do with behavioral and/or socio-organizational matters of project management). Originality/value This paper is among the very few empirical studies consolidating knowledge on high-performance IS project teams (e.g. it is still unclear if there are IS project team-specific factors that influence high performance). For the highly technical IS industry, this study came across human-centric factors transversal to different project teams.",https://www.semanticscholar.org/paper/0dc55c19a36323f077d96e724b12d5d3e2e71daa,IS,Qualitative
Adoption of Machine Learning Systems for Medical Diagnostics in Clinics: Qualitative Interview Study,"Background Recently, machine learning (ML) has been transforming our daily lives by enabling intelligent voice assistants, personalized support for purchase decisions, and efficient credit card fraud detection. In addition to its everyday applications, ML holds the potential to improve medicine as well, especially with regard to diagnostics in clinics. In a world characterized by population growth, demographic change, and the global COVID-19 pandemic, ML systems offer the opportunity to make diagnostics more effective and efficient, leading to a high interest of clinics in such systems. However, despite the high potential of ML, only a few ML systems have been deployed in clinics yet, as their adoption process differs significantly from the integration of prior health information technologies given the specific characteristics of ML. Objective This study aims to explore the factors that influence the adoption process of ML systems for medical diagnostics in clinics to foster the adoption of these systems in clinics. Furthermore, this study provides insight into how these factors can be used to determine the ML maturity score of clinics, which can be applied by practitioners to measure the clinic status quo in the adoption process of ML systems. Methods To gain more insight into the adoption process of ML systems for medical diagnostics in clinics, we conducted a qualitative study by interviewing 22 selected medical experts from clinics and their suppliers with profound knowledge in the field of ML. We used a semistructured interview guideline, asked open-ended questions, and transcribed the interviews verbatim. To analyze the transcripts, we first used a content analysis approach based on the health care–specific framework of nonadoption, abandonment, scale-up, spread, and sustainability. Then, we drew on the results of the content analysis to create a maturity model for ML adoption in clinics according to an established development process. Results With the help of the interviews, we were able to identify 13 ML-specific factors that influence the adoption process of ML systems in clinics. We categorized these factors according to 7 domains that form a holistic ML adoption framework for clinics. In addition, we created an applicable maturity model that could help practitioners assess their current state in the ML adoption process. Conclusions Many clinics still face major problems in adopting ML systems for medical diagnostics; thus, they do not benefit from the potential of these systems. Therefore, both the ML adoption framework and the maturity model for ML systems in clinics can not only guide future research that seeks to explore the promises and challenges associated with ML systems in a medical setting but also be a practical reference point for clinicians.",https://www.semanticscholar.org/paper/59454f24fa6e6539cc8cd9e75eeb029448d8c827,IS,Qualitative
Ethical Issues when Using Digital Platforms to Perform Interviews in Qualitative Health Research,"Qualitative studies have become increasingly common and have been used in different fields such as economics, politics, psychology, sociology and health research for different purposes. Instead of collecting numerical data, qualitative research’s goal is to gather information from participant’s experiences and perceptions. One of the tools to get data related to the participants’ experiences in qualitative research is through interviews. One tool that may be helpful for researchers today are digital and video platforms that fall under the domain of the internet-mediated research. The aim of this work is to identify and describe some ethical controversies when using videoconferencing platforms in qualitative health research for interview purposes. Four cases related to the use of digital platforms (videotelephony systems) to conduct interviews in qualitative research were discussed. Rather than give solutions, we reflect upon the possibility and plausibility of using these telecommunication technologies when using the technique of interviews in qualitative research. The ethical issues that may arise when using these technologies are related to privacy, confidentiality, accuracy of information and expertise when using the platforms. We think that the researcher is committed to making the best decisions in favor of the participant when using digital tools to gather information. In this regard, qualitative researchers may be benefited by the reflections we present insofar they may consider these possible scenarios that may rise ethical issues when collecting data. The activity of research needs to be escorted by constant ethical deliberations to protect participants’ rights during the collection of data phase.",https://www.semanticscholar.org/paper/ff0434e862f76cb7ec05d5db123c5bb987bfeb6a,IS,Qualitative
Public health information systems for primary health care in India: A situational analysis study,"Introduction: Information communication technology (ICT) based health information systems (HISs) are expected to transform health system functionality. The present study was aimed to evaluate HISs in India with a focus on primary health care (PHC). Methodology: The study used a qualitative method to evaluate and understand various ICT-based HIS implemented at the state/union territory (UT) level in India. After initial scoping research on HIS through literature search and observation, in-depth interviews of key informants at various levels (programme managers, analysts, co-ordinators, data entry operator and health care providers) was carried out to have an insight on the user experience of these systems. An inductive applied thematic coding of qualitative data was done for analysing the data. Results: Multiple applications have been developed under national health programmes to meet the health information needs, but at present, there is a limited role of these HISs in enhancing the effectiveness of comprehensive PHC. Many of these systems are proprietary-based, and the long-term sustainability and integration of these systems remain a challenge. Conclusion: A change is required in the approach to design a HIS that will cater to the needs of PHC. Moreover, HIS should be people-centred rather than technology-centric with focus on integration and sustainability.",https://www.semanticscholar.org/paper/b753f9a3e14dac1f73be9a831bf37fc7f14b1599,IS,Qualitative
A process model for implementing information systems security governance,"Purpose The frequent and increasingly potent cyber-attacks because of lack of an optimal mix of technical as well as non-technical IT controls has led to increased adoption of security governance controls by organizations. The purpose of this paper, thus, is to construct and empirically validate an information security governance (ISG) process model through the plan–do–check–act (PDCA) cycle model of Deming. Design/methodology/approach This descriptive research using an interpretive paradigm follows a qualitative methodology using expert interviews of five respondents working in the ISG domain in United Arab Emirates (UAE) to validate the theoretical model. Findings The findings of this paper suggest the primacy of the PDCA Deming cycle for initiating ISG through a risk-based approach assisted by industry-wide best practices in ISG. Regarding selection of ISG frameworks, respondents preferred to have ISO 27K supported by NIST as the core framework with other relevant ISG frameworks/standards forming the peripheral layer. The implementation focus of the ISG model is on mapping ISO 27K/NIST IT controls relevant IT controls selected from ISG frameworks from a horizontal and vertical perspective. Respondents asserted the automation of measurement and control mechanism through automation to assist in the feedback loop of the PDCA cycle. Originality/value The validated model helps academics and practitioners gain insight into the methodology of the phased implementation of an information systems governance process through the PDCA model, as well as the positioning of ITG and ITG frameworks in ISG. Practitioners can glean valuable insights from the empirical section of the research where experts detail the success factors, the sequential steps and justification of these factors in the ISG implementation process.",https://www.semanticscholar.org/paper/cd837e347adb4b0e109364bcd8eb73bed45c8cc4,IS,Qualitative
Exploring the viability of ChatGPT as an alternative to traditional chatbot systems in library and information centers,"Purpose The purpose of this study is to explore the feasibility of using ChatGPT-based chatbot systems as an alternative to traditional knowledge base-based chatbot systems in library and information centers (LICs). Design/methodology/approach The study used a qualitative research approach to examine the viability of ChatGPT-based chatbot systems in LICs. The researchers conducted an extensive literature review and analyzed data collected from interviews with experts in the field of library and information science. Findings The findings of the study reveal that ChatGPT-based chatbot systems are a viable alternative to traditional knowledge base-based chatbot systems in LICs. ChatGPT has the potential to provide more accurate and personalized responses to user queries, improve the user experience and reduce the workload of library staff. However, there are some limitations to the use of ChatGPT in LICs, such as the need for substantial training data and the risk of perpetuating biases. Originality/value To the best of the authors’ knowledge, this study is one of the first to explore the potential of ChatGPT-based chatbot systems in the context of LICs. The research provides insights into the advantages and limitations of ChatGPT-based chatbots and highlights the possibilities for future use of this technology in the information service industry.",https://www.semanticscholar.org/paper/caac37f8ca63d3682ec35ea8d0e9cfb4e7b48402,IS,Qualitative
Factors influencing organizational information systems implementation in Thai public universities,"Purpose This study aims to understand how public sector organizations can successfully implement organizational information systems (IS). It identifies the factors that contribute to the success of organizational IS implementation in public universities. Design/methodology/approach Both qualitative and quantitative research methods are used. The proposed research model is based on previous studies and primary qualitative research, including in-depth interviews, telephone surveys and mail surveys using semi-structured questionnaires to identify the determinants and measures of implementation success. Based on the first mail survey’s results, quantitative research is conducted to test the research hypotheses. The data are gathered from university personnel at 40 public universities, and the study focuses on the implementation of student registration systems. Findings The results suggest that successful implementation of organizational IS includes the decisions of both those in authority and users. The external and internal organization and individual user factors have direct relationships with the measure of implementation success, which suggests significant differences between authorities and users. Research limitations The analysis is based on the viewpoint of public university personnel; however, the findings suggest the need for further research on other public organizational IS as well as other public service operations. Practical implication The study clearly suggests a set of factors to guide public universities in successfully implementing organizational IS for local conditions of a developing country. Originality/value The study contributes to the understanding of effective IS implementation in public universities in a developing country.",https://www.semanticscholar.org/paper/f0d7e0f53ec2bb7e3dc8ff037c20fb61294d0b6d,IS,Qualitative
Develop Accounting Information Systems of Sales in Village-Owned Enterprise,"The purpose of this research is to develop a sale accounting information system in the Village-Owned Enterprises (BUM Desa Satia), especially sales which are source of income. Financial reporting standards use Indonesian Accounting Standards for Entities without Public Accountability (ETAP). This study used a descriptive qualitative study approach and method for developing sales accounting information systems in the Village Owned Enterprise using the Accelerated SAP (ASAP) approach. The results of this research in the development of accounting information systems using odoo accounting. Before implementing ERP using ODOO Accounting can make a blueprint for sales accounting information systems in the Village-Owned Company, which can be simplified by arranging the Odoo Module customization, to create financial reports. Developing a sales accounting information system is expected to help BUMI Satia Village in making transactions and making decisions.",https://www.semanticscholar.org/paper/89350fab68b43f1091b0eaf13b29ba939151168f,IS,Qualitative
Partial least square structural equation modellingâ' use in information systems,"The purpose of many studies in the field of Information Systems (IS) research is to analyse causal relationship between variables. Structural Equation Modelling (SEM) is a statistical technique for testing and estimating those causal relationship based on statistical data and qualitative causal assumption. Partial Least Square Structural Equation Modelling (PLS-SEM) is the technique that is mostly used in IS research. It has been subject to many reviews either in confirmatory or exploratory settings. However, it has recently emerged that PLS occupies the middle ground of exploratory and confirmatory settings. Thus, this paper intends to propose an updated guideline for the use of PLS-SEM in Information Systems Research in exploratory settings maintaining interpretability. A systematic literature review of 40 empirical and methodological studies published between 2012 and 2016 in the leading journal of the field guide Â future empirical work.",https://www.semanticscholar.org/paper/48cd61c74cdb2c46dd4b41346a72ddc6e13409a9,IS,Qualitative
Emerging scenarios of data infrastructure and novel concepts of digital libraries in intelligent infrastructure for human-centred communities: A qualitative research,"This research investigated the strategic development of a large-scale transdisciplinary area, named intelligent infrastructure for human-centred communities, at Virginia Tech. Within such development, this study explored the future vision and anticipated scenarios of data infrastructure and digital libraries for smart community development. It draws upon the mixed-methods approach combining ethnographic participant observation, document analysis and semi-structured interviews. Grounded in socio-technical framework and rooted in empirical methods, this research produces results that augment design thinking and visioning practice for digital data libraries beyond traditional boundaries. The findings reveal the emerging scenarios around complex adaptive systems, intelligent data infrastructure and future digital libraries all in the context of building infrastructure for human-centred communities. Situated in this advancing reality, the results further discuss the next-generation data and information user experience, smart infrastructure data environment and future library capabilities. The article concludes that a smart library system, whether in its conceptual form of a ‘digital octopus’ or a ‘smart village data hub’ or an ‘intelligent virtual assistant’, will provide intelligence in data gathering, processing, summarising, communication and recommendations. By delivering unified and personalised data solutions, it will offer an end-to-end seamless experience for users throughout their journey of knowledge pursuit.",https://www.semanticscholar.org/paper/de7eeee1bbccdfd47dee02de3fd0e2af51ebf2e5,IS,Qualitative
Literature Review of Qualitative Data with Natural Language Processing,"Qualitative research techniques are frequently employed by scholars in the field of social sciences when investigating communities and their communication media. The proliferation of computer-mediated communications has resulted in a substantial volume of textual content. However, the process of coding this vast amount of information necessitates significant time and effort. This article examines the potential for automating specific elements of content analysis through the utilization of natural language processing (NLP) systems, which analyze text in human languages, with a focus on extracting theoretical evidence. In this study, we present a case analysis utilizing NLP to examine the effectiveness of NLP rules in qualitative analysis. Our findings indicate that the NLP rules demonstrated strong performance across multiple codes. The utilization of a NLP system in its current developmental stage has the potential to significantly minimize the text volume, which has to be evaluated using the human coder. This reduction could potentially result in a substantial increase in coding speed, potentially by a factor of ten or more. The research is considered groundbreaking as it pioneers the application of advanced NLP approach to evaluate qualitative data, making it one of the earliest studies in this domain.",https://www.semanticscholar.org/paper/eb764d87d4d8c25a290fbbe83f922cda122ae1ef,CS,Qualitative
Inhibitors of Physicians' Use of Mandatory Hospital Information Systems (HIS),"Physicians' use of information systems continues to be a highly relevant area of information systems research. Although numerous studies have investigated the relationship between hospital physicians and hospital information systems (HIS), a comprehensive framework for assessing the factors which inhibit the use of HIS has yet to be developed. To advance these efforts, this exploratory research takes a qualitative approach to investigate HIS use inhibitors based on interviews with 48 informants and observations from over 40 hours of workplace shadowing in two German hospitals. The findings show that focusing solely on the user does not lead to successful system deployment in hospitals. Rather, the road to success needs to take a combined approach focusing on the user, the process and the system. If these three building blocks are well aligned, correspond with the organizational context and are supported by strong leadership, medical personnel are much more satisfied with the HIS, which results in more effective use.",https://www.semanticscholar.org/paper/a9c4ea825eac5d2ff2bcc309f835216bf78e40ee,IS,Qualitative
Expectations and attitudes towards medical artificial intelligence: A qualitative study in the field of stroke,"Introduction Artificial intelligence (AI) has the potential to transform clinical decision-making as we know it. Powered by sophisticated machine learning algorithms, clinical decision support systems (CDSS) can generate unprecedented amounts of predictive information about individuals’ health. Yet, despite the potential of these systems to promote proactive decision-making and improve health outcomes, their utility and impact remain poorly understood due to their still rare application in clinical practice. Taking the example of AI-powered CDSS in stroke medicine as a case in point, this paper provides a nuanced account of stroke survivors’, family members’, and healthcare professionals’ expectations and attitudes towards medical AI. Methods We followed a qualitative research design informed by the sociology of expectations, which recognizes the generative role of individuals’ expectations in shaping scientific and technological change. Semi-structured interviews were conducted with stroke survivors, family members, and healthcare professionals specialized in stroke based in Germany and Switzerland. Data was analyzed using a combination of inductive and deductive thematic analysis. Results Based on the participants’ deliberations, we identified four presumed roles that medical AI could play in stroke medicine, including an administrative, assistive, advisory, and autonomous role AI. While most participants held positive attitudes towards medical AI and its potential to increase accuracy, speed, and efficiency in medical decision making, they also cautioned that it is not a stand-alone solution and may even lead to new problems. Participants particularly emphasized the importance of relational aspects and raised questions regarding the impact of AI on roles and responsibilities and patients’ rights to information and decision-making. These findings shed light on the potential impact of medical AI on professional identities, role perceptions, and the doctor-patient relationship. Conclusion Our findings highlight the need for a more differentiated approach to identifying and tackling pertinent ethical and legal issues in the context of medical AI. We advocate for stakeholder and public involvement in the development of AI and AI governance to ensure that medical AI offers solutions to the most pressing challenges patients and clinicians face in clinical care.",https://www.semanticscholar.org/paper/8422999b656d173b9da25aec673c87368074f0cd,IS,Qualitative
Exploring the Experiences of Family Caregivers of Children With Special Health Care Needs to Inform the Design of Digital Health Systems: Formative Qualitative Study,"Background Family caregivers of children with special health care needs (CSHCN) are responsible for managing and communicating information regarding their child’s health in their homes. Although family caregivers currently capture information through nondigital methods, digital health care applications are a promising solution for supporting the standardization of information management in complex home care across their child’s health care team. However, family caregivers continue to use paper-based methods where the adoption of digital health care tools is low. With the rise in home care for children with complex health care needs, it is important to understand the caregiving work domain to inform the design of technologies that support child safety in the home. Objective The aim of this study is to explore how family caregivers navigate information management and communication in complex home care for CSHCN. Methods This research is part of a broader study to explore caregivers’ perspectives on integrating and designing digital health care tools for complex home care. The broader study included interviews and surveys about designing a voice user interface to support home care. This formative study explored semistructured interview data with family caregivers of CSHCN about their home care situations. Inductive thematic analysis was used to analyze the information management and communication processes. Results We collected data from 7 family caregivers in North America and identified 5 themes. First, family caregivers were continuously learning to provide care. They were also updating the caregiver team on their child’s status and teaching caregivers about their care situation. As caregiving teams grew, they found themselves working on communicating with their children’s educators. Beyond the scope of managing their child’s health information, family caregivers also navigated bureaucratic processes for their child’s home care. Conclusions Family caregivers’ experiences of caring for CSHCN differ contextually and evolve as their child’s condition changes and they grow toward adulthood. Family caregivers recorded information using paper-based tools, which did not sufficiently support information management. They also experienced significant pressure in summarizing information and coordinating 2-way communication about the details of their child’s health with caregivers. The design of digital health care systems and tools for complex home care may improve care coordination if they provide an intuitive method for information interaction and significant utility by delivering situation-specific insights and adapting to unique and dynamic home care environments. Although these findings provide a foundational understanding, there is an opportunity for further research to generalize the findings.",https://www.semanticscholar.org/paper/ca65877928a72031500a2e0e414b56acc093d460,IS,Qualitative
A qualitative analysis of the feasibility of deploying biometric authentication systems to augment security protocols of bank card transactions,"Background: This study investigated the end-users’ perceptions about the feasibility of deploying biometric authentication systems as intervention solutions to ameliorate card fraud in the South African payment card industry. Objectives: The objective of the study was to determine if the existing banking technology and telecommunications infrastructure were capable of supporting biometric payment systems. Method: In this qualitative research, interviews were conducted with 30 sample elements selected from commercial banks, telecommunications companies and other service providers. The sample included individuals working with banking back-end technologies, telecommunications service providers and credit card fraud experts. Results: The study established that banking technology and telecommunications infrastructure were capable of supporting biometric payment systems. It was revealed that the deployment of biometric systems would mitigate card fraud transactions. However, findings showed that introduction of zero-floor limits led to high traffic volumes, creating congestion on the telecommunications connectivity. Thus, there was a high likelihood that transaction processes would be slow during peak periods. Conclusion: The implementation of biometric systems required highly skilled information technology personnel to oversee and support these technologies. This was identified as a potential hindrance for banks. The study established that existing banking and telecommunications infrastructure was capable and supported biometric systems. Banks were not keen to invest in an outright biometric environment because of the huge costs that would be incurred in implementing advanced technologies.",https://www.semanticscholar.org/paper/9c2658ac9be948f86f42f733409e6690c5b65353,IT,Qualitative
Toward a metadata model for research information management systems,"Purpose The purpose of this paper is to examine the relationships between researcher characteristics and their use of metadata in their ResearchGate profiles. Design/methodology/approach This paper reports on one part of a larger study that examined researchers’ use of and engagement with research information management systems (RIMSs). The study’s design included qualitative, semi-structured interviews with 15 researchers and a survey completed by 412 researchers. Detailed reports of findings from the interviews and survey can be found elsewhere. This paper reports on the part of the study that analyzed the use of metadata elements in the ResearchGate profiles of 126 survey participants. Findings Most researchers shared metadata related to their research rather than their teaching or service. Statistical analyses revealed statistically significant relationships between researchers’ metadata use and their participation levels in RIMSs, as well as between metadata use and researchers’ seniority. Originality/value The study’s findings help to identify researchers’ priorities for different metadata elements, as well as to construct profile metadata templates for each specific participation level.",https://www.semanticscholar.org/paper/775e56039acf70359335211cfc74da8beac1fba2,IS,Qualitative
Virtual Reality Systems as an Orientation Aid for People Who Are Blind to Acquire New Spatial Information,"This research aims to examine the impact of virtual environments interface on the exploration process, construction of cognitive maps, and performance of orientation tasks in real spaces by users who are blind. The study compared interaction with identical spaces using different systems: BlindAid, Virtual Cane, and real space. These two virtual systems include user-interface action commands that convey unique abilities and activities to users who are blind and that operate only in these VR systems and not in real space (e.g., teleporting the user’s avatar or pointing at a virtual object to receive information). This research included 15 participants who are blind, divided into three groups: a control group and two experimental groups. Varied tasks (exploration and orientation) were used in two virtual environments and in real spaces, with both qualitative and quantitative methodologies. The results show that the participants were able to explore, construct a cognitive map, and perform orientation tasks. Participants in both virtual systems used these action commands during their exploration process: all participants used the teleport action command to move their avatar to the starting point and all Virtual Cane participants explored the environment mainly by using the look-around mode, which enabled them to collect spatial information in a way that influenced their ability to construct a cognitive map based on a map model.",https://www.semanticscholar.org/paper/8aebed0da2d7f9df73f0a428e7b9b373e407cb98,CS,Qualitative
Researching the virtual: A framework for reflexivity in qualitative social media research,"Recent years have seen an explosion in social media in our everyday lives, and a corresponding increase in social media research in IS. As social media researchers, we are intrigued by the problem of virtuality and context in social media research, and how we might apply reflexive research principles to such settings. In social media, the absence of a setting's real physical boundaries (to a large extent) limits participants' ability to create a common experience at the present time and develop a history of shared experiences. As a result, we would contend that many social media researchers' interpretations of data in social media settings are often black‐boxed. In this paper, we argue that many of the challenges concerned with social media settings, by nature, are emergent and linked to their virtual and contextual features. We use the Klein and Myers (1999) framework for traditional interpretive field studies as a vehicle for unpacking these challenges. We contend that these challenges may remain unnoticed if researchers do not actively reflect upon their impact on the research process. In this paper, we present a framework for social media research, considering social media research as a reflexive space, building on the notion of three levels of reflexivity: theory, design and practice. Finally, we discuss some implications of reflexivity for qualitative social media research in IS.",https://www.semanticscholar.org/paper/55c3b560a8da2da97651c27822b6d406370b4181,IS,Qualitative
"Webs of Care: Qualitative GIS Research on Aging, Mobility, and Care Relations in Singapore","The connections between time and space have been studied considerably in quantitative and qualitative research on the geographies of care; however, researchers tend to prioritize one approach over the other. Our article integrates analyses of activity spaces and space–time paths with conceptualizations of care developed in qualitative studies to deepen understanding of the spatiotemporal care routines of older adults in Singapore. Using qualitative geographic information systems (QualiGIS), we develop a comparative frame to understand the differential sociopolitical logics that shape the care routines of older adults who are Singapore citizens and nonmigrants vis-à-vis temporary migrants from the People’s Republic of China. Building on recent writing about care assemblages, we conceptualize webs of care as the interlinked chains, in and through which individuals—in this case, older adults—exercise agency to give and receive care in the context of political, institutional, and social structures. Our article shows how the relational thinking of assemblage theory can inform geographic information systems analyses of activity space and space–time paths. We advance research on care assemblages by identifying in actual time–space how various resources within care assemblages (or the lack of) can diminish or enhance the capacity and resilience of older adults to give and receive care.",https://www.semanticscholar.org/paper/6ca61244d49060570e2055babc0d64d243521747,IS,Qualitative
Gap between real-world data and clinical research within hospitals in China: a qualitative study,"Objective To investigate the gap between real-world data and clinical research initiated by doctors in China, explore the potential reasons for this gap and collect different stakeholders’ suggestions. Design This qualitative study involved three types of hospital personnel based on three interview outlines. The data analysis was performed using the constructivist grounded theory analysis process. Setting Six tertiary hospitals (three general hospitals and three specialised hospitals) in Beijing, China, were included. Participants In total, 42 doctors from 12 departments, 5 information technology managers and 4 clinical managers were interviewed through stratified purposive sampling. Results Electronic medical record data cannot be directly downloaded into clinical research files, which is a major problem in China. The lack of data interoperability, unstructured electronic medical record data and concerns regarding data security create a gap between real-world data and research data. Updating hospital information systems, promoting data standards and establishing an independent clinical research platform may be feasible suggestions for solving the current problems. Conclusions Determining the causes of gaps and targeted solutions could contribute to the development of clinical research in China. This research suggests that updating the hospital information system, promoting data standards and establishing a clinical research platform could promote the use of real-world data in the future.",https://www.semanticscholar.org/paper/32b64a216c11ed6d1230027af74e053432090b20,IS,Qualitative
International Benchmarking of Teacher Training Programs: Lessons Learned from Diverse Education Systems,"Effective teacher training programs are critical to the quality of education systems worldwide. Comparison of teacher training programs between countries, known as international benchmarking, provides valuable insight into best practices and areas for improvement. This research aims to conduct an international benchmarking study to examine and compare teacher training programs in various educational systems. The research methodology chosen for this study was a Systematic Literature Review (SLR), which was used to understand the basic principles of Islamic education and their practical application in the Indonesian context. The data used in the SLR approach comes from scientific literature found in academic databases and other reliable sources of information. The analytical process in this research includes a thorough and systematic synthesis of related literature using a qualitative approach. The findings of this research reveal the diverse landscape of teacher training programs in various countries. Successful programs often emphasize practical classroom experience, ongoing professional development, and alignment with the specific needs of students and society. Challenges include limited resources, variations in program quality, and adaptation to rapidly evolving educational technologies and pedagogical approaches. This research underscores the importance of international benchmarking to inform and improve teacher training programs worldwide. Lessons from diverse education systems can guide policymakers, educators, and stakeholders in designing more effective and responsive teacher training initiatives. By promoting best practices and addressing common challenges, this research contributes to improving the overall quality of teacher education and, consequently, the quality of education itself in diverse global contexts.",https://www.semanticscholar.org/paper/f41614575550a23bcf11a6769f73e93ca53d1431,IS,Qualitative
Information and communication technology utilization effectiveness in distance education systems,"Learning by using online application facilities through the Internet is a new service for all users. However, there are many problems and obstacles faced by users, both by students and by lecturers in utilizing online application services via the Internet. The development of a distance education service management software application model provides facilities for academics to facilitate interaction between students and lecturers with using online-based information technology communication services through the Internet. In addition, the management of distance education services is able to provide reports desired by students and lecturers using online-based information technology communication services through the Internet. This study used a descriptive–analytic method by presenting a summary of interviews and survey results in the form of a questionnaire to the faculty member. The method used is a qualitative method because it provides an explanation using analysis. This research uses Moodle application as a Distance Education System. The distance learning model used in the Trilogi University is considered sufficient, in terms of learning goals, learning materials, interactivity, and rules. The results also show that both lecturers and students argue that learning with a distance education system can simplify work, speed up work, accurate work, and be more efficient because it is interactive and user-friendly. This research can be applied to other school units or universities so that distance education services can help academics interact easily, quickly, and accurately. For its development, it can be applied using cellular-based applications.",https://www.semanticscholar.org/paper/cf02e9e3b0546bea013d0662614133a974942865,CS,Qualitative
Barriers and facilitators to data quality of electronic health records used for clinical research in China: a qualitative study,"Objectives There is an increasing trend in the use of electronic health records (EHRs) for clinical research. However, more knowledge is needed on how to assure and improve data quality. This study aimed to explore healthcare professionals’ experiences and perceptions of barriers and facilitators of data quality of EHR-based studies in the Chinese context. Setting Four tertiary hospitals in Beijing, China. Participants Nineteen healthcare professionals with experience in using EHR data for clinical research participated in the study. Methods A qualitative study based on face-to-face semistructured interviews was conducted from March to July 2018. The interviews were audiorecorded and transcribed verbatim. Data analysis was performed using the inductive thematic analysis approach. Results The main themes included factors related to healthcare systems, clinical documentation, EHR systems and researchers. The perceived barriers to data quality included heavy workload, staff rotations, lack of detailed information for specific research, variations in terminology, limited retrieval capabilities, large amounts of unstructured data, challenges with patient identification and matching, problems with data extraction and unfamiliar with data quality assessment. To improve data quality, suggestions from participants included: better staff training, providing monetary incentives, performing daily data verification, improving software functionality and coding structures as well as enhancing multidisciplinary cooperation. Conclusions These results provide a basis to begin to address current barriers and ultimately to improve validity and generalisability of research findings in China.",https://www.semanticscholar.org/paper/2bf00e9d67de53e950fb118ddaf6f3a4c7b510df,CS,Qualitative
Investigating the influence of information complexity on construction quality: a systems thinking approach,"PurposeThis paper investigates the causative relation between information complexity and construction quality. The objectives are to identify the challenges faced in the communication of information and data on construction projects and determine how these barriers influence quality performance of a project.Design/methodology/approachLimitations in the literature on information seeking, coupled with the utilization of Theory of Motivated Information Management as a response led to the development of research problem statement. Through comprehensive review of literature published between the years 2005–2019, the paper first identified barriers of information communication in the industry and key performance indicators of construction quality, followed by content analyses to rank and screen out the least important variables. This paved the way for quantitative and qualitative analyses through 180 structured questionnaires and 11 expert opinion sessions that respectively led to the use of systems thinking approach to establish causality.FindingsThe study finds ineffective communication, unclear details, changes in contract, information delays, unpleasant relationships between stakeholders and project complexity as the most critical factors causing information complexity in a construction project. These factors cause a reduction in quality performance of a project through increased rework and decreased implementation of total quality management.Practical implicationsThe study will assist project managers to diagnose quality-related issues in their projects, trace them back to the challenges and barriers of information communication and then make policy/strategic interventions to reduce such issues through improved information flow.Originality/valueMajority of the studies in this field of work have identified the effects of ineffective information flow on overall project performance, but very few have focused specifically on the quality of construction projects.",https://www.semanticscholar.org/paper/5ecf929819f9dc818459beb52cad18014d81286d,IS,Qualitative
The role of feedback literacy in written corrective feedback research: from feedback information to feedback ecology,"Abstract Research on written corrective feedback (WCF) has received sustained interest in the field of second language acquisition and language education. This viewpoint article extends the theoretical and conceptual discussion on WCF research by introducing the notions of “feedback literacy” and “feedback ecology”. In this article, I first review three strands of WCF research. Then, I argue for the need to shift the focus of our investigation from feedback information (focusing on impact of feedback), to feedback process (focusing on learners “and teachers” perception of feedback), to feedback ecology (focusing on learners’ and teachers’ engagement of feedback and influences of such engagement). Putting forward a “feedback ecology” conceptual framework that is informed by Ecological Systems Theory, Actor-network Theory, and Complex Dynamic Systems Theory, I suggest three research tasks for future WCF studies, highlighting the affordances of qualitative research methodologies such as narrative inquiry.",https://www.semanticscholar.org/paper/94170c783d5d7ff21fac44f5f72a59b7b07007c7,CS,Qualitative
One country with two systems: The characteristics and development of higher education in the Guangdong–Hong Kong–Macau Greater Bay Area,"The creation of bay areas is one way to develop economies and culture based on natural characteristics and regional connections; successful examples include the San Francisco, New York and Tokyo Bay Areas. In 2019, China established the Guangdong–Hong Kong–Macau Greater Bay Area (GBA). As a result of historical and geographical factors, the GBA is uniquely characterised by being subject to ‘one country, two systems’, ‘three customs territories’, and ‘three legal systems’. This study offered ample empirical evidence based on qualitative methods referring to in-depth interviews with academics and managers as well as publicly available policies and literature in the GBA. A thematic analysis was used to explore the context and characteristics of developing higher education (HE) in the GBA. The study emphasised that developing GBA’s HE improved its partnerships from co-operation to strategic co-ordination to resource sharing. This research contributes to HE and its governance in the GBA, a topic on which there is limited information in the extant international research literature. It is also useful to policymakers and scholars as it provides potential strategies and insights regarding the development of regional higher education.",https://www.semanticscholar.org/paper/b30d2dbcb3aba99e6b2c428e341cec1a21a630ea,IS,Qualitative
Aligning the Qualitative Comparative Analysis (QCA) counterfactual approach with the practice of retroduction: Some preliminary insights,"This study offers fresh ontological insights by examining generative causality through the Qualitative Comparative Analysis (QCA) counterfactual lens, in conjunction with Critical Realism and the practice of retroduction. Specifically, it claims that Information Systems (IS) researchers could retroduce generative mechanisms by leveraging the QCA counterfactual approach to causation because retroduction is about conjecturing hypothetical mechanisms that would generate the outcome of interest in a counterfactual fashion. Drawing on an example of typological theorising, this study calls for a renewed effort in the use of retroduction in the study of IS phenomena. In addition, this study sheds new light on the overarching approach for conducting Critical Realist (case study) research. A number of theoretical, methodological, and practical implications are discussed.",https://www.semanticscholar.org/paper/1e2813e427b527c54d01a1de5f69a13296aff06f,IS,Qualitative
Evaluation of Complexity Issues in Building Information Modeling Diffusion Research,"This study aimed to ascertain the research status of complexity issues in building information modeling (BIM) diffusion and identify future research directions in this field. A total of 366 relevant journal articles were holistically evaluated. The visualization analysis indicated that management aspects, emergent trends (such as green building, facility management, and automation), and theme clusters (such as interoperability, waste management, laser scanning, stakeholder management, and energy efficiency) are shaping BIM research towards complexity. Areas such as supply chain, cost, digital twin, and web are also essential. The manual qualitative evaluation classified the complexity issues in BIM diffusion research into three types (complexities of network-based BIM evolution, impact of BIM adoption circumstances, and BIM-based complexity reduction for informed decision making). It was concluded that BIM has been shifting towards information models and systems-based life cycle management, waste control for healthy urban environments, and complex data analysis from a big data perspective, not only in building projects but also in heritage and infrastructure, or at the city scale, for informed decision making and automatic responses. Future research should investigate the co-evolution between collaborative networks and BIM artefacts and work processes, quality improvement of BIM-based complex networks, BIM post-adoption behaviors influenced by complex environmental contexts, and BIM-based complexity reduction approaches.",https://www.semanticscholar.org/paper/363b0b51348f941fe5b84549dc2ac356f96b5758,IS,Qualitative
Development of Grounded Theory in Social Sciences: A Qualitative Approach,"Grounded theory (GT) is a general research method that provides the efficient generation of theory from data, which are collected by a strong, sound, and fair research method. It is an inductive methodology that systematically collects and analyzes data for developing theory on human behavior in social welfare perspectives. It is considered as one of the most popular qualitative research methodologies in the world. It is originally developed by two American sociologists Barney Galland Glaser and Anselm Leonard Strauss in 1967 through the publication of their revolutionary book The Discovery of Grounded Theory. It emphasizes the importance of developing an understanding of human behavior through a process of discovery. Grounded theory has originated in sociology, and at present it has become a key methodological setting in a wide range of other disciplines, such as in nursing, physiotherapy, healthcare, education, anthropology, psychology, management, information systems, software engineering, etc. It is useful both for expert and novice researchers to generate new explanatory theories. This article tries to discuss grounded theory methodology through the discussion of its origin and development, basic principles, characteristics, advantages and disadvantages, and its usefulness in social science researches for qualitative analysis.",https://www.semanticscholar.org/paper/2bd303230f43104843ca38b492045595231a3d5b,CS,Qualitative
"Model Error, Information Barriers, State Estimation and Prediction in Complex Multiscale Systems","Complex multiscale systems are ubiquitous in many areas. This research expository article discusses the development and applications of a recent information-theoretic framework as well as novel reduced-order nonlinear modeling strategies for understanding and predicting complex multiscale systems. The topics include the basic mathematical properties and qualitative features of complex multiscale systems, statistical prediction and uncertainty quantification, state estimation or data assimilation, and coping with the inevitable model errors in approximating such complex systems. Here, the information-theoretic framework is applied to rigorously quantify the model fidelity, model sensitivity and information barriers arising from different approximation strategies. It also succeeds in assessing the skill of filtering and predicting complex dynamical systems and overcomes the shortcomings in traditional path-wise measurements such as the failure in measuring extreme events. In addition, information theory is incorporated into a systematic data-driven nonlinear stochastic modeling framework that allows effective predictions of nonlinear intermittent time series. Finally, new efficient reduced-order nonlinear modeling strategies combined with information theory for model calibration provide skillful predictions of intermittent extreme events in spatially-extended complex dynamical systems. The contents here include the general mathematical theories, effective numerical procedures, instructive qualitative models, and concrete models from climate, atmosphere and ocean science.",https://www.semanticscholar.org/paper/f3bde96b6b835b2054a50f5028f0d1464be38cff,CS,Qualitative
Novel methods of qualitative analysis for health policy research,"BackgroundCurrently, thanks to the growing number of public database resources, most evidence on planning and management, healthcare institutions, policies and practices is becoming available to everyone. However, one of the limitations for the advancement of data and literature-driven research has been the lack of flexibility of the methodological resources used in qualitative research. There is a need to incorporate friendly, cheaper and faster tools for the systematic, unbiased analysis of large data corpora, in particular regarding the qualitative aspects of the information (often overlooked).MethodsThis article proposes a series of novel techniques, exemplified by the case of the role of Institutional Committees of Bioethics to (1) massively identify the documents relevant to a given issue, (2) extract the fundamental content, focusing on qualitative analysis, (3) synthesize the findings in the published literature, (4) categorize and visualize the evidence, and (5) analyse and report the results.ResultsA critical study of the institutional role of public health policies and practices in Institutional Committees of Bioethics was used as an example application of the method. Interactive strategies were helpful to define and conceptualise variables, propose research questions and refine research interpretation. These methods are additional aids to systematic reviews, pre-coding schemes and construction of a priori diagrams to survey and analyse social science literature.ConclusionsThese novel methods have proven to facilitate the formulation and testing of hypotheses on the subjects to be studied. Such tools may allow important advances going from descriptive approaches to decision-making and even institutional assessment and policy redesign, by pragmatic reason of time and costs.",https://www.semanticscholar.org/paper/864bfa0f9660be06454e8204ce368c2019a3ab86,IS,Qualitative
"Perspectives, Experiences, and Practices in the Use of Digital Information Technologies in the Management of Depression and Alcohol Use Disorder in Health Care Systems in Colombia","Digital information technologies are increasingly used in the treatment of mental health disorders. Through this qualitative study, researchers illuminated perspectives, experiences, and practices among diverse stakeholders in the use of digital information technologies in the management of depression and alcohol use disorders in Colombia. In-depth interviews and focus groups were conducted in five primary care institutions across Colombia. Thematic analysis was used to analyze the data. The use of technology in the treatment of mental health disorders can facilitate the evaluation and diagnosis, treatment, and promotion and prevention of mental health disorders, as well as multiple nonmental health applications in the primary care setting. Potential barriers to the use of technology in this setting include challenges of digital literacy, access to technology, confidentiality, and financing. This study can inform the implementation of digital information technologies in the care of depression and problematic alcohol use within health care systems in Colombia.",https://www.semanticscholar.org/paper/0792edca0d807a1b09452afbcc277c49c5180a87,IS,Qualitative
Measuring optimization of digital military programs: an innovation of information and communication systems in industrial digitalization 4.0,"This research examines military digital optimization as an information and communication system innovation in the industrial digitalization era 4.0. Advances in information and communication technology especially on the industrial revolution 4.0. The real impact is seen in several aspects of human- life. The industrial revolution 4.0 also provided a change in the government system into good governance. Technological advances also have an impact on the defense system or military system in Indonesia, especially for the Indonesian National Army, especially the (TNI AD) through the use of an e-military application system to facilitate the search for internal information related to TNI personnel. The research method is descriptive qualitative with a sampling technique that is purposive sampling. Kodam IX / Udayana, Denpasar, Bali as a location of this study. Data collection uses a structured interview method to a number of informants who have been willing to engage a number of 13 people.",https://www.semanticscholar.org/paper/c6395ceb922e01f6b5817f58956c5d1a8658ca32,IS,Qualitative
Healthcare leaders’ use of innovative solutions to ensure resilience in healthcare during the Covid-19 pandemic: a qualitative study in Norwegian nursing homes and home care services,"Background The Covid-19 pandemic introduced a global crisis for the healthcare systems. Research has paid particular attention to hospitals and intensive care units. However, nursing homes and home care services in charge of a highly vulnerable group of patients have also been forced to adapt and transform to ensure the safety of patients and staff; yet they have not received enough research attention. This paper aims to explore how leaders in nursing homes and home care services used innovative solutions to handle the Covid-19 pandemic to ensure resilient performance during times of disruption and major challenges. Methods A qualitative exploratory case study was used to understand the research question. The selected case was a large city municipality in Norway. This specific municipality was heavily affected by the Covid-19 pandemic; therefore, information from this municipality allowed us to gather rich information. Data were collected from documents, semi-structured interviews, and a survey. At the first interview phase, informants included 13 leaders, Head of nursing home (1 participant), Head of Sec. (4 participants), Quality manager (4 participants), Head of nursing home ward (3 participants), and a Professional development nurse (1 participant), at 13 different nursing homes and home care services. At the second phase, an online survey was distributed at 16 different nursing homes and home care services to expand our understanding of the phenomenon from other leaders within the case municipality. Twenty-two leaders responded to the survey. The full dataset was analysed in accordance with inductive thematic analysis methodology. Results The empirical results from the analysis provide a new understanding of how nursing homes and home care leaders used innovative solutions to maintain appropriate care for infected and non-infected patients at their sites. The results showed that innovative solutions could be separated into technology for communication and remote care, practice innovations, service innovations, and physical innovations. Conclusion This study offers a new understanding of the influence of crisis-driven innovation for resilience in healthcare during the Covid-19 pandemic. Nursing home and home care leaders implemented several innovative solutions to ensure resilient performance during the first 6–9 months of the pandemic. In terms of resilience, different innovative solutions can be divided based on their influence into situational, structural, and systemic resilience. A framework for bridging innovative solutions and their influence on resilience in healthcare is outlined in the paper.",https://www.semanticscholar.org/paper/a5138c0339a24d20ea2556dba698ec2a9491a101,CS,Qualitative
Models of Information Technology Use: Meta-Review and Research Directions,"ABSTRACT The considerable body of literature on information technology (IT) use is grounded in different theoretical perspectives and the models have seemingly increased the variance explained in the two major dependent constructs – behavioral intention (BI) and system use (SU). However, not all effects on BI and SU have been significant and the models of IT use have been examined in different contexts. This study conducts a two-part assessment of 35 models of IT use including a qualitative assessment of the model constructs using the STEP (System, Task, Environment, Person) framework and a quantitative assessment of the differences in model effectiveness across models of IT use. Results showed no statistical differences in the proportion of significant effects on BI and SU, but significant differences in the proportion of variance explained in BI and SU. Implications and future directions for research are proposed.",https://www.semanticscholar.org/paper/507fb0cd11030e0522f13aae8194ea2d1815ff2d,CS,Qualitative
Commercial urban agriculture in Florida: a qualitative needs assessment,"Abstract The global trend of urbanization coupled with an increasing awareness of the importance of food systems resilience, has led to an increasing interest in urban agriculture to sustainably feed the rapidly growing urban population and mitigate against food supply chain disruptions. While home and community gardens have been long studied, there has been relatively little empirical research focused specifically on commercial urban agriculture (CUA) operations. The purpose of this study was to characterize commercial urban farms, and to identify their primary barriers to business development and expansion, their perceptions of future opportunities, and their specific informational needs. Because CUA has received relatively less attention in previous empirical research, a qualitative approach was used for this needs assessment to collect rich, contextualized information to help differentiate the specific barriers, opportunities and needs of CUA operations as opposed to their rural counterparts. We conducted semi-structured interviews (n = 29) of CUA producers in Florida. These interviews revealed that CUA operations face many of the same barriers that are common to establishing and growing small farms, with additional barriers due to local government regulations and tensions associated with farming on land that is not traditionally used for agriculture. Despite these difficulties, CUA operators believe their urban location is a key benefit to their operation and they see a variety of opportunities for future business and market expansion.",https://www.semanticscholar.org/paper/6d244d5decf07648f4543d352f12ee7230b7eeaf,IS,Qualitative
Evaluating the Financial Returns on Investment in Sustainable Enterprise Digitalization Initiatives Using AI Information System,"With a focus on sustainability and IT changes, the companies even find an AI based system information system implementation in operations. This study analyzes the financial yield of return on investment (ROI) in the digitalization of leading edge enterprises, as well as finds out how AI information systems could achieve prosperity in both the economic and ecological spheres. The research utilizes a mixed-method methodology, combining the quantitative analysis of business finances from companies that have implemented intelligent environmental activities with qualitative case studies. Credit factors like ROI, NPV (net present value) and IRR (internal rate of return) together with environmental performance parameters are evaluated. Furthermore, by conducting surveys and interviews with trusted experts in the industry, the quantitative data is richer within it giving strategic and operational implications of the current initiatives. A study has been found that companies using AI for sustainability purposes are capable of returning very large financial results, much greater than the ones achieved by other digitalization initiatives. The study demonstrates that AI data systems boost efficiency, cut costs, and enhance market competitiveness. It highlights ecological benefits like reduced carbon footprints, efficient resource use, higher profits, and regulatory compliance. It guides stakeholders on AI investment for future ROI, advocating more AI technology funding. The research calls for an integrative approach that includes financial and sustainability measures to capture value. Incorporating AI in sustainable and eco-based digitalization is crucial for optimal financial and environmental outcomes. The organizations strategically utilizing the loop technology are also believed to have an advantage over others with regards to the current market landscape and will be able to make a balance between profitability and sustainable goals.",https://www.semanticscholar.org/paper/62c7f8a89433b715bccd75d19a0ef5d42985a4c9,IS,Qualitative
Using Simple and Complex Mixed Methods Research Designs to Understand Research in Information Science,"Mixed methods research combines qualitative and quantitative research approaches to describe multiple realities. After identifying a research problem and concluding that it can be comprehensively addressed by collecting quantitative and qualitative data concurrently or in phases, a researcher may choose to either use simple or advanced mixed methods designs. Studies have demonstrated that mixed methods research is not commonplace in library and information science research. The two-eyed seeing principles are given as an example of how indigenous theories and knowledge systems can be combined with Anglo-Saxon philosophical assumptions that dominate the mixed methods research movement to facilitate the production of knowledge that is contextually relevant and useful to the indigenous environment.",https://www.semanticscholar.org/paper/0bcd47ec3c146ee26889cc50dcd4432e5a3c11d0,IT,Qualitative
The Role of Sales Accounting Information System in PT. Petro Gasindo Intiniaga,"The role of sales accounting information systems in a company is very important Importantly, having a sales accounting information system can maximize profits obtained, achieve sales volume, and support sales growth. So that If the company achieves maximum sales levels, the company needs to pay attention sales system in the company. This research focuses on (1) How the sales accounting information system is implemented at PT. Petro Gasindo Intiniaga, (2) What is the internal control system at PT. Petro Gasindo Intiniaga (3) What is the role of the sales information system at PT. Petro Gasindo Intiniaga to improve the internal control system. This research uses a qualitative approach with a descriptive research type. The data sources used are Primary Data and Secondary Data. Research procedure The data used is field search (a) interviews (b) documentation (c) Observation. To check the validity of the data findings, three triangulations were carried out (1) source triangulation (2) technique triangulation (3) time triangulation. The result of this research is an internal control system in Information Systems Sales Accounting at PT. Petro Gasindo Intiniaga is not doing well. In the The company still has several weaknesses, including; (1) There still exists a dual function in the sales system, between the warehouse helper function and admin functions which are only carried out by the warehouse department. (2) The Authorization System for Approval for Delivery of goods is still authorized by the production and warehouse departments. (3) Deep sales system in which the cash received is not fully deposited into the bank on the date the transaction takes place.",https://www.semanticscholar.org/paper/68dc236121f0321f8d570036db50f03752c71b8e,IS,Qualitative
Scaling up population health interventions from decision to sustainability – a window of opportunity? A qualitative view from policy-makers,"Background While known efficacious preventive health interventions exist, the current capacity to scale up these interventions is limited. In recent years, much attention has focussed on developing frameworks and methods for scale-up yet, in practice, the pathway for scale-up is seldom linear and may be highly dependent on contextual circumstances. Few studies have examined the process of scaling up from decision to implementation nor examined the sustainability of scaled-up interventions. This study explores decision-makers’ perceptions from real-world scaled-up case studies to examine how scale-up decisions were made and describe enablers of successful scale-up and sustainability. Methods This qualitative study included 29 interviews conducted with purposively sampled key Australian policy-makers, practitioners and researchers experienced in scale-up. Semi-structured interview questions obtained information regarding case studies of scaled-up interventions. The Framework Analysis method was used as the primary method of analysis of the interview data to inductively generate common and divergent themes within qualitative data across cases. Results A total of 31 case studies of public health interventions were described by interview respondents based on their experiences. According to the interviewees’ perceptions, decisions to scale up commonly occurred either opportunistically, when funding became available, or when a deliberate decision was made and funding allocated. The latter scenario was more common when the intervention aligned with specific political or strategic goals. Decisions to scale up were driven by a variety of key actors such as politicians, senior policy-makers and practitioners in the health system. Drivers of a successful scale-up process included good governance, clear leadership, and adequate resourcing and expertise. Establishing accountability structures and appropriate engagement mechanisms to encourage the uptake of interventions were also key enablers. Sustainability was influenced by evidence of impact as well as good acceptability among the general or target population. Conclusions Much like Kingdon’s Multiple Streams Theory of ‘policy windows’, there is a conceptually similar ‘window for scale-up’, driven by a complex interplay of factors such as political need, strategic context, funding and key actors. Researchers and policy-makers need to consider scalability from the outset and prepare for when the window for scale-up opens. Decision-makers need to provide longer term funding for scale-up to facilitate longer term sustainability and build on the resources already invested for the scale-up process.",https://www.semanticscholar.org/paper/99f673a884e80842bd42074fe6e2ccc9ba508c7c,IS,Qualitative
QCA and the harnessing of unstructured qualitative data,"This paper proposes qualitative comparative analysis (QCA) as a novel method to harness unstructured data sets such as publicly available reports and news articles. It shows how QCA and conventional qualitative IS research can complement each other. In particular, it demonstrates how qualitative IS research can combine typical qualitative coding techniques with a specific type of QCA, namely crisp-set QCA (csQCA). The paper illustrates how QCA offers qualitative IS research an innovative approach to explicate the combination of conditions associated with particular outcomes. Drawing on an empirical study of green IS, it showcases the potential of QCA to harness large unstructured qualitative material and generate deeper insights about emerging IS phenomena. The paper also highlights how QCA can contribute to the data collection, and analysis stages of qualitative IS research.",https://www.semanticscholar.org/paper/2ec32a20565dc87d54cb17f877720cb3ae3d07c4,IS,Qualitative
Using value of information to prioritize research needs for migratory bird management under climate change: a case study using federal land acquisition in the United States,"In response to global habitat loss, many governmental and non‐governmental organizations have implemented land acquisition programs to protect critical habitats permanently for priority species. The ability of these protected areas to meet future management objectives may be compromised if the effects of climate change are not considered in acquisition decisions. Unfortunately, the effects of climate change on ecological systems are complex and plagued by uncertainty, making it difficult for organizations to prioritize research needs to improve decision‐making. Herein, we demonstrate the use of qualitative value of information analysis to identify and prioritize which sources of uncertainty should be reduced to improve land acquisition decisions to protect migratory birds in the face of climate change. The qualitative value of information analysis process involves four steps: (i) articulating alternative hypotheses; (ii) determining the magnitude of uncertainty regarding each hypothesis; (iii) evaluating the relevance of each hypothesis to acquisition decision‐making; and (iv) assessing the feasibility of reducing the uncertainty surrounding each hypothesis through research and monitoring. We demonstrate this approach using the objectives of 3 U.S. federal land acquisition programs that focus on migratory bird management. We used a comprehensive literature review, expert elicitation, and professional judgement to evaluate 11 hypotheses about the effect of climate change on migratory birds. Based on our results, we provide a list of priorities for future research and monitoring to reduce uncertainty and improve land acquisition decisions for the programs considered in our case study. Reducing uncertainty about how climate change will influence the spatial distribution of priority species and biotic homogenization were identified as the highest priorities for future research due to both the value of this information for improving land acquisition decisions and the feasibility of reducing uncertainty through research and monitoring. Research on how changes in precipitation patterns and winter severity will influence migratory bird abundance is also expected to benefit land acquisition decisions. By contrast, hypotheses about phenology and migration distance were identified as low priorities for research. By providing a rigorous and transparent approach to prioritizing research, we demonstrate that qualitative value of information is a valuable tool for prioritizing research and improving management decisions in other complex, high‐uncertainty cases where traditional quantitative value of information analysis is not possible. Given the inherent complexity of ecological systems under climate change, and the difficulty of identifying management‐relevant research priorities, we expect this approach to have wide applications within the field of natural resource management.",https://www.semanticscholar.org/paper/5e36aeb8bdb5cf475f078254ca599be72ec8fc3b,CS,Qualitative
"Participants' Experiences of, and Views About, Daytime Use of a Day-and-Night Hybrid Closed-Loop System in Real Life Settings: Longitudinal Qualitative Study","Abstract Objective: To explore individuals' experiences of daytime use of a day-and-night hybrid closed-loop system, their information and support needs, and their views about how future systems could be improved. Research Design and Methods: Twenty-four adults, adolescents, and parents were interviewed before using a hybrid day-and-night closed-loop system and 3 months later, data were analyzed thematically. Results: Participants praised the closed loop's ability to respond to high and low blood glucose in ways which extended beyond their own capabilities and to act as a safety net and mop up errors, such as when a mealtime bolus was forgotten or unplanned activity was undertaken. Participants also described feeling less burdened by diabetes as a consequence and more able to lead flexible, spontaneous lives. Contrary to their initial expectations, and after trust in the system had been established, most individuals wanted opportunities to collaborate with the closed loop to optimize its effectiveness. Such individuals expressed a need to communicate information, such as when routines changed or to indicate different intensities of physical activity. While individuals valued frequent contact with staff in the initial month of use, most felt that their long-term support needs would be no greater than when using an insulin pump. Conclusions: While participants reported substantial benefits to using the closed loop during the day, they also identified ways in which the technology could be refined and education and training tailored to optimize effective use. Our findings suggest that mainstreaming this technology will not necessarily lead to increased demands on clinical staff.",https://www.semanticscholar.org/paper/d8d94f9f986f25312d1ec26dc0f1d5253930f324,CS,Qualitative
"MSME Performance: Financial Information System, Work Productivity, and E-commerce","This study aims to identify and analyze financial information systems, work productivity, and e-commerce on the performance of MSMEs. The approach in this study used a qualitative approach. The sources of informants were seven SMEs in the fields of food and clothing in Makassar City. Data collection techniques were carried out by interviewing informants and literature studies. The steps for the analysis technique used were data reduction, data presentation, and conclusion. The study results show that work productivity and e-commerce effectively support MSME activities to improve MSME performance. At the same time, the financial information system has not been implemented optimally, so MSME performance has not increased optimally. Some MSMEs have done computerized financial bookkeeping, but some MSMEs still do financial bookkeeping manually due to a lack of managerial knowledge and skills. The implications of this research are very important to help MSMEs identify opportunities and challenges by having a good financial information system that can optimize financial management, good productivity, and utilize e-commerce in their business.",https://www.semanticscholar.org/paper/ed783b5bfe88335a7a98beaaa26c4c1e65a4ba1d,IS,Qualitative
"Recommendation Systems: Algorithms, Challenges, Metrics, and Business Opportunities","Recommender systems are widely used to provide users with recommendations based on their preferences. With the ever-growing volume of information online, recommender systems have been a useful tool to overcome information overload. The utilization of recommender systems cannot be overstated, given its potential influence to ameliorate many over-choice challenges. There are many types of recommendation systems with different methodologies and concepts. Various applications have adopted recommendation systems, including e-commerce, healthcare, transportation, agriculture, and media. This paper provides the current landscape of recommender systems research and identifies directions in the field in various applications. This article provides an overview of the current state of the art in recommendation systems, their types, challenges, limitations, and business adoptions. To assess the quality of a recommendation system, qualitative evaluation metrics are discussed in the paper.",https://www.semanticscholar.org/paper/2d0735c8fc46a3bd5a4219e49748701aa57162cd,IS,Qualitative
Research on critical thinking of pre-service mathematics education teachers in Indonesia (2015-2023): A bibliometric review,"Critical thinking involves being mentally organized and participating in a judgmental process to solve problems. The purpose of this study is to look at the focus of research related to critical thinking of pre-service teacher teachers of mathematics education, especially in Indonesia from 2015 to 2023. The method used in this study is a descriptive bibliometric analysis method. The Scopus database is used to obtain the necessary data. The results of the research show that publications related to the critical thinking of pre-service teacher teachers of mathematics education in Indonesia have increased from 2018 to 2020. Most of the publications were published in conference papers and some were in Q2 journals. The research focus is divided into 4 sections, namely, 1) mathematics teacher , mathematical problems , algebra , and qualitative research ; 2) critical thinking skills , mathematics education , and learning systems ; 3) problem based learning and STEM; 4) data analysis techniques, information analysis. The material that is the focus of research is algebra. New themes for this field are experimental research, digital platforms, quantitative methods, and group theory. The keywords of critical thinking skills are not directly connected with these new themes",https://www.semanticscholar.org/paper/720147d6533f3ff96a77f1f68f37310e9b034b9b,CS,Qualitative
Clinical dilemmas of routine outcome monitoring and clinical feedback: A qualitative study of patient experiences,"ABSTRACT Purpose: Routine outcome monitoring (ROM) and clinical feedback systems (CFS) are becoming prevalent in mental health services, but there are several challenges to successful implementation. ROM/CFS seem to be helpful for some patients, but not for others. To investigate this, we explored patients’ experiences with ROM/CFS as an interpersonal and psychotherapeutic process, in naturalistic settings. Method: We used video-assisted interpersonal process recall interviews to investigate the experiences of 12 patients using ROM/CFS in a Norwegian mental health outpatient clinic. Data were analyzed through systematic text condensation. Results: Our analysis resulted in three pairs of experiences with ROM/CFS: (1) Explicit vs. implicit use of information, (2) Directing focus towards or away from therapeutic topics, and (3) Giving and receiving feedback. These experiences could be helpful or hindering, depending on participants’ needs and preferences. All participants needed to know that the CFS was used in a meaningful way. If not, it could be detrimental to the therapeutic process. Conclusion: In order to be helpful for patients, ROM/CFS should be used in a way that is flexible, meaningful to patients, and sensitive to individual needs and preferences. Future research should further explore this how-to aspect of ROM/CFS with different CFS and populations.",https://www.semanticscholar.org/paper/68c1fde95f61d1254181bc97c8069c0b5d9e7228,IS,Qualitative
A qualitative study of health care providers’ perceptions and experiences of working together to care for children with medical complexity (CMC),"BackgroundChildren with medical complexity (CMC) have a wide range of long term health problems and disabilities that have an adverse impact on their quality of life. They have high levels of family identified health care needs and health care utilisation. There is no Australian literature on the experiences of health care providers working in the Australian tertiary, secondary and primary health care system, whilst managing CMC. This information is essential to inform the design of integrated health care systems for these children. We address this knowledge gap by exploring the perceptions and experiences of health care providers on the provision of health care for CMC aged 0 to 18 years.MethodA qualitative research study was undertaken. Stakeholder forums, group and individual in depth interviews were undertaken using a semi-structured interview guide. The stakeholder forums were audio recorded and transcribed verbatim. Field notes of the stakeholder forums, group and individual interviews were taken. Inductive thematic analysis was undertaken to identify key themes.ResultsOne hundred and three providers took part in the stakeholder forums and interviews across 3 local health districts, a tertiary paediatric hospital network, and primary health care organisations. Providers expressed concern regarding family capacity to negotiate the system, which was impacted by the medical complexity of the children and psychosocial complexity of their families. Lack of health care provider capacity in terms of their skills, time and availability to manage CMC was also a key problem. These issues occurred within a health system that had impaired capacity in terms of fragmentation of care and limited communication among health care providers.ConclusionWhen designing integrated care models for CMC, it is essential to understand and address the challenges experienced by their health care providers. This requires adequate training of providers, additional resources and time for coordination of care, improved systems of communication among services, with timely access to key information for parents and providers.",https://www.semanticscholar.org/paper/d9beb60db6f843c1b7c58130171caab27053ae18,IT,Qualitative
A Comprehensive Survey on the Use of Hypervisors in Safety-Critical Systems,"Virtualization has become one of the main tools for making efficient use of the resources offered by multicore embedded platforms. In recent years, even sectors such as space, aviation, and automotive, traditionally wary of adopting this type of technology due to the impact it could have on the safety of their systems, have been forced to introduce it into their day-to-day work, as their applications are becoming increasingly complex and demanding. This article provides a comprehensive review of the research work that uses or considers the use of a hypervisor as the basis for building a virtualized safety-critical embedded system. Once the hypervisors developed or adapted for this type of system have been identified, an exhaustive qualitative comparison is made between them. an exhaustive qualitative comparison is made between them. To the best of our knowledge, this is the first time that all this information is collected in a single article. Therefore, the main contribution of this article is that it collects and categorizes the information of each hypervisor and compares them with each other, so that this article can be used as a starting point for future researchers in this area, who will be able to quickly check which hypervisor is best suited to their research needs.",https://www.semanticscholar.org/paper/746d999cc2f3387b9a5a92c2df64fdda606ce67f,IT,Qualitative
Qualitative Evidence Synthesis (QES) for Guidelines: Paper 3 – Using qualitative evidence syntheses to develop implementation considerations and inform implementation processes,"This is the third in a series of three papers describing the use of qualitative evidence syntheses (QES) to inform the development of clinical and health systems guidelines. WHO has recognised the need to improve its guideline methodology to ensure that decision-making processes are transparent and evidence based, and that the resulting recommendations are relevant and applicable to end users. In addition to the standard data on effectiveness, WHO guidelines increasingly use evidence derived from QES to provide information on acceptability and feasibility and to develop important implementation considerations. WHO convened a group drawn from the technical teams involved in formulating recent (2010–2018) guidelines employing QES. Using a pragmatic and iterative approach that included feedback from WHO staff and other stakeholders, the group reflected on, discussed and identified key methods and research implications from designing QES and using the resulting findings in guideline development. As members of WHO guideline technical teams, our aim in this paper is to explore how we have used findings from QES to develop implementation considerations for these guidelines. For each guideline, in addition to using systematic reviews of effectiveness, the technical teams used QES to gather evidence of the acceptability and feasibility of interventions and, in some cases, equity issues and the value people place on different outcomes. This evidence was synthesised using standardised processes. The teams then used the QES to identify implementation considerations combined with other sources of information and input from experts. QES were useful sources of information for implementation considerations. However, several issues for further development remain, including whether researchers should use existing health systems frameworks when developing implementation considerations; whether researchers should take confidence in the evidence into account when developing implementation considerations; whether qualitative evidence that reveals implementation challenges should lead guideline panels to make conditional recommendations or only point to implementation considerations; and whether guideline users find it helpful to have challenges pointed out to them or whether they also need solutions. Finally, we need to explore how QES findings can be incorporated into derivative products to aid implementation.",https://www.semanticscholar.org/paper/8cbeff40ee67bdb4c0621ff387f455107d96e6cf,IS,Qualitative
Factors influencing e-government adoption in India: a qualitative approach,"Purpose The purpose of this study is to explore the factors that enable citizens to adopt e-government services in India. Design/methodology/approach The study uses a qualitative approach by conducting semi-structured interviews. Findings The study reveals novel e-government adoption factors, namely, auxiliary facilities, corruption avoidance, transparency and fairness in process, customer support, connectedness and forced adoption, previously unexplored in e-government adoption literature. In addition, the results highlight 17 e-government adoption factors that strengthen the findings from previous literature. Research limitations/implications This study was qualitative in nature, and rather than generalization, the focus was explicitly on obtaining an in-depth understanding. The sample used was sufficient for the purpose of this study and allowed reasonable conclusions to be drawn; however, it cannot be considered representative of a vast country like India. Academicians and information systems researchers can use these findings for further research. Practical implications The findings of this study provide useful insights into the decision-making process of e-government services users in India and similar emerging economies. These findings can be important for government officials tasked with providing e-government services. Originality/value Previous studies in the context of e-government adoption, so far, have tried to integrate adoption factors from previous technology adoption models. Hence, these studies have not been able to capture the complete essence of e-government characteristics. In addition, there are limited studies in e-government adoption in the Indian context.",https://www.semanticscholar.org/paper/9a9fbe81a15ad06272fbce9bc216b9cd708dc05d,IS,Qualitative
Free Digital Learning for Inclusion of Migrants and Refugees in Europe: A Qualitative Analysis of Three Types of Learning Purposes.,"The increasing number of migrants and refugees arriving in Europe places new demands on European education systems. In this context, the role that free digital learning (FDL) could play in fostering inclusion has attracted renewed interest. While the existing literature highlights some general design principles for developing FDL for migrants and refugees, there is little information on the use of FDL at specific education levels, or for specific learning purposes. This paper presents the results of a qualitative study that was carried out as part of the Moocs4Inclusion project of the Joint Research Centre (JRC) between July and December 2016. The study, which has a European focus, disaggregates the analysis of FDL initiatives by what were identified as its three most common purposes: a) language learning, b) civic integration and employment, and c) higher education. For each of these topics, the study sheds light on the approaches used by a wide sample of initiatives, users’ levels of awareness of what is available and take up, and migrants’ and refugees’ perceptions of the current offer. In order to collect the information needed to cover different approaches and perspectives, semi-structured interviews with 24 representatives of 10 FDL initiatives and four focus groups with 39 migrants and refugees were carried out. The results show that there are indeed overlaps between the purposes of FDL initiatives and their design principles. Specific recommendations on how to better design FDL initiatives for migrants and refugees, taking into account their specific purposes, have also been identified.",https://www.semanticscholar.org/paper/c971e51b9e521615c35d095852bfbcb4fd86b87f,CS,Qualitative
"A holistic understanding of information and communication technology for development through context, resilience, and sustainability: Evidence from a local agriculture extension information service in Ethiopia","There exists isolated work that focuses on context, resilience, and sustainability of information and communication technology for development (ICT4D). However, research on how the three concepts inform and influence one another to better meet the development goals and priorities is limited. More research is required to enhance our holistic understanding of ICT4D interventions in terms of the approaches applied to investigate context, resilience, and sustainability from socio‐technical perspectives. This research explores the link among the triple concepts to fill the above void, drawing from qualitative data collected from multiple but networked stakeholders at a local agriculture extension information service. Data is processed in a way to understand: the context of the local development practices and stakeholders, the gaps in the design of information systems applied to support local development practices, and how local communities remain resilient in their information exchange practices. Building on the existing knowledge of context, resilience, and sustainability in ICT4D the research shows how the context of the local development practices inform resilience and in turn how resilience enables the continued operation of the local development practices (context), and how context and resilience lead to sustainable ICT4D interventions. The research contributes to theory by showing how the social and technical resources distributed in the social system and the stakeholders' networks can be unlocked to conceptualize the links between the triple concepts. It informs practitioners on how they can develop a holistic understanding of the socio‐technical contexts of local developments and the socio‐technical factors to design resilient ICT4D that will lead to sustainable digital interventions.",https://www.semanticscholar.org/paper/50135033802bab4ef75969b0fecee6b5b27b3ae0,IS,Qualitative
Blockchain technology-enabled supply chain systems and supply chain performance: a resource-based view,"Using the resource-based theoretical view of the firm, this paper aims to explore how firms’ efforts to integrate blockchain technology (BCT) into their supply chain systems and activities enable certain supply chain capabilities and, consequently, improve their supply chain performance.,Using an abductive research approach, a qualitative content analysis was conducted on 126 cases of firms attempting to implement a blockchain technology-enabled supply chain system (BCTeSCS). These firms spanning across multiple industries were identified using the Nexis Uni database.,Findings reveal that present BCTeSCS efforts are more-oriented toward improving operational-level capabilities (information sharing and coordination capabilities) than strategic-level capabilities (integration and collaboration capabilities). These operational and strategic-level capabilities alongside BCTeSCS deliver several supply chains performance outcomes such as quality compliance and improvement, process improvement, flexibility, reduced cost and reduced process time. However, outcomes may vary by industry type based on their uncertainties.,Given the nascent state of BCT, accessibility to primary data about ongoing BCTeSCS efforts is limited. The presented framework is based on 126 cases of secondary information. Within this constraint, the paper finds scope to future empirical research by proposing a resource-based framework of BCTeSCS and related propositions.,The results and discussion of this study serve as useful guidance for practitioners involved in BCTeSCS integrations.,The paper creates a BCTeSCS scenario for stakeholders to assume its potential socio-economic and socio-environmental pressures.,This paper is one of the initial attempts to examine BCTeSCS efforts across multiple industries, and thus, promises a broad future research scope.",https://www.semanticscholar.org/paper/769757b9c065f659b6699c7f64434df30b9346d2,CS,Qualitative
Multidisciplinary teams and ICT: a qualitative study exploring the use of technology and its impact on multidisciplinary team meetings,"BackgroundMultidisciplinary teams (MDTs) are an integral component in the delivery of health care. This is particularly evident in the delivery of cancer care, where multidisciplinary teams are internationally recognized as the preferred method for service delivery. The use of health information systems and technology are key enabling factors for building the capacity of MDTs to engage in improvement and implementation projects but there is scant research on how MDTs make use of technology and information systems or the kinds of systems needed for them to undertake improvement and implementation research. This paper reports findings on how seven MDTs in cancer care utilized technological and information systems and the barriers and enabling factors that impacted on their uptake.MethodsSeven multidisciplinary teams from two large metropolitan hospitals participated in the study. Qualitative methods including structured observations and semi structured interviews that explored how teams engaged in research and improvement activities were utilized. Participants were also observed and interviewed in relation to their use of data and health information systems. Findings were subject to content analysis and key themes were identified. Interviews were transcribed and de-identified and key themes were subsequently discussed with participants to allow for member checking and further clarification of findings.ResultsA total of 43 MDT meetings across seven tumor streams were observed. Of these, observation notes from 13 meetings contained direct references to emerging technologies and health information systems. Findings from 15 semi-structured interviews were also analyzed in relation to how MDTs used technology in weekly meetings, and the perceived impact of technology. Three broad themes emerged: (1) methods for data collection and use by MDTs, (2) the impact of technology on the MDT meeting environment, and (3) the impact of technology and information systems on clinical decision making.ConclusionThe study demonstrates that real time data collection and imaging may improve patient centered care coordination. However, ICTs can be used sub-optimally by teams. We therefore urge additional research to identify the enabling factors that support better collection and use of outcome data from ICT.",https://www.semanticscholar.org/paper/7b254301392535f0f160fd06dec0eae97dc685d5,IS,Qualitative
Why and How to Strengthen Indigenous Peoples' Food Systems With Examples From Two Unique Indigenous Communities,"Indigenous Peoples' food systems contain extensive and sophisticated knowledge that is often undocumented and underutilized in contemporary society that has increasingly poor nutrition and loss of food biodiversity. Indigenous Peoples in all global regions are among the most vulnerable to marginalization, food insecurity and chronic disease and will benefit greatly from strengthening their resource-rich food systems to make them more resilient and sustainable. It is in this spirit that we contribute to the databases of Indigenous Peoples' food system knowledge with information on unique traditional foods from the Nuxalk Nation in British Columbia, Canada, and the Pwo Karen People of Sanephong Community, Thailand. Several publications from these case studies originated from interdisciplinary mixed-method research, in part through the United Nations Food and Agriculture Organization. We highlight selected foods with nutrient data and various qualitative and quantitative methods used to identify and promote their use within these unique communities. Our intent is to stimulate complementary strengthening efforts among other traditional and Indigenous Peoples that will contribute to global intercultural food system evidence and advances.",https://www.semanticscholar.org/paper/487fb2b90fc184a8214fed3eee95c98a554aecce,CS,Qualitative
Prerequisites for safe intraoperative nursing care and teamwork-Operating theatre nurses' perspectives: A qualitative interview study.,"AIM To describe operating theatre nurses' experience of preconditions for safe intraoperative nursing care and teamwork. BACKGROUND Surgical interventions are often needed for patients' well-being and survival from health problems. Adequate information to professionals responsible within the surgical organisation is of importance for patient safety in connection to the surgery. The members in the surgical team need correct information about the patients' health and planned care. The information is mainly transferred by computerised systems that do not necessarily provide all information needed. METHOD A qualitative descriptive design was chosen. Narrative interviews were carried out with 16 experienced operating theatre nurses in four different hospitals in rural and urban areas in Sweden. The data were analysed using qualitative content analysis. The study complied with criteria to Consolidated Criteria for Reporting Qualitative Research (COREQ). RESULT Operating theatre nurses strived to get adequate information about the patients' care, the surgical intervention and the equipment to be well prepared for intraoperative nursing care. The information from the computerised systems was described as fragmented and obliged the operating theatre nurses to demand a preoperative dialogue between the members of the surgical team. Professional collegial teamwork and committed leadership were considered to enhance patient safety. CONCLUSION From the operating theatre nurses' perspective, prerequisites for intraoperative safe nursing care and teamwork depend upon a preoperative dialogue between the members in the surgical team for collegial teamwork, obtaining a reliable preoperative overall picture based on adequate information transfer, and the support of a committed first-line manager. RELEVANCE TO CLINICAL PRACTICE The operating theatre nurses need a reliable preoperative overall picture in advance, to be able to be well prepared for the patients' surgery. The overall picture should be based on adequate data about the patients' health status and needs, details about the surgical intervention and prescriptions.",https://www.semanticscholar.org/paper/ffd38ca96c0b7addd77b66044b56ae71672e94f7,CS,Qualitative
Modular Service Design of Information Technology-Enabled Services,"The literature has proposed ways to modularize information-technology-enabled services (ITeS) with limited success. We argue that applying design principles (DPs) can address this gap and revitalize the service modularization literature. With a qualitative research study, we develop exemplar DPs and a set of prioritized DPs for ITeS. We contribute to the literature by demonstrating how complex service systems, specifically ITeS, can be modularly designed. Our DPs show how different ITeS design elements or service attribute combinations impact the outcome-driven design of service experience. Based on the findings, we present a modular service design framework and a service design method that adopts DPs to create effective modular ITeS designs. We also offer ways to conceptualize and apply service modularization to improve the adoption of the modular service design by service designers and managers. Graphical Abstract",https://www.semanticscholar.org/paper/934ffe29196167ebfbe56407fdabe86701b54f36,IS,Qualitative
Learning from First-Generation Qualitative Approaches in the IS Discipline: An Evolutionary View and Some Implications for Authors and Evaluators (Part 2/2),"Qualitative research in the information systems (IS) discipline has come a long way, from being dismissed as “exploratory research” or “preresearch,” not worthy of being featured in “scientific” and authoritative journals in the discipline, to a state where such research is seen as legitimate and even welcome within much of the mainstream IS research community. Recent editorials have expressed concerns regarding the research community’s lack of awareness about the diverse nature of qualitative research and the apparent confusion regarding how these diverse approaches are different. In this two-part editorial, Part 1 focused on analyzing first-generation qualitative research approaches based on four key elements (theory, data, analysis, and claims), and discussed how each of these elements might vary depending on the type (i.e., genre) of the qualitative study. In Part 2, we examine qualitative studies published over the past 17 years in four leading journals for evidence related to the genres identified in Part 1 of this editorial. Specifically, our goal was to assess the recognition of various genres in the published papers, and to determine whether there was sufficient internal consistency for a given genre within each paper. Based on the results of the assessment, we offer lessons for authors, reviewers, and editors.",https://www.semanticscholar.org/paper/fc3d747f47490ecb24117de98577e6c4149e7916,IS,Qualitative
Exploring Industry 4.0 technologies to enable circular economy practices in a manufacturing context,"Purpose The purpose of this paper is to explore how rising technologies from Industry 4.0 can be integrated with circular economy (CE) practices to establish a business model that reuses and recycles wasted material such as scrap metal or e-waste. Design/methodology/approach The qualitative research method was deployed in three stages. Stage 1 was a literature review of concepts, successful factors and barriers related to the transition towards a CE along with sustainable supply chain management, smart production systems and additive manufacturing (AM). Stage 2 comprised a conceptual framework to integrate and evaluate the synergistic potential among these concepts. Finally, stage 3 validated the proposed model by collecting rich qualitative data based on semi-structured interviews with managers, researchers and professors of operations management to gather insightful and relevant information. Findings The outcome of the study is the recommendation of a circular model to reuse scrap electronic devices, integrating web technologies, reverse logistics and AM to support CE practices. Results suggest a positive influence from improving business sustainability by reinserting waste into the supply chain to manufacture products on demand. Research limitations/implications The impact of reusing wasted materials to manufacture new products is relevant to minimising resource consumption and negative environmental impacts. Furthermore, it avoids hazardous materials ending up in landfills or in the oceans, seriously threatening life in ecosystems. In addition, reuse of wasted material enables the development of local business networks that generate jobs and improve economic performance. Practical implications First, the impact of reusing materials to manufacture new products minimises resource consumption and negative environmental impacts. The circular model also encourages keeping hazardous materials that seriously threaten life in ecosystems out of landfills and oceans. For this study, it was found that most urban waste is plastic and cast iron, leaving room for improvement in increasing recycling of scrap metal and similar materials. Second, the circular business model promotes a culture of reusing and recycling and motivates the development of collection and processing techniques for urban waste through the use of three-dimensional (3D) printing technologies and Industry 4.0. In this way, the involved stakeholders are focused on the technical parts of recycling and can be better dedicated to research, development and innovation because many of the processes will be automated. Social implications The purpose of this study was to explore how Industry 4.0 technologies are integrated with CE practices. This allows for the proposal of a circular business model for recycling waste and delivering new products, significantly reducing resource consumption and optimising natural resources. In a first stage, the circular business model can be used to recycle electronic scrap, with the proposed integration of web technologies, reverse logistics and AM as a technological platform to support the model. These have several environmental, sociotechnical and economic implications for society. Originality/value The sociotechnical aspects are directly impacted by the circular smart production system (CSPS) management model, since it creates a new culture of reuse and recycling techniques for urban waste using 3D printing technologies, as well as Industry 4.0 concepts to increase production on demand and automate manufacturing processes. The tendency of the CSPS model is to contribute to deployment CE in the manufacture of new products or parts with AM approaches, generating a new path of supply and demand for society.",https://www.semanticscholar.org/paper/f137ad6e0470733c1a71fe8a13b02ebfe91c2076,IT,Qualitative
Examining the Complexity of Patient-Outpatient Care Team Secure Message Communication: Qualitative Analysis,"Background The value of secure messaging in streamlining routine patient care activities is generally agreed upon. However, the differences in how patients use secure messaging, including for communicating both routine and nonroutine issues, and the implications of these differences in use are less well understood. Objective The purpose of this study was to examine secure messaging use to extend current knowledge of how this tool is being used in outpatient care settings and generate new research questions to improve our understanding of the role of secure messaging in the patient-provider communication toolbox. Methods We conducted an in-depth qualitative analysis of secure message threads in 12 US Department of Veterans Affairs outpatient clinics in south Texas. We analyzed 70 secure message threads with a total of 179 unique communications between patients and their outpatient teams for patterns in communication and secure message content. We used theories from information systems and complexity science in organizations to explain our observations. Results Analysis identified content relating to 3 main themes: (1) information management, (2) uncertainty management, and (3) patient safety and engagement risks and opportunities. Within these themes, we identified 2 subcategories of information management (information exchange and problem solving), 2 subcategories of uncertainty management (relationship building and sensemaking), and 3 subcategories of patient safety and engagement risks and opportunities (unresolved issues, tone mismatch, and urgent medical issues). Secure messages were most often used to communicate routine issues (eg, information exchange and problem solving). However, the presence of subcategories pertaining to nonroutine issues (eg, relationship building, sensemaking, tone mismatch, urgent issues, and unresolved issues) requires attention, particularly for improving opportunities in outpatient care settings using secure messaging. Conclusions Patients use secure messaging for both routine and nonroutine purposes. Our analysis sheds light on potentially new patient safety concerns, particularly when using secure messaging to address some of the more complex issues patients are communicating with providers. Secure messaging is an asynchronous communication information system operated by patients and providers who are often characterized as having significant differences in knowledge, experience and expectations. As such, justification for its use beyond routine purposes is limited—yet this occurs, presenting a multifaceted dilemma for health care organizations. Secure messaging use in outpatient care settings may be more nuanced, and thus more challenging to understand and manage than previously recognized. New information system designs that acknowledge the use of secure messaging for nonroutine and complex health topics are needed.",https://www.semanticscholar.org/paper/9738c0eac907420928941eddde34ed94d053f802,IS,Qualitative
A Study on Challenges of Testing Robotic Systems,"Robotic systems are increasingly a part of everyday life. Characteristics of robotic systems such as interaction with the physical world, and integration of hardware and software components, differentiate robotic systems from conventional software systems. Although numerous studies have investigated the challenges of software testing in practice, no such study has focused on testing of robotic systems. In this paper, we conduct a qualitative study to better understand the testing practices used by the robotics community, and identify the challenges faced by practitioners when testing their systems. We identify a total of 12 testing practices and 9 testing challenges from our participants’ responses. We group these challenges into 3 major themes: Real-world complexities, Community and standards, and Component integration. We believe that further research on addressing challenges described with these three major themes can result in higher adoption of robotics testing practices, more testing automation, and higher-quality robotic systems.",https://www.semanticscholar.org/paper/7b1f1556edec5d4d20142dcd1aa52eb492421464,CS,Qualitative
Systematic review of experiences and perceptions of key actors and organisations at multiple levels within health systems internationally in responding to COVID-19,"Background COVID-19 has presented challenges to healthcare systems and healthcare professionals internationally. After one year of the pandemic, the initial evidence on health system responses begins to consolidate, and there is a need to identify and synthesise experiences of responding to COVID-19 among healthcare professionals and other health system stakeholders. This systematic review of primary qualitative studies depicts the experiences and perceptions of organisations and actors at multiple levels of health systems internationally in responding to COVID-19. Methods Six main databases of biomedical information, public health and health administration research were searched over the period October 1, 2019, to October 21, 2020. Information extracted from included studies was analysed thematically. Results Thirty-four studies were eligible for data extraction. Nine of those studies, of lower methodological quality, were removed from the thematic analysis of study results. Considering the professional level experiences, predominant themes of the studies consisted of the new roles and responsibilities of healthcare workers, burnout and distress, recognition of ´unseen´ healthcare workers, and positive changes and emergent solutions amid the crisis. Organisational level findings of the studies included provision of psychological support, COVID-19 as ""catalyst"" for change, and exercise of more ""open"" leadership by managers and health authorities. Continuous training, regulation of working conditions, providing supportive resources, coordinating a diversity of actors, and reviewing and updating regulations were roles identified at the local health system level. Conclusions The experiences of frontline healthcare workers have been the focus of attention of the majority of primary qualitative studies as of October 2020. However, organisational and wider system level studies indicate that some responses to COVID-19 have been characterised by increased emphasis on coordination activities by local health system actors, making service adaptations at pace, and reliance on expanded roles of front-line workers. The need for theory-informed qualitative studies was identified at the organisational level. Trial registration CRD42020202875",https://www.semanticscholar.org/paper/fa8f1f9ffae400ce7397d454fc3c4992b5bf38ad,CS,Qualitative
Mixed language queries in online searches: A study of intra-sentential code-switching from a qualitative perspective,"Purpose With the increasing number of online multilingual resources, cross-language information retrieval (CLIR) has drawn much attention from the information retrieval (IR) research community. However, few studies have examined how and why multilingual searchers seek information in two or more languages, specifically how they switch and mix language in queries to get satisfying results. The purpose of this paper is to focus on Chinese–English bilinguals’ intra-sentential code-switching behaviors in online searches. The scenarios and reasons of code-switching, factors that may affect code-switching, the patterns of mixed language query formulation and reformulation and how current IR systems and other search tools can facilitate such information needs were examined. Design/methodology/approach In-depth semi-structured interviews were used as the research method. In total, 30 participants were recruited based on their English proficiency, location and profession, using a purposive sampling method. Findings Four scenarios and four reasons for using Chinese–English mixed language queries to cover information needs were identified, and results suggest that linguistic and cultural/social factors are of equivalent importance in code-switching behaviors. English terms and Chinese terms in queries play different roles in searches, and mixed language queries are irreplaceable by either single language queries or other search facilitating features. Findings also suggest current search engines and tools need greater emphasis in the user interface and more user education is required. Originality/value This study presents a qualitative analysis of bilinguals’ code-switching behaviors in online searches. Findings are expected to advance the theoretical understanding of bilingual users’ search strategies and interactions with IR systems, and provide insights for designing more effective IR systems and tools to discover multilingual online resources, including cross-language controlled vocabularies, personalized CLIR tools and mixed language query assistants.",https://www.semanticscholar.org/paper/7a32de1ae914e57fa9b622bcde41308b20daedd9,CS,Qualitative
Patient Rounds With Video-Consulted Relatives: Qualitative Study on Possibilities and Barriers From the Perspective of Healthcare Providers,"Background In cancer settings, relatives are often seen as a resource as they are able to support the patient and remember information during hospitalization. However, geographic distance to hospitals, work, and family obligations are reasons that may cause difficulties for relatives’ physical participation during hospitalization. This provided inspiration to uncover the possibility of telehealth care in connection with enabling participation by relatives during patient rounds. Telehealth is used advantageously in health care systems but is also at risk of failing during the implementation process because of, for instance, health care professionals’ resistance to change. Research on the implications for health care professionals in involving relatives’ participation through virtual presence during patient rounds is limited. Objective This study aimed to investigate health care professionals’ experiences in using and implementing technology to involve relatives during video-consulted patient rounds. Methods The design was a qualitative approach. Methods used were focus group interviews, short open interviews, and field observations of health care professionals working at a cancer department. The text material was analyzed using interpretative phenomenological analysis. Results Field observational studies were conducted for 15 days, yielding 75 hours of observation. A total of 14 sessions of video-consulted patient rounds were observed and 15 pages of field notes written, along with 8 short open interviews with physicians, nurses, and staff from management. Moreover, 2 focus group interviews with 9 health care professionals were conducted. Health care professionals experienced the use of technology as a way to facilitate involvement of the patient’s relatives, without them being physically present. Moreover, it raised questions about whether this way of conducting patient rounds could address the needs of both the patients and the relatives. Time, culture, and change of work routines were found to be the major barriers when implementing new technology involving relatives. Conclusions This study identified a double change by introducing both new technology and virtual participation by relatives at the same time. The change had consequences on health care professionals’ work routines with regard to work load, culture, and organization because of the complexity in health care systems.",https://www.semanticscholar.org/paper/15cff525c2944d2ae03ea5b4561a7a171e1d2c14,IS,Qualitative
COVID-19 affected remote workers: a temporal analysis of information system development during the pandemic,"ABSTRACT This paper explores the temporal complexity of remote working within information systems development (ISD). Traditional remote working literature is ill-equipped to deal with the COVID-19 context. An interpretive research approach is used to generate a qualitative data set, based upon the results of field interviews with key ISD professionals. The thematic analysis of the research data is classified based on a temporal complexity framework. The study provides several key takeaways that emerge when temporal complexity theory is applied to study COVID-19 affected remote ISD workers. It explores how temporal characteristics influence remote ISD working, while investigating the resulting challenges. Recommendations are provided which researchers and practitioners may focus on when researching and applying ISD methodology. This is the first paper which examines temporal complexity within remote ISD workers. This research will provide a starting point for ISD researchers and practitioners to test their commonly held temporal assumptions about remote working.",https://www.semanticscholar.org/paper/8adf735a009a353d5c8329ee7214141ae4e38cd6,IS,Qualitative
When enough is enough: Investigating the antecedents and consequences of information security fatigue,"Despite concerns raised by practitioners, the potential downside of the information security demands imposed by organizations on their employees has received limited scholarly attention. Our research focuses on information security fatigue (hereafter security fatigue), which is defined as a socio‐emotional state experienced by an individual who is tired of and disillusioned with security policies and their associated guidelines and procedures. This research delves into the security fatigue concept, investigates its antecedents and reports how fatigue affects employee security policy compliance (and non‐compliance). Since security fatigue is not well articulated in the literature and there is limited understanding of its antecedents and consequences, we take a research approach that affords novel insight into this phenomenon. Specifically, we conduct 38 in‐depth interviews with business and IT professionals, and then use a qualitative approach to construct a model, including seven research propositions, to highlight the key aspects of security fatigue. Our results indicate that four distinct antecedents contribute to security fatigue, which result in three unique consequences. We discuss security fatigue in relation to past theoretical views and related concepts within the security policy compliance literature and identify directions for future research.",https://www.semanticscholar.org/paper/c3f867ec8dcb1e8dd779614567dd46ca7c749ef8,IS,Qualitative
"A Review of Urban Digital Twins Integration, Challenges, and Future Directions in Smart City Development","This review paper explores Urban Digital Twins (UDTs) and their crucial role in developing smarter cities, focusing on making urban areas more sustainable and well-planned. The methodology adopted an extensive literature review across multiple academic databases related to UDTs in smart cities, sustainability, and urban environments, conducted by a bibliometric analysis using VOSviewer to identify key research trends and qualitative analysis through thematic categorization. This paper shows how UDTs can significantly change how cities are managed and planned by examining examples from cities like Singapore and Dubai. This study points out the main hurdles like gathering data, connecting systems, handling vast amounts of information, and making different technologies work together. It also sheds light on what is missing in current research, such as the need for solid rules for using UDTs effectively, better cooperation between various city systems, and a deeper look into how UDTs affect society. To address research gaps, this study highlights the necessity of interdisciplinary collaboration. It also calls for establishing comprehensive models, universal standards, and comparative studies among traditional and UDT methods. Finally, it encourages industry, policymakers, and academics to join forces in realizing sustainable, smart cities.",https://www.semanticscholar.org/paper/ecfc55292af7f14f4fb1f29b253610b5ae64a6e0,IT,Qualitative
Quality in crisis: a systematic review of the quality of health systems in humanitarian settings,"Background There is a growing concern that the quality of health systems in humanitarian crises and the care they provide has received little attention. To help better understand current practice and research on health system quality, this paper aimed to examine the evidence on the quality of health systems in humanitarian settings. Methods This systematic review was based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol. The context of interest was populations affected by humanitarian crisis in low- and middle- income countries (LMICs). We included studies where the intervention of interest, health services for populations affected by crisis, was provided by the formal health system. Our outcome of interest was the quality of the health system. We included primary research studies, from a combination of information sources, published in English between January 2000 and January 2019 using quantitative and qualitative methods. We used the High Quality Health Systems Framework to analyze the included studies by quality domain and sub-domain. Results We identified 2285 articles through our search, of which 163 were eligible for full-text review, and 55 articles were eligible for inclusion in our systematic review. Poor diagnosis, inadequate patient referrals, and inappropriate treatment of illness were commonly cited barriers to quality care. There was a strong focus placed on the foundations of a health system with emphasis on the workforce and tools, but a limited focus on the health impacts of health systems. The review also suggests some barriers to high quality health systems that are specific to humanitarian settings such as language barriers for refugees in their host country, discontinued care for migrant populations with chronic conditions, and fears around provider safety. Conclusion The review highlights a large gap in the measurement of quality both at the point of care and at the health system level. There is a need for further work particularly on health system measurement strategies, accountability mechanisms, and patient-centered approaches in humanitarian settings.",https://www.semanticscholar.org/paper/4c1edca9eae09e3dc9d55c3d5b107a9f6c2aebf7,CS,Qualitative
Practitioner perspectives on the implementation of an electronic information system to enforce practice standards in England,"ABSTRACT Research in England, mainland Europe and Australia has demonstrated how the introduction of electronic information systems (IS) has been highly problematic for frontline social workers, particularly in terms of diverting their time, attention and energy away from direct work with service users. IS have evolved over the years to become increasingly complex and demanding in terms of the nature and volume of data that is required to be entered by social workers. While incorporating workflows in an IS that direct social workers to specific tasks at a particular time is not new, one recently implemented IS incorporates a plethora of workflows that aim to enforce compliance with practice procedures and standards. In this article, the findings from a small qualitative study which aimed to evaluate the reaction of social workers to this new form of IS are reported. Going beyond the problems expected with the introduction of a new IS and drawing theoretically from social informatics and joint cognitive systems, the reasons why this form of IS may be particularly problematic for frontline social workers are explored.",https://www.semanticscholar.org/paper/56761228a180e0eebc4766cba34989d9ad4f0717,IS,Qualitative
"Disentangling Fairness Perceptions in Algorithmic Decision-Making: the Effects of Explanations, Human Oversight, and Contestability","Recent research claims that information cues and system attributes of algorithmic decision-making processes affect decision subjects’ fairness perceptions. However, little is still known about how these factors interact. This paper presents a user study (N = 267) investigating the individual and combined effects of explanations, human oversight, and contestability on informational and procedural fairness perceptions for high- and low-stakes decisions in a loan approval scenario. We find that explanations and contestability contribute to informational and procedural fairness perceptions, respectively, but we find no evidence for an effect of human oversight. Our results further show that both informational and procedural fairness perceptions contribute positively to overall fairness perceptions but we do not find an interaction effect between them. A qualitative analysis exposes tensions between information overload and understanding, human involvement and timely decision-making, and accounting for personal circumstances while maintaining procedural consistency. Our results have important design implications for algorithmic decision-making processes that meet decision subjects’ standards of justice.",https://www.semanticscholar.org/paper/e130c5aa440815105edbf14234f4e3aa4d188ec4,CS,Qualitative
Systems approach to cloud computing adoption in an emerging economy,"PurposeCloud computing (CC) services have given a tremendous boost to the creation of efficient and effective solutions. With developing countries at a slow pace of adoption, this research aims to identify factors and their interrelationships influencing the adoption of CC in a developing country context. The developing countries are enjoying numerous benefits from CC services; however, its low adoption is still a question in developing economies; hence, the authors have selected the context of information and communication technology (ICT) firms in India.Design/methodology/approachThe qualitative research method is used where experts from thirteen ICT firms in India are interviewed.FindingsSixteen factors, twenty-eight subfactors, and 25 interrelationships are revealed through content analysis. Further, causal loop diagrams are proposed to display the behavior of cause and effect of these factors from a system's perspective. This will help to understand the relationships among the factors in order to enhance the speed of CC adoption. Possible financial loss and resistance to change are found as the key barriers to adoption. The proposed interrelationships can guide both policymakers and service providers for designing effective CC policies.Originality/valueThis is the first scholarly work that identifies interrelationships among factors and subfactors, thereby providing a holistic picture to decision-makers while making a choice on whether to adopt cloud services or continue with on premise data centers and servers.",https://www.semanticscholar.org/paper/87bd4b1bd0a792127b0e14df247f4054549aab49,IS,Qualitative
"Clinicians’ Views on Using Artificial Intelligence in Healthcare: Opportunities, Challenges, and Beyond","Introduction: The healthcare industry has made significant progress in information technology, which has improved healthcare procedures and brought about advancements in clinical care services. This includes gathering crucial clinical data and implementing intelligent health information management. Artificial Intelligence (AI) has the potential to bolster further existing health information systems, notably electronic health records (EHRs). With AI, EHRs can offer more customized and adaptable roles for patients. This study aims to delve into the current and potential uses of AI and examine the obstacles that come with it. Method: In this study, we employed a qualitative methodology and purposive sampling to select participants. We sought out clinicians who were eager to share their professional insights. Our research involved conducting three focus group interviews, each lasting an hour. The moderator began each session by introducing the study's goals and assuring participants of confidentiality to foster a collaborative environment. The facilitator asked open-ended questions about EHR, including its applications, challenges, and AI-assisted features. Results: The research conducted by 26 participants has identified five crucial areas of using AI in healthcare delivery. These areas include predictive analysis, clinical decision support systems, data visualization, natural language processing (NLP), patient monitoring, mobile technology, and future and emerging trends. However, the hype surrounding AI and the fact that the technology is still in its early stages pose significant challenges. Technical limitations related to language processing and context-specific reasoning must be addressed. Furthermore, medico-legal challenges arise when AI supports or autonomously delivers healthcare services. Governments must develop strategies to ensure AI's responsible and transparent application in healthcare delivery. Conclusion: AI technology has the potential to revolutionize healthcare through its integration with EHRs and other existing technologies. However, several challenges must be addressed before this potential can be fully realized. The development and testing of complex EHR systems that utilize AI must be approached with care to ensure their accuracy and trustworthiness in decision-making about patient treatment. Additionally, there is a need to navigate medico-legal obligations and ensure that benefits are equitably distributed.",https://www.semanticscholar.org/paper/7644544a3ffae05cd9aa22932886e2e6809c0be3,IS,Qualitative
"The Impact of Information Technology and Sustainable Strategies in Hotel Branding, Evidence from the Greek Environment","Information Systems, Sustainable Development and Hotel Branding are the main study areas of this specific research. In recent years, many technologies have been which have started to be implemented in the hospitality industry, but the full potential of these technologies has not yet been analyzed in order to understand how they could help companies to develop branding strategies and increase the customer’s loyalty and business performance. Moreover, due to the pandemic crisis due to COVID-19, hotel companies have to manage a new crisis, which affects both the economic and social pillars of their sustainable development. Therefore, the hospitality industry must be more flexible in adopting information technologies and sustainable philosophies, firstly for their own survival and secondly to strengthen their position inside the task environment. The methodology approach was based both on the literature review and qualitative methodology, which was conducted through questionnaires. The aim of this research is to analyze the above concepts and see how this combination could lead the hotel companies to develop a competitive advantage inside their task environment. The findings of this research concern the region of Greece. They reveal that there is a strong correlation among the above concepts, and each company should start to adopt sustainable philosophy, use information technologies, and develop new branding strategies.",https://www.semanticscholar.org/paper/0315e0012313f6b1a495ced2b15d60efd145eebd,IS,Qualitative
Dynamical systems theory in cognitive science and neuroscience,"Correspondence Luis H. Favela, Department of Philosophy and Cognitive Sciences Program, University of Central Florida, 4111 Pictor Lane, Suite 220, Orlando, FL 32816-1352. Email: luis.favela@ucf.edu Abstract Dynamical systems theory (DST) is a branch of mathematics that assesses abstract or physical systems that change over time. It has a quantitative part (mathematical equations) and a related qualitative part (plotting equations in a state space). Nonlinear dynamical systems theory applies the same tools in research involving phenomena such as chaos and hysteresis. These approaches have provided different ways of investigating and understanding cognitive systems in cognitive science and neuroscience. The ‘dynamical hypothesis’ claims that cognition is and can be understood as dynamical systems. Common consequences for such an approach include rejecting understanding cognition as information processing in nature, including eschewing explanatory roles for computation or representation. Contemporary applications of DST include mouse-tracking studies in cognitive science and nonrepresentational perspectives on motor control in neuroscience. Such work has philosophical implications concerning the boundaries of cognition, explanation, and representations. DST offers powerful methodology and theories that raise many topics of philosophical significance.",https://www.semanticscholar.org/paper/296d0df174b08489d9acc2372ab2aeb2a54eabe0,CS,Qualitative
Policies and Management Interventions to Enhance Health and Care Workforce Capacity for Addressing the COVID-19 Pandemic: Protocol for a Living Systematic Review,"Background Countries and health systems have had to make challenging resource allocation and capacity-building decisions to promote proper patient care and ensure health and care workers’ safety and well-being, so that they can effectively address the present COVID-19 pandemic as well as upcoming public health problems and natural catastrophes. As innovations are already in place and updated evidence is published daily, more information is required to inform the development and implementation of policies and interventions to improve health and care workforce capacity to address the COVID-19 pandemic response. Objective The objective of this protocol review is to identify countries’ range of experiences with policies and management interventions that can improve health and care workers’ capacity to address the COVID-19 pandemic response and synthesize evidence on the effectiveness of the interventions. Methods We will conduct a living systematic review of quantitative, qualitative, and mixed methods studies and gray literature (technical and political documents) published in English, French, Hindi, Portuguese, Italian, and Spanish between January 1, 2000, and March 1, 2022. The databases to be searched are MEDLINE (PubMed), Embase, SCOPUS, and Latin American and Caribbean Health Sciences Literature. In addition, the World Health Organization’s COVID-19 Research Database and the websites of international organizations (International Labour Organization, Economic Co-operation and Development, and The Health System Response Monitor) will be searched for unpublished studies and gray literature. Data will be extracted from the selected documents using an electronic form adapted from the Joanna Briggs Institute quantitative and qualitative tools for data extraction. A convergent integrated approach to synthesis and integration will be used. The risk of bias will be assessed with Joanna Briggs Institute critical appraisal tools, and the certainty of the evidence in the presented outcomes will be assessed with the Grading of Recommendations, Assessment, Development and Evaluation. Results The database and gray literature search retrieved 3378 documents. Data are being analyzed by 2 independent reviewers. The study is expected to be published by the end of 2023 in a peer-reviewed journal. Conclusions This review will allow us to identify and describe the policies and strategies implemented by countries and their effectiveness, as well as identify gaps in the evidence. Trial Registration PROSPERO CRD42022327041; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=327041 International Registered Report Identifier (IRRID) RR1-10.2196/50306",https://www.semanticscholar.org/paper/aabefc0d61c7f19f378171b92c34b212e06b983d,CS,Qualitative
Investigating the influence of organizational factors on blockchain adoption,"Purpose Blockchain possesses the potential to disrupt and reshape a plethora of industries in the next decade. However, blockchain adoption rates in technology developed countries, such as Ireland, are relatively low. Motivated by blockchain’s potential to transform sociotechnical systems, the lack of systematic inquiry pertaining to blockchain studies from an information system perspective, the authors propose the following research question: “How do organizational factors influence blockchain adoption in organizations based in a developed country?” Specifically, the purpose of this paper is to elucidate the impact of organizational factors on the adoption of blockchain and the adoption of blockchain in companies based in Ireland. Design/methodology/approach A comprehensive literature review was conducted, and the methods of qualitative content analysis were used to identify the most important technology–organization–environment (TOE) blockchain adoption factors. Organizational factors are often viewed as the most significant determinants of IT innovation adoption in organizations. Consequently, using a multiple-case study of 20 companies based in Ireland, the authors investigate how the top three organizational factors identified from the blockchain literature affected these companies decision to adopt or not adopt blockchain. Findings The literature review on blockchain adoption identified specific technological, organizational and environmental factors. Furthermore, the case study findings identified three patterns: top management support and organizational readiness are enablers for blockchain adoption, and large companies are more likely to adopt blockchain than small to medium-sized enterprises (SMEs). The authors explain these patterns by examining the nature of blockchain and the characteristics of Ireland as a developed country. Practical and scientific contributions are also presented. Research limitations/implications This study makes several important scientific contributions. First, the findings revealed that top management support and organizational readiness are significant enablers of blockchain adoption. Ireland is recognized as a technology developed country; however, the findings in relation to top management support contradict existing IT adoption literature pertaining to developed countries. Second, previous IT innovation adoption literature suggests that organizations size has a positive influence on a company’s IT innovation adoption process. This study demonstrates that large organizations are more likely to not only adopt blockchain but are also more likely to conduct increased levels of blockchain research and development activities. Finally, and most significantly, the authors identified several patterns, which relate specifically to Ireland as a developed country that influenced the findings. These findings could hold particular relevance to governments and organizations of other developed countries in terms of accelerating blockchain adoption. Practical implications The findings about the low level of blockchain awareness and the lack of information pertaining to viable business use cases indicate that the Irish government could play a more significant role in promoting the benefits of blockchain technologies. Further, the findings could also encourage IT providers to formulate enhanced strategies aimed at disseminating information pertaining to blockchain technologies. Second, the positive influence of top management support and organizational readiness, particularly about core competencies, on blockchain adoption suggests that equipping managers with the requisite knowledge and skills will be crucial in adopting these IT innovations. Finally, organizations who adopted blockchain used cloud-based blockchain platforms and tools to overcome the constraints of their initial low levels of organizational readiness. Originality/value This is one of the first studies to identify specific TOE blockchain adoption factors. Further, the authors examine how the three most identified organizational adoption factors impact organizations decisions to adopt blockchain. Finally, the authors discuss how the resulting three patterns identified by examining the nature of blockchain and the characteristics of Ireland as a technology developed country.",https://www.semanticscholar.org/paper/f4c8c67418987fcc2431362b427f9bd63c9b7b57,IS,Qualitative
Contemporary issues in north–south health research partnerships: perspectives of health research stakeholders in Zambia,"BackgroundThe late 1990s and early 2000s have seen a growth in north–south health research partnerships resulting from scientific developments such as those in genetic studies and development of statistical techniques and technological requirements for the analysis of large datasets. Despite these efforts, there is inadequate information representing the voice of African researchers as stakeholders experiencing partnership arrangements, particularly in Zambia. Furthermore, very little attention has been paid to capturing the practice of guidelines within partnerships. In this paper, we present achievements and highlight challenges faced by southern partners in north–south health research partnerships.MethodsA qualitative inquiry was employed using in-depth interviews developed using the Bergen Model of Collaborative Functioning with 20 key informants in Lusaka district in Zambia purposively sampled from a wide range of health research partnerships.ResultsPartnerships produce benefits for southern partners, including evidence generation to influence policy, improved service delivery, infrastructure development and designing interventions to improve the healthcare of populations in greatest need. Most importantly, through partnerships, there is availability of financial resources to accomplish partnership goals. For success to be achieved, there must be effective communication and leadership, values and accountability that go into the process of partnership functioning. Trust interacts with different elements that create partnerships where there is co-ownership of study rewards. Challenging aspects of the interaction are largely due to funding mechanisms where 90% of the funding for health research is from northern partners. This funding mechanism results in power imbalances that lead to publication challenges, dictation of research agenda and ownership of samples and data leading to a general lack of motivation to collaborate.ConclusionMistrust has implications on joint working such that partners find it difficult to work together and produce results greater than their individual efforts. Property rights and resource sharing must be resolved early in the partnership and each partner’s contributions recognised. These findings highlight areas that partnerships need to focus on to make the most of guidelines on research partnership with developing countries.",https://www.semanticscholar.org/paper/e7eb56d6f5131a42876122e868468c60a58d2996,CS,Qualitative
Investigating the use of an Artificial Intelligence Model in an ERP Cloud-Based System,"Enterprise Resource Planning (ERP) systems are necessary to improve an enterprise's management performance. However, the perception of information technology (IT) professionals about the integration of artificial intelligence (AI) and machine learning with ERP cloud service platforms is unknown. Few studies have examined how leaders can implement AI for strategic management, but no study has qualitatively explored AIs integration in the cloud ERP system. This qualitative phenomenological study explored IT professionals’ perceptions regarding the integration of AI and Supervised-machine (S-machine) learning into cloud service platforms in the enhancement of the cloud ERP system. Two research questions were developed for this study: 1) What are the perceptions of IT professionals regarding the use of an AI model to integrate SaaS and ERP? and 2) What are the perceptions of IT professionals regarding how AI can be integrated in order to enhance the security of using an ERP cloud-based system? Through a hermeneutical lens and a focus on integrating the Application Programming Interface (API), purposive sampling was used to interview five AI experts, three Machine Learning (ML) experts, five Cybersecurity experts, and two Cloud Service Providers provided their lived experiences with AI and S-machine learning. Five main themes emerged, including 1) use of an AI model to integrate SaaS and ERP helped perform work efficiently, 2) challenges for integrating AI into cloud service ERP and SaaS, 3) resources needed to fully implement an AI into cloud-service ERP or SaaS, 4) the best practices for developing and implementing an AI model for ERP and SaaS, and 5) how security of an ERP clouds-based system is optimized by integrating AI. The culmination of these findings has positive implications for individuals and organizations to improve management performance. While this study does not proposal a new theory, this study extends current literature on the application of theories related to technology integration.",https://www.semanticscholar.org/paper/02b44dd354a69839b5d561a07dfc4d3ff0a020c4,CS,Qualitative
Formation and Mitigation of Technostress in the Personal Use of IT,"Understanding information technology (IT) use is vital for the information systems (IS) discipline due to its substantial positive and negative consequences. In recent years, IT use for personal purposes has grown rapidly. Although personal use is voluntary and can often reflect fun, technostress is a common negative consequence of such use. When left unaddressed, technostress can cause serious harm to IT users. However, prior research has not explained how technostress forms over time or how its mitigation takes place in a personal—rather than organizational—environment. To address these research gaps, we conducted a qualitative study with narrative interviews of IT users who had experienced technostress. This study contributes to (1) the technostress literature by unpacking states in which technostress forms and can be mitigated and (2) the IT affordance literature by explaining the role of affordances and their actualizations in technostress as well as introducing the new concept of actualization cost. In terms of practice, our findings help individuals and societies identify the development of technostress, understand the activities required for its mitigation, and recognize mitigation barriers.",https://www.semanticscholar.org/paper/2c6c7fd6963b1d887647479d4e243d2cefe691d6,IS,Qualitative
Implementing Data-Driven Smart City Applications for Future Cities,"Cities are investing in data-driven smart technologies to improve performance and efficiency and to generate a vast amount of data. Finding the opportunities to innovatively use this data help governments and authorities to forecast, respond, and plan for future scenarios. Access to real-time data and information can provide effective services that improve productivity, resulting in environmental, social, and economic benefits. It also assists in the decision-making process and provides opportunities for community engagement and participation by improving digital literacy and culture. This paper aims to review and analyze current practices of data-driven smart applications that contribute to the smooth functioning of urban city systems and the problems they face. The research methodology is qualitative: a systematic and extensive literature review carried out by PRISMA method. Data and information from different case studies carried out globally assisted in the inductive approach. Content analysis identified smart city indicators and related criteria in the case study examples. The study concluded that smart people, smart living, and smart governance methods that have come into practice at a later stage are as important as smart mobility, smart environments, and smart economy measures that were implemented early on, and cities are opening up to new, transparent participatory governance approaches where citizens play a key role. It also illustrates that the current new wave of smart cities with real time data are promoting citizen participation focusing on human, social capital as an essential component in future cities.",https://www.semanticscholar.org/paper/cb93d3397a67324a77b13e8b265e7c833def632c,IS,Qualitative
The Information System Development Based on Knowledge Management in Higher Education Institution,"The purpose of this study is to express the experiences, ideas, needs, and the use of information technology for the development of management information systems in a higher education institution. This study is qualitative approach research using the case study method. The research data collection uses the techniques of observation, interview, and documentation study. The research procedure used in this research consists of several research steps utilizing the case study method of Robert K. Yin: research planning, research design, research preparation, research data collection, research data analysis, and doing the research report. The research analysis is done by pattern matching. The data validity testing through data source triangulation and technique triangulation. The result of the study presents: (1) the analysis of management information system based on tacit and explicit knowledge through the process of exchanging experience, idea, and initiative, (2) the management information system design based on the needs analysis, and (3) the development of management information system using information technology.",https://www.semanticscholar.org/paper/10c5e0c39560f84b12a098ebe22dab66bc00e77d,IS,Qualitative
Synchrotron Scattering Methods for Nanomaterials and Soft Matter Research,"This article aims to provide an overview of broad range of applications of synchrotron scattering methods in the investigation of nanoscale materials. These scattering techniques allow the elucidation of the structure and dynamics of nanomaterials from sub-nm to micron size scales and down to sub-millisecond time ranges both in bulk and at interfaces. A major advantage of scattering methods is that they provide the ensemble averaged information under in situ and operando conditions. As a result, they are complementary to various imaging techniques which reveal more local information. Scattering methods are particularly suitable for probing buried structures that are difficult to image. Although, many qualitative features can be directly extracted from scattering data, derivation of detailed structural and dynamical information requires quantitative modeling. The fourth-generation synchrotron sources open new possibilities for investigating these complex systems by exploiting the enhanced brightness and coherence properties of X-rays.",https://www.semanticscholar.org/paper/fc6c9c0e9c7f2c62d314842c2240919eb3c8549a,CS,Qualitative
Exploring the socio-cultural factors in the implementation of public financial management information system in Ghana,"This paper aims to explore the socio-cultural factors that emerge in the implementation of integrated financial management information systems (IFMIS) in Ghana, a developing country.,A qualitative research approach was used with a case study design. The data were collected from archival documents and semi-structured face-to-face interviews with participants who played a significant role in the implementation of IFMIS in the Ghanaian public sector.,The findings show that although IFMIS was considered by the World Bank, Department for International Development (DFID), European Union and Danish International Development Agency to be rational, technical, universal and unproblematic, the use of the system in the Ghanaian public institutions was constrained by socio-cultural factors. These factors included power struggles between various technocrats; and negative attitudes such as opportunism and rent-seeking interest towards the IFMIS.,The research is grounded in a single case study, but the findings can be theoretically generalised to information technology (IT)-based financial management system exhibiting the same characteristics.,This study offers a practical implication for governments, consultants and donor agencies.,This study provides additional insight through the application of the sociology and duality of information technology theory to study a particular IT-based public financial management initiative.",https://www.semanticscholar.org/paper/5ebe6b67611825abb2c3b1f8e8f06839c96d01cf,IS,Qualitative
Literature Review: Penerapan Teknologi Informasi dalam Meningkatkan Kualitas Pelayanan Publik,"The number of complaints submitted by the public about the poor process of public services provided is a problem that must be immediately addressed by the Government. Along with the current advances in information technology, many institutions or agencies have begun to overhaul work systems in order to improve the quality of service to the public. This research aims to identify the application of information technology in improving the quality of public services. This research uses descriptive qualitative methods and the research sources used are secondary data, namely studies of literature, articles, journals, regulations, and sites on the internet related to this research. Validity data in study using triangulation. This study uses the theory of service quality, namely tangibles, reliability, responsiveness, assurance, and empathy. The findings of this study are that the application of information technology in improving the quality of public services has not fully run optimally, this can be seen from the dimensions of physical evidence (tangibles) that have not utilized digitalization in optimizing technology so that it requires space and storage space to store files and documents that are increasingly mounting. reliability in technology-based services, information and communication networks is very dependent on the weather, because if there is heavy rain or thunderstorms, the quality of the network becomes weak and even stops so that the process of administering (managing and processing) data becomes slow and can cause inaccuracies, power responsiveness is still not optimal because the services provided are still slow and require a long time",https://www.semanticscholar.org/paper/a5b546dec7f124ec4873d7475a52e539bb65692c,IT,Qualitative
Impact of Digital Farming on Sustainable Development and Planning in Agriculture and Increasing the Competitiveness of the Agricultural Business,"To develop agriculture, it is crucial to introduce digital farming. This is a fundamentally new management strategy based on digital technologies associated with the use of geographic information systems of global positioning, onboard computers, and smart equipment, as well as managerial and executive processes that can differentiate the methods of farming, fertilization, and adding chemical ameliorants and plant protection products. The study aims at determining the applied aspects and key components within a system of digital farming as a tool for the sustainable development of the agricultural business. The authors chose a mixed type of research, with a predominance of qualitative research methods. In particular, to collect data, the authors analyzed scientific sources on the research problem and conducted an expert survey measuring the degree of consistency of expert opinions with mathematical processing of the results obtained. It was determined that in Russia, it is necessary to consistently introduce the use of digital farming. This includes the introduction of parallel stirring, the ability to turn off the sections of the seeder on the floors, the re-equipment of crop protection sprayers to turn off the sections on the floors, and the acquisition of new equipment for differentiated fertilization. The authors conclude that the introduction of digital farming by agricultural producers is a tool for sustainable development and planning in agriculture and increasing the competitiveness of the agricultural business since it increases the economic (increased yields, reduced crop losses, increased land bank efficiency), environmental (production in risky farming areas), and social (increasing the level of personnel qualification and social standards) efficiency of their activities.",https://www.semanticscholar.org/paper/55dff1757a1be2103685c7c57ce37ed15e87871a,IS,Qualitative
Aspect-based Document Similarity for Research Papers,"Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity approach for research papers. Paper citations indicate the aspect-based similarity, i.e., the title of a section in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. According to our results, SciBERT is the best performing system with F1-scores of up to 0.83. A qualitative analysis validates our quantitative results and indicates that aspect-based document similarity indeed leads to more fine-grained recommendations.",https://www.semanticscholar.org/paper/5a0c9bbf0432dac8bd357a4aabf82b83a6c95524,CS,Qualitative
Analysing the Effect of Authentic Learning Activities on Achievement in Social Studies and Attitudes towards Geographic Information System (GIS),"This research aimed to examine the effect of authentic learning activities on achievement in social studies and attitudes towards Geographic Information System (GIS). To this end, it used an explanatory mixed method design where both quantitative and qualitative data were collected. The quantitative data were collected using an academic achievement test and the Geographic Information Systems (GIS) Attitude Scale developed by Baloglu Ugurlu and the qualitative data were collected using a semi-structured interview form. The research was carried out in two fifth-grade sections in a middle school in Kirsehir located in central Turkey during the fall semester of the 2019-2020 academic year. The study group consisted of 60 students, the half of whom were in the control group and the other half were in the experimental group. The experimental group was taught using authentic learning activities, while the control group was taught using textbook activities prepared in accordance with the social studies curriculum. According to the results of the analysis of the quantitative data, there was a significant difference in the test scores in favor of the experimental group and there was a significant positive difference between the experimental group’s mean pretest and posttest scores in the GIS Attitude Scale. The results of the analysis of the qualitative data showed that the students did not get bored with the social studies course, they were more active during the classes and attended the course thanks to the activities they did in the classroom, and they could see the landforms they had not had the chance to see before and learned better thanks to the GIS software. The research explores, for the first time, the effect of authentic learning on GIS attitude.",https://www.semanticscholar.org/paper/08990e2b8c89b53843e89432ad4a0f5fe508a62e,IS,Qualitative
Problem-Based Learning Strategy: Its Impact on Students’ Critical and Creative Thinking Skills,"The ability to think critically and creatively is essential for students to be able to face the challenges of the industrial revolution 4.0. Lectures must be designed to enhance students’ critical and creative thinking skills. This study aims to examine the implementation of problem-based learning in learning management information systems courses to improve students critical and creative thinking skills. The research design carried out was classroom action research. The subject in this study was students of Economics Education, Faculty of Economics, Universitas Negeri Semarang. The research was conducted in April-May 2019. The procedure for implementing class action research are two cycles (plan, action, observation, and reflection). The research data was taken by observation and interview methods. The data analysis method used is descriptive quantitative and qualitative methods. The results showed that two class action research cycles were well implemented. The application of the problem-based learning method can improve students’ critical and creative thinking skills. Students are able to solve a given case by doing the right analysis and being able to provide alternative solutions. Students consider the learning process to be more exciting and challenging. Students can express their opinions well in front of the class. The implication of this research is that lecturers can apply PBL with various combinations of learning strategies to improve students' critical and creative thinking skills.",https://www.semanticscholar.org/paper/3db3e87193b19746cc6d0217a7bc1554d7458459,IS,Qualitative
Pembelajaran Daring dan Luring pada Masa Pandemi Covid-19,"This study aims to explore whether online learning and offline learning can work well, so that educational goals can be achieved. This research is a phenomenological qualitative research to find out how the application of online learning and offline learning in one of the SD Negeri Sepatnunggal 02 in Cilacap Regency, Central Java, Indonesia. The steps of this research include action planning, action implementation, observation stage, and reflection stage. From the results of the study, there were several obstacles in its implementation, but they could be solved well by the teacher in order to educate the students. Both online and offline learning systems are expected for teachers to be creative in educating students, so that learning success can be achieved properly or effectively. This study provides information that teachers actually prefer offline learning where they can interact with students, and also students prefer offline learning with face-to-face interactions.",https://www.semanticscholar.org/paper/b0ccbf96ac4900a7c932c5c4590dd3e6ed429105,CS,Qualitative
Leveraging human factors in cybersecurity: an integrated methodological approach,"Computer and Information Security (CIS) is usually approached adopting a technology-centric viewpoint, where the human components of sociotechnical systems are generally considered as their weakest part, with little consideration for the end users’ cognitive characteristics, needs and motivations. This paper presents a holistic/Human Factors (HF) approach, where the individual, organisational and technological factors are investigated in pilot healthcare organisations to show how HF vulnerabilities may impact on cybersecurity risks. An overview of current challenges in relation to cybersecurity is first provided, followed by the presentation of an integrated top–down and bottom–up methodology using qualitative and quantitative research methods to assess the level of maturity of the pilot organisations with respect to their capability to face and tackle cyber threats and attacks. This approach adopts a user-centred perspective, involving both the organisations’ management and employees, The results show that a better cyber-security culture does not always correspond with more rule compliant behaviour. In addition, conflicts among cybersecurity rules and procedures may trigger human vulnerabilities. In conclusion, the integration of traditional technical solutions with guidelines to enhance CIS systems by leveraging HF in cybersecurity may lead to the adoption of non-technical countermeasures (such as user awareness) for a comprehensive and holistic way to manage cyber security in organisations.",https://www.semanticscholar.org/paper/edbc35c0452cb106132b8ebf911db8f0c78ccbda,IT,Qualitative
Mobile technology for street trading in Tanzania: A design science research approach for determining user requirements,"Street trading is an economic activity that is conducted by street traders in numerous urban parts of Tanzania. Street traders use mobile technology to search for markets. This study mapped the wants and needs of Tanzanian street traders and their customers in order to better understand the potential and pitfalls of technology to help their trade activities. Qualitative data were collected using in‐depth interviews with 22 street traders and 22 customers. In addition, two focus group discussions with 20 participants, including street traders and customers, were conducted. Data were analyzed through content analysis. The results identified a number of technology wants and needs shared by traders and customers that would ease the customers' access to products they want, and support the traders to promote their products, locate where the users are, support business growth, and predict sales potential. This research study contributes to understanding the technology needs of one marginalized group in Tanzania. The study facilitates discussion on the suitability of design research in the context of informal worker communities, and it points toward a path for design research of information systems that are grounded on the needs and knowledge of end‐users in their communities and contexts of use.",https://www.semanticscholar.org/paper/df83b83382d77161b156350bae6e41532a1a610f,IS,Qualitative
Peran Sistem Informasi Manajemen dalam Pengambilan Keputusan di Madrasah Ibtidaiyah Darussalam Pacet Mojokerto,"This article aims to describe: 1) explain the management of the Management Information System, 2) explain the decision making based on data in the Management information system. This research is a qualitative descriptive study. Data was obtained through observation, interviews, and documentation. All collected data were then analyzed using data in a few stages from reducing data, presenting data, to drawing evaluation conclusions. Research informants were the Headmaster of MI Darussalam, the School operator, and Teachers. The results showed that. The Role of Management Information System in Decision Making in MI Darussalam: (1) The components in the Management Information System has been fulfilled, (2) management of management information systems centered on two people, (3) The flow of MIS management in MI Darussalam is the process of data input, data processing, and data storage, (4) The head of the Madrasah puts forward the principle of deliberation to make decisions. (5) The Management Information System is used as a database for making decisions and making program activities. (6) There is a Decisive Support System (DSS) that is used, but it is still constrained.",https://www.semanticscholar.org/paper/d5e299f357335254aad9980d611e81a60662e733,IS,Qualitative
Coupling Mechanistic Approaches and Fuzzy Logic to Model and Simulate Complex Systems,"Several mathematical formalisms can be exploited to model complex systems, in order to capture different features of their dynamic behavior and leverage any available quantitative or qualitative data. Correspondingly, either quantitative models or qualitative models can be defined; bridging the gap between these two worlds would allow us to simultaneously exploit the peculiar advantages provided by each modeling approach. However, to date, the attempts in this direction have been limited to specific fields of research. In this paper, we propose a novel, general-purpose computational framework, named Fuzzy-mechanistic modeling of compleX systems (FuzzX), for the analysis of hybrid models consisting of a quantitative (or mechanistic) module and a qualitative module that can reciprocally control each other's dynamic behavior through a common interface. FuzzX takes advantage of precise quantitative information about the system through the definition and simulation of the mechanistic module. At the same time, it describes the behavior of components and their interactions that are not known in full details, by exploiting fuzzy logic for the definition of the qualitative module. We applied FuzzX for the analysis of a hybrid model of a complex biochemical system, characterized by the presence of positive and negative feedback regulations. We show that FuzzX is able to correctly reproduce known emergent behaviors of this system in normal and perturbed conditions. We envision that FuzzX could be employed to analyze any kind of complex system when quantitative information is limited, as well as to extend existing mechanistic models with fuzzy modules to describe those components and interactions of the system that are not fully characterized.",https://www.semanticscholar.org/paper/957e1c777d7c542afd0f34751da981e1095ec25c,CS,Qualitative
Information and Knowledge Processes in Health Care Value Co-Creation and Co-Destruction,"The purpose of this article is to explore how information and knowledge processes (IKPs) influence co-creation and co-destruction of value in a health service system. A qualitative, single embedded case-approach is taken to develop theory through a systematic combining of theoretical framework, empirical fieldwork, and case analysis. Six theoretical propositions are set to describe the linkage between IKPs and value co-creation (and co-destruction). The article contributes to health marketing and transformative service research by linking organizational activities to the motivation and empowerment of patients and their families, by highlighting the importance of the role of knowledge integration in value co-creation, by introducing a shift toward systems thinking, by conceptualizing value as manifested as health behavior change, and by underlining that health care processes may have a negative (value co-destructing) influence on the well-being of actors.",https://www.semanticscholar.org/paper/b73cd80808c1213b91ed0d3a560ac43ea41cab63,IS,Qualitative
Narrowing the age‐based digital divide: Developing digital capability through social activities,"Healthcare information technologies (HIT) have shown great potential for improving the effectiveness and quality of healthcare services. However, the inequal ability of older adults to use HIT may limit their exploitation of these benefits. To narrow the age‐based “digital divide”, this research further develops the concept of digital capability and emphasises the link between older adults and their social context. Based on a qualitative inductive study of 33 participants, who included Chinese patients and their family members, we generate a novel theoretical model for understanding the process by which social activities may shape older adults' digital capabilities. Based on the model, we suggest two strategies that might encourage older adults to engage with HIT. This research contributes to the information systems (IS) literature by strengthening digital capability as a conceptual lens to investigate individuals' engagement with information communication technologies (ICTs). It also extends research on the social context for ICT use by revealing how social processes at multiple levels influence digital capability development. Finally, this study offers practical implications for governments and private sectors to encourage and promote ICT use by older adults.",https://www.semanticscholar.org/paper/9b953a2d2228803fc778a42c4f2947d39e1e2d4a,IS,Qualitative
Enterprise Architecture Planning for Enterprise University Information System Using the TOGAF Architecture Development Method,"One of the important strategies in dealing with the development of information technology is the usage and increase of information systems support for the enterprise. Enterprise University Information System (EUIS), a product released by the University of Indonesia Computer Science Center, is a cloud-based system that can help the academic community-run main and supporting processes with data that can be tracked in real-time. EUIS is currently used by several clients. In the future, the company is targeting technological developments in EUIS that can be used by many other universities, so it can be easily adjusted and integrated with existing information systems at the client. Therefore, an update of enterprise architecture is needed to support the alignment of IT with existing business needs. The enterprise architecture design method used in Enterprise Architecture Planning (EAP) with the Open Group Architecture Framework (TOGAF). In this research using a qualitative methodology, TOGAF ADM can assist in planning architectural changes from monolithic to micro services by identifying several scenarios. The results showed that the decentralized microservice architecture design can be applied to EUIS. Pusilkom UI currently has implemented this design with a client that has condition 2, so that further research can be tested for other conditions.",https://www.semanticscholar.org/paper/fbf8932040ba9d326ce97677ae994d7d23edbc43,IS,Qualitative
Frontiers of magnetic force microscopy,"Since it was first demonstrated in 1987, magnetic force microscopy (MFM) has become a truly widespread and commonly used characterization technique that has been applied to a variety of research and industrial applications. Some of the main advantages of the method includes its high spatial resolution (typically ∼50 nm), ability to work in variable temperature and applied magnetic fields, versatility, and simplicity in operation, all without almost any need for sample preparation. However, for most commercial systems, the technique has historically provided only qualitative information, and the number of available modes was typically limited, thus not reflecting the experimental demands. Additionally, the range of samples under study was largely restricted to “classic” ferromagnetic samples (typically, thin films or patterned nanostructures). Throughout this Perspective article, the recent progress and development of MFM is described, followed by a summary of the current state-of-the-art techniques and objects for study. Finally, the future of this fascinating field is discussed in the context of emerging instrumental and material developments. Aspects including quantitative MFM, the accurate interpretation of the MFM images, new instrumentation, probe-engineering alternatives, and applications of MFM to new (often interdisciplinary) areas of the materials science, physics, and biology will be discussed. We first describe the physical principles of MFM, specifically paying attention to common artifacts frequently occurring in MFM measurements; then, we present a comprehensive review of the recent developments in the MFM modes, instrumentation, and the main application areas; finally, the importance of the technique is speculated upon for emerging or anticipated to emerge fields including skyrmions, 2D-materials, and topological insulators.",https://www.semanticscholar.org/paper/c0faa5fd1140efcd2625398bb2c8c6cc5c776dfd,IS,Qualitative
Characterisation of the Mycobiota on the Shell Surface of Table Eggs Acquired from Different Egg-Laying Hen Breeding Systems,"Microbial safety is an important factor contributing to the egg quality. During egg acquisition, there is significant risk of contamination of the eggshell surface with microscopic fungi. Mycelial hyphae may grow on the eggshell surface and penetrate into the egg content. However, there is no information on the populations of microscopic fungi on the eggshell surface and, consequently, on possible production of mycotoxins. Therefore, the aim of the study was to identify the species of microscopic fungi present on the eggshell surface acquired from different breeding systems and to measure the number of selected mycotoxins. The qualitative analysis resulted in the identification of 41 isolates on the surface of eggs. There were 7 isolates from the organic production system, 11 from the free-range production system, 14 from the deep litter indoor housing system and 9 from the cage farming production system. The research proved that the diversification in the population of mycobiota on the eggshells depended on the egg-laying hen breeding system. The microscopic fungi isolated from the eggshells included toxigenic and pathogenic species such as Fusarium culmorum and F. equiseti. As the egg storage time increased, fungi, including the pathogenic species, penetrated through the eggshells. In consequence, mycotoxins were identified in the egg whites. Type-A and type-B trichothecenes were found in the eggshell samples containing F. culmorum.",https://www.semanticscholar.org/paper/9147bb9543865c5b35e7cdbc16ad6c509bfb6f2b,CS,Qualitative
Active SLAM: A Review on Last Decade,"This article presents a comprehensive review of the Active Simultaneous Localization and Mapping (A-SLAM) research conducted over the past decade. It explores the formulation, applications, and methodologies employed in A-SLAM, particularly in trajectory generation and control-action selection, drawing on concepts from Information Theory (IT) and the Theory of Optimal Experimental Design (TOED). This review includes both qualitative and quantitative analyses of various approaches, deployment scenarios, configurations, path-planning methods, and utility functions within A-SLAM research. Furthermore, this article introduces a novel analysis of Active Collaborative SLAM (AC-SLAM), focusing on collaborative aspects within SLAM systems. It includes a thorough examination of collaborative parameters and approaches, supported by both qualitative and statistical assessments. This study also identifies limitations in the existing literature and suggests potential avenues for future research. This survey serves as a valuable resource for researchers seeking insights into A-SLAM methods and techniques, offering a current overview of A-SLAM formulation.",https://www.semanticscholar.org/paper/5115f8b41a4897e69bae0457994eb413e38204c2,IS,Qualitative
Data management of digitized indigenous knowledge system in repositories,"This paper assesses the data management of digitized Indigenous knowledge systems (IKS) in IKS repositories in South Africa. The study adopted a qualitative research method. The multiple case study research design was adopted to collect data from eight respondents in four Indigenous Knowledge Systems Documentation Centres (IKSDCs) in repositories spread across three provinces in South Africa. The findings revealed that the Department of Science and Technology (DST) coordinates the national IKS programmes under the National Recordal System (NRS) and they are responsible for the management of digitized IKS. The findings also revealed that although the National Indigenous Knowledge Management System (NIKMAS) was built for the management of the data, the system is still in its planning phase, and work is still in progress. In addition, it was discovered that information professionals like archivists and records managers are currently not involved in the management of IKS data in the repositories. The paper offers recommendations on the data management and storage of IKS, data description, mitigating the challenges, and some measures to help ensure the authenticity of the IKS data collected in the repositories.",https://www.semanticscholar.org/paper/d4c3e850c2d6d8827c2cbc8006030fccf4c6ee37,IS,Qualitative
Transformation strategies for the supply chain: the impact of industry 4.0 and digital transformation,"ABSTRACT This research focuses on the impact of ‘Industry 4.0ʹ and “Digital Transformation” on information sharing and decision making across the supply chain (SC). Following a qualitative approach, the findings are threefold: First, it is shown that the possibility of an entire SC integration based on new technologies is still at distance. Current burdens are the missing willingness to exchange far-reaching information even with long-term partners and the missing technological interface standards in order to enable a trouble-free communication alongside the SC. Second, the impact of Industry 4.0 and the Digital Transformation on decision making is greatly connected to information sharing. An increasing amount of decisions is prepared, recommended or even fully automated by information systems. However, usually, the human being still has the last word. Third, companies’ preparations for these impacts differ greatly. Whereas some companies rely on classical phase-based strategies and long-term visions, others do not have a long-term plan at all.",https://www.semanticscholar.org/paper/cae1074cb85baaab3ad87f22e74222e4ab9808bd,IS,Qualitative
Embodiment for Robotic Lower-Limb Exoskeletons: A Narrative Review,"Research on embodiment of objects external to the human body has revealed important information about how the human nervous system interacts with robotic lower limb exoskeletons. Typical robotic exoskeleton control approaches view the controllers as an external agent intending to move in coordination with the human. However, principles of embodiment suggest that the exoskeleton controller should ideally coordinate with the human such that the nervous system can adequately model the input-output dynamics of the exoskeleton controller. Measuring embodiment of exoskeletons should be a necessary step in the exoskeleton development and prototyping process. Researchers need to establish high fidelity quantitative measures of embodiment, rather than relying on current qualitative survey measures. Mobile brain imaging techniques, such as high-density electroencephalography, is likely to provide a deeper understanding of embodiment during human-machine interactions and advance exoskeleton research and development. In this review we show why future exoskeleton research should include quantitative measures of embodiment as a metric of success.",https://www.semanticscholar.org/paper/931f608e21a5f7a77a189b6ddba449f2038198da,CS,Qualitative
TMI (Too much information)! Effects of given information on organic chemistry students’ approaches to solving mechanism tasks,"We report our qualitative study of twenty-four students enrolled in the second-semester of a second-year undergraduate (sophomore-level) organic chemistry course, Organic Two. We asked the research participants to propose the product and electron-pushing mechanism of elementary mechanistic steps in the absence and presence of the corresponding overall transformation. We also asked the students about their preferences of representational systems when working on tasks common to Organic Two to ascertain the extent to which an external representation, rather than a task, might evoke a problem-solving strategy. In addition to familiarity to instructional materials, the main reason for which the students preferred line-angle formulas for nearly all of the task types is that the representational system allowed them most readily extract relevant, or otherwise useful, information without distracting them. However, line-angle formulas did not seem to cue students to the three-dimensional attributes of molecules; only dash-and-wedge structures and Newman and chair conformers did so. For the electron-pushing tasks, the research participants’ reasoning processes included at least some chemical characteristics of the species involved in the transformation when they were not given the product of reaction. When provided with the overall transformation, however, the students changed their focus to getting to the product. Consequently, they replaced correct answers with incorrect ones when given the reaction products. These results raise the possibility that traditional mechanism tasks may mask students’ mechanistic reasoning ability.",https://www.semanticscholar.org/paper/9a28bb88b1ba1a360fe1cc525ce2cd0d24944b2d,IS,Qualitative
"Kenyan health stakeholder views on individual consent, general notification and governance processes for the re-use of hospital inpatient data to support learning on healthcare systems","Increasing adoption of electronic health records in hospitals provides new opportunities for patient data to support public health advances. Such learning healthcare models have generated ethical debate in high-income countries, including on the role of patient and public consent and engagement. Increasing use of electronic health records in low-middle income countries offers important potential to fast-track healthcare improvements in these settings, where a disproportionate burden of global morbidity occurs. Core ethical issues have been raised around the role and form of information sharing processes for learning healthcare systems, including individual consent and individual and public general notification processes, but little research has focused on this perspective in low-middle income countries. We conducted a qualitative study on the role of information sharing and governance processes for inpatient data re-use, using in-depth interviews with 34 health stakeholders at two public hospitals on the Kenyan coast, including health managers, providers and researchers. Data were collected between March and July 2016 and analysed using a framework approach, with Nvivo 10 software to support data management. Most forms of clinical data re-use were seen as an important public health good. Individual consent and general notification processes were often argued as important, but contingent on interrelated influences of the type of data, use and secondary user. Underlying concerns were linked to issues of patient privacy and autonomy; perceived risks to trust in health systems; and fairness in how data would be used, particularly for non-public sector re-users. Support for engagement often turned on the anticipated outcomes of information-sharing processes, as building or undermining trust in healthcare systems. As reported in high income countries, learning healthcare systems in low-middle counties may generate a core ethical tension between supporting a public good and respecting patient autonomy and privacy, with the maintenance of public trust acting as a core requirement. While more evidence is needed on patient and public perspectives on learning healthcare activities, greater collaboration between public health and research governance systems is likely to support the development of efficient and locally responsive learning healthcare activities in LMICs.",https://www.semanticscholar.org/paper/dd3db61058ae1bb19a65f27018bdeaed90b619ed,IS,Qualitative
The impacts of climate change and variability on crop farming systems in Semi-Arid Central Tanzania: The case of Manyoni District in Singida Region,"This paper focuses on the impacts of climate change and variability on crop farming. The main objective of this paper is to assess the impacts of climate change and variability on crop farming systems in Manyoni district, Tanzania. This paper used mixed research design with both quantitative and qualitative research approaches. The research used different methods in collecting information concerning the impacts of climate change and variability on crop farming systems such as key informants interviews, focus group discussions and observations methods. Secondary data were collected through documentary review. Questionnaires were administered to 362 heads of households from four study villages namely Lusilile, Udimaa, Makanda and Magasai. Findings of the research revealed that majority of the farming households acknowledged occurrences of climate change and variability in their localities for the past 30 years. Heads of households perceived that rainfall has decreased while temperature has been increasing. This findings are in collaboration with that from the Tanzania meteorological data. Moreover, the findings revealed that climate change and variability have impacted crop farming system in different ways such as, damaging of crops and persistent low yields, reduction of crop varieties and species, decreasing soil fertility, increasing crop pests and diseases and drying of water sources. Therefore, this paper recommends that, collective efforts from government and other stakeholders should be harnessed and implemented in order to respond to these impacts so as to improve households’ food security in the stuyy area.",https://www.semanticscholar.org/paper/059428d061aa706e9c55d296913fb021d1d7f4f9,IS,Qualitative
Microservices migration patterns,"Microservices architectures are becoming the defacto standard for building continuously deployed systems. At the same time, there is a substantial growth in the demand for migrating on‐premise legacy applications to the cloud. In this context, organizations tend to migrate their traditional architectures into cloud‐native architectures using microservices. This article reports a set of migration and rearchitecting design patterns that we have empirically identified and collected from industrial‐scale software migration projects. These migration patterns can help information technology organizations plan their migration projects toward microservices more efficiently and effectively. In addition, the proposed patterns facilitate the definition of migration plans by pattern composition. Qualitative empirical research is used to evaluate the validity of the proposed patterns. Our findings suggest that the proposed patterns are evident in other architectural refactoring and migration projects and strong candidates for effective patterns in system migrations.",https://www.semanticscholar.org/paper/539e45841e74ffe553f771e223f5ff5c8c91a5f2,IT,Qualitative
Understanding U.S. Health Systems: Using Mixed Methods to Unpack Organizational Complexity,"Introduction: As hospitals and physician organizations increasingly vertically integrate, there is an important opportunity to use health systems to improve performance. Prior research has largely relied on secondary data sources, but little is known about how health systems are organized “on the ground” and what mechanisms are available to influence physician practice at the front line of care. Methods: We collected in-depth information on eight health systems through key informant interviews, descriptive surveys, and document review. Qualitative data were systematically coded. We conducted analyses to identify organizational structures and mechanisms through which health systems influence practice. Results: As expected, we found that health systems vary on multiple dimensions related to organizational structure (e.g., size, complexity) which reflects history, market and mission. With regard to levers of influence, we observed within-system variation both in mechanisms (e.g., employment of physicians, system-wide EHR, standardization of service lines) and level of influence. Concepts such as “core” versus “peripheral” were more salient than “ownership” versus “contract.” Discussion: Data from secondary sources can help identify and map health systems, but they do not adequately describe them or the variation that exists within and across systems. To examine the degree to which health systems can influence performance, more detailed and nuanced information on health system characteristics is necessary. Conclusion: The mixed-methods data accrual approach used in this study provides granular qualitative data that enables researchers to describe multi-layered health systems, grasp the context in which they operate, and identify the key drivers of performance.",https://www.semanticscholar.org/paper/4db2644c0452f6eaae6d7faad454908ae075d371,IS,Qualitative
Exploring systems thinking in school principals‘ decision-making,"ABSTRACT School principals‘ decisions are made within the complex organizations called schools. This study explored how systems thinking is reflected in principals‘ perceptions about their decision-making processes. Based on a qualitative analysis of interviews and focus groups, principals‘ descriptions of their own effective decision-making reflected systems thinking in the following three areas: (1) expanding the number of choices; (2) identifying possible consequences of various alternatives; and (3) seeking and analysing relevant information. As school is an inherently complex organization, the findings of this study shed light on the connection between the complicated nature of principals‘ decision-making and systems thinking. Implications and further research are discussed.",https://www.semanticscholar.org/paper/1d0fe04e4a1a93703bec71df52176fa24b11e700,IS,Qualitative
"An overview of ARAS method: Theory development, application extension, and future challenge","Multi‐attribute decision‐making (MADM) is one of the most important parts in decision‐making theory, and related research is becoming more and more popular over the past few years. Investigating that the information could be qualitative and quantitative, and the different measurement units cause difficulties in some MADM problems, the additive ratio assessment system (ARAS) method was proposed. The method tries to solve MADM problems through a simple way efficiently, and at the same time eliminates the influence of different measurement units. Till now, the method has received extensive attention and has been extended to different information environments and application fields. To know about the development of the method and improve the method efficiently, this paper reviews the studies on the ARAS method from the perspectives of basic information (including the bibliometrics analyses and the outline of ARAS method), the development on theory (including the development on MADM mechanism, different information environments, and combination with different methods), the development on the application and the future challenge. From the overview, the basic situations and the development of the ARAS method are presented clearly, and the analyses of the challenges can also provide useful and sufficient instructions for the future application and improvement of the method.",https://www.semanticscholar.org/paper/9851bd1c8c9ced70710d0efe6dd04d85462f6ba6,IS,Qualitative
Fourier Transform Infrared Spectroscopy in Oral Cancer Diagnosis,"Oral cancer is one of the most common cancers worldwide. Despite easy access to the oral cavity and significant advances in treatment, the morbidity and mortality rates for oral cancer patients are still very high, mainly due to late-stage diagnosis when treatment is less successful. Oral cancer has also been found to be the most expensive cancer to treat in the United States. Early diagnosis of oral cancer can significantly improve patient survival rate and reduce medical costs. There is an urgent unmet need for an accurate and sensitive molecular-based diagnostic tool for early oral cancer detection. Fourier transform infrared spectroscopy has gained increasing attention in cancer research due to its ability to elucidate qualitative and quantitative information of biochemical content and molecular-level structural changes in complex biological systems. The diagnosis of a disease is based on biochemical changes underlying the disease pathology rather than morphological changes of the tissue. It is a versatile method that can work with tissues, cells, or body fluids. In this review article, we aim to summarize the studies of infrared spectroscopy in oral cancer research and detection. It provides early evidence to support the potential application of infrared spectroscopy as a diagnostic tool for oral potentially malignant and malignant lesions. The challenges and opportunities in clinical translation are also discussed.",https://www.semanticscholar.org/paper/4e6a2918bb5a67737911abcf1c1f2e5bd7917aba,IS,Qualitative
shift of lexicon in traditional technology system in Tolaki community at Konawe district of Southeast Sulawesi,"This research discusses the lexicon used for traditional technology systems in the Tolaki community. Lexicon is a language component containing all information about the meaning and usage of words; the richness of words a language has. Lexicon runs into a shift due to certain factors such as changes in norms, culture, and environment caused by the development of science and technology. This research aims to analyze the level of shift and change in the meaning of the lexicon for traditional technology systems in the Tolaki community in Konawe Regency, Southeast Sulawesi. The method is qualitative descriptive. The data are taken from written sources, literature studies, by examining and recording some lexicons from the book “Tolaki Culture” by Abdurrauf Tarimana related to the lexicon in the traditional technology system of the Tolaki community. The validation of the data is then substantiated by questionnaire distribution in which the informants fill in lexicon data about agricultural technology systems and imply them in Tolaki language. The theory used is the lexical-semantic theory proposed by Pateda. The results showed that the lexicon for the Tolaki agricultural technology system is extinct and there is a shift in the lexicon.",https://www.semanticscholar.org/paper/9bbe2a05b6371da32f4c3e49cf76c1f80c2fac24,CS,Qualitative
Identifying the patterns: Towards a systematic approach to digital platform regulation,"Digital platforms have proven to be efficient matchmakers in our networked economy and society. However, their tremendous potential is a double-edged sword, and concerns about their malpractices and negative impact have risen remarkably. In the light of practical relevance, research on digital platform regulation (DPR) did not contribute very much. Especially information systems (IS) which, as a discipline, seems predestined to explain the problems at the confluence of technology–economy–society, has barely grazed the issue of DPR. Particularly, a contribution is missing that at least explores the problems with digital platforms systematically and comprehensively. In response to this aim, we pursue a two-step approach: First, we apply qualitative meta-analysis of 211 cases as well as a number of regulation papers and actual regulation endeavors to identify and conceptualize the problem scope and potential regulatory approaches for platform problems. Second, we condense our findings into a conceptual model. Our findings show that digital platforms exploit both platform- and monopoly-related problems to assert themselves in different markets. Through their impactful platform governance, they even have become serious challengers to the regulator in controlling market access, key conditions, and resources. At this, they rely on the efficient use of data and digital technologies to effectively orchestrate platform agents and their transactions. The outcome of the paper is a set of two structured instruments that fill a void in interdisciplinary research and provide support to DPR practitioners for a systematic approach to regulation.",https://www.semanticscholar.org/paper/ba9e4b7014d51b10e14e0673f4fe18e58ade7caa,IS,Qualitative
Web-Based Patient Referral System Design from Clinic to Hospital Using Object Oriented Programming System,"The clinic in Tangerang Regency, Indonesia is a healthcare institution where people seek treatment and medical advice. However, the patient referral system to the hospital in the polyclinic still uses the old method, which involves a letter of introduction from the clinic, a conventional way of documenting the chronology of the patient's illness and using a certificate or cover letter at the clinic. Patient data with problems going to the hospital is not stored in the database, and as a result, patients may have to postpone going to the hospital. The purpose of this study is to determine the current patient referral system from clinics to hospitals and design a web-based patient referral information system that can integrate patient data into the hospital. This research uses a descriptive qualitative method to analyze and design the system, employing the Object-Oriented System Design method. The output of this research is software, which is a web-based patient referral information system that displays completed patient data and disease chronology accurately and quickly, supporting digital concepts.",https://www.semanticscholar.org/paper/d8a73f371ccd9fd1d8b0f8de719334b8fa8b85b3,CS,Qualitative
#BlackGirlMagic: The identity conceptualization of Black women in undergraduate STEM education,"Funding information National Science Foundation, Grant/Award Number: 143681 Abstract Much of the research in science education that explores the influence of a racial and gendered identity on science, technology, engineering, and mathematics (STEM) engagement for Black women situate their identities primarily as responses to the oppression and struggles they face in STEM. In this study, we use Phenomenological Variant Ecological Systems Theory as a strengths‐based approach to investigate 10 undergraduate Black women’s perceptions of race and gender on their STEM identity development and engagement. The qualitative analysis of interview and journal data revealed these women enter STEM experiences cognizant of their race and gender identities, naming them in isolation and intersectionally as a potential risk or as being protective, positive, and empowering for their STEM engagement. These findings illuminate the importance of Black women self‐authoring their identities in STEM contexts, both in naming what is salient and defining what those names mean, and have implications for STEM retention and matriculation efforts.",https://www.semanticscholar.org/paper/fd2673743a66891d6af8219e231e6c47ce38769f,IS,Qualitative
A Value-sensitive Design Perspective of Cryptocurrencies: A Research Agenda,"Cryptocurrencies and their underlying blockchain technology have begun to transform numerous industries. Although we have seen an uptrend in the types of created cryptocurrencies, it has not yet translated into mainstream adoption., In this paper, we use value-sensitive design principles to identify values among current and potential cryptocurrency adopters. Using Bitcoin as the context for this qualitative research study, we use grounded theory analytical techniques to discover manifested values among users and non-users. We develop a cryptocurrency value-sensitive design framework to summarize our results. As our main contribution, we offer a research agenda based on the cryptocurrency stakeholders’ underlying value system. This agenda can help information systems scholars apply this value-sensitive design perspective to their own cryptocurrency research.",https://www.semanticscholar.org/paper/16d28d49b24eae278ee3f18646f2a3e364ce5eb7,IS,Qualitative
Implementasi Rekam Medik Elektronik: Sebuah Studi Kualitatif,"Strategy to improve the quality of hospital services through the Hospital Information System (HIS). HIS that contributes to improving services is Electronic Medical Records (EMR), with the aim of supporting integrated, sustainable, efficient and quality health care. This study explores the experience of EMR users in private Islamic hospital, a qualitative research design with a case study approach. Researchers selected 9 participants and conducted semi-structured interviews using interview guidelines. The phenomenological method approach was used for data analysis. Four themes emerged from the data: success factors, implementation barriers, confidentiality or security and the benefits of using EMR. Human resource support, hardwares, finance, leadership, training and technical support are the success factors for implementing EMR. However, there are implementation obstacles where the system error, the system design is not perfect, not compatible with other systems, lack of computer skills, power failure. Meanwhile, the confidentiality of the EMR must be considered regarding the access rights with the username and password to be able to log in and logout, as well as the EMR security risks. However, all of these provide benefits where the contents of the RM are more complete, business efficiency and communication, strategic benefits, and easy access to information. As soon as the hospital benefits from implementing EMR, the EMR implementation process needs to optimize the involvement and participation of organizational members with strong leadership to create a Hi-Tech Hospital. Managed barriers are a challenge in implementing EMR",https://www.semanticscholar.org/paper/fb49f56bc0996a94e5fe51128d19e212fdc417db,IS,Qualitative
A review of mental health and wellbeing under climate change in small island developing states (SIDS),"Small island developing states (SIDS) are often at the forefront of climate change impacts, including those related to health, but information on mental health and wellbeing is typically underreported. To help address this research lacuna, this paper reviews research about mental health and wellbeing under climate change in SIDS. Due to major differences in the literature’s methodologies, results, and analyses, the method is an overview and qualitative evidence synthesis of peer-reviewed publications. The findings show that mental health and wellbeing in the context of climate change have yet to feature prominently and systematically in research covering SIDS. It seems likely that major adverse mental health and wellbeing impacts linked to climate change impacts will affect SIDS peoples. Similar outcomes might also emerge when discussing climate change related situations, scenarios, and responses, irrespective of what has actually happened thus far due to climate change. In the context of inadequate health systems and stigmatisation of mental health diagnoses and treatments, as tends to occur globally, climate change narratives might present an opening for conversations about addressing mental health and wellbeing issues for SIDS.",https://www.semanticscholar.org/paper/d691486cd3302470a4aeb431ece9e3eba5e6a7a6,IS,Qualitative
Exploring Interaction with Remote Autonomous Systems using Conversational Agents,"Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems.",https://www.semanticscholar.org/paper/5731a9a4083f2cfb2cf8df62a84abc597fe799f3,CS,Qualitative
Decentralization: A handicap in fighting the COVID‐19 pandemic? The response of the regional governments in Spain,"Abstract The COVID‐19 pandemic has provided an ultimate testing ground for evaluating the resilience and effectiveness of federal and decentralized systems. The article analyses how the Spanish asymmetrical system of decentralization has responded to the pandemic, focusing on the management developed by the sub‐central governments (Autonomous Communities) during the first two waves of the pandemic in 2020. The research, which is both quantitative and qualitative, employs multidisciplinary tools and information sources, analyzing and linking fiscal and budgetary sources with the available statistics and information on health. Although the health, economic and social crisis caused by COVID‐19 has highlighted appreciable shortcomings related to the decentralized model of territorial organization – in questions of both regional financing and health management – the research concludes that decentralization has not per se been a handicap when confronting the pandemic in Spain.",https://www.semanticscholar.org/paper/2ad0c7bc57c88694cf52b8369b78a31474abec5b,IS,Qualitative
The role of leadership skills in the adoption of business intelligence and analytics by SMEs,"PurposeA key finding in the extant literature on adopting information systems has been the importance of management support and a champion. Further research has indicated that business managers need to have appropriate IT knowledge and skills to lead adoption adequately. In the context of small and medium enterprises (SMEs), this role is usually assumed by the owner/manager. This research aims to synthesise these two tenets by identifying and understanding the type of business intelligence and analytics (BI&A) leadership skills that owners/managers need to develop during the adoption of BI&A.Design/methodology/approachFive BI&A knowledge areas are identified and connected to different types of BI&A leadership skills through qualitative in-depth case studies of fourteen Australian SMEs.FindingsThe case studies reveal that several BI&A leadership skills need to be developed to bring SMEs to higher stages of BI&A adoption.Practical implicationsThis study proposes a BI&A leadership skills development framework that allows practitioners to develop progressive BI&A skills concerning managing data, analytical skills, business processes, social and cultural change, and investment decisions to achieve sustainable operational, management and strategic goals.Originality/valueThe paper takes a unique approach that links five knowledge areas to BI&A leadership skills that owners/managers need to ensure for effective adoption and orchestration of BI&A in their organisations. The BI&A leadership framework includes a developmental approach that relates to the iterative and complex nature of BI&A adoption.",https://www.semanticscholar.org/paper/6203eea4fbf39c7c52e50f2baaf8d6d134eddb23,IS,Qualitative
"The Factors That Promote Vaccine Hesitancy, Rejection, or Delay in Parents","Vaccines are some of the most cost-effective public health interventions for reducing disease burden and mortality. However, in recent years, health systems have faced a growing challenge with increasing number of parents who choose not to vaccinate their children. This decision has important implications for the health of communities worldwide, and despite a considerable amount of research that reinforces vaccine effectiveness and safety, there is uncertainty surrounding the factors that may encourage vaccine hesitancy in parents. In this interpretive review of 34 qualitative studies, we examine the factors that bolster vaccine hesitancy, rejection, and delay, and identify the overlaps and relationships between these factors. We depict our findings using the metaphor of a gear train where each gear represents one of seven factors: previous experiences; “natural” and “organic” living; perceptions of other parents; experiences interacting with health care providers; information sources, challenges, and preferences; distrust in health system players; and mandatory vaccine policies.",https://www.semanticscholar.org/paper/5bd8d65cad1f2e596be6b3cb4bb76c970c8276f0,IS,Qualitative
ANALYSIS OF INTERNAL CONTROL OF INVENTORY ACCOUNTING INFORMATION SYSTEM AT PT. ANDRE LAURENT,"Accounting information system (AIS) is used to assist data processing in a company with the aim to optimize the performance of employees based on the duties and responsibilities of each employee. PT Andre Laurent currently still has obstacles in recording its inventory that causing the difference of inventory stock. This study aims to analyze the procedures of accounting information systems and implementation of internal controls in the company and to design proposals and also to implement a good accounting information system for the company. The research method used in this research is qualitative research method, data collection method by interview, observation or direct observation to the company, documentation and questionnaire is distributed to users of accounting information system proposed based on the discussion. The results of the research are such as the application of FIFO (First-In First-Out) method to inventory recording, improvement of procurement and expenditure procedures, and implementation of Accurate 5 Education Software that will optimize the company's operational activities.",https://www.semanticscholar.org/paper/b62a5dfe99c5fe486a117dc4ebbdd57bc8b5a88b,IS,Qualitative
Eugene Garfield's Ideas and Legacy and Their Impact on the Culture of Research,"Eugene Garfield advanced the theory and practice of information science and envisioned information systems that made the discovery of scientific information much more efficient. The Institute for Scientific Information (ISI), which he founded in Philadelphia in 1960, developed innovative information products that have revolutionized science. ISI provided current scientific information to researchers all over the world by publishing the table of contents of key scientific journals in the journal Current Contents (CC). Garfield introduced the citation as a qualitative measure of academic impact and propelled the concepts of “citation indexing” and “citation linking”, paving the way for today’s search engines. He created the Science Citation Index (SCI), which raised awareness about citations; triggered the development of new disciplines (scientometrics, infometrics, webometrics); and became the foundation for building new important products such as Web of Science. The journal impact factor (IF), originally designed to select journals for the SCI, became the most widely accepted tool for measuring academic impact. Garfield actively promoted English as the international language of science and became a powerful force in the globalization of research. His ideas changed how researchers gather scientific information, communicate their findings, and advance their careers. This article looks at the impact of Garfield’s ideas and legacy on the culture of research.",https://www.semanticscholar.org/paper/a54becd35545a04eac8f9e1e3eb1cba0f3b95ed6,IS,Qualitative
Strategies to improve research capacity across European general practice: The views of members of EGPRN and Wonca Europe,"Abstract Background: The effectiveness of any national healthcare system is highly correlated with the strength of primary care within that system. A strong research basis is essential for a firm and vibrant primary care system. General practitioners (GPs) are at the centre of most primary care systems. Objectives: To inform on actions required to increase research capacity in general practice, particularly in low capacity countries, we collected information from the members of the European General Practice Research Network (EGPRN) and the European World Organization of Family Doctors (Wonca). Methods: A qualitative design including eight semi-structured interviews and two discursive workshops were undertaken with members of EGPRN and Wonca Europe. Appreciative inquiry methods were utilized. Krueger’s (1994) framework analysis approach was used to analyse the data. Results: Research performance in general practice requires improvements in the following areas: visibility of research; knowledge acquisition; mentoring and exchange; networking and research networks; collaboration with industry, authorities and other stakeholders. Research capacity building (RCB) strategies need to be both flexible and financially supported. Leadership and collaboration are crucial. Conclusion: Members of the GP research community see the clear need for both national and international primary care research networks to facilitate appropriate RCB interventions. These interventions should be multifaceted, responding to needs at different levels and tailored to the context where they are to be implemented.",https://www.semanticscholar.org/paper/bb4c33c41d3d068bd22adbe9a0d86a5ab0faca00,IS,Qualitative
Security Estimation of the Simulation Polygon for the Protection of Critical Information Resources,"In this article, the research of information system protection by ana­ ly­ zing the risks for identifying threats for information security is considered. Information risk analysis is periodically conducted to identify information security threats and test the information security system. Currently, various information risk analysis techni­ ques exist and are being used, the main difference being the quantitative or qualitative risk assessment scales. On the basis of the existing methods of testing and evaluation of the vulnerabilities for the automated system, their advantages and disadvantages, for the possibility of further comparison of the spent resources and the security of the information system, the conclusion was made regarding the deter­ mi­ nation of the optimal method of testing the information security system in the context of the simulated polygon for the protection of critical information resources. A simula­ tion ground for the protection of critical information resources based on GNS3 application software has been developed and implemented. Among the considered methods of testing and risk analysis of the automated system, the optimal iRisk methodology was identified for testing the information security system on the basis of the simulated. The quantitative method Risk for security estimation is considered. Generalized iRisk risk assessment is calculated taking into account the following parameters: Vulnerabili­ ty — vulnerability assessment, Threat — threat assessment, Control — assessment of security measures. The methodology includes a common CVSS vul­ nerability assessment system, which allows you to use constantly relevant coefficients for the calculation of vulnerabilities, as well as have a list of all major vulnerabilities that are associated with all modern software products that can be used in the automated system. The known software and hardware vulnerabilities of the ground are considered and the resistance of the built network to specific threats by the iRisk method is calculated.",https://www.semanticscholar.org/paper/edef74e6bcdb63a258d728e7af982d6daa464872,CS,Qualitative
Using Data Mining Techniques for Detecting Dependencies in the Outcoming Data of a Web-Based System,"The increasing amount of data from web systems data is becoming one of the most valuable resources for information retrieval and knowledge discovery. The huge content of information makes it an important area for data mining research. To analyze the dependencies of the outcoming data, expressed as query scenarios, we present a new approach for evaluating the behavior of interactive web systems by applying different data mining techniques to solve the problem. We propose tools that take outcoming logs as input, analyze them, and provide information about web client actions. Qualitative and quantitative automatic evaluation of the data can explain the connections between the most significant parameters of the system in particular scenarios. In this paper, we propose a new method, which can be used to efficiently verify the type of client behavior of a web system or design of the system. The analysis of results demonstrates the possibility of efficient pattern search.",https://www.semanticscholar.org/paper/89a126d90497421f717a2324a5d3c6d0f2589df0,CS,Qualitative
Intelligent Conversational Agents in Mental Healthcare Services: A Thematic Analysis of User Perceptions,"Abstract Background: The emerging Artificial Intelligence (AI) based Conversational Agents (CA) capable of delivering evidence-based psychotherapy presents a unique opportunity to solve longstanding issues such as social stigma and demand-supply imbalance associated with traditional mental health care services. However, the emerging literature points to several socio-ethical challenges which may act as inhibitors to the adoption in the minds of the consumers. We also observe a paucity of research focusing on determinants of adoption and use of AI-based CAs in mental healthcare. In this setting, this study aims to understand the factors influencing the adoption and use of Intelligent CAs in mental healthcare by examining the perceptions of actual users. Method: The study followed a qualitative approach based on netnography and used a rigorous iterative thematic analysis of publicly available user reviews of popular mental health chatbots to develop a comprehensive framework of factors influencing the user’s decision to adopt mental healthcare CA. Results: We developed a comprehensive thematic map comprising of four main themes, namely, perceived risk, perceived benefits, trust, and perceived anthropomorphism, along with its 12 constituent subthemes that provides a visualization of the factors that govern the user’s adoption and use of mental healthcare CA. Conclusions: Insights from our research could guide future research on mental healthcare CA use behavior. Additionally, it could also aid designers in framing better design decisions that meet consumer expectations. Our research could also guide healthcare policymakers and regulators in integrating this technology into formal healthcare delivery systems. Available at: https://aisel.aisnet.org/pajais/vol12/iss2/1/ Recommended Citation Prakash, Ashish Viswanath and Das, Saini (2020) ""Intelligent Conversational Agents in Mental Healthcare Services: A Thematic Analysis of User Perceptions,"" Pacific Asia Journal of the Association for Information Systems: Vol. 12: Iss. 2, Article 1. DOI: 10.17705/1pais.12201",https://www.semanticscholar.org/paper/6df570947d0691b7d65ea8ffd4d6ab093f191606,IS,Qualitative
The Influence of Friends on Teen Vaping: A Mixed-Methods Approach,"Vaping is popular among adolescents. Previous research has explored sources of information and influence on youth vaping, including marketing, ads, family, peers, social media, and the internet. This research endeavors to expand understanding of peer influence. Our hypothesis is that friends’ influence on teen vapers’ first electronic nicotine delivery systems (ENDS) use varies by demographic variables and awareness of ENDS advertising. In August–October 2017, youth (n = 3174) aged 13–18 completed an online survey to quantify ENDS behaviors and attitudes and were invited to participate in follow-up online research in November-December 2017 to probe qualitative context around perceptions and motivations (n = 76). This analysis focused on the ENDS users, defined as having ever tried any ENDS product, from the survey (n = 1549) and the follow-up research (n = 39). Among survey respondents, friends were the most common source of vapers’ first ENDS product (60%). Most survey respondents tried their first ENDS product while “hanging out with friends” (54%). Among follow-up research participants, the theme of socializing was also prominent. ENDS advertising and marketing through social media had a strong association with friend networks; in fact, the odds of friends as source of the first vaping experience were 2 times higher for those who had seen ENDS ads on social media compared with other types of media. The influence of friends is particularly evident among non-Hispanic Whites, Hispanics/Latinos, those living in urban areas, those living in high-income households, those with higher self-esteem, and those who experiment with vaping. These findings support the premise that peer influence is a primary social influencer and reinforcer for vaping. Being included in a popular activity appears to be a strong driving force.",https://www.semanticscholar.org/paper/f4beae0f6d4cedec96aa7b4e0b31ab733add0eea,IT,Qualitative
SISTEM INFORMASI AKADEMIK SMK NEGERI 3 SUMBAWA BESAR BERBASIS WEB,"This study aims to design and build a Web-Based Academic Information System of SMK Negeri 3 Sumbawa in order to (1) Assist Curriculum section work in managing school academic information which was previously still manual by printing and pasting on the bulletin board, so that the information system built makes curriculum part work is more effective and efficient. (2) Facilitate students and teachers in obtaining schoolrelated academic information such as student data, teacher data, lesson schedules, value data whenever needed. This research belongs to descriptive qualitative research. The use of descriptive type is intended as a problem-solving procedure that is investigated by describing the state of the subject and object under study at present based on facts that are visible and as they are. Development of information systems with PHP programming languages and MySQL databases. In this study using the method of software development extreme programming, data collection methods with observation, interviews, and literature studies, system modeling using a structured model namely ERD (Entity Relationship Diagram) and DFD (Data Flow Diagram), while testing the system with the black box method testing is testing the system in terms of functionality.",https://www.semanticscholar.org/paper/1be584de9574eb87012707e6794bd46007993420,CS,Qualitative
Analysis of scientific production on organizational innovation,"Abstract Studies in organizational innovation have shown rapid growth in the last two decades, so it is necessary to perform a qualitative and quantitative analysis of scientific production to know its current status and development in business activity. In this research, bibliometric analysis and mapping were performed on the publications indexed in the Scopus database between 1996 and 2015, obtaining relevant information on scientific production, contributions by region/country, institutions involved, topics and influential authors. A map of terms was generated that establishes research areas related to organizational systems, firm relations and organizational change, as well as a map of citations showing the disciplines of administration, dynamic capacities and organizational learning related to the subject of study and its main exponents. The findings of the study allow identifying areas of current interest and research potential in Organizational Innovation.",https://www.semanticscholar.org/paper/2e07800761c25324ce655560cf7d66ce983d3e9c,IS,Qualitative
Intelligent Assistants,"Intelligent assistants are an increasingly commonplace class of information systems spanning a broad range of form and complexity. But what characterizes an intelligent assistant, and how do we design better assistants? In the paper, the authors contribute to scientific research in the domain of intelligent assistants in three steps, each building on the previous. First, they investigate the historical context of assistance as human work. By examining qualitative studies regarding the work of human assistants, the authors inductively derive concepts crucial to modeling the context of assistance. This analysis informs the second step, in which they develop a conceptual typology of intelligent assistants using 111 published articles. This typology explicates the characteristics (what or how) of intelligent assistants and their use context (who or which). In the third and final step, the authors utilize this typology to shed light on historical trends and patterns in design and evaluation of intelligent assistants, reflect on missed opportunities, and discuss avenues for further exploration.",https://www.semanticscholar.org/paper/cd2f1fa00e078f46c5f20aa413041416cebac2b7,IS,Qualitative
Investigating the Impact of Cloud Computing Vendor on the Adoption of Cloud Computing,"Cloud computing offers significant impacts on organization by changing how information systems are developed, deployed, operated, maintained, and paid for. Therefore, the adoption of cloud computing becomes the focus of relevant research; however, previous studies have mostly studied the factors affecting cloud computing adoption from the perspective of adopters, ignoring the influence of the vendors. This study defines cloud service capability and develops scale to measure it from the perspective of cloud computing vendors to empirically examine the impact of the supply-side of cloud computing. The initial scale of cloud service capability is constructed using qualitative research, and the formal scale is obtained after two rounds of pretest. The statistical results of matched data collected from 132 cloud computing vendors and their users show that cloud service capability significantly affects cloud computing adoption. This study shifts the research perspective on cloud adoption to make theoretical contributions and management insights from the perspective of cloud computing vendors.",https://www.semanticscholar.org/paper/ae2244a3b5c3a32a2801cab517ddaedb802cedb4,IS,Qualitative
Towards Sustainable Development through Higher Education Quality Assurance,"This study aims to identify the role of higher education quality assurance in achieving sustainable development goals. To support this aim, the following objectives were formulated: to discuss and summarize the best practices of QA agencies in promoting SDGs, to identify how Ukrainian universities consider SDGs in their policies, to develop recommendations for internal, and external quality assurance systems regarding SDG achievement. This qualitative study is based on a case study, observation, and questionnaire methods. Ukrainian higher education quality assurance systems are taken as a case study for this article. Primary data are collected through an online questionnaire and observation of Ukrainian universities’ publicly available information regarding sustainable development activities at their official websites. Additionally, some publicly available documents, reports, and materials on the experience of foreign quality assurance agencies are also reviewed, compared, and contrasted. The results of the research can be used at national levels where higher education standards should include competence in sustainable development goals, at institutional level to improve HEIs’ quality assurance system, and at the study program level to include SDG consideration as one of the requirements during internal quality assurance procedures.",https://www.semanticscholar.org/paper/a47ea4312a1e9f95467239c7d9ff987e8233db64,IS,Qualitative
Hexagonal fuzzy approximation of fuzzy numbers and its applications in MCDM,"Numerous research papers and several engineering applications have proved that the fuzzy set theory is an intelligent effective tool to represent complex uncertain information. In fuzzy multi-criteria decision-making (fuzzy MCDM) methods, intelligent information system and fuzzy control-theoretic models, complex qualitative information are extracted from expert’s knowledge as linguistic variables and are modeled by linear/non-linear fuzzy numbers. In numerical computations and experiments, the information/data are fitted by nonlinear functions for better accuracy which may be little hard for further processing to apply in real-life problems. Hence, the study of non-linear fuzzy numbers through triangular and trapezoidal fuzzy numbers is very natural and various researchers have attempted to transform non-linear fuzzy numbers into piecewise linear functions of interval/triangular/trapezoidal in nature by different methods in the past years. But it is noted that the triangular/trapezoidal approximation of nonlinear fuzzy numbers has more loss of information. Therefore, there is a natural need for a better piecewise linear approximation of a given nonlinear fuzzy number without losing much information for better intelligent information modeling. On coincidence, a new notion of Generalized Hexagonal Fuzzy Number has been introduced and its applications on Multi-Criteria Decision-Making problem (MCDM) and Generalized Hexagonal Fully Fuzzy Linear System (GHXFFLS) of equations have been studied by Lakshmana et al. in 2020. Therefore, in this paper, approximation of nonlinear fuzzy numbers into the hexagonal fuzzy numbers which includes trapezoidal, triangular and interval fuzzy numbers as special cases of Hexagonal fuzzy numbers with less loss/gain of information than other existing methods is attempted. Since any fuzzy information is satisfied fully by its modal value/core of that concept, any approximation of that concept is expected to be preserved with same modal value/core. Therefore, in this paper, a stepwise procedure for approximating a non-linear fuzzy number into a new Hexagonal Fuzzy Number that preserves the core of the given fuzzy number is proposed using constrained nonlinear programming model and is illustrated numerically by considering a parabolic fuzzy number. Furthermore, the proposed method is compared for its efficiency on accuracy in terms of loss of information. Finally, some properties of the new hexagonal fuzzy approximation are studied and the applicability of the proposed method is illustrated through the Group MCDM problem using an index matrix (IM).",https://www.semanticscholar.org/paper/76153c42b9b6bb24c679ea981ca00187ed6ec778,IS,Qualitative
The development strategy of smart campus for improving excellent navy human resources,"The globalization process gave birth to an era known as the Industrial Revolution 4.0, which was marked by the existence of Information Communication Technology (ICT) such as artificial intelligence, advance robotic, autonomous vehicles, virtual reality, and cyber-physical systems. ICT raises a major and fundamental change in human life where there has been a shift in activities that were originally carried out in the real world now carried out in cyberspace. All digital and all activities carried out online or internet media. This study aims to obtain the development of the Smart Campus strategy in the Indonesia Naval Technology College (STTAL), to improve excellent human resources. The method applied in this research is to use an operational approach to qualitative analysis on a real strategy implemented. The results obtained are the development of Smart Campus at STTAL carried out with integrated and systemic e-Office, e-Learning, e-Library, and Academic Information System programs. The Smart Campus program development strategy can increase STTAL's human resources to be excellent and advanced. Human resources include professional lecturer resources, smart students, stakeholder partners, and educational employees. The conclusions and contributions obtained are the development of Information Communication Technology (ICT) based on Smart Campus (e-Office, e-Learning, e-Library, and integrated Academic Information System) has a high role to improve professionalism in organizing advanced and quality higher education so that it can produce excellent Indonesian human resources.",https://www.semanticscholar.org/paper/6bb5e9250be379d54afaae3bd28274d2ab3ce92f,CS,Qualitative
Exploring SME cybersecurity practices in developing countries,"ABSTRACT The continued use of information technology systems by small and medium enterprises (SMEs) in developing countries has the potential to bring significant benefits but, at the same time, expose them to online cybersecurity threats. Addressing these threats is, therefore, of paramount importance for developing countries, not only because SMEs are seen as the vehicle for employment and job creation, but because research on SMEs and cybersecurity in this context is limited. This study is a contribution toward addressing this gap. The purpose of this study is, therefore, to explore SME cybersecurity practices and the challenges they face in developing countries. The goal is to sensitize practitioners and government institutions about the challenges and practices faced by SMEs, so that the various parties can work collaboratively in providing context-specific solutions to address these challenges and improve current cybersecurity practices. The study follows a qualitative enquiry approach to solicit information from three South African SMEs that had implemented cybersecurity practices. The findings show that an SME’s perception of cybersecurity is constrained by internal factors of budget, management support, and attitudes. Further findings show that SMEs’ cybersecurity practices are affected by the landscape of cybersecurity, as well as institutional pressures.",https://www.semanticscholar.org/paper/0c75e2324e0e6f7c3c8e7053250163d147972486,IS,Qualitative
The Implementation of E-Government in The Industrial Revolution Era 4.0 in Indonesia,"The implementation of electronic government in the era of the industrial revolution is very influential on people's lives in Indonesia. The transition period towards the industrial revolution era 4.0 is very interesting to discuss, bearing in mind that the Republic of Indonesia is also required to implement industry 4.0 in the government component. This study uses a qualitative method that is literature study. Qualitative research is research that produces information in the form of notes and descriptive data contained in the text under study. The data used is sourced from secondary data relating to the laws and regulations of the Republic of Indonesia related to the application of Electronic Government in the era of the industrial revolution 4.0. The results showed that the development of e-government systems in Indonesia in quantity began to increase but the quality was still inadequate because e-government implementation was not evenly distributed in all regions and still functioned as a provider of static information only. Meanwhile, the fundamental obstacle in the implementation of electronic government in the industrial revolution lies at the local government level. The e-government projection on the development of the industrial revolution must have the best formula in order to achieve the stated goals.",https://www.semanticscholar.org/paper/667558250cf8d9946af373733cff186bbe70fc1c,CS,Qualitative
Factors influencing e-health implementation by medical doctors in public hospitals in Zimbabwe,"Background: Improving access to health care services in both developed and developing countries through information communication technology (ICT) has been getting particular attention from government, medical researchers and practitioners. This has seen many governments proposing the implementation of healthcare systems that are centred on technology, while researchers and practitioners have been arguing for policies that promote the use of technology in healthcare provision. Objective: The main objective of this study was to determine the factors influencing implementation of e-health by medical doctors in public hospitals in Zimbabwe. Methods: The study was guided by a qualitative research in conjunction with multiple-case studies. Qualitative data were collected using 20 semi-structured interviews from selected hospitals concerning the implementation of e-health by medical doctors in public hospitals. Hospitals were selected using random sampling, while purposive sampling was used to select the 20 doctors. In addition, the researcher conducted direct observations at five hospitals. Furthermore, data concerning policy issues in Zimbabwe’s e-health were collected using document review process. Data from the interviews were analysed using data-driven thematic coding. This solo approach was conducted because the researcher intended to reveal e-health influencing factors that could not be revealed by related literature. Results: This study reveals that the implementation of e-health by medical doctors in public hospitals in Zimbabwe is influenced by both internal and external factors. Internal factors include ICT infrastructure and e-health technologies, ICT skills and knowledge, technical support, security concerns, lack of basic medical facilities, demographic factors such as age and doctor–patient relationship. External factors are health policy, funding and bureaucracy. Conclusion: The idea of e-health is relatively new to healthcare centres in Zimbabwe. Its application has not been sufficiently addressed. The study shows that the success of an e-health system depends on internal and external factors. There is a great potential for implementing e-health in Zimbabwe if these factors are taken into consideration. Otherwise, Zimbabwe will continue to lag behind in the implementation of e-health systems in public hospitals.",https://www.semanticscholar.org/paper/eceac4a9dca79d9f951a6b534d03fe9cab7d1959,CS,Qualitative
Democratic governance in an age of datafication: Lessons from mapping government discourses and practices,"There is an abundance of enthusiasm and optimism about how governments at all levels can make use of big data, algorithms and artificial intelligence. There is also growing concern about the risks that come with these new systems. This article makes the case for greater government transparency and accountability about uses of big data through a Government of Canada qualitative research case study. Adapting a method from critical cartographers, I employ counter-mapping to map government big data practices and internal discussions of risk and challenge. I do so by drawing on interviews and freedom of information requests. The analysis reveals that there are more concerns and risks than often publicly discussed and that there are significant areas of silence that need greater attention. The article underlines the need for our democratic systems to respond to our new datafied contexts by ensuring that our institutions make changes to better protect citizen rights, uphold democratic principles and ensure means for citizen intervention.",https://www.semanticscholar.org/paper/e12d7b94beedff68100e7ebb704e8eefc073fed0,CS,Qualitative
The Bundling of Business Intelligence and Analytics,"ABSTRACT Business Intelligence (BI) and analytics are often bundled together in the Information Systems (IS) literature as BI&A. We argue that BI and analytics are different information systems that are used differently, require different tools and technical skills, and are targeted at different audiences. We posit that this bundling is not relevant to the practice, rather the result of a fashion wave inspired by Gartner technology trends. We argue that such misrepresentation disassociates IS research from practice. We performed a pilot qualitative study as well as secondary data analysis on job data from Indeed.com to validate our arguments. The paper contributes to the IS research by critiquing the status quo on BI&A, which has existed since 2012. Implications of the unbundling of BI and analytics for future research and academics are discussed.",https://www.semanticscholar.org/paper/df328b2782ce2d93f02b8e8d4c21bd54c1fdb0e1,IS,Qualitative
Indigenous Australians’ Experiences of Cancer Care: A Narrative Literature Review,"To provide the latest evidence for future research and practice, this study critically reviewed Indigenous peoples’ cancer care experiences in the Australian healthcare system from the patient’s point of view. After searching PubMed, CINAHL and Scopus databases, twenty-three qualitative studies were included in this review. The inductive approach was used for analysing qualitative data on cancer care experience in primary, tertiary and transitional care between systems. Three main themes were found in healthcare services from Indigenous cancer care experiences: communication, cultural safety, and access to services. Communication was an important theme for all healthcare systems, including language and literacy, understanding of cancer care pathways and hospital environment, and lack of information. Cultural safety was related to trust in the system, privacy, and racism. Access to health services was the main concern in transitional care between healthcare systems. While some challenges will need long-term and collective efforts, such as institutional racism as a downstream effect of colonisation, cultural training for healthcare providers and increasing the volume of the Indigenous workforce, such as Indigenous Liaison Officers or Indigenous Care Coordinators, could effectively address this inequity issue for Indigenous people with cancer in Australia in a timely manner.",https://www.semanticscholar.org/paper/21073762b8e2ac80e8647785b84623fba4f5bca2,CS,Qualitative
Measuring instructors continued intention to reuse Google Classroom in Iraq: a mixed-method study during COVID-19,"Purpose The use of learning management systems (LMSs) such as Google Classroom has increased significantly in higher education institutes during the COVID-19 pandemic. However, only a few studies have investigated instructors’ continued intention to reuse LMS. The purpose of this study is to investigate the factors that influence instructors’ intention to reuse an LMS in higher education institutes. Design/methodology/approach This study adopted a mixed-method research design. In the quantitative section, an integrated model of technology acceptance model and information system success model is proposed to explore the effects of system quality, service quality, information quality, perceived ease of use and perceived usefulness on instructors’ satisfaction and how their satisfaction will influence their intention to reuse Google Classroom in the future. In the qualitative section, to gain more understanding, instructors were asked to identify the challenges that inhibit the adoption of e-Learning technologies in public universities in Iraq and what are their recommendations to rectify them. Findings The findings revealed that service quality had no positive influences on the satisfaction of instructors, while other factors had varying levels of influence, the findings further showed that inadequate internet service and students lack of interest are the biggest challenges instructors faced during their experience with Google Classroom. Research limitations/implications To improve the generalizability of the results, future studies are recommended to include larger samples, in addition, further studies are also advised to take individual traits such as age and gender into consideration. Originality/value The outcomes of this study are expected to benefit researchers, policymakers and LMS developers who are interested in factors that affect instructors’ intention to reuse LMS in higher education institutes in developing countries.",https://www.semanticscholar.org/paper/523478e4abc5ebf850301485c4b8cb8843d53a6d,IS,Qualitative
An Untapped Potential in Primary Care: Semi-Structured Interviews with Clinicians on How Patient Portals Will Work for Caregivers in the Safety Net,"Background Patients within safety-net settings are less likely to access health information on patient portals, despite expressed interest. Family and friends are important resources to assist these patients (ie, Medicaid recipients, older patients, patients with limited English proficiency) in navigating health systems, and provider support of the use of patient portals among these groups may also facilitate caregivers’ use of their patients’ portal. Objective Because safety net providers work closely with caregivers to care for their patients, we used qualitative methods to explore safety net providers’ perspectives on portal use among caregivers for their patients, especially as there is limited literature about caregivers’ use of portals in the safety net. Methods We conducted 45- to 60-min semistructured telephone interviews with providers from three large California safety-net health systems. The interviews focused on providers’ experiences with caregivers, caregiver roles, and how the portal could be leveraged as a tool to support caregivers in their responsibilities. A total of three coders analyzed the interview transcripts using both deductive and inductive approaches and established a consensus regarding major themes. Results Of the 16 participants interviewed, 4 specialized in geriatrics, and all held a leadership or administrative role. We described themes highlighting providers’ recognition of potential benefits associated with caregiver portal use and specific challenges to caregiver engagement. Conclusions Providers recognized the potential for portals to improve information delivery and communication by helping caregivers assist socially and medically complex patients in the safety net. Providers in safety net sites also discussed a clear need for better ways to keep in touch with patients and connect with caregivers, yet security and privacy are perhaps of higher importance in these settings and may pose challenges to portal adoption. They noted that caregivers of patients in the safety net likely face similar communication barriers as patients, especially with respect to digital literacy, health literacy, and English proficiency. Further research is needed to assess and support caregivers’ interest and ability to access portals across barriers in health and digital literacy, and English proficiency. Portal platforms and health systems must also address specific strategies to uphold patient preferences while maintaining privacy and security.",https://www.semanticscholar.org/paper/0a2990dda907947ddee59e7a6eff153618c5d53a,CS,Qualitative
"Wearables for Integrative Performance and Tactic Analyses: Opportunities, Challenges, and Future Directions","Micro-electromechanical systems (MEMS) have reduced drastically in size, cost, and power consumption, while improving accuracy. The combination of different sensor technologies is considered a promising step in the monitoring of athletes. Those “wearables” enable the capturing of relevant physiological and tactical information in individual and team sports and thus replacing subjective, time-consuming and qualitative methods with objective, quantitative ones. Prior studies mainly comprised sports categories such as: targeting sports, batting and fielding games as well as net and wall games, focusing on the detection of individual, non-locomotive movements. The increasing capabilities of wearables allow for more complex and integrative analysis expanding research into the last category: invasion sports. Such holistic approaches allow the derivation of metrics, estimation of physical conditions and the analysis of team strategic behavior, accompanied by integrative knowledge gains in technical, tactical, physical, and mental aspects of a sport. However, prior and current researchers find the precise measurement of the actual movement within highly dynamic and non-linear movement difficult. Thus, the present article showcases an overview of the environments in which the wearables are employed. It elaborates their use in individual as well as team-related performance analyses with a special focus on reliability and validity, challenges, and future directions.",https://www.semanticscholar.org/paper/691d66f931412c06693e3ce4e3ae77ac3ed079d1,CS,Qualitative
Implementation of E-Government as a Public Service Innovation in Indonesia,"E-Government implementation is the use of technology, information, and communication to realize more efficient and effective government practices in the process of implementing public services in order to facilitate public access to information and create principles of accountability, transparency and good public participation in the Indonesian government. This study aims to explain the implementation of E-Government in Indonesia which encourages the development of information and data systems and is adapted to existing bureaucratic processes. The method used in this research is qualitative research methods. The results showed that public service innovation organized by the government of the Republic of Indonesia was able to create relationships between elements in a country online, not inline, so that efficiency and speed in public services was not only a symbol but a reality.",https://www.semanticscholar.org/paper/7da4c5fcb8289cbf2a20adb351fe1f2dc7dc063b,IS,Qualitative
Penerapan Aplikasi Keuangan Berbasis Android Si Apik Dalam Penyusunan Laporan Keuangan Berdasarkan SAK EMKM,"Abstract: The application of accounting information systems in companies is important because it can present reliable information effectively and efficiently so as to facilitate operational activities to decision making. Currently, many companies are constrained in presenting accounting information systems in the form of recording transactions and financial statements based on SAK EMKM. Transaction recording and financial statements with SAK EMKM can be done on Android-based accounting software via smartphone devices. This research helps Qaya Laundry provide easy transaction recording and financial reporting using the Si Apik android-based financial accounting application. The method used in this study is based on a qualitative descriptive method with data collection through observation, interviews, and documentation approaches. The results of this study are in the form of guidelines for implementing the Si Apik application in recording transactions and preparing financial reports, making it easier for Qaya Laundry to manage finances for future decision-making needs.",https://www.semanticscholar.org/paper/3e016d989f8800c8b3acc8f11712f762cbda7f6a,IS,Qualitative
Wearable Bioelectronics: Opportunities for Chemistry.,"The practice of human health care may be on the cusp of a revolution, driven by an unprecedented level of personalization enabled by advances in technology, specifically, the transformation of wearable devices from curiosities that provide qualitative information for fitness enthusiasts to sophisticated systems that produce clinical-grade data for physicians. A recent and highly visible example is the Apple Watch Series 4, a platform released in September 2018 that features electrocardiogram measurement capabilities cleared by the US Food and Drug Administration (FDA). Companies such as MC10, iRhythm, Vital Connect, GE Healthcare, and Philips offer nextgeneration devices characterized by intimate skin interfaces and FDA-approved multimodal functionalities, with the potential to allow for medical care that is highly customized to the individual. The designs range from rigid modules mounted to the body with straps and tapes, to thin, stretchable systems that adhere directly onto the skin, much like adhesive bandages or temporary tattoos. New government initiatives in the US, such as those associated with the NextFlex Alliance, the BRAIN initiative, the SPARC program at the NIH, and the Biotechnology Office at DARPA, support research programs in relevant areas via robust levels of funding, although now likely surpassed by combined investments from foundations, venture capital firms, and large corporations. The resulting accelerated rates of technology development and deployment serve as nucleation points for large, growing programs in adjacent areas, most prominently in medical data analytics at Verily, Apple, Philips, Facebook, Intel, Samsung, and others. The outcomes of these collective activities have the potential to lead to unprecedented basic insights into human physiology, with wide-ranging, positive consequences for the cost, efficacy, speed, and global availability of personalized medical care. Successful efforts will directly address an overarching grand challenge for the 21st century, defined by the US National Academy of Engineering as the need for advances in “...the acquisition, management, and use of information in health...”. This special issue highlights the central role of chemical research in establishing the foundations for wearable bioelectronics with advanced capabilities in measurements of physiological state, performed continuously, outside of hospital and laboratory settings but with quantitative correspondence to clinical gold standards. The Accounts that appear in this issue summarize recent research on key aspects, ranging from constituent materials to novel sensors, advanced power supply systems, and skin-compatible integrated platforms. The first area represents a primary focus for the materials chemistry community, where a collection of papers covers, for example, progress in stretchable block copolymers and conjugated organics for structural materials and active layers, respectively, and associated techniques in processing such as spin-casting, printing, and vapor phase deposition onto both planar and textile substrates. Other papers focus on microand nanoscale inorganic materials as ribbons, wires, sheets or fibers in random or organized networks, configured as planar thin films or as buckled, wrinkled, woven, or segmented structures supported by or embedded in elastomeric supports. Liquid metals and two-dimensional materials represent interesting alternatives for stretchable interconnects and active layers, respectively, as described in additional articles. With the availability of new materials, functional subsystems such as sensors and components for power supply are possible. Some Accounts highlight examples of strain, pressure, and temperature gauges based on resistive and capacitive effects, optimized for capturing information on movements, pulsatile blood flow, and thermoregulatory processes, as physical parameters relevant to health status. Additional Accounts highlight the use of organic semiconductors as infrared photodetectors and as transduction elements in transistorbased or enzymatic sensors for measuring biomarkers in interstitial fluid, sweat, tears, and other biofluids, as well as targeted species in the surrounding environment. Power for these devices can be harvested using piezoelectric, thermoelectric, triboelectric, and photoelectric effects in textile or stretchable formats and stored in skin-compatible supercapacitors or batteries, as featured in other Accounts. Further Accounts highlight the ability to build integrated, functional wireless systems, with some emphasis on emerging skininterfaced platforms designed for the analysis of biomarkers in sweat. Another pair of Accounts describes how extensions of some of these same ideas in materials chemistry enable flexible and stretchable devices with neuromorphic operation to mimic biological nerves and artificial muscles for soft mechanical actuation. These Accounts capture some, but not all, of the many exciting developments in this rapidly evolving area of wearable bioelectronic systems. The centrality of novel materials, the rich range of combined topics in fundamental and applied research, and the potential to contribute to global grand challenges in health care form the basis of a dynamic, interdisciplinary space for productive research for chemists over the coming years. The future will involve not only a linear extrapolation of the sorts of capabilities in clinical-grade skininterfaced monitoring devices that are just now beginning to emerge from laboratories around the world but also a transformation that will eventually lead to biointegrated systems with increasing levels of diversity, from systems that not only sense but also actuate, respond, and dynamically deliver therapy in synchrony with natural body processes for improved health and wellness. The role of chemistry is essential to the development of these broadly defined biotic/ abiotic systems. John Rogers, Guest Editor",https://www.semanticscholar.org/paper/1f556734e7c3b02e52e63d7e90a678059003ccad,IT,Qualitative
Chatbots at Digital Workplaces - A Grounded-Theory Approach for Surveying Application Areas and Objectives,"Abstract Background: Chatbots are currently on the rise as more and more researchers tackle this topic from different perspectives. Simultaneously, workplaces and ways of working are increasingly changing in the context of digitalization. However, despite the promised benefits, the changes still show problems that should be tackled more purposefully by chatbots. Application areas and underlying objectives of a chatbot application at digital workplaces especially have not been researched yet. Method: To solve the existing problems and close the research gap, we did a qualitative empirical study based on the grounded-theory process. Therefore, we interviewed 29 experts in a cross-section of different industry sectors and sizes. The experts work in the information systems domain or have profound knowledge of (future) workplace design, especially regarding chatbots. Results: We identified three fundamental usage scenarios of chatbots in seven possible application areas. As a result of this, we found both divisional and cross-divisional application areas at workplaces. Furthermore, we detected fifteen underlying objectives of a chatbot operation, which can be categorized from direct over mid-level to indirect ones. We show dependencies between them, as well. Conclusions: Our results prove the applicability of chatbots in workplace settings. The chatbot operation seems especially fruitful in the support or the self-service domain, where it provides information, carries out processes, or captures process-related data. Additionally, automation, workload reduction, and cost reduction are the fundamental objectives of chatbots in workplace scenarios. With this study, we contribute to the scientific knowledge base by providing knowledge from practice for future research approaches and closing the outlined research gap. Available at: https://aisel.aisnet.org/pajais/vol12/iss2/3/ Recommended Citation Meyer von Wolff, Raphael; Hobert, Sebastian; Masuch, Kristin; and Schumann, Matthias (2020) ""Chatbots at Digital Workplaces – A Grounded-Theory Approach for Surveying Application Areas and Objectives,"" Pacific Asia Journal of the Association for Information Systems: Vol. 12: Iss. 2, Article 3. DOI: 10.17705/1pais.12203 Available at: https://aisel.aisnet.org/pajais/vol12/iss2/3",https://www.semanticscholar.org/paper/067025a2e624c40c7421154c330115de826844f7,IS,Qualitative
Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model,"Image Captioning is an arduous task of producing syntactically and semantically correct textual descriptions of an image in natural language with context related to the image. Existing notable pieces of research in Bengali Image Captioning (BIC) are based on encoder-decoder architecture. This paper presents an end-to-end image captioning system utilizing a multimodal architecture by combining a one-dimensional convolutional neural network (CNN) to encode sequence information with a pre-trained ResNet-50 model image encoder for extracting region-based visual features. We investigate our approach's performance on the BanglaLekhaImageCaptions dataset using the existing evaluation metrics and perform a human evaluation for qualitative analysis. Experiments show that our approach's language encoder captures the fine-grained information in the caption, and combined with the image features, it generates accurate and diversified caption. Our work outperforms all the existing BIC works and achieves a new state-of-the-art (SOTA) performance by scoring 0.651 on BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.",https://www.semanticscholar.org/paper/03d2c5503b16ab689f24ba1d91653c196f4e262e,CS,Qualitative
Defining patient communication needs during hospitalization to improve patient experience and health literacy,"Background In order to play an active role in their health care, patients need information and motivation. Current delivery systems limit patients’ involvement because they do not routinely provide them with enough details of their own clinical results, conditions and other important clinical data. The purpose of this study was to identify, from the perspective of patients, which topics matter the most, who should be communicating them, and when and how should they be provided. Methods We conducted a qualitative, phenomenological study analysing the content of subjective experiences, feelings and behaviours. We organized two focus groups with 13 participants and 15 in-depth interviews. Transcripts of the focus groups and interviews were checked for accuracy and then entered into Atlas ti™ v7.5.13 qualitative software. Two independent researchers performed a qualitative inductive content analysis to classify the data in two levels: themes and categories. Results The qualitative analysis provided 377 units of meaning synthesized into 22 categories and six themes: hospitalization procedure, Health Literacy relating to the patient’s condition, information content, satisfaction, professional-patient relationship, and patient proactivity. Patients described which information they wished for, when they needed it, and who would provide it, usually related to actions such as admission, discharge or diagnostic tests. Oral information was more difficult to comprehend than the written kind, as patients can check written information several times if needed. Nurses were the most available professionals, and patients found easier to relate to them and ask them questions. Moreover, patients identified physicians as those professionals responsible for providing clinical information. Conclusions Our results showed that patients suffered from poor Health Literacy regarding their personal condition, as they were unable to describe the symptoms, the type of tests being performed or their results, and some of them also had difficulties in naming the specific disease or comorbidities they had. During the hospitalization process, patients were in good shape to come with doubts and actively asked for more information. Healthcare organizations and professionals were offered the chance to ensure the correct communication and comprehension to their patients.",https://www.semanticscholar.org/paper/ad2154e08721a95c8f66f49eb6317b5438287cb0,CS,Qualitative
Exploring Affordances of Slack Integrations and Their Actualization Within Enterprises - Towards an Understanding of How Chatbots Create Value,"The rise of chatbots poses new possibilities to link social interactions within instant messengers with third-party systems and business processes. While many companies use chatbots within the enterprise in the form of Slack apps and integrations, little is known about their affordances. Grounded in a qualitative research endeavour, we conducted 12 explorative interviews in 8 organizational settings to inductively gain rich contextual insights. Our results reveal 14 functional affordances in 4 categories, elucidating how their actualization leads to the perception of higher level affordances and constraints. First, we discuss how chatbots augment social information systems with affordances of traditional enterprise systems, and therefore, enable bottom-up automation. Second, we elaborate on how the actualization of an affordance by one user may facilitate its perception by other users. Thus, we contribute towards a better understanding of how chatbots create value.",https://www.semanticscholar.org/paper/32091280aabf7195792135e8c8dee6ecf2f2d59d,IS,Qualitative
Experimenting with Latent Semantic Analysis and Latent Dirichlet Allocation on Automated Essay Grading,"The demand of scoring natural language responses has created a need for new computational tools that can be applied to automatically grade student essays. Systems for automatic essay assessment have been commercially available since 1990's. However, the progress in the field was obstructed by a lack of qualitative information regarding the effectiveness of such systems. Most of the research in automatic essay grading has been associated with English writing due to its widespread use and the availability of more learner collection and language processing software for the language. In addition, there is large number of commercial software for grading programming assignments automatically. In this work, we investigate document semantic similarity based on Latent Semantic Analysis (LSA) and on Latent Dirichlet Allocation (LDA). We use an open-source Python software, Gensim, to develop and implement an essay grading system able to compare an essay to an answer-key and assign to it a grade based on semantic similarity between the two. We test our tool on variable-size essays and conduct experimental tests to compare the results obtained from human grader (professor) and those obtained from the automatic grading system. Results show high correlation between the professor grades and the grades assigned by both modeling techniques. However, LSA-based modeling showed more promising results than the LDA-based method.",https://www.semanticscholar.org/paper/0c090fa8a1fbdb75b4eb9e58b255ee0f4237ead2,CS,Qualitative
"Technology innovations towards reducing hospitality human resource costs in Langkawi, Malaysia","Purpose Hotel labour costs in Malaysia are increasing. This paper aims to explore Langkawi hotel managers’ perceptions about reducing labour costs using various information and communication technology (ICT) innovations. Design/methodology/approach Semi-structured interviews were conducted with managers from 19 budget and boutique resorts on Langkawi Island, Malaysia. Qualitative data were recorded, transcribed and content analysed using latent coding. Findings All hotel managers reported using some form of ICT. The purpose for ICT adoption found was to increase productivity and efficiency. A hotel’s customer mix, the need of the organization and the technology budget available influenced the outcome of technological innovation. Langkawi hotels had successfully implemented Property Management Systems (PMSs), but self-check-in/out kiosks were not seen as important as they do not meet their customers’ service expectations. Research limitations/implications This study identifies some factors influencing uptake by hotels of technological innovations. This initial qualitative exploration of the technology adoption feasibility in Langkawi suggests that implementation to reduce labour cost is more likely for employee-operated devices rather than customer-operated devices. Practical implications This study contributes to the human resource (HR) management literature by providing insight into the reasons hotel managers introduce technology in a developing country context. Results suggests that hotels face challenges in reducing labour costs through technology. These insights may serve to guide policymakers and hotel managers in other developing countries that are planning to use technology to solve their HR issues. Luxury hotels can consider adopting ICT for back-of-the-house operations such as using a HR information system within the HR department and PMS for the hotel overall operation. Social implications These findings can increase the Malaysians awareness of ICT importance, especially in the hotel industry. Originality/value The Malaysian national minimum wage order policy was introduced in 2012. This policy has resulted in increased labour costs and suggests a need to adopt ICT. This paper is the first to examine the viewpoints of hotel practitioners as to the viability of this strategy. Whilst many studies on the adoption of ICT in the hotel industry focus on its impact on productivity and firm’s performance using quantitative methods, this study used qualitative methods to explore hotel managers’ perceptions on its feasibility to reduce dependence on labour.",https://www.semanticscholar.org/paper/b1ee5f8eb14f93f96849f3be9d208e91e0aaa8f1,IS,Qualitative
"Security - Visible, Yet Unseen?","An unsolved debate in the field of usable security concerns whether security mechanisms should be visible, or blackboxed away from the user for the sake of usability. However, tying this question to pragmatic usability factors only might be simplistic. This study aims at researching the impact of displaying security mechanisms on User Experience (UX) in the context of e-voting. Two versions of an e-voting application were designed and tested using a between-group experimental protocol (N=38). Version D displayed security mechanisms, while version ND did not reveal any security-related information. We collected data on UX using standardised evaluation scales and semi-structured interviews. Version D performed better overall in terms of UX and need fulfilment. Qualitative analysis of the interviews gives further insights into factors impacting perceived security. Our study adds to existing research suggesting a conceptual shift from usability to UX and discusses implications for designing and evaluating secure systems.",https://www.semanticscholar.org/paper/ab212fee36bf29d92382f20dbfcbefe3acfe51ea,IS,Qualitative
Social recognition and employee engagement: The effect of social media in organizations,"In the contemporary dynamic business context (characterized by stiff competition, high uncertainty, growth of global as well as virtual organizations, etc.), setting up systems for recognizing and engaging employees which need constant endeavor from present-day organizations. Hence, organizations are actively exploring and adopting novel initiatives through strategies platforms for their employer engaging effort, moving the organization image to the “company of employee’s choice” and connecting their strategic talent acquisition supply chain into that eco-system. Qualitative data analysis through 65 semi-structured interviews with employees working with various Software and Research & Development sectors, Human Resources and Talent Management professionals working for information technologies companies, IT-enabled (Business Process Outsourcing) BPO, and KPO (Knowledge Process Outsourcing) Services, as well as talent acquisition organizations, confirm the utilization of social media for social recognition & employee engagement in their organizations. The study contributes to the fields of talent management and social networks and has several key messages for practitioners regarding the expediency and effectiveness of social networks in organizations.",https://www.semanticscholar.org/paper/51ce11af219158649ebd629684775bba6d053099,IS,Qualitative
"Analysis and Findings of Social Engineering Industry Experts Explorative Interviews: Perspectives on Measures, Tools, and Solutions","Social engineering is one of the biggest threats organizations face today, as more and more organizations are adopting digitalization. In the context of cyber security, social engineering is the practice of taking advantage of human weaknesses through manipulation to accomplish a malicious goal. For better implementation methods against social engineering, this qualitative study will attempt to provide measures against information security challenges faced by organizations. The analysis is then provided by the answers of interviewed experts in the field of cyber security and social engineering. The research herein focuses on the human element of cyber security threats, recognizing that hackers exploit the vulnerabilities and lack of awareness of staff. Then using these issues to create security loopholes and engineer cyber-attacks that include the interruption or infection of information systems, transfer of unauthorized funds, and stealing of credentials. The results of this qualitative study highlight that there is a positive relationship between social engineering and user awareness. The findings build upon the researchers’ ongoing work, which postulates that as an increase in contextual social engineering knowledge leads to a decrease in being victims of social engineering and is, therefore, one of the most effective mechanisms for managing social engineering.",https://www.semanticscholar.org/paper/16e61de584e5a34da67e81c335eeba10c8d4d938,IS,Qualitative
Utilization of Internet of Things on Food Supply Chains in Food Industry,This study aims to analyze the use of the Internet of Things (IoT) in supporting the management of food supply chains (FSCs) in the food industry. This research used qualitative research methods. The results obtained from this study are increasing the effectiveness and efficiency of the existing food supply chain in the food industry by applying the IoT concept to food supply chain management. These results can be obtained because the IoT concept is supported by various systems and technologies that can be implemented and developed so that IoT can help identify and deal with existing problems more quickly while being able to assist in the decision-making process with information obtained through IoT technology so that it will support development food supply chain management in the food industry. This study was conducted to see how much influence the internet of things (IoT) has on food supply chain management in the food industry.,https://www.semanticscholar.org/paper/ed3294a72802455599633d29249713467ed17a67,IS,Qualitative
PENGARUH DUKUNGAN MANAJEMEN PUNCAK TERHADAP KUALITAS SISTEM INFORMASI AKUNTANSI PADA PERUSAHAAN ASURANSI SYARIAH,"This study is objected to test the support of top management that can enhance the quality of Accounting Information Systems (SIA) in Islamic insurance companies in Indonesia. This study uses qualitative methods with a positive paradigm. The data used are primary data with the help of questionnaires as research instruments. Data obtained from returning questionnaires filled out by respondents who are leaders, division heads, department heads or heads of accounting departments. The research population is a sharia insurance company consisting of 59 companies registered as members of the Indonesian Sharia Insurance Association (AASI) as of April 2019. From the population obtained 34 respondents who filled out the questionnaire. Data analysis was performed with descriptive static and PLS-structural equation modelling (PLS-SEM) using SmartPLS software. The results showed that top management's support had a significant effect on the quality of accounting information systems and support of top management has not been maximized.",https://www.semanticscholar.org/paper/64d374c8e4ec65c59ab82a0fe28cc83be8e75faf,IS,Qualitative
"How to improve information technology strategic planning effectiveness using balanced scorecard, risk and maturity analysis, case study health information technology? A qualitative study","Although many health strategic plans have been developed by scholars and organizations, they still suffer from a limited view. Since most health‐related strategies in the future will depend on information technology (IT), as the main driver of today's industry, technology, and society, IT merits attention in health strategic plans. While the majority of the health strategic plan developed based on the interviews and questioner and these plans didn't consider the role of IT in their actions, this research will develop a framework to integrate risk and maturity analysis with the strategic planning process in health information technology strategic plan.",https://www.semanticscholar.org/paper/8041e4c0ad6d9b84dd13c298abbac924d0564b8b,IS,Qualitative
Qualitative research interviews using online video technology – challenges and opportunities,"Purpose This study aims to examine the methodological and method-related challenges and opportunities arising from the use of video interviews in qualitative accounting research, focussed on collecting contextual data and visual cues, enriching communication quality and building and maintaining rapport with interviewees. Design/methodology/approach Prior literature and the authors’ experiences using video technologies for research, including conducting interviews, inform this research. This study uses a transactional conceptual refinement of information richness theory and channel expansion theory to critically analyse the challenges and opportunities of using video technology to conduct qualitative research interviews. Findings The ability, need for and significance of collecting contextual data depend on the researchers’ ontological and epistemological assumptions, and are, therefore, influenced by their research design choices. Video technology enables researchers to view research settings by video. In addition, whilst group/panel interviews have their advantages, it is often difficult to get everyone together in person, something video technology can potentially overcome. The feasibility and the quality of video interviews can be improved if both interview participants are experienced with using video technology, as well as with judicious investment in good quality video technology and through testing and practice. We also discuss how rapport building with interviewees can be facilitated by overcoming the video’s sense of disconnect and enhancing interviewees’ willingness to engage. Originality/value The study builds on the limited prior literature and considers the challenges and opportunities related to methodology and method when conducting video-based qualitative interviews in accounting research. Broadly, qualitative researchers will find the paper useful in considering the use of video interviews and in making research design choices appropriate for video interviews.",https://www.semanticscholar.org/paper/9c954dcb58c13923ad9274bacd6d8bb2929b2a2f,IS,Qualitative
Nurses’ experiences and viewpoints about the benefits of adopting information technology in health care: a qualitative study in Iran,"Background Information technology (IT) plays an important role in nursing practice. Hence, nurses’ experiences and viewpoints about IT integration into healthcare help improve nurses’ adoption of IT. This study aimed to explore the nurses’ experiences and viewpoints about the benefits of IT integration and adoption in healthcare. Methods This study was conducted with a qualitative research approach. Participants included 14 nurses from four hospitals affiliated to a large medical university in Iran, who were selected using a purposive sampling method. Data were collected through semi-structured interviews and analyzed using the conventional content analysis of Lundman and Graneheim. Results Six categories in the study reflected the nurses’ experiences and viewpoints about the benefits of integrating IT into health care. These categories included improving the quality and efficiency of medical services and care, facilitating the communication management in the technological environment, improving information documentation, management, and monitoring, improving resource management, improving management performance and policymaking, and facilitating pathways of organizational and professional growth. Conclusions Lessons learned in this study can help overcoming the barriers of IT adoption, and developing appropriate strategies to familiarize nurses with the benefits of IT in healthcare settings. Healthcare managers are recommended to investigate the experiences of nurses with IT in their hospitals and organize courses to orient hesitant nurses toward adopting IT.",https://www.semanticscholar.org/paper/1df9ede88401343fbc36e46773c90fc6e68bbfba,IS,Qualitative
The Role of Information Technology in Driving Innovation and Entrepreneurial Business Growth,"In the era of globalisation and rapid advances in information technology, the role of IT in driving innovation and growth of entrepreneurial businesses has become increasingly important. In this research, an in-depth analysis of the role of information technology in driving innovation and growth of entrepreneurial businesses will be conducted. This research will involve collecting secondary data from various sources. The focus of this study is primarily qualitative. Methods for gathering data include paying close attention to detail while seeing and recording data, and then using analytical techniques like data reduction, visualization, and inference to draw conclusions. The results of this study conclude that IT has a very important role in driving innovation and growth of entrepreneurial businesses. In today's digital era, entrepreneurs need to utilise IT effectively to accelerate product development, improve operational efficiency, expand market reach, drive business innovation, and enhance customer experience.",https://www.semanticscholar.org/paper/cb27e7e859e0cf29f2202dd9ac20adf4082e87db,IS,Qualitative
The Role of Information Technology in Improving the Efficiency and Effectiveness of Talent Management Processes,"Amidst the current era of globalization and heightened business competition, organizations across diverse sectors encounter obstacles in the recruitment, cultivation, and retention of optimal talent. The strategic management of talent has emerged as a critical area of emphasis for organizations seeking to cultivate a skilled and proficient labor force. The present study aims to investigate the impact of information technology, specifically the implementation of a human resource management system, on enhancing the efficiency and effectiveness of talent management procedures. This research is qualitative in nature. The techniques used to obtain information involve careful observation and meticulous note-taking, followed by analytical procedures such as data reduction, visualisation, and inference. The results of this study show that the role of IT in improving the efficiency and effectiveness of talent management processes is significant. The use of IT in employee recruitment and selection, skills development, employee engagement, and data management provide many benefits to organisations. However, challenges such as data privacy and security, digital divide, organisational complexity, ethical aspects, and the importance of human interaction need to be addressed to achieve optimal results.",https://www.semanticscholar.org/paper/9ab0a0c022dfafe5b5d3df6929d1823f8a6c030a,IS,Qualitative
The Role of Information Technology in Improving the Efficiency and Productivity of Human Resources in the Workplace,"The ubiquity of information technology in contemporary society has rendered it an indispensable component across a wide range of domains. The utilization of information technology has had a noteworthy influence on enhancing efficiency and productivity in diverse domains, encompassing the professional milieu. The Human Resources function is a crucial element within an organization, playing a significant role in driving the success and expansion of the company. The objective of this investigation is to scrutinize the function of IT in enhancing HR efficacy and output within the workplace. It is crucial to comprehend the ramifications of IT employment within an organizational framework. The primary emphasis of this investigation is qualitative in nature. The process of data collection involves meticulous observation and recording of data, followed by the application of analytical methods such as data reduction, visualisation, and inference to derive meaningful insights. The research has deduced that in the constantly changing digital era, the significance of information technology in enhancing the effectiveness and output of human resources in the work environment is pivotal. IT facilitates the automation of mundane tasks, enhances collaboration, provides expedient access to information, enables effective training, supports intelligent data analysis, promotes efficient performance management, allows for work flexibility, and ensures information security.",https://www.semanticscholar.org/paper/24bdb62e1bed1181eb62dd10a3502e80dc6c8f3c,IS,Qualitative
A Study on Gender Roles in the Information Technology Profession and its Impact on Human Resources,"Information Technology (IT) is one of the fields that has an important role in the economic and social development of the world today. One important issue that continues to be of concern is the gender gap in the IT industry. This research aims to examine the role of gender in the IT profession and its influence on human resources. This research uses a qualitative methodology in the form of a literature review, which implies that the research will examine and explain data by utilising information and textual content from various sources. The study results show that gender roles in the Information Technology profession is an issue that is gaining increasing attention in the IT industry. Despite the increasing participation of women in IT careers, gender imbalance remains a significant problem. Factors such as gender stereotypes, inequality in opportunities, and differences in education contribute to this imbalance. As a result, unequal gender roles in the IT industry create inequalities in compensation and career opportunities, and also result in underrepresentation of women in leadership positions.",https://www.semanticscholar.org/paper/e82e844fe416b80e22e1640bf0da3a49ff08c247,CS,Qualitative
The Role of Leadership in Managing Information Technology Change and its Impact on Organisational Human Resources,"The development of information technology (IT) has significantly changed the way organisations operate and communicate. In the digital era, organisations that want to remain competitive and relevant must be able to manage information technology changes well. The purpose of this study is to examine the role of leadership in managing information technology change and its impact on organisational HR. This research is a literature review that utilises qualitative methods, which means that it will analyse and interpret data by utilising information and text derived from various sources. The results of the study show that the role of leadership in managing information technology change is very important in the ever-evolving digital era. Leaders must have a deep understanding of technology, formulate a digital transformation strategy, and guide the organisation through the change. The impact of effective leadership is increased productivity, innovation, and organisational competitiveness. However, technological change also impacts the organisation's people, and leaders must ensure that people have the necessary skills and knowledge, and create a culture that supports innovation and continuous learning.",https://www.semanticscholar.org/paper/aa3133251c5623b200d6493cbcd713abb96049bf,IS,Qualitative
Investigating the Perceptions of Primary Care Dietitians on the Potential for Information Technology in the Workplace: Qualitative Study,"Background Chronic diseases are the leading cause of morbidity and mortality worldwide. The primary health care setting is an effective avenue for the management and prevention of chronic diseases. Dietitians working in this setting assist with the management of modifiable risk factors of chronic diseases. However, health care professionals report challenges in providing care in this setting because of time and financial constraints. Information technology offers the potential to improve health care quality, safety, efficiency, and cost-efficiency, but there exists limited understanding of dietitians’ application of technology in this setting. Objective The objective of this study was to explore the perceptions of primary care dietitians about using information technology in their workplace. Methods We recruited 20 Australian primary care dietitians using purposive and snowball sampling for semistructured telephonic interviews. Interview questions aimed to gain an understanding of dietitians’ perceptions about sharing patient outcomes through a national database and the benefits, disadvantages, feasibility, and barriers of using information technology. Interviews were audiorecorded, transcribed verbatim, and thematically analyzed for emerging themes and subthemes. Finally, the technologies used by participants were collated by name and researched for their key attributes. Results The following 4 distinct themes emerged from the data: information technology improving the efficiency of practice tasks, experiencing barriers to using information technology in practice, information technology enhancing outcomes through education and monitoring, and information technology for sharing information with others. Participants identified several advantages and disadvantages of using technology and expressed willingness to share patient outcomes using a Web-based database. Conclusions This study suggests that information technology is perceived to have benefits to dietitians and patients in primary health care. However, to achieve the optimal benefit, support is required to overcome barriers to integrate information technology into practice better. Further development of patient management systems and standardized Web-based data collection systems are needed to support better usage by dietitians.",https://www.semanticscholar.org/paper/a053da5264dab456ea4468f14bfe8210cec4cc09,CS,Qualitative
The Role of Leadership in Managing Organisational Culture Change in the Context of Information Technology Implementation,"The rapid development of information technology has affected many organisations in various sectors. The implementation of information technology often involves changes in work processes, organisational structure and overall organisational culture. The background of this study aims to investigate the role of leadership in managing organisational culture change in the context of information technology implementation. The current research type is qualitative. Data collection techniques include listening and recording important information to conduct data analysis through data reduction, data display, and conclusion drawing. The study arrived at a statement that in the context of information technology implementation in organisations, organisational culture change becomes a critical factor affecting the success and sustainability of the use of such technology. Leadership plays an important role in managing such organisational culture change. Through effective communication, appropriate education and training, active participation and involvement of organisational members, strong team building, recognition and rewards, gradual change management, and continuous evaluation and adjustment, leadership can create an enabling environment for the adaptation and utilisation of information technology.",https://www.semanticscholar.org/paper/fd2a3166571502beb1821b64604d542e45a88421,IS,Qualitative
Transforming Learning Environments with Information Technology: Trends and Best Practices,"This research was conducted as a way to examine contextual factors in the effective application of information technology to help the learning environment which can help carry out transformation in the field of education as a new learning method. The research method used is a qualitative descriptive method by prioritizing library or literature studies in the research process. By referring to the design components and also the implementation of initiatives in the field of education as a way of educational transformation, this research can describe what happens with the advancement of information technology to transformation in the world of education and learning which is also related to the integration of technology, policy recommendations, and also resources. sustainable power. The transformation of the education system of course supports national competitiveness as one of the long-term efforts that receives continuous support and also visionary leadership in providing input for the world of education with the help of advanced information technology. Keywords: Learning Environments, Information Technology, Effective Application",https://www.semanticscholar.org/paper/cd6f59228a14d818fbdc71dc57caa43e171c896b,IS,Qualitative
User‐centered approaches in software development processes: Qualitative research into the practice of Hungarian companies,"Integrating user‐centered approaches into development processes is one of the main challenges nowadays that derives from different objectives of software engineering (SE) and human‐computer interaction (HCI) fields. For SE experts, the main goal is quality code creation, whereas for HCI professionals, it is the continuous product interaction with the users. The major question is what tools and timings can be used together to achieve these goals effectively. Therefore, this article provides comparative, exploratory, and qualitative research about possible solutions on how practitioners transfer HCI values and practices to SE processes. The current practice of software companies was studied by conducting interviews on a sample of 13 Hungarian Information Technology companies to explore the SE processes in respect of several dimensions (applied development models, the integrity of user‐centered methods, and the user experience [UX] maturity). According to preliminary expectations, the development processes of the various companies proceed in different steps; nevertheless, they can be well grouped together based on the UX methods applied. The results representing the various user‐centered processes can be considered useful for future decision makers of software companies worldwide.",https://www.semanticscholar.org/paper/0f488dc1fcadcea58da0699845a0c26dfc934bfa,CS,Qualitative
Information technology and management in higher education and science,"Today IT is one of the most important factors that has a strong impact on the education and science system quality in the whole world as well as in Latvia. The basis of the use of information technologies in higher education and science research is the system analysis qualitative combination of methods. During the research the existing base of scientific literature, methods of analysis, synthesis, induction, deduction, concretization, methods of generalization and analogies were used. As a result of the research, it was found that the introduction of information technology in the field of science and higher education is of great importance for the actual development in these fields. It will be helpful for the professional training of specialists capable of developing selected spheres and competing in the modern labor market. In the course of the research, the main indicators which conduct to the increasing the level of information literacy were identified. A number of advantages and disadvantages of information technologies in the scientific and educational sphere use are highlighted. One identified the main tasks of using information technologies in the field of science and higher education. The main advantages of information system in the process of learning in higher educational institutions use are highlighted. The results of the research are of great practical importance, as they can be used as a basis for further work in this direction.",https://www.semanticscholar.org/paper/4eb8811caffa7946357e0e902463aa3b1369ad43,IS,Qualitative
The Effect of Information Technology on The Performance of MSMEs During the Covid-19 Pandemic,"Micro, Small, and Medium Enterprises (MSMEs) are essential in improving a nation's economy. MSMEs can survive in a declining economic condition and also have the ability to absorb a large number of workers. However, the COVID-19 pandemic has had an impact on the global economy. This study aims to analyze the effect of information technology variables on the performance of MSMEs during the Covid-19 pandemic. This type of research is qualitative with a literature review approach. Several articles relating to information technology and MSME performance were used as references. The results obtained by MSMEs feel the positive impact of the role of information technology during the Covid-19 pandemic. Further research is expected to examine the role of information technology in the MSME sector. For example, SMEs in food, clothing, and others. SMEs must choose technology adaptations from the business activities and adjust to their existing financial capabilities. The government must help create adaptations for MSME actors during the Covid-19 pandemic, such as providing training and adequate technology infrastructure.",https://www.semanticscholar.org/paper/188517c2a2d0a376b554b11687599b5a6741d563,IS,Qualitative
The Impact of Information Technology Development on Cybercrime Rate in Indonesia,"Information technology is growing rapidly in society. In the period of technological development not only gives us a positive impact, but can also have a negative impact. One of the negative impact of the development of information technology is the misuse of the technology, so that it can harm others. Technological developments have an effect on the occurrence of cybercrime in Indonesia, the crimes that have occurred have also developed and varied. The method used in this research are a qualitative approach and descriptive methods. The results of the study show the impact of Information Technology development on the level of cybercrime in Indonesia occurs such as loss of privacy, unauthorized access to important data, data theft and others. Prevention and countermeasures that can be carry out to prevent cybercrime are preventive, pre-emptive and repressive measure",https://www.semanticscholar.org/paper/989c135ff576f0e4265c73ac41cf0b506833a3ff,IS,Qualitative
The Development of Information Technology and Its Influence on the Field of Management Accounting,"This study aims to determine the impact of the development of information technology in the field of management accounting. The method used is descriptive qualitative research methods and library research methods. The results of this study include the development of information technology that has brought important changes to the business world. There are many types of information systems that use emerging information technologies, including electronic data processing systems, data processing systems (DPS), decision support systems (DSS), data processing systems, management information (MIS), executive information systems (EIS), expert system (ES)) and Accounting Information System (AIS). The development of information technology has also influenced the field of management accounting because it is a field where information is generated in the context of management planning, control, and decision making. These impacts can be beneficial or detrimental to the business.",https://www.semanticscholar.org/paper/8fb960634c8a025035432f6f42ba77f5c85e8358,IS,Qualitative
The Implementation Of Information Technology In The Development Of Left And Right Brain At An Early Age In The World Of Education,"The purpose of the study is to develop the use of the right and left brain in early childhood through the implementation of information technology in this case learning using video with words. Education in learning that is done must keep up with the times by utilizing information technology we often mention with the era of revolution 4.0. Research methods use descriptive qualitative methods by means of observation and interview of teachers, Then the data obtained in the test using spss for windows with a population of 60 early childhood results obtained Fcount = 23,347 > Ftable = 3.16 so it can be said that the implementation of information technology in learning in the form of video display is very good in the development of the right and left brain in each individual early childhood.",https://www.semanticscholar.org/paper/c7886bf80c59444d8727f3f38a68d706915f0385,CS,Qualitative
"Ethnography, Its Strengths, Weaknesses and Its Application in Information Technology and Communication as a Research Design","Ethnography was originally developed for the study of foreign cultures by the anthropologists. It involves the observation of situations and carrying out interviews with the study population. There are two basic characteristics of ethnography where the observation takes place in a natural setting and secondly, where researchers must understand how an event is perceived and interpreted by the people in a community. Ethnography is therefore a qualitative research method that is used to study people and cultures for in-depth knowledge about a socio-technological realities surrounding everyday software development practice. Ethnography can help to uncover not only what practitioners do, but also why they do it in terms of human computer interaction and user interfaces design. This is due to its unique strength to involve the researcher, the research process and the research, making it a potential ideal method for undertaking research where the community and its members interact with each other. The main objective of this paper is to examine through literature review, the strengths and weaknesses of ethnography as a research design method for researchers in the information communications and technology (ICT) field. This will therefore provide more insight on how ethnography can be applied in conducting some of the qualitative information communication and technologies studies, especially where in-depth understanding is required.",https://www.semanticscholar.org/paper/c2be7b8b5976c530581bd535a4b5081dd5eeaca2,IS,Qualitative
Information Technology in the Development of Language Aspects of Early Childhood,"This study aims to analyze the use of Information Technology in the development of language aspects of early childhood in Raudhatul Athfal Tarbiyatul Ula and Raudhatul Athfal Darul Arofah Sumberkare Wonomerto Probolinggo, East Java. This research uses a qualitative approach, it is used to find more in-depth information about Information Technology in the Development of Early Childhood Language Aspects, while the type of research used is a case study focused on a particular case to be observed and analyzed carefully to completion. The results showed that: the use of information technology in the development of language aspects of early childhood, as follows; Children can develop language through audiovisuals, children can develop language with busy books, develop children's language through play media, and can develop children's language through teaching aids.",https://www.semanticscholar.org/paper/ca250fd460429eb09ae3e275e7f3699e249f343e,CS,Qualitative
Information technology as a resource to counter domestic sex trafficking in the United States,"Globally, millions of individuals are victims of sex trafficking and are compelled to perform sexual acts through force, fraud, or coercion. Law enforcement agencies, non‐profit organisations, and social entrepreneurs increasingly are using information technology as a resource to locate, identify, and rescue victims and find, arrest, and convict traffickers. In this qualitative case study, we partnered with a non‐profit organisation that trains law enforcement officers to use information technology to counter sex trafficking. For this research study, we observed training courses, interviewed law enforcement officers and non‐profit staff, and reviewed technology usage logs and other data sources. Some officers readily used the new information technology post‐training, while others failed to use the new technology. Using conservation of resources theory as a sensitising lens, we identify two factors affecting the use of new technology post‐training: the level of organisational resources available to individuals and the individual's perceptions of the new information technology as a resource. With these findings, we develop the Resources Model of Information Technology Use to explain how perceptions of organisational and technology resources affect information technology usage patterns and outcomes.",https://www.semanticscholar.org/paper/81debe31adc84c55d57438a69c9d56e4e7aa6267,CS,Qualitative
Application of blockchain information technology in Ṣukūk trade,"Purpose This study aims to explore the opportunities and challenges in activating a Smart Contract to enhance the efficiency and effectiveness of Ṣukūk offerings in the Islamic capital market. Design/methodology/approach The study adopts a mono-method qualitative approach. Data were obtained from survey interviews of two issuances on the fusion of smart contracts in Ṣukūk structures that were Sharīʿah-compliant. A thematic approach was further used to analyze the interview data based on the onion research method while opportunities and challenges of activating the Smart Ṣukūk (SṢ) relied on doctrinal evidence. Findings The results from the issuances across two jurisdictions showed that deployment of SṢ can resolve contractual ambiguities arising from Sharīʿah interpretations, jurisdictional policies and legal regime issues, which affect Ṣukūk origination and issuances especially on the right of investors in the event of Ṣukūk defaults. Although SṢ is automated, the third party’s presence is not eliminated as the blockchain platform still relies on the validators who are usually blockchain developers functioning as a third party in the Ṣukūk chain. Research limitations/implications The study relies on doctrinal literature to explain the features and requirements of SṢ. The empirical approach is limited to interview data based on local SṢ issuances. Future studies need to explore regulators’ role and global standards in cross-border issuance of SṢ with multiple jurisdictions/laws. Practical implications The paper concludes that the offering of SṢ using local currency has been successful in the two issuances because of the facilitative regulatory environment. However, addressing Ṣukūk’s challenges in cross-border offerings would require guidance from international standard-setters such as the Accounting and Auditing Organization for Islamic Financial Institutions and the Islamic Financial Services Board. Originality/value This study is an advanced application of smart contracts to alleviate the related Ṣukūk challenges in the Islamic capital market.",https://www.semanticscholar.org/paper/deeaac112067adfbc7b96d7443c10e110d5f2d8e,IS,Qualitative
Utilization of information technology for learning in Covid-19 disaster conditions,"The learning process during pandemic Covid-19 is done online to suppress the spread. The current pandemic conditions the role of information technology is significant in online learning. The purpose of this study is to find out “Utilization of Information Technology for Online Learning in Covid-19 Disaster Conditions”. This research uses qualitative and quantitative surveys. Data sources are lecturers and students in the Geography Study Program at Lambung Mangkurat University. Online learning here uses e-Learning, Google Class, WhatsApp, Zoom, other information media and internet networks that can connect lecturers and students. The findings of this study are that information technology strongly supports the success of online learning in Covid-19 disaster conditions. The limitations of this study limit the information technology understudy and define the learning process during the Covid-19 pandemic in certain subjects. The results showed that the use of information technology is significant to support the success of online learning in Covid-19 pandemic conditions. The most widely used are E-learning and Watsapp. Whatsapp is most effective used as a medium in online learning because it does not require large quotas and good signals. The obstacles are some students in remote areas, and the ability to buy a variety of quotas.",https://www.semanticscholar.org/paper/8f5823628fe94b140849ec686d250dc7fdc69bb9,CS,Qualitative
User Experience Design Practices in Industry (Case Study from Indonesian Information Technology Companies),"User Experience (UX) is a term that has received a lot of attention in the last decade. The number of industries whose consider the importance of implementing the UX design process within their development cycle has increased. Therefore, we think it is important to investigate how UX design processes are implemented in the industries. In this research, we take a qualitative approach with descriptive methods by investigating six information technology companies in Indonesia. As a result, we found that most of these information technology companies implement the UX design process as part of their operation and consider that the UX design process is an important part of software development. Each company has its order and priorities in regard to the UX design processes and only follows their established UX design process framework in order to meet their product development requirements. We also found that there are different UX design process approaches from these six companies.",https://www.semanticscholar.org/paper/c5f2da20cca2854f2bfc0ae7f816f86b68cc0798,IS,Qualitative
Evolution of hospitality and tourism technology research from Journal of Hospitality and Tourism Technology: a computer-assisted qualitative data analysis,"Purpose Information and communication technologies have been widely implemented and made radical changes for several decades in the hospitality and tourism industry. This rapid development also generates considerable data in social media. This trend opens the door to analyze unstructured data and gain increased attention of a qualitative research approach from hospitality and tourism researchers and industry professionals. Therefore, this paper aims to describe how a computer-assisted qualitative data analysis (CAQDA) approach can be used in the hospitality and tourism technology literature to uncover the trends and thematic concepts of hospitality and tourism technology research and their dynamics in Journal of Hospitality and Tourism Technology (JHTT) Design/methodology/approach To achieve the proposed research goals, the current study used CAQDA software, Leximancer, to analyze 218 articles published in JHTT between Volume 1(1) in 2010 and Volume 10(4) in 2019. Based on the rigorous CAQDA processes, the study performed the thematic analysis using all articles and subgroup analyses in the five-year periods. Findings Using CAQDA, the study reveals the critical research trends and insights on hospitality and tourism technology for 10 years in the JHTT. The findings of this study can provide strong evidence of what hospitality and tourism technology research topics have been examined and how these topics were connected and changed over time. More importantly, the current study illustrates how the CAQDA approach can be applied to uncover the hidden trends and thematic concepts from text data in the hospitality and tourism literature. Originality/value This study is the first attempt to apply CAQDA software to identify research trends and thematic concepts and gain insights from past JHTT’s articles. Moreover, this study applies this software to describe how hospitality and tourism researchers can use one of the modern computer-assisted qualitative techniques. Based on the findings of this study, theoretical and methodological implications for hospitality and tourism researchers are provided. More importantly, the current study presents the specific guidelines of how the CAQDA approach can be used for the literature review.",https://www.semanticscholar.org/paper/49af7a74517db01cfd49ccf74efa673396bc1e70,CS,Qualitative
The impact of outsourcing information technology services on business operations,"Background: Organisations outsource Information Technology (IT) services in order to keep up with the IT evolution and to remain competitive. Although the IT operations department is responsible to manage service provider service quality, they are not in a position to evaluate IT services performance because of the loss of control and capability in the IT environment.Objectives: This article investigated the impact of outsourcing in-house IT services on the performance of IT operations and how it affects the performance of the organisation.Methods: The qualitative approach was used in this research. Data was collected using structured interviews and were analysed using thematic analysis.Results: Outsourcing of IT services was found to have negative and positive impact on the organisation. Knowledge of the operating environment proved to be significant in the provision of relevant services that add value to the organisation. The results of the analysis revealed that in-house IT and the quality of IT services provided have a direct impact on the performance of the organisation.Conclusion: The study demonstrated that outsourcing IT services impacts the quality and the performance of the organisation positively and negatively. The study recommended that in-house IT and outsource service providers need to collaborate to ensure smooth service delivery, process alignment and also to equip in-house IT with skills to handle first-line support.",https://www.semanticscholar.org/paper/07cf52f9b7194dbd53a5e7b1ed7da245588ad445,IT,Qualitative
Trust and Health Information Exchanges: Qualitative Analysis of the Intent to Share Personal Health Information,"Background Digital health has the potential to improve the quality of care, reduce health care costs, and increase patient satisfaction. Patient acceptance and consent are a prerequisite for effective sharing of personal health information (PHI) through health information exchanges (HIEs). Patients need to form and retain trust in the system(s) they use to leverage the full potential of digital health. Germany is at the forefront of approving digital treatment options with cost coverage through statutory health insurance. However, the German population has a high level of technology skepticism and a low level of trust, providing a good basis to illuminate various facets of eHealth trust formation. Objective In a German setting, we aimed to answer the question, How does an individual form a behavioral intent to share PHI with an HIE platform? We discussed trust and informed consent through (1) synthesizing the main influence factor models into a complex model of trust in HIE, (2) providing initial validation of influence factors based on a qualitative study with patient interviews, and (3) developing a model of trust formation for digital health apps. Methods We developed a complex model of the formation of trust and the intent to share PHI. We provided initial validation of the influence factors through 20 qualitative, semistructured interviews in the German health care setting and used a deductive coding approach to analyze the data. Results We found that German patients show a positive intent to share their PHI with HIEs under certain conditions. These include (perceived) information security and a noncommercial organization as the recipient of the PHI. Technology experience, age, policy and regulation, and a disposition to trust play an important role in an individual’s privacy concern, which, combined with social influence, affects trust formation on a cognitive and emotional level. We found a high level of cognitive trust in health care and noncommercial research institutions but distrust in commercial entities. We further found that in-person interactions with physicians increase trust in digital health apps and PHI sharing. Patients’ emotional trust depends on disposition and social influences. To form their intent to share, patients undergo a privacy calculus. Hereby, the individual’s benefit (eg, convenience), benefits for the individual’s own health, and the benefits for public welfare often outweigh the perceived risks of sharing PHI. Conclusions With the higher demand for timely PHI, HIE providers will need to clearly communicate the benefits of their solutions and their information security measures to health care providers (physicians, nursing and administrative staff) and patients and include them as key partners to increase trust. Offering easy access and educational measures as well as the option for specific consent may increase patients’ trust and their intention to share PHI.",https://www.semanticscholar.org/paper/aaa6aba7053e3879a1a582a79fc0e00c3a6fcbbe,IS,Qualitative
Negative Emotions Induced by Work-Related Information Technology Use in Hospital Nursing,"There is a lack of research into the implications of information technology-related issues for nurses' experiences and well-being at work. However, negative work experiences can generate negative emotions, which, in turn, can negatively affect well-being. Despite this, research has not systematically addressed negative emotions generated by work-related information technology use in hospital nursing. Drawing on data collected through focus groups and interviews with a total of 15 ward nurses, this paper identifies the discrete negative emotions that emerge from work-related information technology use in hospital nursing and maps the identified emotions onto the perceptions associated with and triggering them. The analysis was qualitative and included process, emotion, and causation coding alongside extensive memo writing. We identified six primary negative emotions: frustration, moral distress, alienation, psychological distress, anxiety, and perplexity. All of the identified emotions can be associated with four types of experiences of feeling hindered: mental effort, inability to carry out a task, doing extra or unnecessary work, and failing to complete a task successfully. The framework we present may support healthcare organizations in identifying potentially harmful information technology-related configurations in their infrastructure and implementing appropriate measures to foster nurses' well-being at work.",https://www.semanticscholar.org/paper/51cda2a91384b1766dbbc17289033316a292314c,IS,Qualitative
Women Strengthening Through Information Technology Literacy in Tourist Village,"Women have positive potentials that support their strategic role in development. This research aims to analysis implementing information technology literacy model guidelines in empowering women through community-based education and how increasing the knowledge, skills, and awareness of women in managing the productive potential of information technology-based environments. By the research and development method or Research a Development, this research tries to produce a product and test the effectiveness of the product in accordance with development goals. Women's empowerment activities through information technology including providing information technology-based entrepreneurship motivation, forming business groups, training in information technology-based business management, managing businesses and assisting business groups. Data was collected using the method of observation and interview. The collected data were analyzed with qualitative descriptive. The findings of the study are; 1) implementation program which includes: a. preparation for program implementation which includes program socialization, selection of participants, program objectives, materials, strategies, media, teaching materials, assessment, b. implementation of programs, providing information technology-based entrepreneurship, information technology-based business management training, managing businesses and assisting business groups, c. evaluating the implementation of the program by making direct observations during the process, 2) increasing the knowledge of the culinary group.",https://www.semanticscholar.org/paper/902e40d02521ed00c0955b62bcef1b66ce3d9dc6,IS,Qualitative
Utilization of Information Technology at the Mangarabombang District Office,"Information technology brings together high-speed computing and communications for data, voice, and video. This study aims to determine the use of information technology at the Mangarabombang District Office, Takalar Regency, which focuses on managing Family Cards. To find out these objectives, the researchers used a qualitative descriptive type of research. There were 9 informants in this study and used data collection techniques that were carried out through observation, interviews, and documentation. The data obtained from the research results are processed using data analysis techniques, namely data collection, data reduction, data presentation, conclusion/verification. The results showed that the utilization of information technology at the Mangarabombang District Office, Takalar Regency was optimal. Viewed from the aspect of facilitating communication and obtaining information, developing skills and awareness, increasing the quality and quantity of services, encouraging the growth of democracy. This research can be used as a reference in the realm of using information technology at the sub-district level, because previous research has focused on the district or city level.",https://www.semanticscholar.org/paper/3ac2ca6f263e21e80b61bb84fbe4205997bb3b76,IS,Qualitative
Formation of information culture of students through information technology,"The aim of this research is to evaluate the formation of students' knowledge culture through information technology with student views. In the research, qualitative methods were used to collect the data, and the content analysis method was used for the analysis of the data. The data were collected by the researcher through semi-structured interview forms prepared for the teacher and the students. The participants of the research consisted of 40 primary school 4th grade students studying in Almaty, Kazakhstan in the 2020-2021 academic year. The results of the research reveal that students sometimes benefit from information technologies while doing research for information purposes and for doing their school homework. At the same time, as a result of the research, it was determined that the students always benefited from information technologies for activity purposes. The results obtained from the research reveal that education systems, teachers and parents have great duties in the effective use of information technologies by students. Keywords: information culture, information Technologies, information technology; student competencies, student opinions",https://www.semanticscholar.org/paper/c416dbac68045445a58196909990f2771012aec1,CS,Qualitative
E-Government and information technology coursework in public administration programs in Asia,"New technologies such as artificial intelligence (AI), the Internet of Things (IoT), and blockchain are changing how the public sector serves constituents. Academic programs in public administration and public policy must adapt their coursework to best serve students in an increasingly technology-based world. This qualitative research analyzed 84 Asian public administration graduate programs to determine the degree to which information technology was taught throughout the region. Our findings indicate that considerable variation existed between the number of information technology classes offered and the types of information discussed in the classes. A majority of public administration programs throughout Asia offered coursework in information technology; but, core classes and IT-specific specializations in public administration programs were observed at a much lower rate. Discussions of the tactics taken by programs to educate students in information technologies provide actionable suggestions for practitioners, educators, researchers, and administrators alike.",https://www.semanticscholar.org/paper/8a0dd5620fd9c6c1f8dff7919645e72f8e3826fa,IT,Qualitative
A Model of Factors Affecting Entrepreneurial Intention among Information Technology Students in Vietnam,"In recent decades, the research field of entrepreneurship phenomenon has significantly increased in both quantity and sophistication. In Vietnam, paradoxically, while creating a new business venture has become a tendency, the interest in studying entrepreneurs seems not to be thoroughly investigated. This research aims to evaluate the factors that affect the entrepreneurial intention of information technology (IT) students in Vietnam. The authors make use of mixed methods including both quantitative research method and qualitative research method. The qualitative research method is employed to identify meanings, confirmations, adjustments, and compliments for concept-measurement variables in the conceptual model. Quantitative research is conducted from a sample of 424 IT senior students across many universities in Vietnam. Questionnaires have been sent to students to evaluate the measurement scale and appropriateness of the research model. Results from multiple regression highlighted five independent variables affecting the dependent variable, the entrepreneurial intention, in a descending order as following: entrepreneurial educational environment, personal characteristics, perception of feasibility, entrepreneurial supports, and financial accessibility. In addition, this research has proved that the variable attitudes towards entrepreneurship partially mediated among the interrelationship of the aforementioned variables. From this research, the authors make some recommendations to enhance entrepreneurial intentions of IT students in Vietnam.",https://www.semanticscholar.org/paper/a139a8b9de0169f30e1f458bd3ac36a481e24faa,IS,Qualitative
Role Of Information Technology for Successful Responses to Covid-19 Pandemic,"The purpose of this paper is to examine the role of information technology to response COVID-19 pandemic. By drawing on an understanding of research synthesis as the interpretation of qualitative evidence gained from literature review of previous articles, journals, and research. The main research question of this paper is “What is the role of information technology in the successful handling of the COVID-19 pandemic?”. From this research we can conclude that during the COVID-19 pandemic there was a clear evidence that technology played an important role in the success of pandemic handling.",https://www.semanticscholar.org/paper/78f7656ebacd9987691bf89fea6a223f6ef677d1,IS,Qualitative
Proposal guidelines to implement the concepts of industry 4.0 into information technology companies,"The goal of this work is to propose guidelines for the Information Technology (IT) companies in Brazil to implement the concepts of Industry 4.0.,This research study used inductive method, exploratory and descriptive research, and a bibliographic search was performed, besides using a qualitative research as field research. The IT professionals of companies in Brazil were used as the research university.,With the advances of technology and the advents of industrial revolutions, the profile of professionals and the relationship between employees and companies have been altered, fact that was demonstrated in the field research, making it possible to highlight: the knowledge degree of managers about the theme, it means, the perception of what Industry 4.0 is; how the organizations have already been adjusting to the new industrial revolution; the main challenges for this adjustment; the relevance for clients and opponents; among other discoveries. Therefore, companies must adapt to the new market demands, by improving their processes, investing in new technologies and training their employees.,Actions for implementing Industry 4.0 in IT companies were proposed, such as understanding its organization, defining in which area of the business model the organization intends to make/have changes, discovering tendencies and developments that will have influence over the organization when applying the concepts of Industry 4.0, overcoming the major challenges and adhering to new technologies.,With the contribution toward the organization, it is expected that this study can fulfill the demands of clients and provide cost, personnel and time savings the best way possible. For the academy, the contribution lies in presenting a research study with a new theme and with focus on IT.",https://www.semanticscholar.org/paper/d29e480c0a00ddd18cabfbf03d88fb1508b6399d,IS,Qualitative
How Prefrail Older People Living Alone Perceive Information and Communications Technology and What They Would Ask a Robot for: Qualitative Study,"Background In the last decade, the family system has changed significantly. Although in the past, older people used to live with their children, nowadays, they cannot always depend on assistance of their relatives. Many older people wish to remain as independent as possible while remaining in their homes, even when living alone. To do so, there are many tasks that they must perform to maintain their independence in everyday life, and above all, their well-being. Information and communications technology (ICT), particularly robotics and domotics, could play a pivotal role in aging, especially in contemporary society, where relatives are not always able to accurately and constantly assist the older person. Objective The aim of this study was to understand the needs, preferences, and views on ICT of some prefrail older people who live alone. In particular, we wanted to explore their attitude toward a hypothetical caregiver robot and the functions they would ask for. Methods We designed a qualitative study based on an interpretative phenomenological approach. A total of 50 potential participants were purposively recruited in a big town in Northern Italy and were administered the Fried scale (to assess the participants’ frailty) and the Mini-Mental State Examination (to evaluate the older person’s capacity to comprehend the interview questions). In total, 25 prefrail older people who lived alone participated in an individual semistructured interview, lasting approximately 45 min each. Overall, 3 researchers independently analyzed the interviews transcripts, identifying meaning units, which were later grouped in clustering of themes, and finally in emergent themes. Constant triangulation among researchers and their reflective attitude assured trustiness. Results From this study, it emerged that a number of interviewees who were currently using ICT (ie, smartphones) did not own a computer in the past, or did not receive higher education, or were not all young older people (aged 65-74 years). Furthermore, we found that among the older people who described their relationship with ICT as negative, many used it in everyday life. Referring to robotics, the interviewees appeared quite open-minded. In particular, robots were considered suitable for housekeeping, for monitoring older people’s health and accidental falls, and for entertainment. Conclusions Older people’s use and attitudes toward ICT does not always seem to be related to previous experiences with technological devices, higher education, or lower age. Furthermore, many participants in this study were able to use ICT, even if they did not always acknowledge it. Moreover, many interviewees appeared to be open-minded toward technological devices, even toward robots. Therefore, proposing new advanced technology to a group of prefrail people, who are self-sufficient and can live alone at home, seems to be feasible.",https://www.semanticscholar.org/paper/0f689b44feebf0c83c49d50c8e11f705a4189be7,CS,Qualitative
The Impact of Information Technology Capabilities of Manufacturing Enterprises on Innovation Performance: Evidences from SEM and fsQCA,"With the development of national strategies (such as Industrial 4.0 and Made in China 2025), how to build digital enterprises and cultivate innovation capabilities of enterprises has become a critical problem to Chinese manufacturing enterprises. However, the literature on the specific path of information technology (IT) capabilities to the innovation of enterprises is still lacking a body of relevant empirical research. In particular, it has not yet thought to explore the information technology capabilities, digital transformation, and then innovation performance of manufacturing enterprises. By performing a questionnaire investigation for 138 Chinese manufacturing enterprises, this study adopted both a fuzzy-set qualitative comparative analysis (fsQCA) and structural equation modeling (SEM) to explore the set relations of the conjunctions and conditions and the statistical associations by studying the relationships among information technology capabilities, digital transformation and innovation performance. The results show that the positive impacts of information technology capabilities on the process innovation performance and the digital transformation, as well as the positive impacts of digital transformation on both process innovation performance and product innovation performance. Specifically, digital transformation takes on a new function of partial mediation of IT capabilities and process innovation performance, and digital transformation functions as a complete mediator for IT capabilities and product innovation performance. The combinations of causal recipes related to innovation performance are provided by a fuzzy-set qualitative comparative analysis (fsQCA). Through the analyses of SEM and fsQCA, this research develops the formation mechanisms of both process innovation performance and product innovation performance, and provides guidance for both IT and innovation management of manufacturing enterprises in China.",https://www.semanticscholar.org/paper/6e2cbaa1c95ce737b44e9ed8139512ffbfcc20a1,IS,Qualitative
Benefits and concerns associated with blockchain-based health information exchange (HIE): a qualitative study from physicians' perspectives,"Background Blockchain technology has the potential to revolutionize information sharing in companies. Many studies suggest using blockchain-powered platforms to replace existing mechanisms for health information exchange (HIE) across healthcare organizations. However, very few blockchain-based projects have been implemented in the healthcare sector. This study takes a qualitative approach to explore benefits, concerns, and barriers to the rollout of blockchain in HIE projects from physicians' perspectives. Methods The Promoting Action on Research Implementation in Health Services (PARIHS) framework was used to help us better understand root causes, existing problems, perceived risks, perceived benefits, and suggestions. In-depth interviews have been conducted with 38 physicians in six months. The data were analyzed and coded using NVIVO to classify conceptually similar themes mentioned by the interviewees. Results In total, seven themes have been identified. The key benefits are categorized into three themes: innovative technological features, collaborative ecosystem, and system performance. The main concerns and risks are categorized into four themes: individual, organizational, technological, and market-related issues. The findings can contribute to knowledge by highlighting key values expected from blockchain technology in HIEs. The results also explore obstacles to leveraging the blockchain in healthcare from the perspectives of an important stakeholder (physicians). Conclusions The results show that although blockchain technology may create several benefits (e.g., innovative technological features, collaborative ecosystem, and system performance), its applications in healthcare are still in their early stages. The perceptions of the individual issues (e.g., lack of knowledge), organizational issues (e.g., implementation issues), technological issues (e.g., blockchain model types), and market-related issues (e.g., regulatory concerns) indicate that blockchain-based applications in healthcare continue to be an emerging field. This study has practical implications as understanding these concerns can help developers and healthcare managers identify potential issues in the planning, developing, and implementing blockchain-based HIE systems. Addressing these barriers would support the widespread use of blockchain-based HIEs in different healthcare settings and facilitate interoperability and connectivity in regional and community health information networks.",https://www.semanticscholar.org/paper/270101aec4e31a133fa40adbb2107131cd9fdb3d,IS,Qualitative
Relevance of the technology acceptance model (TAM) in information management research: a review of selected empirical evidence,"Purpose- The purpose of the study was to examine the relevance of the Technology Acceptance Model (TAM) in information management research, and how it has been extended in relation to its perceived usefulness and perceived ease of use. Methodology- A desk study approach was used to review some of the studies that have used the model. Search engines, such as google scholar, yahoo search, and answers. com, were used to search through internationally renowned journals like Emerald, Science direct, IJRIC, South African Journal of Information Management and others. In all, twenty two (22) articles that were published from 1999 to 2016 were used. The 22 articles were those which have used the TAM in empirical studies and have well-described methodologies and clear findings. Findings- The review showed that TAM is still recognized as the right model for quantitative based information management research, and to a lesser extent qualitative information management research and desk studies. However, while some researchers concluded that the TAM is relevant in determining and assessing users’ behaviour regarding technology usage with respect to time, others have criticised the TAM as too limited in the areas of theoretical assumptions and practical effectiveness. These critics have concluded that the model lacks the necessary attributes as a good theory for information system research. Conclusion- In essence, the conflicting views create inconclusiveness about usage of TAM as a theoretical model. Such inconclusiveness calls for further research, and such research should set clear boundaries with respect to measurement of the issues, sampling procedures, and the analytical procedures.",https://www.semanticscholar.org/paper/fb21edb2db75757d6f2a8f789d41904a6c06c587,IS,Qualitative
Investigating information technology skills retention challenges in South Africa’s public sector,"South Africa’s democracy in 1994 triggered the promotion of socio-economic development in the public sector, with specific emphasis on improving infrastructure and bridging the information Technology (IT) skills gap. In this paper, the factors that influence the retention of competent IT resources for the State Information Technology Agency (SITA) are examined. The Theory of Planned Behaviour (TPB) is the underlying theoretical framework. A case study approach is employed as a suitable qualitative research design to investigate contemporary occurrences in real-life settings for exploratory and theory-building research. Empirical insights, regarding excessive power accumulation, lack of accountability, and skills imbalance, in state enterprises, are provided to manage IT infrastructure, effectively. The authors assert that reliance on consultants, promotes opportunistic bargaining that could be detrimental to the government’s strategy to retain skilled IT resources. The results reveal that recruiting and retaining appropriately skilled IT professionals, would address challenges of information asymmetry in the SITA, and facilitate the building of a strong professional network to manage the state enterprises’ IT platforms. The analyzed data emanate from the minutes of meetings, as well as published media sources, and validated by the project’s respondents. One limitation is that the stakeholders did not disclose all the facts.",https://www.semanticscholar.org/paper/84159f856995ef50205d91f12e2870eaa938c5a9,IT,Qualitative
INFORMATION TECHNOLOGY-BASED MANAGEMENT EDUCATION IN VOCATIONAL HIGH SCHOOLS,"This study aims to analyze the role of information technology-based management education in vocational schools. The implementation of technology was conducted in several schools, covering managerial aspects as well as academic aspects. This research utilizes a qualitative approach and descriptive method. Data obtained through literature review and analyzed descriptively and qualitatively. The results show that technology-based management education is essential to provide information to support and facilitate the management and academic processes daily. Schools must invest in technology-based school systems to increase the effectiveness and efficiency of school management generally.",https://www.semanticscholar.org/paper/98b2593cd44ca65289d79bf438939cb6c5cc8c42,IS,Qualitative
Making the Information Technology (IT) Business Alignment Works: A Framework of IT-based Competitive Strategy,"Seamless IT-business integration enables superior performance and provides value creation opportunities for an enterprise to achieve a sustainable competitive advantage. Yet, creating IT-business alignment remains a challenge. Some users consider the existing IT-business alignment methodologies too complicated for practical implementation. This paper proposes a simplified and practical framework, an IT-based competitive strategy framework, to align an IT strategy with enterprise businesses based on a design science research methodology. The framework consists of three elements to formulate a comprehensive IT-based strategy, i.e., value drivers of IT implementation, competitive factors and an IT competitive strategy. The framework evaluation includes interviewing experts and practitioners, applying the proposed framework in an Indonesian enterprise and assessing the framework benefits on a qualitative basis. The observational evaluation with the experts concludes that the proposed framework is helpful for the targeted users. The framework application in a company also demonstrates the advantage and the usefulness of the proposed framework.",https://www.semanticscholar.org/paper/0fe1884545bc1029959f2332352c2e1f71ec6c8e,IS,Qualitative
Information technology adoption on digital marketing communication channel,"This research aimed at contributing academically related to the effort of information technology adoption particularly in digital channels of marketing communication, such as website and social media, to be optimized by Small-Medium size Enterprises (SMEs) in Bali. The main focus is on various travel agents and ticketing in small and medium scale which using the media of information technology in order to build brand awareness of their services and attracting their potential target market. Snowball sampling was used to 16 informants through the semi-structured interview as a qualitative research method. This research aims at addressing how marketers knowing and using information technology to deliver their business activities, while they tend to face a number of barriers to realize and process data from those marketing channels as an analysis supporting tools for crafting marketing campaign strategies and optimizing as a business strategic decision. Furthermore, the paradigm of society that traditional channels and digital channels of marketing communication as a different thing, which lead to the failure of the integration of marketing communication.",https://www.semanticscholar.org/paper/0b735a29a3ba632c81bb62a1478bf529906a4951,IS,Qualitative
The information technology role in supplier-customer information-sharing in the supply chain management of South African small and medium-sized enterprises,"Background: The study background looked at the advent of supply chain management in the last generation which ushered in technology that drives information-sharing within, and across enterprises. The information flow facilitates synchronisation of business activities, such as relationship-building, supply chain management among others.Aim: The aim of the study was to investigate how information technology (IT) application in the South African small and medium-sized enterprises (SMEs) enhanced supplier-customer information sharing.Setting: Interviews were conducted with SMEs samples that comprised mixed owner-managers from food, and general trading SMEs in Gauteng Province of South Africa.Methods: A qualitative research methodology was used, and a non-probability sampling process was pursued.Results: The results indicated that IT application in the South African SMEs enhanced supplier-customer information-sharing, as it improved interaction through supply chain collaboration and integration.Conclusion: The conclusion of the study highlighted that IT application in enterprises as obtained from South African SMEs enhanced supplier-customer information-sharing.",https://www.semanticscholar.org/paper/9e0d54e093c3b7912cbe6fe8686be259018d9e0a,IS,Qualitative
Progression and development of information and communication technology research in hospitality and tourism,"Purpose This study aims to present a state-of-the art review on information and communication technology (ICT) research in hospitality and tourism published between 2014 and 2017. Design/methodology/approach A total of 288 full-length articles from eight top-tier hospitality and tourism journals were gathered by harnessing a systematic literature search approach. Subsequently, the authors used a qualitative content analysis to review, analyse and assign all included articles into a framework with six consumer-related and five supplier-related research streams. Findings In terms of volume (i.e. the amounts and ratios of ICT research in top-tier journals by publication year) and variety (i.e. the diversity of research topics), a significant progression of ICT research in hospitality and tourism is observed. However, some old and new knowledge gaps are still inadequately addressed, thus requiring scholars and practitioners to conduct additional research in the future. Practical implications The accumulation of knowledge and actionable clues in this study is expected to keep practitioners updated with the overwhelming volume of ICT research. Originality/value This study contributes to the literature by accelerating the accumulation of knowledge on research topics and setting forth an agenda for future research. The findings also complement prior literature reviews by providing an overview of how knowledge on ICT research in hospitality and tourism has progressed since 2014.",https://www.semanticscholar.org/paper/4f19edfd48cba422aa0a2309145e822f27a4eebb,CS,Qualitative
Translational research in action: The use of technology to disseminate information to parents during the COVID‐19 pandemic,"Abstract This paper addresses the research problem of how to reach, engage and support parents in home‐educating young children during the first national COVID‐19 lockdown in England (March–June 2020), which was addressed through using technology. An internet‐mediated research (IMR) approach is used to investigate the effectiveness of using technology and translational research as strategies for disseminating a rapidly produced digital guide, for promoting play‐based learning at home, to parents. Lockdown with the closure of early years provision led to parents finding themselves isolated at home with young children. Early years educators were managing a unique set of circumstances where communication with families, including those ‘harder‐to‐reach’ was contextually problematic. Qualitative data using IMR captured online interactions by unobtrusive and obtrusive methods; unsolicited emails and social media comments and questionnaire responses. Conventional content analysis identified emerging themes of access, availability, reliability and readability. Analysis showed a combination of factors impacted on the speed and scale of sharing and downloading the digital guide. First, being digitally ready as platforms were already used by early years educators and Local Authorities. Second, the professional drive of Local Authorities and early years educators to support families during the crisis and third, the availability of an easily accessible online resource seen as valuable in improving play‐based learning at home. Practitioner notes What is already known about this topic? There are high levels of digital readiness in the United Kingdom. Technology is one method used by early years settings to communicate with parents. Parental engagement is challenging. What this paper adds? A translational research strategy (to share research‐informed‐knowledge with stakeholders) and internet‐mediated research (to gather data from stakeholders) combine effectively for use within the early years sector to disseminate research knowledge to parents and support home learning environments. The high levels of technology readiness of early years educators and parents in England provides opportunities for disseminating information and improving home learning environments. Accessing and sharing documents online may involve parents but is insufficient to engage. Implications for practice and/or policy Early years settings need to be more proactive in engaging with parents online. Technology provides opportunities to develop interaction and the sharing of information with parents. Digital media should be used as additional communication strategies and should not replace the fundamental importance of face‐to‐face‐interaction.",https://www.semanticscholar.org/paper/fbcc43b200c6d50d7ac6eb5af9e18b0f6b6f7756,IS,Qualitative
Harnessing Artificial Intelligence for Advancing Sustainable Development Goals in South Africa's Higher Education System: A Qualitative Study,"Artificial intelligence (AI) presents opportunities in transforming higher education system and contribute significantly to achieving Sustainable Development Goals (SDGs). This study seeks to leverage AI technologies to advance SDGs within South Africa’s higher education system. It examines AI technology adoption in South African higher education institutions and challenges, strategies, and potential future directions. This qualitative research employed the constructivist principle to unravel the dynamics of AI in advancing SDGs in South Africa. Lecturers from the Department of Information Sciences were the participants of the study. The participants were purposefully selected based on their experience and knowledge of AI technologies. In-depth interview and focused group discussion was employed to generate and estimate responses using thematic content analysis. The result revealed that participants used AI technology to increase students' learning and engagement so students would not doze in class. It was discovered that AI technology has increased the chances for collective learning. The study further proved that AI technology can improve personalized learning experiences of students with diverse learning styles and abilities. This has led to a more inclusive and interactive classroom environment where students feel more motivated and supported in their learning journey. Integrating AI technology into education has shown promising results in improving student outcomes and fostering a more collaborative learning atmosphere. Based on the results, it implies that harnessing AI would advance SDGs in South Africa’s higher education institutions. As such, we recommended that the South African government should formulate comprehensive national AI education policy guidelines for higher education to regulate and harmonize the usage of AI in the higher education system.",https://www.semanticscholar.org/paper/de94ba92bf8009a40de80131a414aecd29e95d36,CS,Qualitative
Overview of ChatGPT Technology and its Potential in Improving Tourism Information Services,"Tourism is one of the important economic sectors for many countries, including Indonesia. Along with the development of technology and the internet, the way people seek information about tourism has also changed. This research aims to conduct a review of ChatGPT technology and its potential in improving tourism information services. The focus of this research is qualitative. Methods for gathering information included paying close attention and taking detailed notes, with subsequent analysis including data reduction, visualisation, and inference. The study arrived at the conclusion that the use of AI technology specifically ChatGPT has great potential in improving services and traveller experience in the tourism industry. ChatGPT can assist service providers in providing solutions and answering tourists' questions quickly and efficiently, as well as strengthening the position of the company or organisation amid increasingly fierce competition.",https://www.semanticscholar.org/paper/9afbac611ddc4556ac3bd6be548163eb8b3ed2be,IS,Qualitative
Robotics in Education: Examining Information Technology Teachers’ Views,"This study examining the robotic codings and robotic tournaments from the perspectives of Information Technology (IT) teachers was carried out using the qualitative case study method. In the study, a total of 20 teachers from private schools and state schools were asked for their views. The research data collected from the IT teachers helped determine the advantages and limitations of robotic coding and robotic tournaments. The results revealed that the IT teachers generally reported positive views about robotic coding and robotic tournaments. In addition, the teachers reported that robotic coding activities arose the students’ curiosity and attention. As for the negative aspects, the teachers thought that students had to perform the tasks in a very limited time; that there was no equality between the materials owned; that the environment was encouraging; that the juries did not act objectively during the scoring phase; and that the tournaments served for corporate advertising. Consequently, it was found that the students achieved active learning with the help of the robotic coding trainings and tournaments, which increased their motivations.",https://www.semanticscholar.org/paper/1609b81cb7ff21d087a4792a99d572cd7a057b3c,CS,Qualitative
Identifying Clusters and Themes from Incidents Related to Health Information Technology in Medical Imaging as a Basis for Improvements in Practice,"Beyond identifying and counting the things that go wrong, understanding how and why they go wrong requires qualitative research, especially for low-frequency events. The purpose of this study was to identify and characterize patient safety and quality issues related to health information technology (HIT) in medical imaging by collecting and analyzing incident reports through the lens of thematic analysis. In this article, we analyze 5 clusters: Staff related issues (16%), issues with diagnosis (15%), HIT incidents that involved “paper record” (12%), information and communication related (4%), and “action taken” related issues (4%). Human factors involved people failing to scan forms into the computer system (consents, requests, bookings, questionnaires, assessments, treatments and prescriptions), and another 4% involved failure to enter verbally imparted information into the system (about infectious patients, cancelled cases, and the status of reports). All of these problems had their genesis in human errors and violations. Human factors were found to cause more deleterious effects than technical factors. Of three instances of deaths caused by diagnostic issues, two were triggered by human factors, missed diagnosis. However, “staff or organizational outcome” was evenly distributed for both human and technical factors. It was therefore important to identify and characterize these incidents related to health information technology in medical imaging through the lens of thematic analysis, to provide a basis for improvements in preventing issues and improving clinical practice.",https://www.semanticscholar.org/paper/66c6b3899af0ad540853665fab60c0d240f5ba22,IS,Qualitative
Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,"Background Artificial Intelligence (AI) applications are widely researched for their potential in effectively improving the healthcare operations and disease management. However, the research trend shows that these applications also have significant negative implications on the service delivery. Purpose To assess the use of ChatGPT for mental health support. Methods Due to the novelty and unfamiliarity of the ChatGPT technology, a quasi-experimental design was chosen for this study. Outpatients from a public hospital were included in the sample. A two-week experiment followed by semi-structured interviews was conducted in which participants used ChatGPT for mental health support. Semi-structured interviews were conducted with 24 individuals with mental health conditions. Results Eight positive factors (psychoeducation, emotional support, goal setting and motivation, referral and resource information, self-assessment and monitoring, cognitive behavioral therapy, crisis interventions, and psychotherapeutic exercises) and four negative factors (ethical and legal considerations, accuracy and reliability, limited assessment capabilities, and cultural and linguistic considerations) were associated with the use of ChatGPT for mental health support. Conclusion It is important to carefully consider the ethical, reliability, accuracy, and legal challenges and develop appropriate strategies to mitigate them in order to ensure safe and effective use of AI-based applications like ChatGPT in mental health support.",https://www.semanticscholar.org/paper/b2a492ab1dc7636f2972349823881ed93100085d,CS,Qualitative
‘They just came with the medication dispenser’- a qualitative study of elderly service users’ involvement and welfare technology in public home care services,"Background Public home care for the elderly is a key area in relation to improving health care quality. It is an important political goal to increase elderly people’s involvement in their care and in the use of welfare technology. The aim of this study was to explore elderly service users’ experience of user involvement in the implementation and everyday use of welfare technology in public home care services. Method This qualitative study has an explorative and descriptive design. Sixteen interviews of service users were conducted in five different municipalities over a period of six months. The data were analysed using reflexive thematic analysis. Results Service users receiving public home care service are not a homogenous group, and the participants had different wishes and needs as regards user involvement and the use of welfare technology. The analysis led to four main themes: 1) diverse preferences as regards user involvement, 2) individual differences as regards information, knowledge and training, 3) feeling safe and getting help, and 4) a wish to stay at home for as long as possible. Conclusion The results indicated that user involvement was only to a limited extent an integral part of public home care services. Participants had varying insight into and interest in welfare technology, which was a challenge for user involvement. User involvement must be facilitated and implemented in a gentle way, highlighting autonomy and collaboration, and with the focus on respect, reciprocity and dialogue.",https://www.semanticscholar.org/paper/41ef8fcbd0c170ff6622518dc781290daa8629b9,CS,Qualitative
THE ROLE OF INFORMATION TECHNOLOGY IN EDUCATION WORLD (PERAN TEKNOLOGI INFORMASI DALAM BIDANG PENDIDIKAN; E-EDUCATION),"This research aims to explore the importance of information technology in the field of education. As an instrument of the modern digital world, the existence of information technology is possible to answer the level of learning difficulties that are currently considered conventional. This study uses a qualitative approach to the study of literature. As a result of this research, the influences of globalization, future education will be more open and two-way, diverse, multidisciplinary, and related to work productivity. With the development of information technology in the field of education, it is now possible to hold distance learning by using internet media to connect students with their lecturers, view student grades online, check finances, view class schedules, send assignments files given by lecturers etc. Penelitian ini bertujuan mengungkap pentingnya tekhnologi informasi dalam bidang pendidikan. Sebagai instrumen dunia digital modern, keberadaan teknologi informasi dimungkinkan dapat menjawab tingkat kesulitan pembelajaran yang saat ini dirasa masih konvensional. Penelitian ini menggunakan pendekatan kualitatif pendekatan kajian studi literatur. Dihasilkan dari penelitian ini, masuknya pengaruh globalisasi, pendidikan masa mendatang akan lebih bersifat terbuka dan dua arah, beragam, multidisipliner, serta terkait pada produktivitas kerja. Dengan adanya perkembangan teknologi informasi dalam bidang pendidikan, maka pada saat ini sudah dimungkinkan untuk diadakan belajar jarak jauh dengan menggunakan media internet untuk menghubungkan antara mahasiswa dengan dosennya, melihat nilai mahasiswa secara online, mengecek keuangan, melihat jadwal kuliah, mengirimkan berkas tugas yang diberikan dosen dan sebagainya.",https://www.semanticscholar.org/paper/8de6f3b005fe53e8ae5c1e4ed1aad7a474090ad1,IS,Qualitative
The role of information technology applications in profitability,"Purpose This paper aims to investigate the role of information technology (IT) applications in the profitability of the Indian travel and tourism industry. Design/methodology/approach Qualitative research was conducted and an in-depth interview technique was applied for the collection of primary data. Traditional travel agents, tour operators, online travel agents and hoteliers in Gurgaon were investigated to explore issues pertaining to the study, and all responses were recorded and later transcribed. The identities of interviewees and their properties are not revealed to preserve confidentiality. Findings The analysis reveals that IT plays a significant role in the profitability of the tourism and travel industry. These include competitive pricing attained via dynamic pricing especially in the hotel industry, promotion and improved efficiency in rendering services to tourists and access to tourists virtually anywhere and at any time. Practical implications The findings have direct and indirect implications for different stakeholders – notably, travelers, owners and executives of businesses in the travel industry, as well as for researchers. Executives and owners can discern the critical implications of adopting IT to take their businesses to a new level. They should also be able to appreciate the fact that to remain competitive, practitioners must explore the potential opportunities emerging through IT. Originality/value Organizations in this sector are moving from traditional business models to more technologically dependent approaches. In particular, this study shows how the use of IT applications is influencing the profitability of the travel and tourism industry especially in India.",https://www.semanticscholar.org/paper/07de48aed8810c03b59b7bb8d488d32d2a1e2603,IS,Qualitative
Perception of Teachers in Higher Education towards Ethical Issues of Information Technology Use,"Our paper presents the results of a type of qualitative research meant to identify the perspective of teachers in higher education on the ethical issues related to information technology use. The research is based on the analysis of the data content obtained by applying a semi-structured interview to 31 teachers working in higher education institutions in Romania. The data processing and interpretation allowed the identification and definition of the general and specific thematic categories related to the perception of teachers in higher education on the ethical aspects of information technology use. The research results are analyzed from the perspective of the following thematic directions: ethical problems of the use of information technology by teachers in teaching-learning-evaluation activities, ethical problems of the exploitation of technological resources in carrying out research activities, problematic aspects of the ethical use of technological tools in online communication, teacher training in higher education in the field of ethical use of IT, introduction of these topics in academic courses, existence of rules at the university level for teacher ethical use of IT.",https://www.semanticscholar.org/paper/903170d0f4027e252096a631faf784d507f8a7a4,IS,Qualitative
The effect of information technology relatedness on union performance mediated by knowledge management capability,The purpose of this research are: (1) to know the effect of information technology relatedness on knowledge management capability; (2) to know the effect of knowledge management capability on the union performance; (3) to know the effect of information technology relatedness on the union performance; and (4) to know the effect of information technology relatedness on the union performance mediated knowledge management capability. The type of data in this study is the type of qualitative data and quantitative data. The sample used in this research is manager/head of cooperative and manager/head of accounting department at Multipurpose Cooperative in Badung regency of Bali Province using Information Technology Relatedness with Full Integrated System. While the data analysis techniques used using multiple regression analysis. Based on the results of research indicate that: (1) Information technology relatedness have a positive effect on knowledge management capability; (2) Knowledge management capability has a positive effect on union performance; (3) Information technology relatedness has a positive impact on union performance; and (4) Information technology relatedness has a positive influence on the union performance which is mediated by knowledge management capability.,https://www.semanticscholar.org/paper/f3b7496ae551ad2070d3cbfb095c15dd36479941,IS,Qualitative
EVALUATION OF DIGITAL COMPETENCE BY INFORMATION TECHNOLOGY TEACHERS IN TURKEY IN THE CONTEXT OF 21ST CENTURY SKILLS AND THE QUALITY FRAMEWORK OF MINISTRY OF EDUCATION,"This research which examines information technology teachers’ opinions on digital competence is a phenomenological qualitative research and was carried out with 10 information technology teachers in Turkey. The data were collected through semi-structured interview form developed by the researchers and analyzed by using content analysis method. The findings showed that information technology teachers explained digital competence with 193 utterances. These utterances were identified in 3 themes as “digital competence and its components”, “importance and effects”, “digital competence and education”. These themes were separated into 9 categories as “meaning”, “sub-dimensions”, “supporting competences”, “importance”, “positive effects for future”, “negative effects for future ”,“ acquisition by formal education ”,“ acquisition by informal education ”,“ digital competence and educational problems”. It was seen that information technology teachers expressed their opinions on digital competence with 61 codes from these categories. According to the results of the research, information technology teachers frequently produced “digital literacy”, “knowledge and “communication”, “foreign language knowledge”, “necessity of the knowledge society”, “necessity to be information literate”, “fast communication”, “technological antisocialism”, “useless knowledge acquisition”, “social regression”, “school education”, “early technology introduction” and “objective level mismatches” codes while expressing digital qualifications that individuals are expected to have. Article visualizations:",https://www.semanticscholar.org/paper/70b1d00bbe8f0ef891175df724978d2ed90dd878,CS,Qualitative
Disposal and reuse of the information technology waste: a case study in a Brazilian university,"Purpose The purpose of this paper is to identify the factors that define the management practices of a center of electrical and electronic waste and of reuse of equipment aiming to contribute to the sustainable development. It is known that the effort to achieve the green IT, including recycling and sustainable disposability of equipment does not follow the same pace as industry production. Design/methodology/approach The paper draws on the existing sustainable development, on the computer equipment and its composition and in the disposal of electronic waste as literature of orientation. The center for disposal and reuse of the information technology waste from a Brazilian university was approached by the methodology of qualitative case study. Findings The research revealed some findings related to the concepts of integrated waste management, product life cycle assessment, stakeholder involvement and inventory and information system of electrical and electronic equipment. Research limitations/implications The research involved the analysis of documents and the website of the center and the technician responsible for the center was interviewed. The results can contribute as a benchmark for other universities and organizations who intend to create or implement a center for collection and recycling of computer equipment. Practical/implications The paper shows the importance of communication and relationship between the center and the units of the university and with the stakeholders related to the waste electrical and electronic equipment (WEEE) management. Originality/value The study has its focus on a more comprehensive WEEE approach that shows insights that can be used or adapted to any university or even companies.",https://www.semanticscholar.org/paper/18e6c9bd9bc4edefc781cfbf348e683084b24f36,IS,Qualitative
An exploration of patients’ experience of nurses’ use of point-of-care information technology in acute care,"The rapid introduction of technology into acute healthcare settings, specifically the presence of point-of-care health information technology at patients’ bedsides, is expected to impact patients’ healthcare experience by altering nursepatient interactions. This research was a multi-method naturalistic pilot study designed to explore patients’ perception of their interactions with nurses using bedside point-of-care health information technology in acute care. Data were collected using observation, interviews and surveys. Twenty-four participants were purposefully recruited from medical and surgical wards, to capture variability in their self-reported confidence with information technology; 29% were not confident, 38% were somewhat confident and 33% were completely confident with information technology. Participants’ mean age was 68.6 years (SD 11.1) and 63% were male. Qualitative observation, interview and survey data showed some nurses directly involved patients and explained or demonstrated how the point-of-care health information technology was being used to complement and enhance their care; while others used the point-of-care health information technology as an electronic documentation tool without engaging their patients. Patients’ experiences of point-of-care health information technology differed with their self-reported confidence with information technology; those with complete information technology confidence were better at recognising the potential and opportunities for point-of-care health information technology to support self-directed care than those with less confidence using information technology. Some participants reported that the use of point-of-care health information technology impeded interpersonal communication with nurses. Participants recognised the benefits of point-of-care health information technology to support clinical practice but generally desired greater engagement with the nurses when they used the system.",https://www.semanticscholar.org/paper/1dc9ee936632e87eac1be58d10def16635945b03,IS,Qualitative
"Perspectives of Patients, Health Care Professionals, and Developers Toward Blockchain-Based Health Information Exchange: Qualitative Study","Background Although the electronic health record system adoption rate has reached 96% in the United States, implementation and usage of health information exchange (HIE) is still lagging behind. Blockchain has come into the spotlight as a technology to solve this problem. However, there have been no studies assessing the perspectives of different stakeholders regarding blockchain-based patient-centered HIE. Objective The objective of this study was to analyze the awareness among patients, health care professionals, and information technology developers toward blockchain-based HIE, and compare their different perspectives related to the platform using a qualitative research methodology. Methods In this qualitative study, we applied grounded theory and the Promoting Action on Research Implementation in the Health Service (PARiHS) framework. We interviewed 7 patients, 7 physicians, and 7 developers, for a total of 21 interviewees. Results Regarding the leakage of health information, the patient group did not have concerns in contrast to the physician and developer groups. Physicians were particularly concerned about the fact that errors in the data cannot be easily fixed due to the nature of blockchain technology. Patients were not against the idea of providing information for clinical trials or research institutions. They wished to be provided with the results of clinical research rather than being compensated for providing data. The developers emphasized that blockchain must be technically mature before it can be applied to the health care scene, and standards of medical information to be exchanged must first be established. Conclusions The three groups’ perceptions of blockchain were generally positive about the idea of patients having the control of sharing their own health information. However, they were skeptical about the cooperation among various institutions and implementation for data standardization in the establishment process, in addition to how the service will be employed in practice. Taking these factors into consideration during planning, development, and operation of a platform will contribute to establishing practical treatment plans and tracking in a more convenient manner for both patients and physicians. Furthermore, it will help expand the related research and health management industry based on blockchain.",https://www.semanticscholar.org/paper/dc044147410900b7454f9954ed175264cdd89367,IS,Qualitative
Leadership and Service in the Police Context_A Qualitative Study,"This research aims to explore the leadership and service in the Indonesian National Police. This research is very significant for improving police policy due at the ontological level and sociological level. The problem is very interesting to be explored by conducting qualitative research method. Data were collected through in-depth interview, documentation and observation. Data were analyzed by using data reduction, data display and data verification and supported by triangulation. The results are categorized into several themes. Based on these results as per ontological level and sociological level are useful for improving police policy and practice and providing information to stakeholders related as inputs for making better regulation on police policy as well as for public officials and practitioners.",https://www.semanticscholar.org/paper/14a250dee49c095ec49e6c6dd84bf8fb892d0a1b,IS,Qualitative
"Synergistic Effect of Integrated Project Delivery, Lean Construction, and Building Information Modeling on Project Performance Measures: A Quantitative and Qualitative Analysis","In the recent years, owners and construction management companies have shown an increasingly more interest in adopting approaches that result in enhanced quality and less risks, conflicts, and wastes on their projects despite potentially higher initial cost. Implementing advanced technology trends and incorporating more integrated methods of delivering projects have proven to be highly value-adding and forward-thinking approaches. The objective of this research was to evaluate the effectiveness of and the synergy between three of such trending concepts in the construction industry, namely, integrated project delivery (IPD), lean principles, and building information modeling (BIM) in terms of cost and schedule performance measures. Data analysis was conducted on 72 vertical projects through interviews and study of the published articles, reports, and case studies. Qualitative analysis was performed through grounded theory while quantitative analysis was implemented using univariate and multivariate analysis of variance tests on schedule performance and cost performance. Results of the grounded theory analysis summarize six crucial characteristics required for an effective coordination between IPD, lean construction, and BIM. Statistical analysis on different combination of these three components revealed considerable effectiveness in terms of schedule performance while the effect on cost performance was not as much significant. This study contributes to the body of knowledge and practice in the field of construction by demonstrating the cost and schedule benefits realized through the use of IPD, lean construction, and BIM and identifying their collective conceptual advantages.",https://www.semanticscholar.org/paper/7ab38c9f2ee804bf24864c97cd3e64209b08b1cf,IS,Qualitative
Trend of Technology Pedagogical Content Knowledge (TPACK) Research in 2012-2022: Contribution to Science Learning of 21st Century,"The use of TPACK in 21st Century Science learning can facilitate teachers and students to be more active in learning and make it easier for students to have the expected 21st-century competencies. This study aims to identify and analyze TPACK research trends in 21st-century science learning in the form of TPACK documents, classification of journal rankings, classification of authors and their country of origin, and classification of keywords. This research is qualitative research. The data used in this study were obtained from documents indexed by Google Scholar from 2012-2022 using Publish or Perish and dimension.ai. Research procedures using PRISMA guidelines. Methods of data analysis using bibliometric analysis assisted by VOSviewer software. The results of the analysis show that the trend of writing TPACK articles in 21st Century Science learning has increased significantly from 2016 to 2020. Most journals that contain articles about TPACK in 21st Century Science learning are Educational and Information Technologies and Computers & Education. The results of the density mapping analysis show that the themes that are rarely researched are ICT investigation, curriculum, effectiveness, teacher knowledge, foreign language, teacher education, and TPACK instrument.",https://www.semanticscholar.org/paper/3bb2ebe5252a5d845f3d1a83cdac65def1aff7f4,CS,Qualitative
BIM research vs BIM practice: a bibliometric-qualitative analysis from China,"PurposeUnderstanding the frontier difference between building information modeling (BIM) research and practice is a top priority to guarantee the engineering significance and feasibility of academic achievements, yet such research gap has not been well-explored. The purpose of this paper is to provide an objective and accurate analysis of BIM knowledge using 551 published BIM-related papers and 68 documents of frontier BIM projects in China.Design/methodology/approachThis paper adopts the mixed method, combining the bibliometrics method with the qualitative method. Bibliometrics was used to analyze 551 BIM-related literatures from China with Citespace 5.0. Qualitative research was used to analyze 68 project documents from China with Nvivo. Finally, the analysis results are compared to obtain the final conclusion.FindingsThe analysis results of the collected BIM-related papers, given by bibliometrics analysis, show that the subject categories of engineering, civil engineering, and construction and building technology, and 8 key research clusters are extremely important for development of BIM knowledge. The analysis results of the collected project documents, given by qualitative analysis, indicate that visualization, aided management, intelligent construction, simulation and analysis are the hot applications of BIM practice.Originality/valueThrough comparison, certain research gaps between the research and practice community in China was identified, which are useful for identification of research trends and practice frontier in BIM community. This study offers useful and new insights to summarize the status quo of BIM and can be used as a reference to integrate future BIM developments.",https://www.semanticscholar.org/paper/b50741eb374eb85d96c46b55e7adba93e4c5265c,IS,Qualitative
User Requirements for Technology to Assist Aging in Place: Qualitative Study of Older People and Their Informal Support Networks,"Background Informal support is essential for enabling many older people to age in place. However, there is limited research examining the information needs of older adults’ informal support networks and how these could be met through home monitoring and information and communication technologies. Objective The purpose of this study was to investigate how technologies that connect older adults to their informal and formal support networks could assist aging in place and enhance older adults’ health and well-being. Methods Semistructured interviews were conducted with 10 older adults and a total of 31 members of their self-identified informal support networks. They were asked questions about their information needs and how technology could support the older adults to age in place. The interviews were transcribed and thematically analyzed. Results The analysis identified three overarching themes: (1) the social enablers theme, which outlined how timing, informal support networks, and safety concerns assist the older adults’ uptake of technology, (2) the technology concerns theme, which outlined concerns about cost, usability, information security and privacy, and technology superseding face-to-face contact, and (3) the information desired theme, which outlined what information should be collected and transferred and who should make decisions about this. Conclusions Older adults and their informal support networks may be receptive to technology that monitors older adults within the home if it enables aging in place for longer. However, cost, privacy, security, and usability barriers would need to be considered and the system should be individualizable to older adults’ changing needs. The user requirements identified from this study and described in this paper have informed the development of a technology that is currently being prototyped.",https://www.semanticscholar.org/paper/1a7109d77173cce328250402d9a6c0db2d63723d,IT,Qualitative
The interplay between literacy and digital technology: a fuzzy-set qualitative comparative analysis approach,"Introduction. Information literacy and digital literacy skills have become increasingly important capabilities in the digital world, as such it is of the utmost importance to assess how individual’s literacy skills impact people’s intention to use digital technology. Method. In this paper, based on the current literature, we design our research and through an empirical study conducted in Finland and Italy, we examine how these skills impact the decision of university students to use digital technology. Analysis. Data was analysed through a novel method (fuzzy-set qualitative comparative analysis). Results. The fuzzy-set Qualitative Comparative Analysis results show that students in these two countries differ from one another and factors influencing the intention to use digital technology vary among students. For Finnish students’ digital literacy and for Italian students’ information literacy were important factors. Conclusions. The results of this paper contribute to the information and digital literacy research and provide unique insights and practical implications.",https://www.semanticscholar.org/paper/44ad323f03ed9e1964d729b16f9d53679b1ce3ad,IS,Qualitative
Proposing a conceptual model of the sustainable digital supply chain in manufacturing companies: a qualitative approach,"PurposeIn the digital age, emerging technologies have affected every industry. Information and communications technology and digital technologies have transformed traditional supply chains into smart and more resilient ones, enabling effective management of challenges. Given the importance of the two topics, namely sustainable supply chain management and Industry 4.0 in supply chain management, on the one hand, and the dearth of theoretical research performed in this area on the other, this study aims to propose a conceptual model on a sustainable digital supply chain management in manufacturing companies.Design/methodology/approachThis study utilized a qualitative approach. First, an in-depth review of the relevant literature was done. Then, following a multi-grounded theory methodology, relevant data were gathered by reviewing 92 papers and conducting nine semi-structured interviews with industry experts. These data were analyzed using the MAXQDA software.FindingsA total of 41 concepts, ten sub-components and three main components (dimensions) were extracted, and the proposed conceptual model was presented. Finally, based on this conceptual model, three propositions were suggested.Research limitations/implicationsConsidering that the present study was performed in the context of Iranian manufacturing companies, caution should be exercised in relation to the generalizability of the obtained results. Also, due to the problems in the digital technology infrastructure and the limited use of these technologies by manufacturing companies (emphasized by the interviewees), this study focused on the theoretical dimension of using digital technologies by these companies.Practical implicationsThe proposed comprehensive model can help academicians as well as practitioners to focus better and explore the variables and constructs of the model, paving the way toward successful implementation of digital technologies in the manufacturing supply chain.Originality/valueTo the best knowledge of the authors, this study is among the first of its kind which presents a holistic and comprehensive digital supply chain model aimed at guiding companies to consider sustainability from all the main dimensions and their relevant indicators.",https://www.semanticscholar.org/paper/85223e72c6e56b0f88ff7bcab5e30899a018a25c,CS,Qualitative
Family Separation and the Impact of Digital Technology on the Mental Health of Refugee Families in the United States: Qualitative Study,"Background Conflicts around the world have resulted in a record high number of refugees. Family separation is a critical factor that impacts refugee mental health. Thus, it is important to explore refugees’ ability to maintain contact with family members across the globe and the ways in which they attempt to do so. It is increasingly common for refugees to use information and communication technologies (ICTs), which include mobile phones, the internet, and social media sites, such as Facebook, WhatsApp, Skype, and Viber, for these purposes. Objective The aim of this study was to explore refugees’ perceptions of the impact of communication through ICTs on their mental health, the exercise of agency by refugees within the context of ICT use, especially their communication with their families, and logistical issues that affect their access to ICTs in the United States. Methods We used a constructivist grounded theory approach to analyze in-depth interviews of 290 adult refugee participants from different countries, who were enrolled in a randomized controlled trial of a community-based mental health intervention. Results Analyses showed that communication through ICTs had differing impacts on the mental health of refugee participants. ICTs, as channels of communication between separated families, were a major source of emotional and mental well-being for a large number of refugee participants. However, for some participants, the communication process with separated family members through digital technology was mentally and emotionally difficult. The participants also discussed ways in which they hide adversities from their families through selective use of different ICTs. Several participants noted logistical and financial barriers to communicating with their families through ICTs. Conclusions These findings are important in elucidating aspects of refugee agency and environmental constraints that need to be further explicated in theories related to ICT use as well as in providing insight for researchers and practitioners involved in efforts related to migration and mental health.",https://www.semanticscholar.org/paper/32b8cb2f361b0a049b90828ce89203dabcdfc40c,IS,Qualitative
Information Communication Technology as A Tool for Improving Nigerian Education at All Levels: A Theoretical Perceptive,"The importance and integration of ICT equipment and facilities in teaching and learning are needed in Nigerian schools. However, the use of ICT in learning is still experiencing problems. The aims of this study are information and communication technology as a tool to improve Nigerian education at all levels. This type of research is qualitative research. The methods used in data collection are documentation study, literature study, and observation. The primary data collection instrument is the researcher observing, asking, listening, asking, and taking research data in qualitative research. The data analysis technique used is descriptive qualitative analysis. The result of the research is that Nigeria is experiencing problems with the integration and application of ICT tools due to many factors such as; Limited ICT facilities in Nigerian Schools, Lack of experienced technical tutors, Electrical Problems, Environmental Factors, and Expensive IC tools, and Lack of professional skills. The research concludes that the government should make efforts to offer solutions to the problems mentioned in this paper to improve Nigerian education at all levels. The researcher asks education stakeholders to encourage tutors, facilitators, and students to develop positive attitudes towards using ICT tools at all levels of education in Nigeria.",https://www.semanticscholar.org/paper/bffb7d7372cdc7372756b2312cb0bd0aa628d53f,IS,Qualitative
Exploring User Needs for a Mobile Behavioral-Sensing Technology for Depression Management: Qualitative Study,"Background Today, college students are dealing with depression at some of the highest rates in decades. As the primary mental health service provider, university counseling centers are limited in their capacity and efficiency to provide mental health care due to time constraints and reliance on students’ self-reports. A mobile behavioral-sensing platform may serve as a solution to enhance the efficiency and accessibility of university counseling services. Objective The main objectives of this study are to (1) understand the usefulness of a mobile sensing platform (ie, iSee) in improving counseling services and assisting students’ self-management of their depression conditions, and (2) explore what types of behavioral targets (ie, meaningful information extracted from raw sensor data) and feedback to deliver from both clinician and students’ perspectives. Methods We conducted semistructured interviews with 9 clinicians and 12 students with depression recruited from a counseling center at a large Midwestern university. The interviews were 40-50 minutes long and were audio recorded and transcribed. The interview data were analyzed using thematic analysis with an inductive approach. Clinician and student interviews were analyzed separately for comparison. The process of extracting themes involved iterative coding, memo writing, theme revisits, and refinement. Results From the clinician perspective, the mobile sensing platform helps to improve counseling service by providing objective evidence for clinicians and filling gaps in clinician-patient communication. Clinicians suggested providing students with their sensed behavioral targets organized around personalized goals. Clinicians also recommended delivering therapeutic feedback to students based on their sensed behavioral targets, including positive reinforcement, reflection reminders, and challenging negative thoughts. From the student perspective, the mobile sensing platform helps to ease continued self-tracking practices. Students expressed their need for integrated behavioral targets to understand correlations between behaviors and depression. They also pointed out that they would prefer to avoid seeing negative feedback. Conclusions Although clinician and student participants shared views on the advantages of iSee in supporting university counseling, they had divergent opinions on the types of behavioral targets and feedback to be provided via iSee. This exploratory work gained initial insights into the design of a mobile sensing platform for depression management and informed a more conclusive research project for the future.",https://www.semanticscholar.org/paper/75b069e90c072bc1d00c3d327de49ef8d559d50e,IS,Qualitative
Artificial intelligence (AI) technology in OpenAI ChatGPT application: A review of ChatGPT in writing English essay,"ChatGPT is a product of AI that is currently being widely discussed on Twitter. This research reviews how ChatGPT writes English essays. This research is descriptive qualitative. The analysis shows that we can access ChatGPT on openai.com or chat.openai.com on the browser. If we do not have an account, we register via email, Google, or Microsoft account. After login, enter a question or statement in the conversation column provided. Send it and ChatGPT will respond and the answer appear quickly. The researcher tries ChatGPT “Can you help me in doing my English assignment?"", and the ChatBot replies ""Of course! I'd be happy to help you with your English assignment. What do you need help with? Do you have a specific question or task that you're working on, or is there a broader topic that you'd like help with? It would be helpful to have some more information so that I can better understand how I can assist you"". Based on several tries, ChatGPT can answer all questions on various topics such as English essays including a descriptive text about Solo and My Family, recount text about personal experience and unforgettable moments, resolution in 2023, and future career. ChatGPT considers the event orders and writing order, including using main, explanatory sentences, and a conclusion. It uses two voices both active and passive voice. Besides, it considers tenses use related to the given topic essay. However, from examples of English essays produced by ChatGPT, it certainly requires further research to find out that the essay results are grammatically accurate.",https://www.semanticscholar.org/paper/c03452648d60b4acf3e98a582f660783a3be0ccc,CS,Qualitative
Investigator Experiences Using Mobile Technologies in Clinical Research: Qualitative Descriptive Study,"Background The successful adoption of mobile technology for use in clinical trials relies on positive reception from key stakeholders, including clinical investigators; however, little information is known about the perspectives of investigators using mobile technologies in clinical trials. Objective The aim of this study was to seek investigators’ insights on the advantages and challenges of mobile clinical trials (MCTs); site-level budgetary, training, and other support needs necessary to adequately prepare for and implement MCTs; and the advantages and disadvantages for trial participants using mobile technologies in clinical trials. Methods Using a qualitative descriptive study design, we conducted in-depth interviews with investigators involved in the conduct of MCTs. Data were analyzed using applied thematic analysis. Results We interviewed 12 investigators who represented a wide variety of clinical specialties and reported using a wide range of mobile technologies. Investigators most commonly cited 3 advantages of MCTs over traditional clinical trials: more streamlined study operations, remote data capture, and improvement in the quality of studies and data collected. Investigators also reported that MCTs can be designed around the convenience of trial participants, and individuals may be more willing to participate in MCTs because they can take part from their homes. In addition, investigators recognized that MCTs can also involve additional burden for participants and described that operational challenges, technology adoption barriers, uncertainties about data quality, and time burden made MCTs more challenging than traditional clinical trials. Investigators stressed that additional training and dedicated staff effort may be needed to select a particular technology for use in a trial, helping trial participants learn and use the technology, and for staff troubleshooting the technology. Investigators also expressed that sharing data collected in real time with investigators and trial participants is an important aspect of MCTs that warrants consideration and potentially additional training and education. Conclusions Investigator perspectives can inform the use of mobile technologies in future clinical trials by proactively identifying and addressing potential challenges.",https://www.semanticscholar.org/paper/bb68614958d4e15e76bc3e6254657317f39418de,IS,Qualitative
A Research Framework of Mitigating Construction Accidents in High-Rise Building Projects via Integrating Building Information Modeling with Emerging Digital Technologies,"The construction of high-rise building projects is a dangerous vocation due to the uniqueness and nature of the activities, as well as the complexity of the working environment, yet safety issues remain crucial in the construction industry. Digital technologies, such as building information modeling (BIM), have been identified as valuable tools for increasing construction productivity, efficiency, and safety. This research aimed to mitigate the accident safety factors in high-rise building projects via integrating BIM with emerging digital technologies in the construction industry, such as photogrammetry, GPS, RFID, augmented reality, (AR), virtual reality (VR), and drone technology. Qualitative research was conceived in the ground theory approach. Forty-five online interviews with construction stakeholders and qualitative data analysis were carried out using the NVivo 11 software package. According to the findings, interviewees were more motivated to use photogrammetry and drone technologies in high-rise building projects in order to increase construction safety. Positive, negative, and neutral attitudes about BIM integration with emerging digital technologies were discovered. Furthermore, a research framework was developed by consolidating research findings that articulate the measures and future needs of BIM integration with other digital technologies to mitigate construction accidents in high-rise building projects. The framework also renders practical references for industry practitioners towards effective and safer construction.",https://www.semanticscholar.org/paper/1a576584c322f288b0d124378206c6f1c39dd1b2,CS,Qualitative
Multi-stakeholder perspectives on information communication technology training for older adults: implications for teaching and learning,"Abstract Objectives: The objective of this research was to identify and conceptualize barriers and strategies for effective implementation of information communication technology (ICT) training for older adults. Methods: A grounded theory approach was used to collect and analyze qualitative data from 61 participants in three stakeholder groups: older-adult ICT trainees, care providers and ICT trainers. Results: Care providers expressed older adults’ reluctance, lack of affinity, fears and socio-contextual attributes as barriers to overcome with ICT training. ICT trainers highlighted motivation, trainer–trainee relationship, patience, self-reliance and mutual value as strategic themes. ICT trainees elucidated occupational accomplishment, challenges and a sense of competence as themes from their experience with training. Discussion: While digital literacy and skill building have traditionally been the key focus of ageing-centred ICT training, a deeper approach to address internal (personal) and external (socio-contextual) barriers, as conceptualized in the study finding, is vital in yielding successful outcomes. Implications for rehabilitation Information communication technology (ICT) is a vital resource for older adults to age-in-place and for health professionals in delivery of tele-rehabilitation. Family members and care providers realize the scope of ICT for ageing-in-place but raise doubts on the inherent motivation and abilities of older adults to adopt ICT. On the other hand, older adults who engage in one-on-one ICT training value their new-found sense of accomplishment and competence in using the Internet and social media. Graduate students who provided the training greatly appreciate their own learning experience, and stress the need for mutual trust, patience and simplicity in teaching ICT. A major precursor to imparting digital literacy and skills in older adults who lack ICT exposure is to help them overcome deep-seated attitudinal and socio-contextual barriers through a one-on-one approach.",https://www.semanticscholar.org/paper/6e51429afcc535948bacce85f661ae532a01aa97,CS,Qualitative
Academic Supervision to Improve Teachers' Readiness in Utilizing Information and Communication Technology in Vocational High Schools,"The study aims to explain the implementation of academic supervision to improve teachers' readiness in utilizing information and communication technology. The implementation includes several aspects, such as planning, implementation, and evaluation of academic supervision. The research is a qualitative type with a case study approach. The subjects are the headmaster and teachers in SMK Muhammadiyah Karangmojo. The data were gathered through observation, interview, and documentation. They were then analyzed by reducing, displaying, and drawing a conclusion. The research has three findings. First, the supervision program is arranged by the headmasters, consisting of the supervision instruments emphasizing the use of digital learning media by the teachers. Second, the teachers have utilized various and attractive digital communication media, allowing the students to be more active and creative during the learning process. In implementing the program, the main obstacle encountered by the headmasters is related to the teachers, in that the senior teachers are reluctant to learn to use information and communication technology. Besides, a low internet connection becomes another impediment that occurs if the school members use it at the same time. Third, the evaluation and follow-up were carried out by facilitating the teacher forum (MGMP) at school and sending the teachers to follow the training and education program to increase the literacy in information and communication technology. The findings prove that the supports of the headmaster as the top leader through academic supervision help teachers improve their readiness in utilizing the technology in the classroom.",https://www.semanticscholar.org/paper/1bd22327cb7f34464f0a44f128c7b894bd8ac257,CS,Qualitative
Application of computer vision for construction progress monitoring: a qualitative investigation,"Purpose Construction progress monitoring (CPM) is considered a difficult and tedious task in construction projects, which focuses on identifying discrepancies between the as-built product and the as-planned design. Computer vision (CV) technology is applied to automate the CPM process. However, the synergy between the CV and CPM in literature and industry practice is lacking. This study aims to fulfil this research gap. Design/methodology/approach A Delphi qualitative approach was used in this study by conducting two interview rounds. The collected data was analysed using manual content analysis. Findings This study identified seven stages of CPM; data acquisition, information retrieval, verification, progress estimation and comparison, visualisation of the results and schedule updating. Factors such as higher accuracy in data, less labourious process, efficiency and near real-time access are some of the significant enablers in instigating CV for CPM. Major challenges identified were occlusions and lighting issues in the site images and lack of support from the management. The challenges can be easily overcome by implementing suitable strategies such as familiarisation of the workforce with CV technology and application of CV research for the construction industry to grow with the technology in line with other industries. Originality/value This study addresses the gap pertaining to the synergy between the CV in CPM literature and the industry practice. This research contributes by enabling the construction personnel to identify the shortcomings and the opportunities to apply automated technologies concerning each stage in the progress monitoring process.",https://www.semanticscholar.org/paper/457d523900577dce397fd51431dde5eb41164eca,CS,Qualitative
TEACHERS’ OBSTACLES IN UTILIZING INFORMATION AND COMMUNICATION TECHNOLOGY,"This study aims to study teacher barriers to utilizing information and communication media (ICT) for learning media. This research was carried out at Public Elementary School 24 Pulau Rimau located in Karang Manunggal village, Pulau Rimau sub-district, Banyuasin district. The subjects of this study were teachers at Public Elementary School 24 Pulau Rimau who tested 16 people. In conducting this research, data from certain sources are needed that are as expected in this study. The technique of collecting data is by observation and interview. Data were analyzed using percentages to see the percentage of respondents' answers. Furthermore, all data is processed by analyzing qualitative data, namely data reduction, data display, and data verification. The formulation of the problem in this study is what forms are questioned by the teacher in utilizing technology-based media in Public Elementary School 24 Pulau Rimau ? From the results of research on teacher barriers to using Information Technology (ICT) in Public Elementary School 24 Pulau Rimau, two aspects related to the ability of teachers and Information and Communication Technology (ICT) were inadequate. The obstacle of the teacher's ability to master ICT is that the teacher is not proficient in using computers, judging from the percentage of teacher answers, 75% say they are not proficient in using technology-based media, whereas using computers is one of the activities that support computer-based media. In addition, teachers do not have an advanced educational background in the use of ICT media. Another obstacle in terms of facilities, in Public Elementary School 24 Pulau Rimau there is no electricity network (PLN). In addition to not having a PLN, there is also the unavailability of computer facilities and laptops at the Public Elementary School 24 Pulau Rimau.",https://www.semanticscholar.org/paper/92e61ee9be0f8640246b75ab25b9f555e6e203be,CS,Qualitative
INTEGRATION OF INFORMATION AND COMUNICATION TECHNOLOGY INTO ISLAMIC RELIGIOUS EDUCATION TEACHER TRAINING,"Integrating ICT in teacher training has become an absolute necessity over recent years. This research highlights the role of training as a means to improve Islamic religious education teacher competence in ICT. Specifically, the case-study research aim is to describe the level of ICT integration into the Islamic religious education training, referring to ‘education-and-training’, at a religious training center (RTC) in Palembang, Indonesia. Thirty teachers of Indonesian Islamic school (madrasah) and two facilitators at the RTC acted as informants in the research. Using the interactive model of qualitative analysis by Miles and Huberman, the research results indicated that the participants have already integrated ICT but were restricted to using presentations, especially the PowerPoint computer program. Secondly, age and technical problems became the major constraints to realize such integration. With respect to the availability of facilities, the RTC is in good category.",https://www.semanticscholar.org/paper/2d9cd57a9a8dc323eb68359784b4bd6beeba307d,IS,Qualitative
Information and Communication Technology in Foreign Language Classes in English: Roles and Practices,"Teaching and learning process in 21st century has been embedded with technology. Use of ICTs plays crucial role in ameliorating knowledge acquisition in general and enhancing English Language mastering in particular. The government of Nepal has made provision of integrating ICTs in school education with the aim of enhancing quality education and make both teachers and learners confidence and competitive in the present global world. In this context, this research intended to investigate roles and practices of ICTs at English as Foreign language classes (EFL) of Nepal. To achieve the objective, explanatory sequential mixed research design was adopted. Survey questionnaire, unstructured interview and non-participant observation were used to as the tools of data collection and forty secondary English teachers (20 from public schools & 20 from private schools) were the sample population. Two datasets; quantitative and qualitative were collected and analyzed sequentially. The findings of the research reveal that majority of the teachers were positive towards roles of ICTs in language teaching even they were doubt if ICTs could develop creative and critical abilities of the students. Though the teachers practiced ICTs in their classes, they felt discomfort in using new ICT tools and applications. The findings imply that teachers should be given training on ICT use for building up their skills; knowledge and confidence to achieve optimum advantages form it.",https://www.semanticscholar.org/paper/33926f0a648d9dbf0d717a12c11a30adc9b3d701,CS,Qualitative
Medicine Information Record Based on Blockchain Technology,"Drug's production recording is one of the most critical processes pharmacy industry. The benefit of recording the drugs is to avoid counterfeit drug. By knowing the exact number of drugs produced will make it easier to track. Counterfeit drugs are still circulating today in the market. Counterfeit drugs are very dangerous even cause death for people who consume it. This qualitative research was conducted using Blockchain technology, which has characteristics such as immutable, unchangeable, and peer-to-peer, which will minimize the possibility of counterfeit drugs. Simulations are carried out using the Multichain application to get a clear picture of how drug production can be recorded in the blockchain (Multichain). This research involved one of Indonesia's largest drug industry companies to give input on Blockchain technology (Multichain) for drug production records. The results, blockchain technology (Multichain), can be used to record drug production. So, it is possible to track the drugs record. This research is very important and useful for the drug industry to ensure the quality of drugs production.",https://www.semanticscholar.org/paper/49ea71869d005ea10449cffefa508c8d3c65b6f9,CS,Qualitative
The Influence of Teachers' Digital Literacy and the Use of Technology Media on Students' Ability to Identify Hoaxes in the Digital Era,"Purpose of the study: This study aims to examine the influence of teacher digital literacy and the use of media technology on students' ability to identify hoaxes in the digital era. Methodology: A mixed-method approach was employed, combining quantitative analysis using Multiple Linear Regression with Statistical Package for the Social Sciences (SPSS) and qualitative analysis with the Miles and Huberman model through interviews with teachers. The research sample consisted of 50 teachers and 200 students from secondary schools in both urban and rural areas, selected through purposive sampling. The research instrument used a questionnaire and interview guide. Main Findings: The results indicated that both teacher digital literacy and the use of media technology positively influenced students' ability to identify hoaxes. Teachers with higher digital literacy levels were able to effectively use media technology to guide students in critical thinking and information verification. The use of media technology helped students compare information from different sources and better assess its validity. Novelty/Originality of this study: This study contributes to the field by exploring the combined impact of teacher digital literacy and media technology usage on students' ability to recognize and evaluate misinformation, addressing a critical issue in the context of education in the digital age.",https://www.semanticscholar.org/paper/df3ba04b349f1198213c75b1dd381f4f85d79f7a,IS,Qualitative
Digital Technology Innovation in Improving Financial Access for Low-Income Communities,"Access to finance is one of the key factors in building an inclusive and sustainable economy. Low-income communities often experience difficulties in accessing adequate financial services, such as savings, credit and insurance. This research aims to investigate how digital technology innovation has contributed to improving financial access for low-income people. This research is a literature analysis that utilises a qualitative approach. This approach involves analysing and interpreting data based on information and text taken from various sources. The study results show that digital technology innovation has opened up great opportunities in improving financial access for low-income people. Digital banking services, e-payments and alternative funding models have helped overcome traditional barriers, such as geographical distance and administrative costs, which often hinder access to financial services.",https://www.semanticscholar.org/paper/dd8e9c40ff3e8390a4f465c3b4076ddbfac7221c,IS,Qualitative
Students' Perceptions of the AI Technology Application in English Writing Classes,"With the rapid advancement of information technology, numerous cutting-edge techniques and technologies have been developed to improve learning generally and English learning specifically. The utilization of artificial intelligence (AI) technology in teaching and learning at universities is an inevitable trend in the Industry 4.0 era. This paper aims to investigate students' perceptions of AI technology application in English writing classes. 100 students from four General English classes at Vietnam National University (VNU) took part in this study. To achieve the research purpose, I combined quantitative and qualitative research methods through questionnaires and in-depth interviews. The findings of the study indicate that students' attitudes regarding AI writing tools were favourable in terms of their accessibility, adaptability, and simplicity. However, some challenges are unavoidable when employing these tools, resulting from factors like learners' technology anxiety and lack of tool variety. I hope this research serves as a valuable resource for teachers looking to diversify their teaching methods and encourages students to enhance their interest and motivation in using AI tools for English learning.",https://www.semanticscholar.org/paper/89993c94830b0489240216a38ee220321cef409e,CS,Qualitative
Digital Technology Transformation in Enhancing Public Participation in Democratic Processes,"In the era of globalisation and the development of digital technology, the role of technology in various aspects of life is increasing. One of the areas significantly affected is the democratic process in society. This research aims to analyse the transformation of digital technology in increasing people's participation in the democratic process. The current research type is qualitative. Data collection techniques include listening and recording important information to conduct data analysis through data reduction, data display, and conclusion drawing. The results showed that the transformation of digital technology has brought a significant impact on public participation in the democratic process. The availability of online platforms, easy access to information, and the ability to interact with political leaders have changed the way individuals engage in political decision-making. Despite the positive potential, there are challenges and risks that need to be addressed in order for public participation in the digital era to truly contribute to the strengthening of democratic principles.",https://www.semanticscholar.org/paper/3fcd55b214a5e3a8a6385a4542fc830d9f1160f8,IS,Qualitative
A qualitative study of students' perspective on e-learning adoption in India,"PurposeThis paper will try to uncover how e-learning is giving a new shape to the education industry. Also, it will encompass the students' perspective and experience of e-learning.Design/methodology/approachThe present study employed interpretative phenomenological analysis (IPA) to intensely scrutinize the lived-in experiences of the participants. In the present study, the respondents were selected from Delhi NCR of India. Semi-structured interviews were conducted to collect the primary data to understand the student's perspectives on the impact of information and communications technology (ICT) in education industry.FindingsThe findings have been grouped under two sections referred to as “themes,” which include “drivers for e-learning adoption” and “inhibitors which restrict the adoption of e-learning.”Practical implicationsAt present, India does not have a big market for e-learning, but there is huge potential in the country. The present study may be helpful for the educational institutions in India and in similar developing countries in understanding the students' perspectives on e-learning adoption. The educational institutions may improve their systems accordingly so that they can not only retain the students of their own countries but also attract students from other countries for further education.Social implicationsE-learning can be employed to give users quick access to ideas and experiences from a wide range of people, communities, and the cultures to increase the tangibility.Originality/valueThe study will be useful to the policymakers in the higher education sector of developing nations like India, in understanding the students' mindsets. This study makes a contribution to the growing literature on e-learning, where the researchers have determined the relative importance of various motivating and inhibiting factors which influence the adoption of e-learning. Additionally, the study has used IPA as the methodology to determine the factors, which is a novel contribution.",https://www.semanticscholar.org/paper/d033473fccd8c185bbca75559006e67b0a635cad,IS,Qualitative
The influence of personal factors on resistance to technology adoption in university libraries in Bangladesh,"This research explores the impact of personal factors on resistance to technology adoption in university libraries in Bangladesh, particularly among LIS (Library and Information Science) professionals. The study aims to reveal specific challenges and opportunities influencing attitudes toward technology integration, offering a nuanced understanding of resistance factors in this unique cultural and educational context. Through a qualitative case study with 21 LIS professionals from seven university libraries, conducted via semi-structured interviews guided by a questionnaire, recurring themes such as fear of job displacement, perceived technological self-efficacy, and concerns about disruptions to workflows emerge. Fear of job displacement refers to professionals’ apprehension about potential job loss due to technological advancements, while perceived technological self-efficacy reflects individuals’ confidence in using and adapting to new technologies. Concerns about disruptions to workflows highlight worries regarding the impact of technology implementation on existing work processes. Despite challenges, participants acknowledge potential benefits, emphasizing improved services. The study advocates for crucial strategies like comprehensive training programs and inclusive decision-making processes to alleviate resistance. This research provides a focused exploration, original insights, and practical value for advancing technology adoption in university libraries in Bangladesh, contributing to a deeper comprehension of complexities in technology integration within academic institutions.",https://www.semanticscholar.org/paper/3f5e9febcb655add2f6be756200a90a38ca6639a,IS,Qualitative
Skype in Qualitative Interviews: Participant and Researcher Perspectives,"As Internet usage has increased, web-based technologies such as Skype and Face Time have become more common alternatives for qualitative interviewing, especially for research participants who are geographically distant from the researchers. Challenges to the use of these tools have been identified, but as technology is currently changing at a rapid pace, more recent research is needed to provide up-to-date information on the feasibility of web and video conferencing technologies for qualitative interviewing. This paper reflects on the experience of using Skype for qualitative research interviews (n=14) in a study of pregnancy and parenting in doctoral programs, including feedback from research participants who chose to complete the qualitative interview via Skype instead of telephone or face-to-face interviews. Twelve participants who completed Skype interviews provided feedback on their experiences using Skype for qualitative interviews. Feedback from participants highlight an overall positive perception of Skype interviews due to the availability of visual cues from researchers and flexibility, but participants also shared challenges in terms of technology issues and participants’ lack of expertise with the technology. Recommendations include the use of videoconferencing and digital technologies as an additional or alternative interview tool for qualitative interviews, especially for participants who have logistical challenges meeting researchers face-to-face.",https://www.semanticscholar.org/paper/205960e9675f8e2a7157bf03ec8ab5764250097f,CS,Qualitative
Effects of using educational technology tools to enhance EFL students’ speaking performance,"Educational speaking technology is a digital expertise used to enhance speaking performance. This research examined the effects of using educational speaking technology tools: FORVO, YouGlish, and OALD 8th ed. to enhance students’ speaking performance. A quasi-experimental pretest-posttest two groups design was used. Test, questionnaire, interview, and teacher-log were used to gather the data from 82 first-year Information communication and Technology (IT) students selected through comprehensive sampling. The experimental group students had learned speaking skills through educational speaking technology tools while the control group students learned using the conventional method. When the quantitative data were analyzed through independent samples T-test, the qualitative data were analyzed through thematic analysis. The findings of the study uncovered that there was a statistical difference between the experimental and control group students in their speaking performance. Accordingly, the learners who had learned through educational speaking technology have enhanced their speaking performance compared to the students that learned conventionally. Predominantly, students who learned through educational speaking technology were fluent, coherent, and accurate in their speech, rich in lexical resources, used a variety of grammatical ranges, and better in pronunciation. Besides, the students’ have positive perceptions towards using educational speaking technology tools. Consequently, this study recommends researchers, teachers, and students to make the use of educational technology and to go in line with the state of the art.",https://www.semanticscholar.org/paper/1176eaefc7eb3f23b18e847a398c44267ad404ce,CS,Qualitative
The behavior of Extension Agents in Utilizing Information and Technology to Improve the Performance of Extension Agents in Lampung Province,"The ability of individuals to utilize information and technology (IT) is needed, it also happens in the agriculture field. Extension agents are required to have the ability to use IT to support their performance. The ability to utilize IT can encourage the presence of professional, broad-minded and high-capacity extension agents. The high capacity of agents will facilitate them to carry out its performance. In fact, the problem in the field is the low performance of agricultural extension agents. It can be due to various issues such as low capacity, lack of infrastructure availability, low ability to access information and others, these issues were also found in extension agents in Lampung Province. The aim of this study is to measure the behavior of agricultural, fisheries and forestry extension agents in utilizing IT. This research is a descriptive study using a survey method with quantitative and qualitative approaches. The study was conducted in Lampung Province with 355 respondents. The results suggested that the knowledge of the extension agents in utilizing IT classified as the high category, however, the behavior of the extension agents in utilizing IT still needs to be improved to be good behavior in utilizing IT. Likewise, the extension agent’s skill in utilizing IT is still in the medium category, therefore the support of all parties is needed to increase extension agent’s behavior in utilizing IT becomes better and can support the performance of extension agents.",https://www.semanticscholar.org/paper/4e9c85421dd133841fa9dd2ae1f29d844d541e97,IT,Qualitative
Continuing Professional Development opportunities in Information and Communication Technology for academic librarians at the Durban University of Technology,"Continuing Professional Development (CPD) is a vital tool for maintaining the skills and expertise of staff, especially with regard to the use of Information and Communication Technology (ICT). There is little information available about the involvement of librarians in South Africa with CPD: this study focused on academic librarians at the Durban University of Technology (DUT) Library, seeking their attitudes towards CPD and its provision. It employed a qualitative approach in its research design. Questionnaires were utilised to collect information from twenty-five academic librarians. Follow-up interviews were conducted with five respondents. The overall study indicated that, although the respondents were aware of the importance of CPD and the impact of ICT on library resources and services, not all of them kept abreast of CPD activities within their field. The respondents believed that the institutions and the professional body, the Library and Information Association of South Africa (LIASA), should work together to encourage and promote CPD activities. By encouraging CPD activities within the Library and Information Science (LIS) sector, the quality of librarianship and service delivery within the LIS would improve. Half of the respondents agreed about the importance of CPD becoming compulsory within the LIS profession and 55% of respondents considered that LIASA had a potentially important role to play in promoting CPD. However, in separate interviews, several respondents expressed doubt about the capacity of LIASA to fulfil this role.",https://www.semanticscholar.org/paper/17bdd59c04a957a9e3d297c774980d939b3a3dde,IS,Qualitative
Peranan Guru Dalam Penggunaan Media Pembelajaran Berbasis Information And Communication Technology (ICT) Di SDN RRI Cisalak,"The most important thing in a learning process is the teacher and learning media. Learning media appropriately selected and used by a teacher will certainly have an effect on the learning process and result. The information and Communication Technology (ICT)-based learning media is very important in the development of education. ICT is a program used as a tool for manipulating and conveying information. With ICT, the learning process in the classroom becomes easier because the knowledge and information can be manipulated, manageed and transferred easily. The ICT-based learning media can develop students' thinking skills and professionally improve the ability of teachers. Teachers become more motivated and creative during the class. The purpose of this study is to know the role of teachers in using ITC-based learning media at RRI Cisalak Elementary School. The method used in the research is a qualitative descriptive method. The research conducts in-depth interview, observation and literature review. The samples are taken using a purposive sampling technique. The data are analyzed in some steps, namely data reduction, data presentation, and conclusion. The results of the research show that teachers at SD RRI Cisalak have been able to carry out the learning process by using ICT-based learning media. Teachers play a very big role in the continuity of ICT-based learning. The role of the teacher is also supported by the school principal and school committee by increasing the number of facilities, infrastructures and ICT-based learning media in schools",https://www.semanticscholar.org/paper/66e8defb8ca8ade902dbe1c93485a72027bae0f6,IT,Qualitative
Technology Education in the Quran,"The type of research that the author uses is a type of qualitative research using a content analysis approach (Content Analysis), or what can be called a content study. This analysis is a research technique for making a conclusion or inference that can be replicated and the correctness of the data by considering the context. The object of this research is explored through various information in the form of books, interpretations, and journals. This journal discusses how children use technology in their early years, both at home and school. because in this increasingly sophisticated era, children cannot be separated from technology. The existence of technology is undeniable, and children cannot be prevented from using it. However, rights to property and assistance may be the best option for children. Teachers and parents alike are expected to learn a lot from the findings of this research regarding the process of using technology and its applications.",https://www.semanticscholar.org/paper/8650238755a88b82939f6509a83791c3642df05b,IS,Qualitative
Research on the improvement of teachers' teaching ability based on machine learning and digital twin technology,"The qualitative analysis results of teachers’ abilities are difficult to quantify, and ability problems in the teaching process are difficult to be effectively measured. In order to study methods to improve teachers’ teaching abilities, this paper builds a corresponding teacher competence evaluation model based on machine learning and digital twin technology, establishes a data collection model for teachers’ professional competence, and establishes a data fusion model. It includes data cleaning model based on XML information template, data integration model, multi-index screening mechanism and clustering strategy based on perturbation attributes. On this basis, this paper uses decision tree algorithm, random forest algorithm and neural network algorithm to construct three scheduling rule mining models aiming at teachers’ professional ability. In addition, this paper establishes a digital twin-driven multi-knowledge model scheduling optimization architecture that uses the three scheduling rules mined. The research results show that the model constructed in this paper has good performance.",https://www.semanticscholar.org/paper/c2fff40cc4786aa364836cc4ae68f6c95db80f42,CS,Qualitative
Application of artificial intelligence in libraries and information centers services: prospects and challenges,"Purpose Artificial intelligence (AI) is one of the emerging technologies of this time. AI is a widely used technology in library services that can transform the best services in the age of information technology. This paper aims to highlight the use of AI in library operations. Several research has been undertaken on this subject, but that only address a few applications. AI and libraries have a substantial nexus; nevertheless, the use and awareness of AI in library services are still creating question marks addressed in this paper. This study will help the policy stakeholder, librarians and scholars in the field to address these issues before the deployment of AI in library services. Design/methodology/approach This study is based on a qualitative method using content analysis techniques. An extensive review of literature on “artificial intelligence”, “smart libraries” was carried to ascertain the emerging technologies in the smart library domain. Literature was searched against various keywords like artificial intelligence, smart technologies, Internet of Things, electronic resource management, data mining and ambient intelligence. This study highlights the pros and cons of AI in library services and its possible solutions. Findings The findings of this study show that AI is a vibrant technology that can be used in library services; however, some obstacles like adequate funds, the attitude of librarians and technical skills are a few obstacles that hamper AI in library operations. The findings also reveal that using AI in library operations will accelerate libraries in the right direction. Furthermore, this study highlights various applications that can be deployed without spending costs. Practical implications This paper may be of interest to academic, librarians, policymakers, researchers and the government to have a perspective on initiatives in the country on application of technology in library services. This study can introduce the current status and potential of this technology to bring the technology revolution in library and information center services. Social implications This study will motivate library professionals to take advantage of AI in library services and further accelerate library operations in the right direction. Originality/value This study covers the understanding of AI in library services that will help the librarian’s and information professionals leverage AI in library scenarios. Furthermore, the practical implication of AI in library services will bring positive change in implementing AI.",https://www.semanticscholar.org/paper/cbbc90e37ddf7d4276df9044ebfa8a5f3f3643d0,CS,Qualitative
Identifying the Roles of Healthcare Leaders in HIT Implementation: A Scoping Review of the Quantitative and Qualitative Evidence,"Despite major investment, health information technology (HIT) implementation often tends to fail. One of the reasons for HIT implementation failure is poor leadership in healthcare organisations, and thus, more research is needed on leaders’ roles in HIT implementation. The aim of the review was to identify the role of healthcare leaders in HIT implementation. A scoping review with content analysis was conducted using a five-step framework defined by Arksey and O’Malley. Database searches were performed using CINAHL, Business Source Complete, ProQuest, Scopus and Web of Science. The included studies were written either in English or Finnish, published between 2000 and 2019, focused on HIT implementation and contained leadership insight given by various informants. In total, 16 studies were included. The roles of healthcare leaders were identified as supporter, change manager, advocate, project manager, manager, facilitator and champion. Identifying healthcare leaders’ roles in HIT implementation may allow us to take a step closer to successful HIT implementation. Yet, it seems that healthcare leaders cannot fully realise these identified roles and their understanding of HIT needs enforcement. Also, healthcare leaders seem to need more support when actively participating in HIT implementation.",https://www.semanticscholar.org/paper/8c264100a15b5ec639846632804a12f85ff6779a,CS,Qualitative
The Role of ChatGPT in Enhancing the Information Search and Decision-Making Process of Travellers,"In today's digital era, the internet and technology are growing rapidly. One of the significant technological advances is ChatGPT which is able to accelerate the process of information search and decision making in the world of tourism. This research aims to discuss the role of ChatGPT in improving the process of information search and decision making of tourists. This research is qualitative in nature. The techniques used to obtain information involve careful observation and meticulous note-taking, followed by analytical procedures such as data reduction, visualisation, and inference. This study arrives at the conclusion that in today's digital era, artificial intelligence technologies such as ChatGPT have an important role to play in improving travellers' information search and decision-making process. ChatGPT can provide recommendations, information, and customer service quickly and effectively, thus helping travellers to plan their trips more easily and efficiently.",https://www.semanticscholar.org/paper/2f4b4fbb20fd7645b19ac7f3ce64ce34f73355db,CS,Qualitative
A qualitative evidence synthesis of users’ experience of mobile health applications in the self-management of type 2 diabetes,"Aim The aim of this qualitative evidence synthesis was to identify and synthesise qualitative research relating to experiences of using mobile health (mHealth) applications to aid self-management of Type 2 Diabetes. Methods Using a systematic search strategy, 11 databases were searched (Medline, CINAHL, PsychInfo, PubMed, Web of Science, Embase, Cochrane Library, Scopus, ProQuest A&1, ProQuest UK & Ireland, Mednar). “Best fit” framework synthesis was used guided by the Health Information Technology Acceptance Model (HITAM). Assessment of methodological limitations was conducted using Critical Appraisal Skills Programme (CASP) and confidence in the review findings were guided by GRADE-CERQual. Results Fourteen eligible studies were included in the synthesis (7 qualitative and 5 mixed methods). Key themes identified under the health, information and technology zones of the HITAM revealed the benefits of mHealth apps, barriers to their use, their perceived usefulness and ease of use. Discussion Most people used the apps for feedback on their self-management and found them helpful in their communication with health care providers. Some embraced the technology and found it easy to use while others found mHealth apps to be counterintuitive.",https://www.semanticscholar.org/paper/7c4c71f09ad41d5ec5e47247ed138aebde4e79f5,IS,Qualitative
The Role of Blockchain Technology in Increasing Economic Transparency and Public Trust,"Blockchain technology has become one of the most promising technological innovations in recent years. The main advantages of blockchain technology are high transparency, security, and data integrity. Therefore, this technology has attracted attention in various sectors, including the economy. This research aims to examine the role of blockchain technology in enhancing economic transparency and public trust. This study is a literature review that adopts a qualitative method, which means it will analyse and interpret data by utilising information and texts derived from various sources. The results of the study show that blockchain technology has proven itself to be a highly effective tool for achieving this goal. In an increasingly connected and data-dependent society, economic transparency and public trust are key factors in sustainable economic growth and social stability.",https://www.semanticscholar.org/paper/6ff3535edc9ebda287cf2fa430d619a2973e2902,IS,Qualitative
Revisiting personal information management through information practices with activity tracking technology,"Personal information management (PIM) is an interdisciplinary research area with established theoretical foundations from information science and applied empirical research from human‐computer interaction (HCI). The increasingly popular activity tracking technology (ATT) has given rise to a new type of personal information about one's daily physical activities, which has not been scrutinized under theoretical frameworks in PIM. This article presents an in‐depth qualitative interview study supplemented by participant‐driven photo elicitation with 20 long‐term activity tracker users about how they manage personal information generated by ATT. Key findings include the identification of two distinctive user groups (i.e., consistent casual users and powers users) among long‐term activity tracker users and an in‐depth portrayal of these groups' PIM behaviors with ATT. These behaviors include concurrent and subsequent PIM practices, 6 types of PIM activities, and the use of a spectrum of PIM tools. This research provides timely theoretical updates to PIM under the background of self‐tracking technologies, outlines empirical implications to improve ATT in support of PIM, and offers recommendations for incorporating participant‐driven photo elicitation as a supplementary method for qualitative interviews in information behavior research.",https://www.semanticscholar.org/paper/4392f2a26337fa9d71c42492a4e6d2d1d4efabfb,IS,Qualitative
Teachers’ Skill and Motivation in Using Information and Communication Technology,"Teaching and learning in the 21st century has been embedded with technology. Use of Information and Communication Technology (ICT) plays a crucial role in ameliorating knowledge acquisition in general and enhancing English Language mastering in particular. The government of Nepal has made provision of integrating ICTs in school education to enhance quality education and make both teachers and learners confident and competitive in the present global world. The teachers are the real implementers of the policy into practice. In this context, this research intended to investigate secondary level English teachers’ skill and motivation in using ICTs while teaching the English language. To achieve the objective, an explanatory sequential mixed research design was adopted. The survey questionnaire, unstructured interview and non-participant observation were used to determine the skills and motivation of forty secondary English teachers (20 from public and 20 from private schools). Two datasets, i.e. quantitative and qualitative were collected and analyzed sequentially. The findings of the research reveal that majority of the teachers have high motivation in using ICTs even though they are not skilful, knowledgeable and confident in handling and using new ICT tools and applications. The findings imply that teachers should be given opportunities for training on ICT use for building up their skill, motivation, knowledge and confidence.",https://www.semanticscholar.org/paper/4803820da0156380107715b16e3bdd7b33560d82,CS,Qualitative
A survey of research trends in assistive technologies using information modelling techniques,"Abstract Background Despite the rapid proliferation and emphasis on technology, the use of assistive technology among individuals with varying disabilities and age is different. This situation instigates the need for a systematic review to gain a realistic understanding of prominent issues, research trends and assistive technology applications with minimal bias. Objective Identification of leading researchers and prominent publications in assistive technologies. Subsequently, semantic relation between qualitative and quantitative research literature on assistive technologies was explored to future research directions. Methods A manual search across reputed research databases was done to find out relevant literature from January 2005 to April 2020. In this paper, latent semantic analysis (LSA) was done to develop an information model for achieving defined objectives. Results A corpus of 367 research papers published during 2005–2020 was processed using LSA. Term frequency, inverse document frequency of high loading terms provided five major topic solutions. Marcia Scherer, Rory Cooper and Stefano Federici are most noticed authors in assistive technology research. “Smart Assistive Technologies” and “Wearable Technologies for Rehabilitation” came out as contemporary research trends within assistive technologies. Conclusions The manuscript concludes the fact that assistive technologies for rehabilitation are experiencing a transition from standalone mechanical devices towards smart, wearable and connected devices. Implications for Rehabilitation Customized assistive devices could be programmed for multiple uses. User data privacy and internet dependency of smart assistive technologies must be taken care of while designing smart assistive devices for rehabilitation. Fog devices could eliminate the latency issues associated with cloud-based rehabilitation services.",https://www.semanticscholar.org/paper/c527d80feb0f78863a0d98d822f33a4f85a585f2,CS,Qualitative
How Blockchain Innovation could affect the Audit Profession: A Qualitative Study,"Blockchain is transforming not only the way of recording, processing and storing financial transactions and information, but also the way audit firms can practice their profession. The purpose of this article is to examine how this technology will affect the audit profession. Based on a qualitative study carried out on a sample of 17 auditors, this research shows that this technology could affect audit firms at six key levels. Blockchain will allow an auditor to (1) save time and improve the efficiency of their audit, (2) favor an audit covering the whole population instead of an audit based on sampling techniques, (3) focus the audit on testing controls rather than testing transactions, (4) set up a continuous audit process, (5) play a more strategic audit role and (6) develop new advisory services. The results underline the need for the establishment of a clear and coherent legislative system and new audit standards, allowing auditors to embed this technology and enhance audit practices. JEL Codes: M42",https://www.semanticscholar.org/paper/2307910252b2e242a3ada872b27661259cc59eb3,CS,Qualitative
Optimising Organisational Performance Through Human Resource Management Strategy and Technology Integration to Enhance Innovation,"In the evolving era of digitalization and globalization, organizations face increasingly complex and dynamic challenges. Rapidly advancing information technology and intense global market competition necessitate organizations to evaluate and enhance their performance to remain relevant and competitive. This research aims to investi-gate best practices in HRM and technology integration to promote innovation, with a focus on its positive impact on organizational performance. The approach entails conducting an in-depth examination of literature through qualitative analysis, aiming to attain a comprehensive comprehension of the topic spanning the years 2001 to 2023. The study's findings indicate that human resource management (HRM) strategies focusing on employee recruitment, training, and development, along with effective performance management, form the basis for a competent team. A positive work environment with constructive feedback enhances productivity. Technology integration, through information and communication technology (ICT) and technology based HRM systems, improves operational efficiency. The use of technology for decision-making, administration, and employee data analysis accelerates business processes. Technology integration also stimulates innovation through collaboration and artificial intelligence, aiding organizations in identifying new opportunities in the digital era.",https://www.semanticscholar.org/paper/4a3178ddf096cfab6c2fb756ad43b5ad1e1b91ca,IS,Qualitative
Immersive Technologies-Driven Building Information Modeling (BIM) in the Context of Metaverse,"At present, considering the novelty of Immersive Technologies (ImTs) associated with Digital Twin (DT), Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) in the context of the metaverse and its rapid and ongoing development in Building Information Modeling (BIM), knowledge of specific possibilities and methods for integrating ImTs into building process workflows remains fragmented and scarce. Therefore, this paper aims to explore the research progress and trends of immersive technology-driven BIM applications, providing a helpful reference for understanding the current knowledge system and stimulating future research. To the best of the authors’ knowledge, this is the first attempt to use macro-quantitative bibliometric analysis and micro-qualitative analysis methods to explore the research topic of ImTs-driven BIM. This study obtains 758 related studies in the past decade, year 2013 to 2022, through a series of keywords from the Web of Science Core Collection database and uses VOSviewer software to conduct keywords co-occurrence analysis and overlay visualisation to visualise the relationship between ImTs and BIM, which contains six clusters, namely VR, Internet of Things (IoT), DT, 3D model, design, and AR. The macro-quantitative analysis on ImTs-driven BIM applications throughout all the stages of the building lifecycle reveals the themes, content, and characteristics of the applications across the stages, which tend to be integrated with emerging advanced technology and tools, such as Artificial Intelligence (AI), blockchain, and deep learning.",https://www.semanticscholar.org/paper/4149a00eb8a450a7d2e6c321c41c78428b53deba,CS,Qualitative
Patients’ Needs and Requirements for eHealth Pain Management Interventions: Qualitative Study,"Background A growing body of evidence supports the potential effectiveness of electronic health (eHealth) interventions in managing chronic pain. However, research on the needs and preferences of patients with chronic pain in relation to eHealth interventions is scarce. Eliciting user input in the development of eHealth interventions may be a crucial step toward developing meaningful interventions for patients for potentially improving treatment outcomes. Objective This study aimed to explore the experiences of patients with chronic pain with regard to information and communication technology, understand how an eHealth intervention can support the everyday needs and challenges of patients with chronic pain, and identify possible facilitators and barriers for patients’ use of an eHealth pain management intervention. Methods Twenty patients living with chronic pain and five spouses participated in individual interviews. Semistructured interview guides were used to explore participants’ needs, experiences, and challenges in daily life as well as their information and communication technology experiences and preferences for eHealth support interventions. Spouses were recruited and interviewed to gain additional insight into the patients’ needs. The study used qualitative thematic analysis. Results The participants were generally experienced technology users and reported using apps regularly. They were mainly in favor of using an eHealth self-management intervention for chronic pain and considered it a potentially acceptable way of gathering knowledge and support for pain management. The participants expressed the need for obtaining more information and knowledge, establishing a better balance in everyday life, and receiving support for improving communication and social participation. They provided suggestions for the eHealth intervention content and functionality to address these needs. Accessibility, personalization, and usability were emphasized as important elements for an eHealth support tool. The participants described an ideal eHealth intervention as one that could be used for support and distraction from pain, at any time or in any situation, regardless of varying pain intensity and concentration capacity. Conclusions This study provides insight into user preferences for eHealth interventions aiming to address self-management for chronic pain. Participants highlighted important factors to be considered when designing and developing eHealth interventions for self-management of chronic pain, illustrating the importance and benefit of including users in the development of eHealth interventions. Trial Registration ClinicalTrials.gov NCT03705104; https://clinicaltrials.gov/ct2/show/NCT03705104.",https://www.semanticscholar.org/paper/5fd0fe9a8418b4cda73c0509176f52edfc054135,CS,Qualitative
Barriers and Facilitators Affecting Patient Portal Implementation from an Organizational Perspective: Qualitative Study.,"Background The number of patient portals is rising, and although portals can have positive effects, their implementation has major impacts on the providing health care institutions. However, little is known about the organizational factors affecting successful implementation. Knowledge of the specific barriers to and facilitators of various stakeholders is likely to be useful for future implementations. Objective The objective of this study was to identify the barriers to and facilitators of patient portal implementation facing various stakeholders within hospital organizations in the Netherlands. Methods Purposive sampling was used to select hospitals of various types. A total of 2 university medical centers, 3 teaching hospitals, and 2 general hospitals were included. For each, 3 stakeholders were interviewed: (1) medical professionals, (2) managers, and (3) information technology employees. In total, 21 semistructured interviews were conducted using the Grol and Wensing model, which describes barriers to and facilitators of change in health care practice at 6 levels: (1) innovation; (2) individual professional; (3) patient; (4) social context; (5) organizational context; and (6) economic and political context. Two researchers independently selected and coded quotes by applying this model using a (deductive) directed content approach. Additional factors related to technical and portal characteristics were added using the model of McGinn et al, developed for implementation of electronic health records. Results In total, we identified 376 quotes, 26 barriers, and 28 facilitators. Thirteen barriers and 12 facilitators were common for all stakeholder groups. The facilitators’ perceived usefulness (especially less paperwork) was mentioned by all the stakeholders, followed by subjects’ positive attitude. The main barriers were lack of resources (namely, lack of staff and materials), financial difficulties (especially complying with high costs, lack of reimbursements), and guaranteeing privacy and security (eg, strict regulations). Both similarities and differences were found between stakeholder groups and hospital types. For example, managers and information technology employees mainly considered guaranteeing privacy and security as a predominant barrier. Financial difficulties were particularly mentioned by medical professionals and managers. Conclusions Patient portal implementation is a complex process and is not only a technical process but also affects the organization and its staff. Barriers and facilitators occurred at various levels and differed among hospital types (eg, lack of accessibility) and stakeholder groups (eg, sufficient resources) in terms of several factors. Our findings underscore the importance of involving multiple stakeholders in portal implementations. We identified a set of barriers and facilitators that are likely to be useful in making strategic and efficient implementation plans.",https://www.semanticscholar.org/paper/801731975b0c029fef6199083be52d64811a85c8,IS,Qualitative
Technology supports me: Perceptions of the benefits of digital technology in adolescents,"Background Technology plays a significant role in the lives of adolescents. Our knowledge is predominantly based on research exploring the risks associated with it, but adolescents also feel that technology supports their lives. This has received less consideration. Therefore, we aim to examine how adolescents perceive the benefits of digital technology. Methods We used qualitative data collected as part of the international Health Behaviour in School-Aged Children study. We conducted online, semi-structured interviews with 15 Slovak adolescents who came from three different types of secondary schools based on their graduation systems (mean age: 15.33; 20% boys). The data were analyzed using consensual qualitative research and thematic analysis. Results We identified five main themes based on the comments of adolescents: 1. I know (source of information, formal and non-formal education); 2. I can (smart device, helpful tool); 3. I am connected/included (social interactions); 4. I have my comfortable place (leisure time, creating my alternative world); and 5. I work on my future (self-development). Conclusion Adolescents perceived digital technology as mostly supportive and a helpful tool in their lives. The potential benefits of digital technology should be better reflected in public perception and policy, as the societal debate is mostly dominated by perceived disadvantages and risks.",https://www.semanticscholar.org/paper/3e87d8680c4162dcfcbad34e54f3493f47c7e713,IS,Qualitative
Lessons Learned from Institutional Responses to COVID-19: Evidenced-Based Insights from a Qualitative Study of Historically Black Community Colleges,"ABSTRACT This qualitative study employs narrative inquiry methodology to elicit information from 10 mid- and senior-level administrators at eight of the 13 federally-designated historically Black community colleges (HBCCs) in the nation about their institution’s response to the challenges of change presented by COVID-19. Specifically, the semi-structured interview protocol was designed to focus on what worked, what did not work, and any “lessons learned” along the way. Rich, thick description of the study sites and participants, as well as a sequential approach to constant comparison method, yielded several interpretable themes: what worked (e.g., safety, technology), what did not work (e.g., plans, traditional pedagogy), and four key lessons learned. Implications for policy, practice, and future research are discussed.",https://www.semanticscholar.org/paper/be3318210e944f15653693b62aa44b5cc5865061,IS,Qualitative
RESEARCH AND DEVELOPMENT: AS THE PRIMARY ALTERNATIVE TO EDUCATIONAL RESEARCH DESIGN FRAMEWORKS,"A model for better educational practices is being developed as a result of the expansion of educational research as a branch of science, according to the researchers working in this area. Research and Development (R&D) Method is one of the best model designs. This study employs a descriptive qualitative research methodology that depicts the scenario as it relates to an indicator or the current status of the findings in the field. In this study, the methods for gathering data and information were interviewing, and documenting studies of the necessary data sources. This article presents a number of models for development research, including the Borg and Gall Development model, the Sadiman Development model, the ADDIE Development model, the Dick and Carey Development model, and the Data Center for Education and Culture Information Technology of Ministry of National Education",https://www.semanticscholar.org/paper/e237ddfba1d2eb652f2ae7f594898c84231c90fa,IT,Qualitative
Students’ Perception on the Integrating of Information and Communication Technology (ICT),"The new trends of the integrating of ICT in English Class used by the researchers in Teaching English as Foreign Language (TEFL) methodology class that enables the students get more experience, knowledge and understanding. This research aims to know whether the students have positive perception on the integrating ICT in TEFL methodology class, describe the students’ attitude in learning TEFL methodology Class through the integration of ICT and get the information on the strengths and weaknesses of the integrating of ICT in TEFL methodology class from the students’ perception. The researchers conducted the research in TEFL Methodology Class. The students involved in this research were the students of fifth semester that consist of 30 students in English Department of STKIP Muhammadiyah Pringsewu Lampung. This research is qualitative research which involves case study. In collecting the data, the researchers used questionnaire, in-dept interview and observation. In getting the validity of the data the triangulation was applied by the researcher. From the research finding, the researchers got the information that the students have positive perception in ICT for TEFL methodology class, the students have positive attitude in learning, and integration of ICT for TEFL brings both strengths and weakness.",https://www.semanticscholar.org/paper/58b8158efd1f8a3fc986cada26fe94130f8d67c8,CS,Qualitative
“Call a Teenager… That’s What I Do!” - Grandchildren Help Older Adults Use New Technologies: Qualitative Study,"Background Although family technical support seems intuitive, there is very little research exploring this topic. Objective The objective of this study was to conduct a subanalysis of data collected from a large-scale qualitative project regarding older adults’ experiences in using health information technology. Specifically, the subanalysis explored older adults’ experiences with technology support from family members to inform strategies for promoting older adults’ engagement with new health technologies. Although the primary analysis of the original study was theoretically driven, this paper reports results from an inductive, open-coding analysis. Methods This is a subanalysis of a major code identified unexpectedly from a qualitative study investigating older adults’ use experience of a widespread health technology, the patient portal. A total of 24 older patients (≥65 years) with multiple chronic conditions (Charlson Comorbidity Index >2) participated in focus groups conducted at the patients’ primary clinic. While conducting the primary theoretically driven analysis, coders utilized an open-coding approach to ensure important ideas not reflected in the theoretical code book were captured. Open coding resulted in 1 code: family support. This subanalysis further categorized family support by who provided tech support, how tech support was offered, and the opinions of older participants about receiving family tech support. Results The participants were not specifically asked about family support, yet themes around family assistance and encouragement for technology emerged from every focus group. Participants repeatedly mentioned that they called their grandchildren and adult children if they needed help with technology. Participants also reported that family members experienced difficulty when teaching technology use. Family members struggled to explain simple technology tasks and were frustrated by the slow teaching process. Conclusions The results suggest that older adults ask their family members, particularly grandchildren, to support them in the use of new technologies. However, family may experience difficulties in providing this support. Older adults will be increasingly expected to use health technologies, and family members may help with tech support. Providers and health systems should consider potential family support and engagement strategies to foster adoption and use among older patients.",https://www.semanticscholar.org/paper/4a19ac0aa4284e1a7c3e5422d27ac54239085871,IS,Qualitative
Construction risk management research: intellectual structure and emerging themes,"Abstract This study aims to undertake a holistic review of global construction risk management (CRM) research published between 2000 and 2021 and identify the intellectual structure and emerging themes of the CRM research. A total of 2034 primary documents and 68727 secondary documents were collected from Web of Science core collection database. Document co-citation and bibliographic coupling techniques were adopted with qualitative discussion to show the intellectual structure of the CRM knowledge domain and emerging themes. The CRM knowledge domain consists of the key themes relating to CRM steps, RM in construction projects with specified characteristics, RM in international construction and management of particular risk categories. In addition, the emerging themes include advanced risk analysis techniques, information and communication technology-driven CRM, integration of CRM into other management functions, as well as human factors in CRM. This review study is more inclusive than any prior reviews on CRM and provides an in-depth understanding of the CRM research and benefits industry practitioners and researchers.",https://www.semanticscholar.org/paper/08a2cfb3fb18e8b825df68741ef3d5eb0ae6dbb2,IS,Qualitative
Development and pilot evaluation of a pregnancy-specific mobile health tool: a qualitative investigation of SmartMoms Canada,"BackgroundMobile technology is ubiquitous. Women of childbearing age have embraced health information technology for pregnancy-related counsel as prenatal care provider communication is increasingly scarce and brief. Pregnant women and new mothers place high value in the use of online sources to support their pregnancy information needs. In Canada, over 300,000 women are pregnant annually, with approximately 60% exceeding evidence-based weight gain recommendations. Mobile health (mHealth) tools, such as mobile applications (app), have the potential to reduce excessive gestational weight gain, offering pregnant women trustworthy guidance, ultimately improving the health outcomes of mothers and infants. Therefore, the primary aim of this study was to implement a qualitative, descriptive research design to assess the receptiveness, functionality, and future prospective of the SmartMoms Canada mHealth app.MethodsTwo focus groups (n = 13) involving both currently pregnant and recently postpartum women were organized on the same day. Focus groups were transcribed verbatim and thematic analysis was undertaken using manual coding and NVivo software. Participants who took part in the focus groups (n = 13) and those who could not attend (n = 4) were asked to complete a Likert-scale survey. All survey responses (n = 17) were analyzed using simple tabulation and percentage analysis.ResultsParticipants were technologically proficient and interacted with several mHealth tools prior to testing the SmartMoms Canada app. Six major themes emerged from thematic analysis: knowledge of pregnancy-specific mHealth services, knowledge and attitudes of weight gain guidelines, weight tracking, strengths of the app, critique and lastly, future suggestions for the app.ConclusionsOur thematic analysis found that women positively viewed the future potential of our app and offered constructive feedback to improve the next version. Participants sought more personalization and enhanced app interactivity, along with promotion of overall maternal health including nutrition and mental health, in addition to weight tracking.",https://www.semanticscholar.org/paper/5841280b49352103d272c7831a0435eedd627b58,CS,Qualitative
Qualitative analysis to determine decision-makers' attitudes towards e-government services in a De-Facto state,"The manner in which people, businesses and governments perform is changing because of the spread of technology. Digitalization of governments can be considered a necessity as we are now entering the era of the Internet-of-Things. The advantages and disadvantages of electronic governments have been examined in several research studies. This study aims to examine the attitudes of decision-makers towards e-government. The research aims are as follows: to determine the problems related with e-government usage, to establish the factors which decrease the usage of e-government services and to propose recommendations for the effective application of e-government practices.,Qualitative research has been used for the study. Participants were chosen by the snowball sampling method, and face-to-face in-depth interviews were conducted with all decision-makers. In-depth interviews are more efficient and enable the acquisition of better qualitative information, in-depth knowledge and statistics, as the distance between the interviewer and interviewee is reduced (Stokes and Bergin, 2006). Questions asked can be categorized under two sections, where the questions in the first section are related to the decision-maker’s management style/managerial proposition, and in the second section, technological questions are asked in terms of the preferred communication method and the decision-makers’ attitudes towards e-government practices.,Decision-makers perceive electronic government to be important, while the level of importance is observed to be different among the decision-makers. Chronic problems exist in many countries, such as nepotism, where the decision-makers have conflicting arguments about e-government and the resulting effect on nepotism. Furthermore, the study also indicates that decision-makers are aware of the importance of mobile government, although they acknowledge that more time is required, as their country is still developing. Electronic voting is also perceived to be important, although the decision-makers believe that security and privacy issues need to be solved before related projects can be initiated.,This research can be a benchmark study for the decision-makers of small island developing states by means of e-government. The impediments preventing the effective application of e-government practices are also discussed in the study. This study will be useful to highlight the triggers and obstacles for e-government development in the context of a developing country. Internet penetration has increased significantly since the 2000s, and therefore, decision-makers need to consider the shift in citizens’ behaviour, such as the high usage of smartphones and the emergence of the Internet-of-Things (Kaya and Bicen, 2016; Kumar et al., 2017).",https://www.semanticscholar.org/paper/c739a05e4f7819e6caaff2ae13a91872dae50961,IS,Qualitative
Creating a technology-enhanced constructivist learning environment for research ability development in a BA Thesis Writing course,"Abstract This qualitative case study investigates the technology-enhanced constructivist learning environment (CLE) for developing undergraduate English-majors’ research abilities at a Chinese university. The faculty group attempted at transforming an English Writing course into BA Thesis Writing, supported by a free online learning management system (Moodle) with multi-faceted scenario, resources, tools and management. Situated in the broader context of the 10-year curriculum reform, two BA Thesis Writing classes from two semesters were examined using observation notes, interviews and student products. Findings show the CLE has enhanced the students’ problem-based learning (PBL) processes and research ability development in terms of problem awareness, information literacy, reasoning and research designing. The CLE contributed to the whole learning experience by continuous instructional support, forum discussions, and information resources and tools. However, faculty tended to avoid teaching such a course because it is not only demanding but it clashes with norms of Chinese higher education in terms of professional values, pedagogical philosophy and institutional culture. The paper concludes by assessing the reform efforts’ overall gains and challenges, and its implications for future technology integration in foreign language learning and teaching in China.",https://www.semanticscholar.org/paper/0a7a4da1bf9a3047f61c1aa25f41505aea7a7f0a,CS,Qualitative
Pandemic designs for the future: perspectives of technology education teachers during COVID-19,"Purpose The disruption caused by the pandemic declaration and subsequent public health measures put in place have had a substantial effect on teachers’ abilities to support student engagement in technology education (TE). The purpose of this paper is to explore the following research question: How do TE teachers see emergency remote teaching (ERT) transitions to blended learning into the next academic year affecting their profession? Design/methodology/approach A snowball and convenience sampling design was used to recruit specialist teachers in TE through their professional organization and were asked to respond to the question: What are your concerns about the future of teaching TE remotely? The qualitative data collected from the participants (N = 42) was analyzed thematically (Braun and Clarke, 2006). Findings The analysis revealed that the switch to ERT impacted the teachers’ ability to support hands-on competency development owing to inequitable student access to tools, materials and resources, all of which affected student motivation and engagement. As a result, teachers raised questions about the overall effectiveness of online learning approaches and TE’s future and sustainability if offered completely online. Originality/value This research is the first of its kind exploring the experiences of TE teachers during the COVID-19 pandemic. In answer to the challenges identified by teachers, the authors offer a blended learning design framework informed by pandemic transformed pedagogy that can serve as a model for educators to use when designing blended instruction.",https://www.semanticscholar.org/paper/28f6fa87b6c7cb4b05cb8f53b6f722e8868ba866,IS,Qualitative
Peer-to-Peer Health Communication in Older Adults’ Online Communities: Protocol for a Qualitative Netnographic Study and Co-Design Approach,"Background Online communities provide an environment in which people with similar health concerns can interact and access content that can support the self-management of long-term conditions (LTCs). Recently, the importance of online social networks as sources of health information and social support has been brought into focus with the emergence and widespread societal impacts of COVID-19. Although online communities exist for older adults, little is known about the specific health and self-care topics that older people discuss in such environments and how these relate to users’ support needs and outcomes. A better understanding of users’ needs and peer-to-peer communication in these communities is necessary to inform the design of information and communication technology (ICT) interventions that are relevant to older people and their peer supporters. Objective This study aims to use a two-phase, web-based ethnographic (netnography) and co-design approach to explore specific health care and self-care topics that older adults discuss in a UK-based online community and how peer supporters respond to these queries with informational and/or social support and engage with stakeholders to define the needs and requirements for new ICT-based interventions capable of reducing social isolation and facilitating LTC self-management support. Methods The first phase of the research will involve a qualitative netnographic analysis of posts in discussion forums in a publicly accessible online community. The second phase will involve co-design workshops with health care consumers (ie, older adults and carers) and service providers to determine the needs and requirements for new ICT-based interventions and digital innovations. Constructivist grounded theory will be used in the first phase; in the second phase, the co-design workshops will be audiorecorded and analyzed thematically. Results This research project is in progress. Permission was obtained from the website administrator to use materials from the social media forum; data collection for the first phase began in April 2020. The second phase of the study is expected to begin in late 2020. This study is due to be completed by the end of 2021. Conclusions This study is the first, to the best of our knowledge, to combine qualitative netnography with an iterative co-design framework to specify the needs and requirements for new ICT-based interventions. The findings from this study will inform the next phase of the multiphase knowledge translation project and will provide insights into the potential of online peer health communities to reduce social isolation and facilitate chronic illness self-management support and self-care. International Registered Report Identifier (IRRID) PRR1-10.2196/19834",https://www.semanticscholar.org/paper/7978658d4a172f807ddcec5094263b88457d34db,IS,Qualitative
STATUS QUO AND FUTURE DIRECTIONS OF FACILITY MANAGEMENT: A BIBLIOMETRIC – QUALITATIVE ANALYSIS,"Facility management (FM) has received extensive attention from practitioners and researchers. While FM is continuously maturing as a scientific discipline and relevant studies are constantly growing, there are no holistic reviews of current research. The information in previous studies is generally scattered, and existing literature reviews mostly focused on specific aspects of FM. It is necessary for researchers and practitioners to obtain a thorough view of the current status in the FM field and future development trends that have been summarized and discussed in depth. Using a bibliometricqualitative analysis, a total of 724 academic journal papers on FM, between 1995 and 2018, were reviewed. A number of the latest advancements and emergent trends were identified based on knowledge maps in FM, including changing circumstances, enhancing information technology, all-around facility manager, strategic performance management, sustainable FM and innovative FM practice. It is hoped that this review can help researchers understand the current body of FM knowledge. The future directions were also highlighted in this study to help researchers identify areas where research is most needed. This study could also help practitioners to address upcoming challenges in the FM field.",https://www.semanticscholar.org/paper/ef3be07c1abc5cd6113231c09adeeba59664a50f,IS,Qualitative
Virtual Reality Technology: Analysis based on text and opinion mining.,"The purpose of this research is to highlight the importance of periodically analyzing the data obtained from the technological sources used by customers, such as user comments on social networks and videos, using qualitative data analysis software. This research analyzes user sentiments, words, and opinions about virtual reality (VR) videos on YouTube in order to explore user reactions to such videos, as well as to establish whether this technology contributes to the sustainability of natural environments. User-generated data can provide important information for decision making about future policies of companies that produce video content. The results of our analysis of 12 videos revealed that users predominantly perceived these videos positively. This conclusion was supported by the findings of an opinion and text analysis, which identified positive reviews for videos and channels with many followers and large numbers of visits. The features such as the quality of the video and the accessibility of technology were appreciated by the viewers, whereas videos that are 100% VR and require special glasses to view them do not have as many visits. However, VR was seen to be a product which viewers were interested in and, according to Google, there are an increasing number of searches and sales of VR glasses in holiday seasons. Emotions of wonder and joy are more evident than emotions of anger or frustration, so positive feelings can be seen to be predominant.",https://www.semanticscholar.org/paper/91f43b211baf55a2798090998eb010f0d6900116,CS,Qualitative
Therapy interventions for children with neurodisabilities: a qualitative scoping study.,"BACKGROUND Therapy interventions emerged four times in the top 10 research priorities in a James Lind Alliance research prioritisation exercise for children with neurodisabilities (Morris C, Simkiss D, Busk M, Morris M, Allard A, Denness J, et al. Setting research priorities to improve the health of children and young people with neurodisability: a British Academy of Childhood Disability-James Lind Alliance Research Priority Setting Partnership. BMJ Open 2015;5:e006233). The National Institute for Health Research (NIHR) commissioned this study as part of an information-gathering exercise in response to this. OBJECTIVES The objectives were to (1) describe the current practice, approaches and schools of thought in relation to physiotherapy, occupational therapy and speech and language therapy for children with neurodisability; (2) explore clinical decision-making; (3) investigate views on outcomes and their measurement, particularly participation as an outcome, that is, the child's ability to have the opportunity to be involved in life situations and activities (e.g. communication, mobility, interpersonal interactions, self-care, learning and applying knowledge); (4) seek views on the aspects of therapy interventions that have an impact on outcomes; and (5) elicit stakeholder views on research needs and priorities. DESIGN, SETTING AND PARTICIPANTS More than 70 professionals (therapists, service leads, paediatricians and education staff) and 25 parents participated in a qualitative interview (either individually or as part of a focus group). RESULTS Professional thinking and models of service delivery are in a state of flux and development. There is a move towards goals-focused, family-centred approaches. Work tends to be highly individualised, with few protocols. Parents are certain of the value of therapies, although they may experience difficulties with provision and may seek (additional) private provision. Therapy interventions are conceived as three components: the therapist, the procedures/equipment, etc., and the wider therapeutic environment. They are believed to be highly complex and poorly understood. Although participation is widely endorsed as a core intervention objective of therapy interventions, its suitability, or appropriateness, as an outcome measure was questioned. Other child and/or parent outcomes were identified as more or equally important. Notions of intermediate outcomes - in terms of body structure/function, and the achievement of activities - were regarded as important and not counter to participation-focused approaches. Among therapists, research on intervention effectiveness was (cautiously) welcomed. A number of methodological challenges were identified. A portfolio of study designs - quantitative and qualitative, experimental and observational - was called for, and which included economic evaluation and clear pathways to impact. LIMITATIONS The study was not successful in recruiting children and young people. Further work is required to elucidate the views of this key stakeholder group. CONCLUSIONS Therapy interventions are poorly understood. There was strong support, tempered a little by concerns among some about the feasibility of demonstrating impact, for investment in research. FUTURE WORK The identification of research priorities was a core study objective, and a wide-ranging research agenda was identified. It included 'foundational' research into neurodisability, the active components of therapy interventions and the concept of participation. Three areas of evaluation were identified: overall approaches to therapy, service organisation and delivery issues, and the evaluation of specific techniques. Parents regarded evaluations of approaches to therapy (e.g. goals-focused; supporting family-self management) as priorities, along with evaluations of models of service provision. Professionals' views were broadly similar, with an additional emphasis on methodological research. In terms of specific techniques, there was no shared agreement regarding priorities, with views informed by personal interests and experiences. FUNDING The NIHR Health Technology Assessment programme.",https://www.semanticscholar.org/paper/28e3b0d2db051b7e486e85cd66511f72e5fa317f,IS,Qualitative
The Technology Use and Information Flow at a Municipal Telemedicine Service,"Health care services facechallenges with providing individualised treatment to an ageing population prone to chronic conditions and multi-morbidities. The research project Patients and Professionals in Productive Teams aims to study patient-centred teamworkservice models. This paper presents an evaluation of a telemedicine service for chronic obstructive pulmonary disease patients integrated with municipal health care services. Qualitative methods were used to study the technology use and information flow. The results showed that the telemedicine technology was a standalone system, not integrated with the electronic health record of the municipality. A benefit of the system was a function to provide the patient with written instructions on agreements and advices. As a constraint for the patient-centred team approach, the information in the telemedicine system was available only for the telemedicine nurses and not to other health care professionals.",https://www.semanticscholar.org/paper/a9c1b79b05b21ff09a8bac26e03201ee3bbbb176,CS,Qualitative
How do technology equipment companies implement new billing strategies?,"Objective: To evaluate the implementation of electronic invoicing in companies that sell technology equipment in the municipality of Florencia, Caquetá. Methods: the research approach is mixed, qualitative due to the implementation of 116 surveys, and qualitative due to the development of three interviews with municipal actors from Florencia - Caquetá, in the same way the type of descriptive-explanatory research given to the object of the investigation. Results: 78 of 116 entrepreneurs stated the implementation of the electronic invoicing system, where serious training and teaching problems were found in the use of information and research technologies. In this sense, the interviewees express resistance to change and lack of flexibility in knowledge relating to technologies. Conclusion: The implementation of electronic invoicing in companies that sell technological equipment in the municipality of Florencia is being strengthened, taking into account that they have had different types of growth and development barriers due to costs and lack of training.",https://www.semanticscholar.org/paper/78476929a3a03966bbcb56c96ee32fa995ef39f3,CS,Qualitative
Optimizing zakat governance in East Java using analytical network process (ANP): the role of zakat technology (ZakaTech),"Purpose This study aims to formulate a strategy for optimizing zakat governance in zakat institutions in East Java Province by identifying priority problems, creating solutions and developing strategies. Design/methodology/approach This qualitative research uses the analytical network process method. Ten respondents representing practitioners, academics, associations and regulators were selected for their expertise in zakat governance. The data were processed using Super Decision software program and Excel. Findings Priority issues in optimizing zakat governance found in this study are lack of information on Mustahik’s needs and development to Muzakki and the low motivation and ability of Mustahik to develop. Improving the quality and capacity of Amil’s, especially in the technological aspect, is a priority solution. The priority strategy considers intensification (by developing Amil’s ability to use technology) and extensification (by increasing Amil’s numbers who master technology). Practical implications The results highlight the urgency of increasing Amil’s capacity and capability in technology-based zakat management. Zakat institutions need to prepare for management’s transformation toward zakat technology as one of the priorities in optimizing zakat governance. Originality/value Problems, solutions and strategies for optimizing zakat governance are collected by connecting it to the Zakat Core Principles, namely, the ninth principle of collection management and the tenth principle of distribution management. Further, for identifying problems, solutions and strategies, four aspects must be considered of Amil, Muzakki, Mustahiq and other supporting elements to present better policies to optimize zakat governance.",https://www.semanticscholar.org/paper/2e329f19de7d60da42a562930d28e217881ef25f,IS,Qualitative
Education and Technology Management Policies and Practices in Madarasah,"In order to promote education in madrasahs, this study seeks to assess policies from a sociopolitical perspective. This research, it continues, was carried out using a descriptive qualitative method to the kind of library research, with book data serving as the data source. Content analysis, whose scope includes the depth of information content, was used to carry out the data analysis technique. The findings demonstrated that madrasah-based management involves the principal and is participatory management of the madrasah. The community, stakeholders, instructors, and students must work together to attain the goals for educational quality. Since1960–1990, there has been a global push toward decentralization in the area of education reform. The New Order's fall in 1998 ushered in a period of transition that included the constitutional reformation of education in Indonesia. since the Regional Government Law No. 22 of1999, which was later revised by Law No. 32 of the same year, established regional autonomy. In2004, the local government received control of the educational matters.",https://www.semanticscholar.org/paper/413f83d31b6bdbaaba871c2605522bfd540b5fdc,IS,Qualitative
Pengembangan Kurikulum Merdeka untuk Meningkatkan Kualitas Pembelajaran,"Developing a curriculum that is in accordance with the times is one of the efforts made to improve the quality of education. The education system is expected to realize students to be able to have the ability to think critically, solve problems, be creative, innovative, communication skills, collaboration skills, search skills, management skills, skills to convey information and skills to use information and technology needed today. Based on the policy of the Indonesian Minister of Education and Culture, the Independent Curriculum is expected to create a happy learning atmosphere for both teachers and students. The method used in this research is qualitative through literature, namely by tracing the sources of writing that have been made previously to solve the problems discussed in a study. The results of this study indicate that in curriculum development, educators play an important role. The professionalism of educators when teaching in class is one of the things that can show the increasing quality of learning with the curriculum development used.",https://www.semanticscholar.org/paper/07c17863256483f8bf2ee18447d20c7ea17a50c1,IS,Qualitative
Analysing the Role of ChatGPT in Improving Student Productivity in Higher Education,"Student productivity in higher education is still a problem. In the digital era, technology is increasingly developing and provides convenience for doing various things, including in terms of learning. The purpose of this study will be an analysis of the role of ChatGPT in helping to improve the quality of student productivity. This research is qualitative in nature. Data collection techniques include listening and recording important information to conduct data analysis through data reduction, data display, and conclusion drawing. This study concludes that ChatGPT can make a significant contribution in improving the quality of student productivity. This language model can help students in various ways, such as providing useful information and resources, helping to improve language skills, facilitating collaboration, increasing time efficiency and effectiveness, and providing support and motivation.",https://www.semanticscholar.org/paper/81d3d446d90c2484785eb3974d8757a7b8b1fe1e,IS,Qualitative
InfoSwarms: Drone Swarms and Information Warfare,": Drone swarms, which can be used at sea, on land, in the air, and even in space, are fundamentally information-dependent weapons. No study to date has examined drone swarms in the context of information warfare writ large. This article explores the dependence of these swarms on information and the resultant connections with areas of information warfare—electronic, cyber, space, and psychological—drawing on open-source research and qualitative reasoning. Overall, the article offers insights into how this important emerging technology fits into the broader defense ecosystem and outlines practical approaches to strengthening related information warfare capabilities.",https://www.semanticscholar.org/paper/31b3e26627e35e7f899de588f84d5e99155ab5a3,CS,Qualitative
THE DIGITAL MEDIATION OF MIGRATION: A QUALITATIVE THEMATIC SYNTHESIS,"The highly complex economic, political, social and cultural processes of migration are increasingly digitally mediated. A rich area of research examining this phenomenon is emerging at the confluence of information and communication technology (ICT) and social sciences. However, little has been done to systematically review this literature. Through a qualitative thematic synthesis (QTS), this paper presents a fusion of recent research to identify: 1) what technologies are being used by migrants to the EU and how and 2) what are the impacts of technology on migrants and migration processes. By bridging the findings of multidisciplinary works, this QTS demonstrates how digital media and technology affect migrant experiences in both positive and negative ways as well as evidence of their impact on migratory trends. It highlights key gaps in the literature and suggests further areas for intervention, identifying the need for a cross-disciplinary research agenda that addresses causal relationships between effects of technology and migration processes.",https://www.semanticscholar.org/paper/3403aecfda5cdaff255064e15c710df81f2a3b53,IS,Qualitative
Blockchain Technology in Wine Chain for Collecting and Addressing Sustainable Performance: An Exploratory Study,"Sustainability standards have not yet been commonly adopted by the whole wine chain, and indicator assessments are not widely spread. A deep understanding of how embedding sustainability into business while controlling costs related to the adoption of sustainability certification standards such as data collection and management practices could allow one to overcome most barriers relevant to sustainability compliance. Blockchain technology (BCT) may answer these needs. In order to verify BCT potential to be used as a sustainability management tool in the wine industry, with a qualitative triple bottom line research approach, this article explores the connections among BCT adoption in agri-food, issues posed by wine sustainability certification, and whether wine companies that already own a wine sustainability certification are prepared to adopt it. Results show that (1) the blockchain allows collecting data and information that are relevant for monitoring and improving sustainability: Soil and water features, climate conditions, treatment with pesticides and fertilizers, production process, traceability, transparency, labor and human rights, quality and safety, waste reduction, authenticity, relationship with stakeholders; (2) wine companies that already own a sustainability certification have little familiarity with blockchain applications (57.1%, n.21) and only 14% of the respondents support their intention to invest in BCT in the coming years; (3) the case study shows improvements in traceability and transparency along the supply chain and an increase in consumers’ trust that was reflected in sales growth, and the main costs are linked to complexity in data management.",https://www.semanticscholar.org/paper/563d4e1825d7dfbea321286b776257b9719b8f2d,IS,Qualitative
Utilizing a Prototype Patient-Controlled Electronic Health Record in Germany: Qualitative Analysis of User-Reported Perceptions and Perspectives,"Background Personal electronic health records (PHR) are considered instrumental in improving health care quality and efficiency, enhancing communication between all parties involved and strengthening the patient’s role. Technical architectures, data privacy, and applicability issues have been discussed for many years. Nevertheless, nationwide implementation of a PHR is still pending in Germany despite legal regulations provided by the eHealth Act passed in 2015. Within the information technology for patient-oriented care project funded by the Federal Ministry of Education and Research (2012-2017), a Web-based personal electronic health record prototype (PEPA) was developed enabling patient-controlled information exchange across different care settings. Gastrointestinal cancer patients and general practitioners utilized PEPA during a 3-month trial period. Both patients and physicians authorized by them could view PEPA content online and upload or download files. Objective This paper aims to outline findings of the posttrial qualitative study carried out to evaluate user-reported experiences, perceptions, and perspectives, focusing on their interpretation of PEPA beyond technical usability and views on a future nationwide implementation. Methods Data were collected through semistructured guide-based interviews with 11 patients and 3 physicians (N=14). Participants were asked to share experiences, views of perceived implications, and perspectives towards nationwide implementation. Further data were generated through free-text fields in a subsequent study-specific patient questionnaire and researcher’s notes. Data were pseudonymized, audiotaped, and transcribed verbatim. Content analysis was performed through the Framework Analysis approach. All qualitative data were systemized by using MAXQDA Analytics PRO 12 (Rel.12.3.1). Additionally, participant characteristics were analyzed descriptively using IBM SPSS Statistics Version 24. Results Users interpreted PEPA as a central medium containing digital chronological health-related documentation that simplifies information sharing across care settings. While patients consider the implementation of PEPA in Germany in the near future, physicians are more hesitant. Both groups believe in PEPA’s concept, but share awareness of concerns about data privacy and older or impaired people’s abilities to manage online records. Patients perceive benefits for involvement in treatment processes and continuity of care but worry about financing and the implementation of functionally reduced versions. Physicians consider integration into primary systems critical for interoperability but anticipate technical challenges, as well as resistance from older patients and colleagues. They omit clear positioning regarding PEPA’s potential incremental value for health care organizations or the provider-patient relationship. Conclusions Digitalization in German health care will continue to bring change, both organizational and in the physician-patient relationship. Patients endorse and expect a nationwide PEPA implementation, anticipating various benefits. Decision makers and providers need to contribute to closing modernization gaps by committing to new concepts and by invigorating transformed roles.",https://www.semanticscholar.org/paper/88df5b99c68b9fce231e1098c7aab5af652bb3ec,IS,Qualitative
Experiences of Using Web-Based and Mobile Technologies to Support Self-Management of Type 2 Diabetes: Qualitative Study,"Background The prevalence of type 2 diabetes is rising, placing increasing strain on health care services. Web-based and mobile technologies can be an important source of information and support for people with type 2 diabetes and may prove beneficial with respect to reducing complications due to mismanagement. To date, little research has been performed to gain an insight into people’s perspectives of using such technologies in their daily management. Objective The purpose of this study was to understand the impact of using Web-based and mobile technologies to support the management of type 2 diabetes. Methods In-depth interviews were conducted with 15 people with type 2 diabetes to explore experiences of using Web-based and mobile technologies to manage their diabetes. Transcripts were analyzed using the framework method. Results Technology supported the users to maintain individualized and tailored goals when managing their health. A total of 7 themes were identified as important to participants when using technology to support self-management: (1) information, (2) understanding individual health and personal data, (3) reaching and sustaining goals, (4) minimizing disruption to daily life, (5) reassurance, (6) communicating with health care professionals, and (7) coordinated care. Conclusions Patients need to be supported to manage their condition to improve well-being and prevent diabetes-related complications from arising. Technologies enabled the users to get an in-depth sense of how their body reacted to both lifestyle and medication factors—something that was much more difficult with the use of traditional standardized information alone. It is intended that the results of this study will inform a new questionnaire designed to assess self-management in people using Web-based and mobile technology to manage their health.",https://www.semanticscholar.org/paper/deb6a091e74e2a9de29b39d9651293e8f13e183f,IS,Qualitative
The Role of Social Media in Shaping Public Opinion and Its Influence on Economic Decisions,"Social media has become an integral part of modern society's daily life. With the rapid development of information and communication technology, social media platforms have changed the way people interact, share information, and express their views. The purpose of this study is to analyse the role of social media in shaping public opinion and its influence on economic decisions. This research uses a qualitative method. Data collection involved literature review and social media content analysis, focusing on opinion patterns and their influence. Reliability and validity were emphasised through triangulation and participation of secondary source participants in the analysis process. The study results show that in a digital age characterised by the central role of social media, it can be concluded that social media has a significant role in shaping public opinion and also influencing economic decision-making. Through social media, individuals and groups can interact with each other, share information, and participate in discussions that shape collective views on various issues.",https://www.semanticscholar.org/paper/9718b516e4e5dbc4336fcdf06758421c2f71911e,IS,Qualitative
Managing cyber and information risks in supply chains: insights from an exploratory analysis,"Purpose The purpose of this paper is to explore how companies approach the management of cyber and information risks in their supply chain, what initiatives they adopt to this aim, and to what extent along the supply chain. In fact, the increasing level of connectivity is transforming supply chains, and it creates new opportunities but also new risks in the cyber space. Hence, cyber supply chain risk management (CSCRM) is emerging as a new management construct. The ultimate aim is to help organizations in understanding and improving the CSCRM process and cyber resilience in their supply chains. Design/methodology/approach This research relied on a qualitative approach based on a comparative case study analysis involving five large multinational companies with headquarters, or branches, in the UK. Findings Results highlight the importance for CSCRM to shift the viewpoint from the traditional focus on companies’ internal information technology (IT) infrastructure, able to “firewall themselves” only, to the whole supply chain with a cross-functional approach; initiatives for CSCRM are mainly adopted to “respond” and “recover” without a well-rounded approach to supply chain resilience for a long-term capacity to adapt to changes according to an evolutionary approach. Initiatives are adopted at a firm/dyadic level, and a network perspective is missing. Research limitations/implications This paper extends the current theory on cyber and information risks in supply chains, as a combination of supply chain risk management and resilience, and information risk management. It provides an analysis and classification of cyber and information risks, sources of risks and initiatives to managing them according to a supply chain perspective, along with an investigation of their adoption across the supply chain. It also studies how the concept of resilience has been deployed in the CSCRM process by companies. By laying the first empirical foundations of the subject, this study stimulates further research on the challenges and drivers of initiatives and coordination mechanisms for CSCRM at a supply chain network level. Practical implications Results invite companies to break the “silos” of their activities in CSCRM, embracing the whole supply chain network for better resilience. The adoption of IT security initiatives should be combined with organisational ones and extended beyond the dyad. Where applicable, initiatives should be bi-directional to involve supply chain partners, remove the typical isolation in the CSCRM process and leverage the value of information. Decisions on investments in CSCRM should involve also supply chain managers according to a holistic approach. Originality/value A supply chain perspective in the existing scientific contributions is missing in the management of cyber and information risk. This is one of the first empirical studies dealing with this interdisciplinary subject, focusing on risks that are now very high in the companies’ agenda, but still overlooked. It contributes to theory on information risk because it addresses cyber and information risks in massively connected supply chains through a holistic approach that includes technology, people and processes at an extended level that goes beyond the dyad.",https://www.semanticscholar.org/paper/2727536acc743270317388a4693dd41d3e236532,IS,Qualitative
Analysis framework for the interactions between building information modelling (BIM) and lean construction on construction mega-projects,"Purpose The construction industry encounters substantial challenges in its evolution towards sustainable development and to the adoption of building information modelling (BIM) technology and lean construction (LC) practices on construction mega-projects. This study aims to present critical challenges and to investigate the interactions of BIM and LC on construction mega-projects encountered by key stakeholders in their efforts to integrate BIM and LC. Design/methodology/approach A qualitative research approach is adopted to introduce and validate LC principles and BIM functionalities resulting from a detailed analysis of extant literature, followed by a conceptual analysis of the interactions between BIM and LC on construction mega-projects. A quantitative questionnaire survey is then used. Descriptive and inferential statistical tests are used for data analysis, and analysis of variance tests elaborate and validate results. Findings The research yielded ten BIM functionalities and ten LC principles, which are categorised in four principle areas and four BIM functionality groups. A research framework for analysis of the interaction between BIM and LC is then compiled. Originality/value Research findings and the proposed framework will enhance the adoption of BIM and LC practices on construction mega-projects and allow project key stakeholders to place emphasis on tackling crucial challenges and barriers identified in this research. The framework will guide and stimulate research; and as such, the approach adopted up to this point is constructive. The identified interactions between BIM and LC on construction mega-projects show positive synergies between the two.",https://www.semanticscholar.org/paper/11ace0e23ba541e95d7e42e0a9f49eeb5e0ba4fe,IS,Qualitative
Mixed Reality (MR) for Generation Z in Cultural Heritage Tourism Towards Metaverse,"Generation Z is transforming tourism by demanding the cocreation of transformative experiences. Cultural heritage professionals must comprehend the needs and desires of the Gen Z to support the cocreation of transformative experiences. This study analysed the role of Mixed Reality (MR) from the perspective of Gen Z guests through 18 semi-structured interviews and inductive qualitative research. Participants believe that cultural heritage experiences can benefit from immersive technology. Technology supports cocreation of experiences between developers, service providers, DMOs, and consumers. Cultural heritage sites, as a key element of tourism destinations, should consider how to use MR to enhance consumer experiences. Participants express the opinion that cultural heritage sites and tourism destinations require considerable modernisation to create transformative experiences. Metaverse in tourism and cultural heritage sites will undoubtedly support Gen Z to cocreate transformational experiences.",https://www.semanticscholar.org/paper/5277b63c23d925924e9b77fdfe7f138f26fa5012,IS,Qualitative
Teachers' Views on the Use of Information and Communication Technologies (ICT) in Education Environments,"Developments in information and communication technologies enable more information services to be used in education applications. In this respect, it is important for educators to adopt technology, follow it closely and show a positive attitude towards technology in order to be able to use the developing technology in the classroom. Today, using technology is not a privilege but a necessity. Technological developments affect the structure and functions of educational institutions. For this reason, teachers are expected to integrate their lessons with technology in order to train individuals of the information society. This study aims to determine teachers’ views on the use of information and communication technologies in education. Mixed research method was used in the study including experimental dimension and qualitative research. A total number of 58 teachers participated in the study. Results of the study provided useful implications for teachers in developing the ability to work in the information and educational environment.",https://www.semanticscholar.org/paper/ed2fd6d294a1dab50448e49bb9c535002490f78c,IS,Qualitative
Are primary education teachers trained for the use of the technology with disabled students?,"Incorporating information and communication technology (ICT) in inclusive classrooms requires competent teachers, both technological and pedagogical. To contrast these theoretical assumptions, this study aims to identify the level of training and technical knowledge of primary school teachers in Spain regarding the use of ICTs for supporting students with special needs. The research methodology used was a mixed research design (quantitative and qualitative method), analysing 777 questionnaires supplied to primary school teachers and 723 interviews conducted with key informants (members of management teams, ICT coordinators, directors and technological advisors of teacher training centres). The results informed teachers' knowledge about ICT and disability and barriers or obstacles to their training. Among the conclusions, teachers' inadequate training regarding ICTs for students with special needs stands out and the lack of training experiences in this field.",https://www.semanticscholar.org/paper/26826e292840126f5015cb304764b6fe55c85b5f,IS,Qualitative
Improving Customer Service Quality in MSMEs through the Use of ChatGPT,"In the current era of digitalisation, technological developments are accelerating and changing the way humans communicate and interact, including in MSME businesses. This study aims to evaluate the effect of using ChatGPT in improving the quality of customer service in MSMEs. By using ChatGPT technology, it is expected that MSMEs can provide better and effective services to their customers. The focus of this research is qualitative. Methods for gathering information included paying close attention and taking detailed notes, with subsequent analysis including data reduction, visualisation, and conclusions. The results of this study concluded that the use of ChatGPT can help MSMEs in improving the quality of customer service and efficiency in operations. MSMEs need to pay attention to several important things in using this technology, such as customer context and situation, staff training, privacy and security of customer data, and constraints in the use of technology. In the long run, the use of ChatGPT can help MSMEs in gaining better insights into customer preferences and improving customer satisfaction.",https://www.semanticscholar.org/paper/679e40e5fb432e2ee419a25f725a057bff12806e,CS,Qualitative
BIM-driven energy simulation and optimization for net-zero tall buildings: sustainable construction management,"The growing demand for sustainable and energy-efficient buildings, particularly in the context of tall structures, has prompted increased attention to innovative solutions. Despite advancements in Building Information Modelling (BIM) technology, there exists a critical gap in understanding its comprehensive application for achieving net-zero energy consumption in tall buildings, particularly in the Malaysian construction industry. This research addresses this gap by presenting a novel strategy that integrates BIM technology with energy analysis tools for net-zero tall buildings in Malaysia. The aim of the study is to contribute valuable insights to the construction industry, policymakers, and researchers by conducting empirical research, utilizing case studies, validating the proposed framework, advancing sustainable design practices, and supporting the transition towards net-zero energy tall buildings in Malaysia. The methodology involves a three-phase approach, including qualitative analysis, a pilot survey, and a main questionnaire. Exploratory factor analysis (EFA) validates the categorization derived from qualitative interviews, while Partial Least Squares Structural Equation Modelling (PLS-SEM) assesses the convergent and discriminant validity of the measurement model. Hypotheses testing using bootstrapping establishes the significance of correlations between BIM deployment and key factors such as early design integration, enhanced energy efficiency, optimized system integration, predictive performance analysis, and validation of sustainable design. The research findings support the positive associations between BIM deployment and the mentioned factors, providing statistical significance through T-statistics and p-values. The implications of this research extend beyond the Malaysian context, offering valuable insights for architects, engineers, and stakeholders involved in designing and managing sustainable tall buildings. By addressing the identified gaps and leveraging BIM technology effectively, stakeholders can contribute to the construction of net-zero energy structures, aligning with global efforts towards sustainable and energy-efficient building practices.",https://www.semanticscholar.org/paper/9f77c81d15f28d9b81f6ae0cb16f20f20e0d6e3b,CS,Qualitative
An investigation of teachers' perceptions of using ChatGPT as a supporting tool for teaching and learning in the digital era,"The widespread use of information and communication technology (ICT) has led to significant changes in societal aspects, resulting in the emergence of a “knowledge society.” However, students and teachers have faced challenges in adapting to this digitalization. In the United Arab Emirates (UAE), transitioning to a knowledge‐based economy is a primary national agenda goal, aligning with Sustainable Development Goal 4 (SDG4) of ensuring high‐quality education.This research investigates teachers' perceptions of using ChatGPT as a digital supporting tool for teaching and learning practices. This includes lesson planning, teaching and learning activities, assessment and feedback and the challenges and benefits explored.This study employs an explanatory sequential mixed‐methods design involving quantitative and qualitative data collection methods. An online survey was used with closed‐ended items to collect quantitative data, while semi‐structured interviews were conducted to collect qualitative data. The study participants are middle and high school teachers (n1 = 40) from different Dubai and Abu Dhabi private schools.The most noticeable result is that teachers feel the benefits of using ChatGPT in lesson planning, teaching and learning and less in assessment and feedback. Some challenges and benefits were highlighted in each area and recommendations were suggested. However, teachers' biggest challenge was the bias and accuracy of information received and the lack of human interaction.The findings provide valuable insights into the potential of ChatGPT in education and inform future research in this area. Specifically, the study provided insights into the effectiveness of ChatGPT in enhancing students' learning outcomes, engagement and motivation, as well as its impact on teaching practices and paedagogical beliefs.",https://www.semanticscholar.org/paper/fb798c84269b34da2477b602c9de7066956dd987,IS,Qualitative
The use of technology in informal English language learning: evidence from Yemeni undergraduate students,"Purpose This study aims to explore the use of technology-based strategies by Yemeni undergraduate students to develop their English as a foreign language skills in informal learning settings. Design/methodology/approach A mixed methods research design was used to collect quantitative and qualitative data from 110 undergraduate students enrolled at the English Departments in two universities in Yemen. In the first phase of the study, 10 students were interviewed to obtain information about their use of technology to develop their English language skills and subsystems in informal settings. Following the analysis of the interview data, a questionnaire was built to collect quantitative data, and the second phase of the study was carried out with 100 undergraduate students. Findings The findings revealed that students developed four technology-based strategies that they used in informal settings. These strategies included using social media, being inspired by someone, accessing social networks and websites. Students reported that these strategies helped them develop their listening, speaking and reading skills, while they also reported that their vocabulary was enhanced over grammar and pronunciation. Research limitations/implications The study findings can be of benefit not only for helping to raise students’ awareness of informal learning strategies to develop their English skills outside the classroom but also for teachers to rethink the importance of integrating technology tools and digital resources in their teaching practice. The results could also guide curriculum designers to augment textbook materials by integrating technology-based informal learning strategies. Originality/value This is the first study on this topic conducted in the context of higher education in Yemen and offers unique insights into informal learning practices of Yemeni students. In addition, the findings of the study open new dimensions for rethinking the classification of language learning strategies to include those that are technology-based and mainly useful for independent informal learning.",https://www.semanticscholar.org/paper/102b73fcf44d5510d5eacd3aec1f89b8ae295340,IT,Qualitative
Understanding in-store interactive technology use: a uses and gratifications theory (UGT) perspective,"PurposeThe purpose of this research is twofold: (1) to identify and understand consumer motivations to use interactive technologies in stores through the lens of the uses and gratifications (UGT) approach and (2) to understand how these gratifications differ between different interactive technologies (interactive kiosks and self-checkouts).Design/methodology/approachThis research presents a dual qualitative study based on 32 in-depth interviews with 20 consumers, eight salespersons and four phygital experts.FindingsThe data analysis identified three specific gratifications sought in using interactive kiosks (information-seeking, hedonic and social interaction) and two gratifications common to both interactive kiosks and self-checkouts (control and time-saving).Originality/valueFrom a media perspective (UGT), this research provides a deeper understanding of gratifications sought in using interactive technologies in a phygital store. It also contributes significantly to previous research by highlighting that gratifications differ between different technology types.",https://www.semanticscholar.org/paper/7526fdb8e272ae03337ce645aaf9d6db7dc82d79,IS,Qualitative
The enhanced participant-driven photo elicitation method for everyday life health information behaviour research,"The purpose of this paper is to report the design and implementation of the enhanced participant-driven photo elicitation method in a qualitative interview study, to assess the performance of the method to investigate a research topic in everyday life health information behaviour and to provide insights on how to effectively use this method in future research.,The author embedded the enhanced participant-driven photo elicitation in a qualitative interview study to examine people’s everyday life health information behaviour with activity tracking technology. The author assessed the types of visual data collected by the method, categories of elicitation enabled by the method and how the method contributed to key research findings of the interview study.,The enhanced participant-driven photo elicitation generated rich, unique and meaningful data that would be otherwise difficult to collect through conventional qualitative interviews. The method also elicited explanation, rationalisation and reflection during the interviews, which enriched and triangulated key research findings. This work validated the benefits of the general photo elicitation method such as aiding participants’ recall of experiences, enriching research findings and improving research validity. It also demonstrated that the enhancement techniques used in this study could generate rich and even research data across interviews.,This paper describes the design and implementation of the enhanced participant-driven photo elicitation method to augment a qualitative interview study with activity tracker users. The author provides recommendations for researchers to take full advantage of the method in future everyday life health information behaviour research.",https://www.semanticscholar.org/paper/b05e6232842cb4da96467f629eea237d001842c3,CS,Qualitative
Review of augmented reality in academic and research libraries,"Purpose The paper aims to provide substantive information regarding augmented reality (AR) and its usage in academic and research libraries. Design/methodology/approach Qualitative research based on the content analysis method was used for this study. Articles from top databases such as Science Direct, Emerald and Taylor & Francis were consulted, and most cited articles were extracted from those searches for this study. Findings The research reveals that most libraries in developed countries are executing this technology to give maximal information to their patrons in the augmented world. The study also shows that advanced technologies are needed these days, and without adopting these technologies, no advancement in the library sector can be made. Originality/value This paper sheds light on AR conversation and, by extension, implementation; it will further boost the morale of academic and research librarians. Additionally, it sets the framework for researchers and practitioners that the application service is lacking despite the rise of technological advancements.",https://www.semanticscholar.org/paper/1e04bd9ed743858d11e61d930ac1c71e3c4dc7bb,CS,Qualitative
The Impact of Digital Transformation on Business Models and Competitive Advantage,"Digital transformation has become a dominant trend in the business world in recent years. The rapid development of information and communication technology has enabled organisations to change the way they operate, interact with customers, and compete in the market. This research aims to analyse the impact of digital transformation on business models and competitive advantage. This research is a literature review that uses a qualitative approach, which implies that data will be analysed and interpreted using information and text obtained from various sources. The study results show that in the ongoing era of digital transformation, its impact on business models and competitive advantage is an aspect that cannot be ignored. This digital transformation has changed the fundamentals of how organisations operate and interact with customers, and how they can win the competition in an increasingly fierce market. Digital transformation has brought about a significant business paradigm shift. Today, more organisations are integrating digital technology as an integral part of their business model. It is no longer an option, but a necessity to maintain relevance and competitiveness in the market.",https://www.semanticscholar.org/paper/97b2a4430df3b9fe6b16000897348ebf1994bcae,IS,Qualitative
Facebook and the creation of the metaverse: radical business model innovation or incremental transformation?,"Purpose In a move characterized by ambiguity, Facebook changed its name to Meta in October 2021, announcing a new era of social interaction, enabled by the metaverse technology that appears poised to become the future center of gravity for online social interactions. At first glance, the communicated change signals a radically new business model (BM) based on an unprecedented configuration of the three following components: value creation, value proposition and value capture. The purpose of this paper is to analyze Facebook’s announced changes in its BM to clarify whether the change is as radical as communicated or rather represents an incremental transformation of the current BM.Design/methodology/approach This investigation adopted an in-depth case study research method. The process included using a structured approach to collect 153 data points, including academic studies and publicly available information, followed by qualitative content analysis.Findings The results of our analysis of Facebook’s entrepreneurial journey indicate that the communicated strategic refocusing does not correspond to a radical BM innovation pattern. Even though Facebook’s BM might evolve into the innovation phase, as the current changes appear very futuristic, the authors estimate that the core elements of the BM will change incrementally. The investigation indicates that the underlying logic of the straightforward communicative efforts primarily serves two purposes: to improve the external perception of the company and to disseminate an internal change signal within the organization.Originality/value This paper is the first study that takes an entrepreneurship and BM perspective in analyzing Facebook’s approach in rebranding to Meta and refocusing its strategy on building the metaverse. The academic and practical relevance, as well as the potential future impact on business and society, makes the investigation of this case an intriguing prospect. Additionally, the study illuminates the difference between the communicated vision and the real impact on the business, suggesting critical questions about future large-scale rebranding efforts and their effects.",https://www.semanticscholar.org/paper/bc11d671731a55967460c5602f85d01f336bcd3c,IS,Qualitative
REGIONAL,"Financial accountability is the responsibility for financial management in the form of financial reports through the use of information technology, which is expected to be able to support regional governments in carrying out better regional financial management as well as having competent human resources in their management. This study aims to provide information about the usefulness of information technology and human resource competence in the accountability region financial report. This research was conducted in three regional expansion in Lampung Province with a descriptive qualitative method with the help of NVivo software for data analysts. The result showed that to achieve regional financial accountability is supported by utilizing information technology in the form of a regional management information system, while to be able to operate the SIMDA application program require adequate human resource and training is given to increase competence.",https://www.semanticscholar.org/paper/a46ff0bb25b1bba25f5040490de327a0d0c75a7f,IS,Qualitative
Barriers to digital inclusion among older people: a intergenerational reflection on the need to develop digital competences for the group with the highest level of digital exclusion,"Over the past thirty years and with the rise of the digital society, the process of digital exclusion has become increasingly noticeable and represents a sub-type of social exclusion. Shaping digital competences in the era of the intensive development of the information society requires constant reflection on the effectiveness of such activities. This article looks at what kind of barriers are currently blocking the development of digital competences among older people. Using structured interviews, responses were obtained from 30 respondents in Poland, though the respondents themselves did not belong to the demographic of older people. The respondents identified eight main types of barriers to digital inclusion for older people: 1) Fear of new technologies, 2) No need to use ICT, 3) Self-marginalisation in the information society, 4) The characteristics of new media, 5) Attitude to Life-Long Learning, 6) Physical limitations, 7) Economic determinants, and 8) Infrastructural limitations. The results of the qualitative research provide a fresh look at the process of the formation of digital competence among vulnerable groups within the wider process of digital inclusion. This article is the result of an international project REMEDIS supported in Poland by the National Science Centre - NCN [021/03/Y/HS6/00275].",https://www.semanticscholar.org/paper/3273302b780436b3f9b5e31970109ae7983c6086,IT,Qualitative
Analisis Kurikulum Merdeka Belajar Dalam Memfasilitasi Pembelajaran Abad Ke-21 Pada Siswa Menengah Atas,"The independent curriculum is an educational approach that aims to provide independence to students in their learning so that they can optimally develop their potential. In the era of globalization and the rapid development of information technology, 21st-century learning is an urgent need. This research aims to describe the role of the independent learning curriculum in facilitating 21st-century learning in senior high schools. The method used in this research is descriptive-qualitative with a case study design. The research was conducted at SMAN 2, Merekau, South Papua. Data collection techniques used in this research include in-depth interviews, observation, and documentation. The results showed that: 1) the independent learning curriculum for facilitating 21st century learning is good. It is proven that the learning tools of SMAN 2 Merauke have successfully integrated 21st century skills (collaboration, communication, critical thinking, creativity, character, and citizenship) into the learning tools. 2) The role of the principal and teachers: The principal as the person in charge and teachers as the implementers of learning play an important role in the successful implementation of an independent curriculum with 21st century skills.",https://www.semanticscholar.org/paper/f772f0e3d189a67b11cdc114e8fe0bafe80f3e5f,CS,Qualitative
An overview of Indonesian regulatory framework on Islamic financial technology (fintech),"The advancement of technology has touched many sectors, including financial industries. The emergence of Financial Technology or Fintech has changed the way people do business transactions. Indonesia as a country with a separate regulatory model under the Central Bank of Indonesia (BI) and Financial Services Authority (OJK) has issued several regulations to regulate fintech, which includes Islamic fintech. This paper aims to analyse the existing regulatory framework of Islamic fintech in Indonesia and gives some recommendations, if any, to improve Islamic fintech development in Indonesia. It uses qualitative methodology by extracting information from works of literature and existing regulations. This research finds that both BI and OJK have significant roles in regulating Islamic fintech without issuing any provision to address it specifically. However, it is complemented by the fatwa issued by the National Shariah Board of Indonesian Ulama (DSN MUI).",https://www.semanticscholar.org/paper/d7fb6a8d26be0dcfc5deb4b626a902806a8f0dde,IS,Qualitative
Changes in Media Consumption Patterns and their Implications for People's Cultural Identity,"In the era of globalisation and the rapid development of information technology, mass media has become an integral part of people's lives. Media is not only a source of information, but also plays an important role in shaping a society's views, values and cultural identity. This research aims to analyse changes in media consumption patterns and their implications for people's cultural identity. This research is a literature review that uses a qualitative method approach, which means it will analyse and interpret data by relying on information and texts from various sources. The results show that changes in media consumption patterns in the era of globalisation have a significant impact on people's cultural identity. Easier access to different types of media content from around the world has opened up opportunities for cross-cultural interaction and information dissemination. However, this also brings challenges in maintaining a balance between the preservation of local cultural identity and the influence of global culture. People's cultural identity is formed through a complex dynamic between traditions, values and the influence of modern media.",https://www.semanticscholar.org/paper/f0d0a1750927d19d4580585e206b6a43f9bba6da,IS,Qualitative
Agricultural Technology Transfer Preferences of Smallholder Farmers in Tunisia’s Arid Regions,"The objective of this research study was to assess the sources of information on two improved agricultural and livestock technologies (barley variety and feed blocks) as well as the efficacy of numerous agricultural technology diffusion means introduced in the livestock–barley system in semi-arid Tunisia. The research used primary data collected from 671 smallholder farmers. A descriptive statistical analysis was conducted, and Kendall’s W-test and the chi-squared distribution test were deployed to categorize and evaluate the efficacy of the different methods of technology diffusion used by the Tunisian extension system. To address farmers’ perceived opinions and classify the changes from the use of the improved technologies, a qualitative approach based on the Stapel scale was used. Farmer training, demonstration, and farmer-to-farmer interactions were perceived as the most effective agricultural extension methods. The access to technology, know-how, adoption cost of that technology, and labor intensity for adoption influenced its adoption level. Farmers’ opinions about the changes resulting from the adoption of both technologies revealed that yield and resistance to drought were the most important impacts of the two technologies. The study recommends empowering the national extension system through both conventional and non-conventional technologies (ICT, video, mobile phones, etc.), given the cost-effectiveness and their impact on the farmers’ adoption decisions.",https://www.semanticscholar.org/paper/640c3fc01a67684740517e6bbbc19a99a9b178ab,IS,Qualitative
The Research Progress of Electrical Impedance Tomography for Lung Monitoring,"Medical imaging can intuitively show people the internal structure, morphological information, and organ functions of the organism, which is one of the most important inspection methods in clinical medical diagnosis. Currently used medical imaging methods can only be applied to some diagnostic occasions after qualitative lesions have been generated, and the general imaging technology is usually accompanied by radiation and other conditions. However, electrical impedance tomography has the advantages of being noninvasive and non-radiative. EIT (Electrical Impedance Tomography) is also widely used in the early diagnosis and treatment of some diseases because of these advantages. At present, EIT is relatively mature and more and more image reconstruction algorithms are used to improve imaging resolution. Hardware technology is also developing rapidly, and the accuracy of data collection and processing is continuously improving. In terms of clinical application, EIT has also been used for pathological treatment of lungs, the brain, and the bladder. In the future, EIT has a good application prospect in the medical field, which can meet the needs of real-time, long-term monitoring and early diagnosis. Aiming at the application of EIT in the treatment of lung pathology, this article reviews the research progress of EIT, image reconstruction algorithms, hardware system design, and clinical applications used in the treatment of lung diseases. Through the research and introduction of several core components of EIT technology, it clarifies the characteristics of EIT system complexity and its solutions, provides research ideas for subsequent research, and once again verifies the broad development prospects of EIT technology in the future.",https://www.semanticscholar.org/paper/2b818186ef2cef813ed2391c76c14c29cd2fc9f9,CS,Qualitative
impact of technology on teaching and teaching English to elementary school students,"This study reveals the impact of technology on teaching and teaching English to elementary school students, and shows how technology can help the process teaching and learning. The aim of the study is to show the importance of technology and to present the main points of teaching with technology. To have a wider understanding about the topic a survey and the overall observation is conducted in three elementary schools. The data are collected by using qualitative and quantitative research methods, using questionnaires with 400 students and the observations in the classrooms. The study was completed using primary data which were the questionnaires and the secondary data which were books, journals, articles and different websites. This study provides important information about teaching English with technology and how to integrate technology in everyday teaching. It helps teachers and educators understand technology and improve teaching.",https://www.semanticscholar.org/paper/a8e40cdec4ec9d8b42a8302faad93ac3b1f04b0e,IS,Qualitative
Internet of Things (IoTs) Effects and Building Effective Management Information System (MIS) in Vietnam Enterprises and Human-Computer Interaction Issues in Industry 4.0,"In recent years there is a rising need in Vietnam enterprises to build an effective management information system in order to deliver better information for management levels to make a proper decision. This paper mainly use qualitative analysis with statistics, synthesis and inductive methods, combine with dialectical materialism methods. Research results indicate that in industry 4.0 under Internet of Things (IoTs) effects, Vietnam enterprises such as hospitals and renewable energy companies or manufacturing firms can use advanced cloud technology to store and process big data of clients to serve for a better MIS system or risk MIS system. IoTs together with Big Data and cloud technology also prove better solutions to accounting and human resources of businesses. Last but not least, this study also propose some solutions to deal with challenges in constructing risk information system in Vietnam enterprises during covid 19 impacts. For instance, we need to invest more on technology infrastructure and take advantage of IoTs effects to build effective MIS for companies to make sound decisions.",https://www.semanticscholar.org/paper/c52f07b7e4f1c0ad81b922eaa3411e16c4a49392,IS,Qualitative
Artificial intelligence (AI) library services innovative conceptual framework for the digital transformation of university education,"PurposeArtificial intelligence (AI) is one of the latest digital transformation (DT) technological trends the university library can use to provide library users with alternative educational services. AI can foster intelligent decisions for retrieving and sharing information for learning and research. However, extant literature confirms a low adoption rate by the university libraries in using AI to provide innovative alternative services, as this is missing in their strategic plan. The research develops (AI-LSICF) an artificial intelligence library services innovative conceptual framework to provide new insight into how AI technology can be used to deliver value-added innovative library services to achieve digital transformation. It will also encourage library and information professionals to adopt AI to complement effective service delivery.Design/methodology/approachThis study adopts a qualitative content analysis to investigate extant literature on how AI adoption fosters innovative services in various organisations. The study also used content analysis to generate possible solutions to aid AI service innovation and delivery in university libraries.FindingsThis study uses its findings to develop an Artificial Intelligence Library Services Innovative Conceptual Framework (AI-LSICF) by integrating AI applications and functions into the digital transformation framework elements and discussed using a service innovation framework.Research limitations/implicationsIn research, AI-LSICF helps increase an understanding of AI by presenting new insights into how the university library can leverage technology to actualise innovation in service provision to foster DT. This trail will be valuable to scholars and academics interested in addressing the application pathways of AI library service innovation, which is still under-explored in digital transformation.Practical implicationsIn practice, AI-LSICF could reform the information industry from its traditional brands into a more applied and resolutely customer-driven organisation. This reformation will awaken awareness of how librarians and information professionals can leverage technology to catch up with digital transformation in this age of the fourth industrial revolution.Social implicationsThe enlightenment of AI-LSICF will motivate library professionals to take advantage of AI's potential to enhance their current business model and achieve a unique competitive advantage within their community.Originality/valueAI-LSICF development serves as a revelation, motivating university libraries and information professionals to consider AI in their strategic plan to enable technology to support university education. This act will enable alternative service delivery in the face of unforeseen circumstances like technological disruption and the present global COVID-19 pandemic that requires non-physical interaction.",https://www.semanticscholar.org/paper/2cbe9b8ec3f192b2d1deb54e37bc2faf3989cabe,IS,Qualitative
Telemedicine Practice: Review of the Current Ethical and Legal Challenges,"Background: Telemedicine involves medical practice and information and communications technology. It has been proven to be very effective for remote health care, especially in areas with poor provision of health facilities. However, implementation of these technologies is often hampered by various issues. Among these, ethical and legal concerns are some of the more complex and diverse ones. In this study, an analysis of scientific literature was carried out to identify the ethical and legal challenges of telemedicine. Materials and Methods: English literature, published between 2010 and 2019, was searched on PubMed, Scopus, and Web of Science by using keywords, including “Telemedicine,” “Ethics,” “Malpractice,” “Telemedicine and Ethics,” “Telemedicine and Informed consent,” and “telemedicine and malpractice.” Different types of articles were analyzed, including research articles, review articles, and qualitative studies. The abstracts were evaluated according to the selection criteria, using the Newcastle–Ottawa Scale criteria, and the final analysis led to the inclusion of 22 articles. Discussion: From the aforementioned sample, we analyzed elements that may be indicative of the efficacy of telemedicine in an adequate time frame. Ethical aspects such as informed consent, protection data, confidentiality, physician's malpractice, and liability and telemedicine regulations were considered. Conclusions: Our objective was to highlight the current status and identify what still needs to be implemented in telemedicine with respect to ethical and legal standards. Gaps emerged between current legislation, legislators, service providers, different medical services, and most importantly patient interaction with his/her data and the use of that data.",https://www.semanticscholar.org/paper/776479f3de3f520717b8fbd6ffe109a02759132f,IS,Qualitative
Factors Influencing Crisis Management: A systematic review and synthesis for future research,"Abstract The purpose of this study is to provide a comprehensive systematic literature review (SLR) of factors influencing crisis management (CM). The study attempts to assess the main areas that have been linked to and studied CM, and the research outlets that have been provided these research. The study adopts a qualitative approach and uses SLR method to collect relevant data. The study surveyed 223 studies from different research outlets, including the most reputed publishers; Emerald, Wiley, Elsevier, Springer, Taylor & Francis, SAGE, and Inderscience. The extracted articles are categorized into 8 areas based on their effect on CM. The most important factors are communication and social media, which have 66 studies with 4039 citations, leadership which have 40 studies with 2315 citations, followed by knowledge, governance, information technology, strategic planning, and professional entities, which have 38, 24, 23, 16, and 16 manuscripts with 2109, 1738, 301, 548, and 160 citations, respectively. The current study provides an open insight for academicians and researchers on the main areas of CM investigated by prior studies. It provides a novel contribution and comprehensive understating through highlighting what has been done and what is left to be done in respect to crisis management.",https://www.semanticscholar.org/paper/5a19dec076388889de51afe031c31fa7752d5529,CS,Qualitative
Opportunities in emerging technologies for Southern Africa: How the Global South should adopt to take advantage?,"The manufacturing sector, energy sector, and mobility businesses can all benefit from the use of emerging technology. A crucial part of modernizing industries is utilizing emerging technologies. This research aimed to examine how the Global South can adopt and maximize emerging technology opportunities. The research was qualitative in nature, and the focus was on economies in the Global South. Random sampling to select 40 economies from the four continents that fall under the Global South which were Africa, Asia and the Pacific, the Arab Region, Latin America, and the Caribbean. Research articles on the subject matter were collected from Google Scholar and other online repositories. Document analysis was used under the data analysis section. This research aimed to examine how the Global South can adopt and maximize emerging technology opportunities. The research findings described challenges, such as poor technological policies, high prices, and taxation, a lack of financial resources, a lack of information and communication technology (ICT) infrastructure and limited experts, digital illiteracy, and how illicit financial flows adversely affect the Global South's ability to tap into emerging technologies. In terms of policy recommendations, it was suggested that the Global South needs to design and implement robust ICT policies, mobilize financial resources, and invest in research and development. These policies must be implemented in the primary, secondary, and tertiary sectors to ensure meaningful benefits of using emerging technologies.",https://www.semanticscholar.org/paper/e98ebfc4e99066835e44db5a6d0e1dd2969663fc,CS,Qualitative
University Students’ Engagement in Mobile Learning,"The implementation of mobile learning seems to be an emerging topic in many educational institutions. As recently noticed, mobile technology has employed wireless technologies to communicate, think, learn, and share in order to spread and exchange information. Therefore, using mobile technologies in learning and teaching can create a positive environment in higher education. Hence, the purpose of this study is to evaluate mobile learning engagement among educational technology students. Data from three focus group discussions and 15 semi-structured interviews with students who experienced mobile learning were gathered using a qualitative approach design. A total of seventeen basic themes and four organizing themes were extracted, where the researchers categories of engagement, i.e., social engagement, cognitive engagement, emotional engagement, and behavioral engagement. In the present research, the findings indicate that social engagement themes included social–mobile interaction, building community, developing relationships, and competition. The cognitive engagement themes included attention, cognitive and meta-cognitive strategies, immersion, and cognitive curiosity. Emotional engagement themes included excitement and enjoyment, instructor comforting students, motivation, and emotional safety. Behavioral engagement themes included effort and time on task, attendance, participation, and positive conduct.",https://www.semanticscholar.org/paper/ccdfcce99e157cfbdaf03c77c1ce5a612855fea7,IS,Qualitative
Teacher Learning in Difficult Times: Examining Foreign Language Teachers’ Cognitions About Online Teaching to Tide Over COVID-19,"The sudden global outbreak of COVID-19 in late 2019 has led to thriving online teaching, including the teaching of languages, across the world. As the online teaching of English-as-a-foreign-language (EFL) in Chinese universities is facing new challenges, EFL teachers have been positively exploring new solutions. To understand how EFL teachers were coping with the challenges, we set up this research as part of a larger study to examine EFL teachers’ cognitions about online teaching in response to the disruption of normal teaching plans. We did so by taking a qualitative approach through analyzing in-depth interviews with three EFL teachers from a Chinese university. Through thematic analysis we found that teachers had clear cognitions about features, advantages, and constraints of online EFL teaching and that they acquired information and communication technology (ICT) literacy through understanding students’ learning needs, online teaching practice, and the necessity of integrating traditional classroom teaching methods into online delivery. We conclude this study with a discussion on its pedagogical implications for similar contexts or colleagues facing similar challenges in other parts of the world.",https://www.semanticscholar.org/paper/dd7fa92c3f82f5fc74c1dc96ad56bb4ca7c6006c,CS,Qualitative
Product and Service Innovation Strategies to Expand MSME Markets,"Micro, Small, and Medium Enterprises are one of the important pillars in a country's economy. Despite their strategic role, MSMEs often face a number of challenges, such as intensifying competition, changing consumer tastes, evolving technology, and access to a wider market. This study aims to analyse product and service innovation strategies to expand the MSME market. This research employs a qualitative research approach, more precisely, a literature review, where data will be examined and explained by drawing upon information and textual materials collected from a variety of sources. The study results show that innovation in products and services plays a key role in the growth and sustainability of micro, small and medium enterprises. MSMEs play a vital role in the economy by creating jobs and contributing to sustainable economic growth. Innovation differentiates MSMEs from their competitors in an increasingly globalised and dynamic business environment. Innovative products and services can attract customer attention, provide a competitive advantage, improve product quality, and build long-term relationships with customers.",https://www.semanticscholar.org/paper/f588e4ae775d2b276628d3a90d41e623e28581f7,IS,Qualitative
Determinasi Kemudahan Akses Informasi bagi Keputusan Investasi Gen Z,"This research aims to determine the ease of access to information for Gen Z investment decisions. This research uses descriptive qualitative methods and literature reviews. Qualitative research is also intended to understand how someone thinks, acts and obtains information from a phenomenon. The data used is narrative and qualitative, obtained from previous research documents and obtained novelty in this research. Data analysis used in this qualitative research is the Miles and Huberman interactive model. This qualitative research is exploratory in nature. The type of data used is secondary data. The focus of this research is ease of access to information. The results of this research are: 1) That the use of technology and digital platforms for easy access to information for Gen Z in investing is quite good, where Gen Z uses social media as a platform to gain knowledge regarding good investment instruments, a means of discussion and exchanging views between Gen investors. Z, then the platform that provides easy access to information for Gen Z in investing is quite varied, from the Bibit, DANAeMAS, Ajaib, Tokopedia to Shopee applications, apart from that, payments made can also utilize digital banking and digital wallet payments; 2) Generation Z or Gen Z chooses forms of investment instruments with (high risk high return) high risk high returns such as stocks and cryptocurrencies, while generation Y and generation X choose forms of investment instruments with (low risk low return) low risk low returns such as jewelry gold and mutual funds; and 3) The amount of investment made by Gen Z varies, where in the July 2023 period it was IDR 7,658,506, while in the August 2023 period it was IDR 7,645,956.",https://www.semanticscholar.org/paper/e60c30ae728840370bd1265e4364125fd42eeebb,IS,Qualitative
Sustainable Digital Marketing Strategy for Long-Term Growth of MSMEs,"In recent years, the advancement of digital technology has brought fundamental changes in consumer behavior regarding information seeking, product comparison, and purchasing. Small and Medium Enterprises (MSMEs) need to adapt to these trends to remain competitive and thrive. This research aims to examine sustainable digital marketing strategies for the long-term growth of MSMEs. The approach involves a qualitative analysis of literature, intending to obtain a comprehensive understanding of the subject from 2018 to 2023. The study's findings indicate that, in the face of increasingly fierce competition in the digital era, MSMEs must adopt sustainable digital marketing strategies to ensure the long-term growth of their businesses. Market and audience understanding, continuity in branding, SEO optimization, social media utilization, content marketing, data analysis, email marketing, and investment in learning and innovation are key elements in achieving these goals.",https://www.semanticscholar.org/paper/32df58a0350a4476fddbfb804b4ea371aab07902,IS,Qualitative
A systematic literature review of healthcare supply chain and implications of future research,"Purpose This paper aims to review the healthcare supply chain (HSC) literature along various areas and to find out the gap in it. Design/methodology/approach In total, 143 research papers were reviewed during 1996-2017. A critical review was carried out in various dimensions such as research methodologies/data collection method (empirical, case study and literature review) and inquiry mode of research methodology (qualitative, quantitative and mixed), country-specific, targeted area, research aim and year of publication. Findings Supply chain (SC) operations, performance measurement, inventory management, lean and agile operation, and use of information technology were well studied and analyzed, however, employee and customer training, tracking and visibility of medicines, cold chain management, human resource practices, risk management and waste management are felt to be important areas but not much attention were made in this direction. Research limitations/implications Mainly drug and vaccine SC were considered in current study of HSC while SC along healthcare equipment and machine, hospitality and drug manufacturing related papers were excluded in this study. Practical implications This literature review has recognized and analyzed various issues relevant to HSC and shows the direction for future research to develop an efficient and effective HSC. Originality/value The insight of various aspects of HSC was explored in general for better and deeper understanding of it for designing of an efficient and competent HSC. The outcomes of the study may form a basis to decide direction of future research.",https://www.semanticscholar.org/paper/3b477901f8ef2bc50e03b6505e493afdd18bd16d,IS,Qualitative
Learning Purposive Communication: A Personal Narrative Experience of Non-Teacher Education Students,"This research explored the learning experiences of non-teacher education students enrolled in Purposive Communication at Sultan Kudarat State University-Kalamansig Campus. The study employed a qualitative design and non-probability purposive sampling. This study included six(6) non-teacher education students participants, such as students taking BS in Information Technology, BS in Biology, and BS in Fisheries. For data collection and collection, a semi-structured interview guide question was used. Colizzi’s thematic analysis was employed to analyze the data. The learning experiences of non-teacher education students in purposive communication revealed that they enjoyed learning Purposive communication through hybrid learning. Various performance tasks were provided for the students to perform, and these tasks are based on real-life experience. In performing these tasks, English was evident as a medium of instruction. Non-teacher education students can integrate responsive examples in the community and the global arena. Further, the most common challenges experienced by non-teacher education students were technical issues, distractions, time management, understanding course expectations, lack of in-person interaction, adapting to unfamiliar technology, and uncertainty about the future. It is recommended that the institution implement a monitoring strategy that would sustain the teaching methods employed by the teachers to make the teaching-learning process effective and productive in Purposive communication. The institution must provide sound-based classroom management during the new normal education and religiously implement hybrid learning to maintain the learning process. The institution should provide learning tools for poor but deserving non-teacher education students to assist them in learning Purposive communication even in times of educational crisis.",https://www.semanticscholar.org/paper/c8febd2a183c905520a8a3d3f82095fa0719099f,IS,Qualitative
The Used of Technology to Improve Health Social Security Agency Services in Indonesia,"The purpose of this study is to find out and understand the use of technology carried out by BPJS management in providing services to the Indonesian people. This study uses a qualitative method is a method that focuses on in-depth observations. Therefore, the use of qualitative methods in research can result in a more comprehensive study of a phenomenon. The researcher uses reviews from several JKN Mobile users by dividing them into 2 groups for closed discussions using a Zoom breakout room. There were 112 participants who were also informants and respondents who came from various regions. The development of the BPJS Health Mobile application is a tangible manifestation of BPJS Health's commitment to providing easy access and optimal services for participants. Through this application, participants can access various information related to the National Health Insurance program organized by BPJS Health quickly and easily, wherever and whenever.",https://www.semanticscholar.org/paper/9e8421b511f4cb6d9c466479652b481c5195682d,IS,Qualitative
The Bright and Dark Sides of Technostress: A Mixed-Methods Study Involving Healthcare IT,"Today’s healthcare workers, specifically nurses, are experiencing technostress associated with the use of healthcare information technology (HIT). Technostress is often characterized by IS researchers as negative, or as being on the “dark side” of technology. However, a broader reading of the stress literature suggests that technostress may be both positive and negative, and can therefore have a “bright side” in addition to a dark side. The objective of this study is to conceptualize a holistic technostress process that includes positive and negative components of technostress embedded in two subprocesses: the techno-eustress subprocess and the techno-distress subprocess, respectively. The study instantiates this holistic technostress model through a sequential mixed-methods research design in the context of HIT. Phase 1 of the design is a qualitative, interpretive case study involving interviews with 32 nurses. Based on the findings from the case study, the paper builds a research model that operationalizes the concepts embedded in the holistic technostress model and identifies contextually relevant challenge and hindrance technostressors and outcomes. In Phase 2, the research model is empirically validated by analyzing survey data collected from 402 nurses employed in the United States. Results reveal that several challenge and hindrance technostressors are related to positive and negative psychological responses, respectively, and that such responses are related to job satisfaction and attrition, which impact turnover intention. Contributions to theory and practice are also discussed.",https://www.semanticscholar.org/paper/1897c28ab22f19ed634820ca58810d782b87cb39,IS,Qualitative
The Rising Importance of AI in Boosting the Efficiency of Online Advertising in Developing Countries,"There hasn't been enough study into artificial intelligence. Digital marketing may be taken to greater levels with the help of deep learning machines and AI. The potential of online marketing is likely to lie in the positive reactions of a wide range of Indian consumers to sales automation technology, thus this research seeks to assess the results of that research. We discover that software engineers need to work in tandem with online marketing who use algorithms with deep learning to account for consumer mentality, behavior, and preferences while designing the architecture. As a result, in the future, firms will reap enormous advantages as precise consumer data is readily accessible to marketers. Predicting the machines' behavior under different situations is made possible by a causal model based on regression analysis. Data was analyzed using SPSS version 20 and R software, and information was gathered about customers' actions, preferences, and feelings in order to determine, through a fuzzy-set qualitative analysis (fsQCA) approach, whether or not the engine's services could be effective in persuading them to make use of the latter.",https://www.semanticscholar.org/paper/c166150cd44b5d17b680b7835bb0e6708306f30c,CS,Qualitative
Digital leadership role in developing business strategy suitable for digital transformation,"Businesses must respond to the ecology in which they operate. Especially the rapid transformation of technology has increased the degree of dependency on the system. The main reason for this is perceived only as the technology costs brought by digital transformation. However, we understand from the bankruptcy of economically strong companies that this is not the real problem. This study looks at it from the perspective of leadership, which is an important skill for businesses. The research focuses on leadership roles needed to adapt to digital transformation. At this point, the roles of digital leadership and its contribution to businesses were investigated. At this point, we try to reveal the role of digital leadership with two different qualitative analyzes. In the research, semi-structured interviews were conducted with senior managers, phenomenological and content analysis was performed using Nvivo and MAXQDA qualitative analysis programs, and relevant confidential information was revealed. As a result of the research, it has been determined that there is an important link between time management and productivity while supporting system efficiency and transformation adaptation. In other words, a positive relationship has been determined between the success of digital transformation and digital leadership roles. In digital transformation, digital leadership has a role in the positive development of the relationship between the digital transformation process and business strategies. As a result, a perspective on how digital leadership can contribute to businesses that want to develop strategies suitable for the digital transformation process is presented.",https://www.semanticscholar.org/paper/1d928d66aafca1a2d8353f7c66f3e81f1c39374d,IS,Qualitative
Analysis of Factors Leading to E-commerce Adoption,"E-commerce, in particular, is making extensive use of the latest innovations in information technology. The rise of online shopping provides companies with a new channel for expanding their operations. Despite this, many individuals lack an understanding of what drives businesses to embrace e-commerce. The purpose of this research is to illuminate the considerations that companies take into account when deciding to implement e-commerce. This research used a qualitative descriptive approach and a literary analysis to gather information. The findings of this research indicate that there are a variety of organizational elements, including technological, organizational, and environmental, that contribute to the adoption of e-commerce. The study's findings should provide further context for discussions of the elements that influence businesses' decisions to adopt e-commerce.",https://www.semanticscholar.org/paper/09c437d794bdeb5cff670f5b80d0317de377a7d7,IS,Qualitative
Understanding security in the government's use of blockchain technology with value focused thinking approach,"The purpose of the research is to assess security concerns in government's use of Blockchain technology through the lens of value focused thinking (VFT) approach.,Qualitative methods such as open ended questions and interviews were used to gather data and the data was analysed in a structured approach to formulate a means-ends network of objectives. The data were collected from eight interviews of selected participants.,The findings are 35 objectives of which ten are fundamental objectives related to the values of identity, trust, data privacy, transparency, integrity, public service delivery, cost, availability of public information, responsibility and usability.,The study implies that governments or private organisations building blockchain solutions for governments can benefit from insights on values to focus on in blockchain security; and the current research serves as a base to understand more on blockchain use in the governments as well as security values and application of VFT approach for future academic researchers in this area.,It is a need-driven approach in which blockchain is assessed to ensure its fit to societal needs and public values.,It is first of its kind in studying security in blockchain use by government through the lens of VFT approach, and it provides insights of values that are of importance to further blockchain use in the government.",https://www.semanticscholar.org/paper/cc5b6e784c7bfe0c4091de2dcfcb316a5f82c005,CS,Qualitative
"Big data empowered agility for dynamic, volatile, and time-sensitive service industries: the case of tourism sector","Purpose Dynamic, volatile, and time-sensitive industries, such as tourism, travel and hospitality require agility and market intelligence to create value and achieve competitive advantage. The aim of the current study is to examine the influence of big data (BD) on the performance of service organizations and to probe for a deeper understanding of implementing BD, based on available technologies. Design/methodology/approach An ethnographic study was conducted following an abductive approach. A primary qualitative research scheme was used with 35 information technology and database professionals participating in five online focus groups of seven participants each. Analytical themes were developed simultaneously with the literature being revisited throughout the study to ultimately create sets of common themes and dimensions. Findings BD can help organizations build agility, especially within dynamic industries, to better predict customer behavioral patterns and make tailor-made propositions from the BD. An integrated BD-specific framework is proposed to address value according to the dimensions of need, value, time and utility. Research limitations/implications Little research exists on the key drivers of BD use for dynamic, real-time and agile businesses. This research adds to the developing literature on BD applications to support organizational decision-making and business performance in the tourism industry. Originality/value This study responds to scholars’ recent calls for more empirical research with contextual understanding of the use of BD to add value in marketing intelligence within business ecosystems. It delineates factors contributing to BD value creation and explores the impacts on the respective service encounters.",https://www.semanticscholar.org/paper/93fcbe7932b1d6fe77e8ab1a5791058c80e229d9,IS,Qualitative
Information and communication technologies as a source of education,"It is well known that the application of ICT in education helps us to develop skills for competent and efficient problem solving in the field of professional activities. This paper aims to examine the attitudes of students towards the use of ICT as a source of education. The methodology adopted was that of original research, with a qualitative and quantitative approach. This study included 175 students from Serbia and Bulgaria. The variables of this study were gender and the years of study. In order to assess the attitudes of students towards the use of ICT in education a five-point Likert scale was used across forty-five statements. The results of the research show that students are very interested in using ICT as a source of education. Thus, it is evident that information and communication technologies have become very important pedagogical resources in approaching the teaching and learning processes in an innovative way. Keywords: attitudes of students; ICT in education; ICT; education technology; education.",https://www.semanticscholar.org/paper/178d4ff8909aa7f38960e49875f9cc866fd5904d,IS,Qualitative
An investigation on the influencing factors of elderly people's intention to use financial AI customer service,"PurposeWith the great changes brought by information technology, there is also a challenge for the elderly's acceptance. This study aimed to determine the antecedents of elderly people's usage intention of financial artificial intelligent customer service (FAICS) and to examine the relationships between various factors and thus to help them better adapt to the digital age.Design/methodology/approachA mixed method, including the qualitative and quantitative study, was utilized to explore answers of the research questions. As the qualitative study, the authors used semi-structured interviews and data coding to uncover the influencing factors. As the quantitative study, the authors collected data through questionnaires and tested hypotheses using structural equation modeling.FindingsThe results of data analysis from interviews and questionnaires suggested that perceived anthropomorphism and virtual identity of elderly users have a positive impact on their perceived ease of use, and the perceived intelligence of elderly users positively influences their perceived ease of use, satisfaction and perceived usefulness. Additionally, the elderly's cognition age can moderate the effects of perceived usefulness and satisfaction on their usage intention of FAICS.Originality/valueThis study contributes to the literature by taking the elderly group as the research participants and combining those influencing factors with technology acceptance model and information systems success model. The findings provide a basis for accelerating the promotion of FAICS and help address the problem that the elderly have difficulty adapting to a new technology.",https://www.semanticscholar.org/paper/6a07334d5a6a2fd8c6b6be20f0d565d97f39010c,IS,Qualitative
Emerging Big Data Sources for Public Transport Planning: A Systematic Review on Current State of Art and Future Research Directions,"The rapid advancement of information and communication technology has brought a revolution in the domain of public transport (PT) planning alongside other areas of transport planning and operations. Of particular significance are the passively generated big data sources (e.g., smart cards, detailed vehicle location data, mobile phone traces, social media) which have started replacing the traditional surveys conducted onboard, at the stops/stations and/or at the household level for gathering insights about the behavior of the PT users. This paper presents a systematic review of the contemporary research papers related to the use of novel data sources in PT planning with particular focus on (1) assessing the usability and potential strengths and weaknesses of different emerging big data sources, (2) identifying the challenges and highlighting research gaps. Reviewed articles were categorized based on qualitative pattern matching (similarities/dissimilarities) and multiple sources of evidence analysis under three categories—use of big data in (1) travel pattern analysis, (2) PT modelling, and (3) PT performance assessment. The review revealed research gaps ranging from methodological and applied research on fusing different forms of big data as well as big data and traditional survey data; further work to validate the models and assumptions; lack of progress on developing more dynamic planning models. Findings of this study could inform transport planners and researchers about the opportunities/challenges big data bring for PT planning. Harnessing the full potential of the big data sources for PT planning can be extremely useful for cities in the developing world, where the PT landscape is changing more rapidly, but traditional forms of data are expensive to collect.",https://www.semanticscholar.org/paper/ab0e4b8c50929d9f32250f7cc059f77847919398,CS,Qualitative
Acceptance of an Informational Antituberculosis Chatbot Among Korean Adults: Mixed Methods Research,"Background Tuberculosis (TB) is a highly infectious disease. Negative perceptions and insufficient knowledge have made its eradication difficult. Recently, mobile health care interventions, such as an anti-TB chatbot developed by the research team, have emerged in support of TB eradication programs. However, before the anti-TB chatbot is deployed, it is important to understand the factors that predict its acceptance by the population. Objective This study aims to explore the acceptance of an anti-TB chatbot that provides information about the disease and its treatment to people vulnerable to TB in South Korea. Thus, we are investigating the factors that predict technology acceptance through qualitative research based on the interviews of patients with TB and homeless facility personnel. We are then verifying the extended Technology Acceptance Model (TAM) and predicting the factors associated with the acceptance of the chatbot. Methods In study 1, we conducted interviews with potential chatbot users to extract the factors that predict user acceptance and constructed a conceptual framework based on the TAM. In total, 16 interviews with patients with TB and one focus group interview with 10 experts on TB were conducted. In study 2, we conducted surveys of potential chatbot users to validate the extended TAM. Survey participants were recruited among late-stage patients in TB facilities and members of web-based communities sharing TB information. A total of 123 responses were collected. Results The results indicate that perceived ease of use and social influence were significantly predictive of perceived usefulness (P=.04 and P<.001, respectively). Perceived usefulness was predictive of the attitude toward the chatbot (P<.001), whereas perceived ease of use (P=.88) was not. Behavioral intention was positively predicted by attitude toward the chatbot and facilitating conditions (P<.001 and P=.03, respectively). The research model explained 55.4% of the variance in the use of anti-TB chatbots. The moderating effect of TB history was found in the relationship between attitude toward the chatbot and behavioral intention (P=.01) and between facilitating conditions and behavioral intention (P=.02). Conclusions This study can be used to inform future design of anti-TB chatbots and highlight the importance of services and the environment that empower people to use the technology.",https://www.semanticscholar.org/paper/c93eee857d5065e6cd6f0842e9be8caaf647ff85,IS,Qualitative
Implementation of the “Merdeka Belajar” Curriculum in the Industrial 4.0 Era,"Revolution 4.0 is marked by the era of digitalization, namely by the rapid development of science and technology. Technology in the industrial era 4.0 caused the human role to be shifted. This causes changes in how to work and relate to one another. In the digital era, the education system is expected to realize that students have skills capable of critical thinking and problem solving, creative and innovative skills, communication and collaboration skills, and skilled use of information and technology. Therefore, the purpose of this article is to determine the effectiveness of the independent learning curriculum in implementing the learning process and the constraints and solutions that can be provided in implementing the independent learning curriculum in the learning process. The research method used is the method of literature study and qualitative research. The results obtained from the research that has been carried out are the application of the “Merdeka” curriculum, the effectiveness of the “Merdeka” curriculum, the advantages of implementing an “Merdeka” curriculum, and the constraints of implementing an “Merdeka” curriculum. Implementing the independent learning curriculum is more straightforward than the 2013 one. The effectiveness of the 2013 curriculum is quite good and beneficial in terms of students who are free to choose according to their interests and talents and teachers who only need to explain some of the material's content. Of course, in implementing this “Merdeka” curriculum, there are obstacles, such as various media.",https://www.semanticscholar.org/paper/e8f5d49f563dfc902cb53cbad89044cf5610f6c3,IS,Qualitative
Transformasi Komunikasi Dakwah dalam Era Digital: Peluang dan Tantangan dalam Pendidikan Islam Kontemporer,"This study aims to examine the transformation of da'wah communication in the digital era and identify opportunities and challenges faced in the context of contemporary Islamic education. By studying communication theory, information technology, and Islamic education, this research focuses on the impact of changing times marked by advances in information and communication technology. The research method uses a qualitative approach, with literature studies, interviews, and observation as data collection techniques. Through analysis of the latest data and literature, this research finds that the digital era has opened up opportunities for access to information and a wider global reach for Islamic da'wah through various social media platforms and other digital technologies. However, challenges that arise, such as the risk of content not by religious teachings and the digital divide among Muslims, are a serious concern in efforts to spread religious messages wisely. Therefore, this study highlights the importance of a religious values-based approach in integrating technology into contemporary Islamic education so that da'wah can be carried out effectively and consistently with prevailing religious principles. The results of this study conclude that it is important to provide valuable insights for preachers and practitioners of Islamic education in taking advantage of opportunities in the digital era as well as overcoming challenges that arise to optimize the transformation of da'wah communications and carry out Islamic education that is adaptive in the midst of changing times.",https://www.semanticscholar.org/paper/3f12358477625c307e67f5512bbc830e2ce6fc62,IS,Qualitative
Formulasi Model Perkuliahan Daring Sebagai Upaya Menekan Disparitas Kualitas Perguruan Tinggi,"A one-step using network technology and information technology for the development of learning system in universities is online lecturing system. This learning system is assessed as highly efficient, because the same learning resources can be used by hundred people at the same time. So, college student that lives in isolated area can access the subjects of reputable universities in Indonesia. Hence, the quality disparity of universities will be decreased. This study is qualitative descriptive research. The object of this study is on website pditt.belajar.kemdikbud.go.id. This website is developed by the Ministry of Education and Culture Indonesia. The selection of that locus is because the website is an official website of government which collaborate with some reputable universities in Indonesia, such as University of Indonesia, Bandung Institute of Technology, Gajahmada University, Sepuluh November Institut of Technology, Aptikom, and Bina Nusantara University. The result showed that online lecturing system has positive contribution for pushing the quality disparity of universities in Indonesia. The indications such as 1) Minimize the limitation of access to higher education that have a certain quality. 2) Cut-off the limitation of facilities that had been cosidered as one of obstacles of the lower quality of higher education. 3) Eliminate the limitation of understanding to certain material. 4) Online lecturing system gives wide access to educational resources, especially in reputable universities.",https://www.semanticscholar.org/paper/1f57fb78c5fa9a31ad0b07376b09374c28c6445e,CS,Qualitative
"Design, Development and Validation of an Educational Methodology Using Immersive Augmented Reality for STEAM Education","The main objective of this study is the design and validation of an educational methodological model based on the use of immersive technological resources (Augmented Reality – AR) to improve learning processes in secondary education science subjects (Biology and Geology). The process was developed based on three main quantitative studies: an exploratory study, a study of performance divided into three cases studies, and an attitudinal study. The information obtained was completed with a fourth qualitative study of the training of teachers who participate in educational technology. This research provides empirical evidence that allows validation of the methodological model developed to explain key concepts and to improve the level of motivation and acceptance of AR technology by students. The proposed model can induce improvements in educational processes in the field of STEAM when used with an immersive AR technological resource and an adapted digital evaluation system. It also demonstrates that teachers require specific training in connection with the creation and the adequate use of AR educational resources, and of digital evaluation systems as well. The results of this study have important implications for the field of education, demonstrating the potential of AR technology to improve learning outcomes and the need for teacher training in its use.",https://www.semanticscholar.org/paper/4387a74b19af0a7b6fdb5de8252edceb39ba952f,CS,Qualitative
"Smart dining, smart restaurant, and smart service quality (SSQ)","Purpose Have you been to a smart restaurant, and how were its services? A common limitation of hospitality studies stems from the lack of research on how service quality is shaped within smart technology. This study aims to fill this literature void not merely to reiterate the importance of technology but also to recast service quality through the lens of information technology. It synthesizes the 5-S model of smart service quality (AKA SSQ) as a new conceptualization of service quality application in smart hospitality contexts such as smart restaurants. Design/methodology/approach This study undertook a qualitative research design based on theoretical synthesis from service quality, information technology and attention restoration. Drawing from online review comments and semistructured interviews from smart restaurants, the authors improvised the SSQ model to identify the essence of smart service in smart dining establishments. Findings “5-S” reflects an extension of the literature to denote a new SSQ abstraction pertinent to s-servicescape, s-assurance, s-responsiveness, s-reliability and s-empathy. A nomological network was posited to better understand the importance of smart design and consequence of SSQ. Research limitations/implications The emergence of smart dining gives rise to smart restaurants, which puts technology at center stage. As consumers are becoming increasingly comfortable with self-service technology, auto-payment and ordering systems and robotic services, technology in foodservice will continue to play an essential role to better serve diners. Geared with advanced innovations and intelligent devices, smart restaurants are now more than mere eateries. It is a trend and a lifestyle. Originality/value This novel SSQ concept adds new nuances to the literature by acknowledging the technological essence in today’s hospitality industry. By integrating smart technology into the service quality paradigm, the authors are able to observe several interesting behaviors exhibited during smart dining, including tech-induced restoration, which opens a new avenue to understand how attention restoration could be attained through immersion in a technologically advanced setting. By synthesizing theoretical essence from service quality, attention restoration and information technology, the authors are able to create a new dialog that should warrant a forum of discussion in future studies.",https://www.semanticscholar.org/paper/d7b14a9ac90382df1a4a485408233477256181b4,CS,Qualitative
The Role of Partnerships and Business Networks in the Growth of MSMEs in the Digital Age,"MSMEs have proven their vital role as economic pillars in various countries around the world. MSMEs have now transformed a lot. Major changes in the business landscape brought about by the digital age have also affected MSMEs. In fact, partnerships and business networks offer the potential to help MSMEs overcome the challenges they face. This study aims to examine the role of partnerships and business networks in the growth of MSMEs in the digital era. This research is a literature review using a qualitative approach, which means that it will analyse and interpret data using information and text from different sources. The study results show that the role of partnerships and business networks in the growth of MSMEs in the digital era is very important. Partnerships with large enterprises and other MSMEs allow MSMEs to expand access to markets and resources, while business networks help them connect with other business actors, increase visibility and provide collaboration opportunities. However, the success of these partnerships and networks depends on easy access to information technology and digital infrastructure.",https://www.semanticscholar.org/paper/d1a0a1bb9d33a15c099ebbb4bbcf013bd214c969,IT,Qualitative
Investigating Building Information Modelling (BIM) Adoption in Indonesia Construction Industry,"The development of information technology continues to evolve to respond the increasing demand and challenges in the construction industry. Building Information Modelling (BIM) emerges in recent years as the fresh solution to make the project lifecycle more efficient by encouraging collaborative working of all stakeholders involved in the construction project, i.e. owners, consultants, and contractors. This study aims to investigate BIM adoption and implementation in Indonesia construction industry and explore challenges and opportunities related to the implementation of BIM. A combination of qualitative and quantitative research method has been adopted for this exploratory research. Data was collected through interviews and questionnaires survey with snowball sampling from twenty entities, consisting of 12 contractors, 4 consultants, and 4 owners. The results show that 60% of respondents have already acknowledged and implemented BIM. The examples of BIM software used by the respondents include Revit, Tekla, and SmartPlan®. Benefits of BIM implementation as perceived by most respondents, e.g. design collision detection, clear project simulation, reduced reworks, and efficient use of resources. The adoption of BIM, however, still faces challenges, such as the absence of requirement and demand, and high investment cost. This research provides an initial understanding of current BIM adoption in Indonesia, which can be used as the basis to develop a national strategic framework for BIM adoption in Indonesia construction industry.",https://www.semanticscholar.org/paper/dcc0236a98a81a336b3d4d71bb7747f0f7cbf94a,CS,Qualitative
Towards a user-centric theory of value-driven information security compliance,"The purpose of this paper is to fill a gap in the literature, by investigating the relationship between users’ perceptions of the value of the information that they are handling, and their resultant level of compliance with their organisation’s information security policies. In so doing, the authors seek to develop a theory of value-driven information security compliance.,An interpretive, grounded theory research approach has been adopted to generate a qualitative data set, based upon the results of 55 interviews with key informants from governmental agencies based within Brunei Darussalam, complemented by the results of seven focus groups. The interviews and focus groups were conducted in two phases, so that the results of the first phase could be used to inform the second phase data collection exercise, and the thematic analysis of the research data was conducted using the NVivo 11-Plus software.,The findings suggest that, when assigning value to their information, users take into account the views of members of their immediate work-group and the espoused views of their organisation, as well as a variety of contextual factors, relating to culture, ethics and education. Perhaps more importantly, it has been demonstrated that the users’ perception of information value has a marked impact upon their willingness to comply with security policies and protocols.,Although the authors have been able to develop a rich model of information value and security compliance, the qualitative nature of this research means that it has not been tested, in the numerical sense. However, this study still has important implications for both research and practice. Specifically, researchers should consider users’ perceptions of information value, when conducting future studies of information security compliance.,Managers and practitioners will be better able to get their colleagues to comply with information security protocols, if they can take active steps to convince them that the information that they are handling is a valuable organisational resource, which needs to be protected.,The central contribution is a novel model of information security compliance that centre stages the role of the users’ perceptions of information value, as this is a factor which has been largely ignored in contemporary accounts of compliance behaviour. This study is also original, in that it fills a methodological gap, by balancing the voices of both user representatives and senior organisational stakeholders, in a single study.",https://www.semanticscholar.org/paper/93723e4e2a9073e43628b233e1a5e647c92374d4,IS,Qualitative
Technology-Based Google Classroom In English Business Writing Class,"In this 4.0 era, digital technology and information system technology have integrated with teaching media and tool. From previous studies,Google Classroom is one of technologies whichhas been implemented inScience, Social Science, and Engineering Departments of Higher Institution. Based on PEOU and PU, the resultsare satisfying. Unfortunately, there weren’t any studies that showwhether Google Classroom is also appropriate for Language Department. Therefore, a research was conducted in English Bussiness Writing Class of English Department Politeknik Negeri Padang. The research was carried out in 2A and 2Bwith 54 total number of students. The data was collected using fieldnote of observation, questionnaire, and interview. In this descriptive qualitative research, LMS feature was used as the indicator to analyze the collected data. The result is Google Classroom has caused efficiency and effectiveness in the process of teaching and learning of English Business Writing class. Thus, teacher’s creativity is needed, and the role of a teacher to decide the use of the technology in the class is really big. But overall, using this LMS in a writing class supports the paper-less program",https://www.semanticscholar.org/paper/47960d214da1c19b57541ac6fa1aa4cf9a33ad0a,IS,Qualitative
Kecakapan Abad 21: Kompetensi Digital Pendidik Masa Depan,"This study aims to know about the 21st Century skills discourse comprehensively. Such knowledge is the basic capital of the educators' paradigm for developing students to be skilled in accordance with their times. Given the reality of educators who are reluctant to actually use technology that every child has been in and even capable of. This reluctance might be caused by technophobia, skepticism or other reasons. This research is qualitative research. The qualitative approach emphasizes its analysis of descriptive data. The data collected will be analyzed using descriptive analysis method which consists of three main activities, namely: data reduction, data presentation and drawing conclusions by exploring the deepest meaning systematically. The results of this study indicate that digital competence must be possessed by educators to present new content in 21st century learning. Digital competence is a new ability for educators in 21st century learning. Digital competence shows the use of information and communication technology based on pedagogical principles by realizing its implications for methodology education. The digital competencies include mastering information and communication, creating learning content, and solving educational problems",https://www.semanticscholar.org/paper/34177fb834b6c8e5cb89631f4f2fe86ea22a2551,CS,Qualitative
Information experience in personally meaningful activities,"Information behavior in activities that are freely chosen has been little explored. This article conceptualizes personally meaningful activities as a site for information behavior research. Personal meaning is discussed as a necessity for human beings. In the information age, there is an ethical directive for developers of information technology to promote and afford personally meaningful activities. This article builds on discussions of the pleasurable and profound in information science conceptually and empirically. First, it argues for the necessity of phenomenology in these discussions, which heretofore has been mostly absent. Next, it presents results from a qualitative, empirical study on information in personally meaningful activities. The empirical study uses interpretative phenomenological analysis to examine information experience in three domains of personal meaning: Bible reading, ultramarathon running, and art‐making. The following themes emerge and are discussed: identity, central practice, curiosity, and presence. Opportunities for technological development and further research are outlined.",https://www.semanticscholar.org/paper/eb4f74b089129c8f1c907915452493bfa49ec9ce,IS,Qualitative
Exploring the Potential of Artificial Intelligence and Computing Technologies in Art Museums,"The research intends to explore how Artificial Intelligence (AI) and computing technology can be used to create a more immersive and enjoyable experience within the context of a museum visit. Specifically, the study aims to identify ways in which AI and computing technologies can be leveraged to enrich the visitor’s experience, including by providing interactive content, automated personalization, and real-time access to relevant information. Additionally, the research will assess the potential for AI and computing technology to support improved data analytics and utilization of resources within museums, such as enhanced curation, digital preservation, and increased engagement with audiences. The study employed a qualitative methodology, utilizing interviews with museum professionals and surveys of museum visitors to collect data on visitor experiences. An analysis of the data was conducted to identify current and potential uses of AI and computing technology in art museums. The findings reveal that AI and computing technology are currently being used to facilitate access to collections, tour guidance, and educational activities while emerging technologies show promise for providing even more immersive and personalized visitor experiences. The results of this study suggest that AI and computing technology can play an important role in enhancing the visitor’s museum experience. The research provides recommendations for art museums to leverage AI and computing technology to optimize visitor engagement and foster more meaningful connections with works of art.",https://www.semanticscholar.org/paper/6628966e0353c485e93e091fb07d2bcdcc477c8f,CS,Qualitative
Whole Genome Resequencing Helps Study Important Traits in Chickens,"The emergence of high-throughput sequencing technology promotes life science development, provides technical support to analyze many life mechanisms, and presents new solutions to previously unsolved problems in genomic research. Resequencing technology has been widely used for genome selection and research on chicken population structure, genetic diversity, evolutionary mechanisms, and important economic traits caused by genome sequence differences since the release of chicken genome sequence information. This article elaborates on the factors influencing whole genome resequencing and the differences between these factors and whole genome sequencing. It reviews the important research progress in chicken qualitative traits (e.g., frizzle feather and comb), quantitative traits (e.g., meat quality and growth traits), adaptability, and disease resistance, and provides a theoretical basis to study whole genome resequencing in chickens.",https://www.semanticscholar.org/paper/ac8e2eb0d31548a2d7cea2ed87bbb6fa7d1546b2,CS,Qualitative
"Factors Affecting Information Quality of Local Government Financial Statement of West Bandung District, West Java Province, Indonesia","This study aims to determine the factors that affect the information quality of West Bandung District local government financial information. Government financial statement should meet qualitative characteristics.This research uses qualitative method and data completion with interview technique and report to 17 informants which are heads of sub finance section in Regional Work Units of West Bandung District. Data analysis is done by data reduction then presents data and draw the conclusion on data obtained according to data analysis method for qualitative research. The results of this study indicate the factors that affect the information quality of West Bandung District Financial Statements is the application of Government Accounting Standards, human resources quality, internal control system, utilization of information technology, organizational commitment, the role of internal auditors, assets, external factors, and the operational fund management of special schools for West Bandung District Education Office",https://www.semanticscholar.org/paper/3c0bd9e92129877c09006e2681f268877654c6fc,IS,Qualitative
"Career Break, Not a Brake on Career: A Study of the Reasons and Enablers of Women’s Re-entry to Technology Careers in India","Career re-entry of women in the technology sector remains an unexplored area. With the increasing focus of information technology (IT) organisations to attract, retain and promote women at the workplace, career re-entry among women professionals’ merits attention. The purpose of this study is to investigate the reasons and enablers of career re-entry among women who plan a re-entry in the IT sector in India. This study employed a qualitative research method and used interviews as a tool for data collection. Data collected through the interviews of re-entry women (n = 28) was analysed with the help of qualitative analysis software ATLAS.ti. Further, text analysis was also performed through Voyant tools. Findings suggest that a strong career identity, a high level of work centrality and an urge to regain financial independence motivated women to return to IT careers. Findings revealed seven distinct enablers of career re-entry. Based on this finding, a model of the support ecosystem is discussed that presents an intricate relationship between the enablers of career re-entry, support ecosystem and career resumption. Moreover, findings indicate that an active agency of women, a support ecosystem and favourable life events lead to career re-entry. Managerial and theoretical implications of findings are discussed. The article concludes with limitations and future research agenda.",https://www.semanticscholar.org/paper/23aa88b51eb21e145bf49ba58e6d4a1ab1bf69ac,CS,Qualitative
Peran Guru Penggerak dalam Kurikulum Merdeka Belajar,"Pioneer teachers and the independent learning curriculum are one of the new breakthroughs that are created to develop the educational aspects by implementing technology 5.0 based on the direction of the Indonesian Minister of Education. This article aims to present data regarding the role of the pioneer teacher in the independent learning curriculum. This study applied qualitative research in a type of literary study. It means that the author examines published articles that convey the research results about pioneer teachers and independent learning curriculum. The discussion presented includes the understanding of the pioneer teachers, the role of the pioneer teachers, the meaning of independent learning and the implementation of the independent learning curriculum. The research results were expected to be used as a study or information that can be used as a theoretical basis for further research in the discussion of pioneer teachers in the independent learning curriculum as well as to optimize learning success applying technology-based learning 5.0.",https://www.semanticscholar.org/paper/252b371e2fee5dbb2ae3633f1053402ccc8e0149,CS,Qualitative
Kesenjangan Digital dan Solusi yang Diterapkan di Indonesia Selama Pandemi COVID-19,"The digital divide is a crucial issue in developing countries. Indonesia needs to find elite solutions to solve the issue. By looking at the speedy development of telecommunications and the cheapening of information technology, especially the internet, the expectation is to lower the digital gap problems. By this time, the digital divide discoursing is mostly associated with the availability of access and infrastructure, even though good digital capabilities are a prerequisite for digital equality. This study aims to observe how the conditions of the digital divide in Indonesia and the way to solve it presented by the government or individuals to overcome the digital divide. Using the descriptive qualitative method in this research is to illustrate the restrictions of the digital divide between provinces and regions in Indonesia and what solutions have been taken to address the digital divide in Indonesia. This study concludes that the enlargement of internet access in Indonesia has not been linear with the digital competency of its people. It includes motivation to be more productive using the internet, get useful information and use it for productive activities that can improve the economy.",https://www.semanticscholar.org/paper/3da7f2bc08e968ac316e358d8fba8aa9b917d6ee,CS,Qualitative
Quality Evaluation of Digital Twins Generated Based on UAV Photogrammetry and TLS: Bridge Case Study,"In the current modern era of information and technology, emerging remote advancements have been widely established for detailed virtual inspections and assessments of infrastructure assets, especially bridges. These technologies are capable of creating an accurate digital representation of the existing assets, commonly known as the digital twins. Digital twins are suitable alternatives to in-person and on-site based assessments that can provide safer, cheaper, more reliable, and less distributive bridge inspections. In the case of bridge monitoring, Unmanned Aerial Vehicle (UAV) photogrammetry and Terrestrial Laser Scanning (TLS) are among the most common advanced technologies that hold the potential to provide qualitative digital models; however, the research is still lacking a reliable methodology to evaluate the generated point clouds in terms of quality and geometric accuracy for a bridge size case study. Therefore, this paper aims to provide a comprehensive methodology along with a thorough bridge case study to evaluate two digital point clouds developed from an existing Australian heritage bridge via both UAV-based photogrammetry and TLS. In this regard, a range of proposed approaches were employed to compare point clouds in terms of points’ distribution, level of outlier noise, data completeness, surface deviation, and geometric accuracy. The comparative results of this case study not only proved the capability and applicability of the proposed methodology and approaches in evaluating these two voluminous point clouds, but they also exhibited a higher level of point density and more acceptable agreements with as-is measurements in TLS-based point clouds subjected to the implementation of a precise data capture and a 3D reconstruction model.",https://www.semanticscholar.org/paper/a1f4df396bb4403bc7732a9209b2738b76c0cfc9,IT,Qualitative
Hybrid Learning Innovation: Challenges for Developing Teachers Skills in Indonesia,"The world of education receives a fairly high impact due to developments in technology and information. In the 21st century there has been a shift from industrial technology to information technology, so that in the future this causes teachers as educators and essential elements in the field of education to be able to learn new things in the teaching and learning process, including learning innovations. This research will be conducted to see how the challenges and readiness of teachers as educators face the influx of technological changes and innovations in implementing learning. The approach used in this study is a descriptive qualitative approach with descriptive analysis methods. This research uses secondary data that comes from the results of research and previous studies. In selecting data, the researcher selects various studies and studies that are still relevant to this research. This study found that the rapid development of technology and information requires educators to keep abreast of technological developments by continuously improving themselves, innovating learning, and adapting to the needs of society. In facing the various challenges that exist in the future, educators need to have various skills in life and career, learning and innovation skills, as well as skills in using media and technology.",https://www.semanticscholar.org/paper/74d24697500f9585f80d7d2e50c9d795416caa8f,IS,Qualitative
Pemanfaatan Quizizz Sebagai Media Pembelajaran Berbasis Game Pada Masa Pandemi Covid-19,"The Covid-19 pandemic has hit all countries in the world has had an impact on many sectors, including the education sector. Teaching and learning activities are transferred to a virtual face room. Online learning is carried out with the help of technology, one of those is quizizz which is a game-based learning media. This research is a qualitative research that aims to obtain information about the use of the Quizizz application in economic learning. The research subjects were high school and vocational high school students in Salatiga City, Boyolali Regency, and Semarang Regency. The results showed that the use of quizizz had a positive impact on students. Quizizz helped them to better understand the material, did not make them bored in learning, made them more enthusiastic, focused, and active in learning, relaxes when they were bored, makes students think critically, competitively, and broadly.",https://www.semanticscholar.org/paper/b17398b6c631f7cf94d5da537dab8c7429f5b013,IS,Qualitative
Exploring the Potential of ChatGPT in Improving Online Marketing and Promotion of MSMEs,"In the ever-evolving digital era, the use of technology and the internet has changed the way businesses operate, including in product marketing and promotion. ChatGPT is allegedly an example of AI technology that can improve online marketing and promotion of MSMEs. This research will explore the potential of ChatGPT in improving online marketing and promotion of MSMEs. This research is qualitative in nature. The techniques used to obtain information involve careful observation and meticulous note-taking, followed by analytical procedures such as data reduction, visualisation, and inference. The study arrived at the conclusion that exploring the potential of ChatGPT in enhancing online marketing and promotion of MSMEs offers an exciting opportunity for MSMEs to reach potential customers, improve customer interactions, and optimise marketing efforts efficiently. With ChatGPT's ability to provide natural responses, provide product information, provide recommendations, and run marketing campaigns, MSMEs can expand their market reach and provide a better customer experience.",https://www.semanticscholar.org/paper/484923f4db65eb68000b5e1c9da0c4cf22465ed3,IS,Qualitative
Self-paced cybersecurity awareness training educating retail employees to identify phishing attacks,"ABSTRACT This generic qualitative inquiry study examined the features needing improvement in self-paced cybersecurity awareness training to educate retail employees to identify phishing attacks. The researcher used a thematic analysis to investigate the responses postulated by participants. The researcher evaluated the responses solely based on the transcription of each participant. NVivo 12 performed queries and matched data [1]]. The qualitative analysis revealed the emerging themes. The emerging themes are topics included in self-paced cybersecurity awareness training, methods to improve it, factors affecting motivation to finish it, and its frequency and duration. Findings from this generic qualitative research study will assist security practitioners of the cybersecurity and information technology field in implementing practical self-paced cybersecurity awareness training to affect employees’ safe computing practices to identify phishing attacks.",https://www.semanticscholar.org/paper/5ea8de4f43e5485805777b3667b64f320c162728,CS,Qualitative
Developing Civicpedia as a Civic Education E-Learning Media To Improve Students’ Information Literacy,"The relevance of the research stems from the need to construct literacy information technology in 21st century learning. The essential idea of article was improving students` information literacy through “Civicpedia” as a civic education e-learning media (learning material on website, e-dictionary, video, poster, valued story, and interactive quiz). The research aims to describe : 1) the design of civicpedia as a civic education e-learning media to improve students’ information literacy; 2) the intensity of the use of civicpedia in the learning process; 3) the responses of the civicpedia users. Qualitative and quantitative approach with research and development design was used in the study. The data were obtained through observation, documentation, interview, and questionnaire. Data collection, data reduction, data presentation, and data presentation were performed to analyze qualitative data, and quantitative data analysis were shown in percentage. The participants of the research were 447 students from 54 junior high school in Bandung, Indonesia.The following was studied: 1) the concept of the civicpedia design consists of home page, dictionary page, media page, quiz and contact page; 2) steps in developing teaching materials were designed based on Curriculum of 2013, compiled on the basis of the formal education level, and contextually formulated on the current real-life controversial cases, collaborated with authentic assignments, which enhanced the students’ critical thinking, and related to unknown terms with suitable images and videos; 3) the students’ responses regarding the implementation civicpedia in the learning process were positive. The program display was considered good and the interactivity aspect was deemed very good. Most students very positively perceived the use of Civicpedia in civic education learning to improving information literacy.",https://www.semanticscholar.org/paper/a9b7b30197e0004aef7614b9cd0398ecd68dd907,IS,Qualitative
Information literacy education in primary schools: A case study,This study focuses on teachers’ perspectives concerning information literacy teaching in two primary schools in Israel—one school that joined the national information and communications technology program and a second school that did not. The researchers used a qualitative research method during the 2015 academic year. Eighteen teachers were interviewed. The findings suggest that participation in the national information and communications technology program did not lead to the integration of information literacy in the school’s curriculum. A significant gap was discovered in both schools between the teachers’ perceptions—who understood the importance of teaching information literacy and its actual implementation.,https://www.semanticscholar.org/paper/bb3f29aea15b37d317773c3ef1ac4f485eb1ac60,IS,Qualitative
Understanding Teacher Educators’ Perceptions and Practices about ICT Integration in Teacher Education Program,"This study explored the perceptions and practices of teacher educators in integrating information and communication Technology (ICT) in teacher education programs. The study adopted a phenomenological design under the qualitative research approach that included eighteen selected participants from a teacher education university college. Data collection employed semi-structured interviews, observations, and documentary reviews. The authors adopted the Braun and Clarke (2006) thematic analysis model for data analysis. The findings showed that while young and inexperienced teachers showed readiness to use ICT, some teacher educators do not understand the logic behind using technology and hence question the rationale for applying it to their teaching. At the same time, equipment challenges, large teaching burdens, and time limits were the critical barriers to integration. Again, the findings revealed that teacher educators use different software and learning platforms, use social media, gather online information, and access learning materials through journal subscriptions to enhance preservice teachers’ learning. Thus, integrating ICT during teacher training is paramount, and teacher educators should be assisted and encouraged to develop positive attitudes in learning and to apply ICT in their teaching practices. Concomitantly, equipping preservice teachers with ICT-based pedagogical skills, not only through specialized ICT courses, but also through observing how teacher educators use it, has a significant impact on transforming teaching practices in their future classrooms.",https://www.semanticscholar.org/paper/7c8d8769c06c93df391d70224aff4f81056cc771,CS,Qualitative
"Application of Terrestrial Laser Scanning (TLS) in the Architecture, Engineering and Construction (AEC) Industry","As a revolutionary technology, terrestrial laser scanning (TLS) is attracting increasing interest in the fields of architecture, engineering and construction (AEC), with outstanding advantages, such as highly automated, non-contact operation and efficient large-scale sampling capability. TLS has extended a new approach to capturing extremely comprehensive data of the construction environment, providing detailed information for further analysis. This paper presents a systematic review based on scientometric and qualitative analysis to summarize the progress and the current status of the topic and to point out promising research efforts. To begin with, a brief understanding of TLS is provided. Following the selection of relevant papers through a literature search, a scientometric analysis of papers is carried out. Then, major applications are categorized and presented, including (1) 3D model reconstruction, (2) object recognition, (3) deformation measurement, (4) quality assessment, and (5) progress tracking. For widespread adoption and effective use of TLS, essential problems impacting working effects in application are summarized as follows: workflow, data quality, scan planning, and data processing. Finally, future research directions are suggested, including: (1) cost control of hardware and software, (2) improvement of data processing capability, (3) automatic scan planning, (4) integration of digital technologies, (5) adoption of artificial intelligence.",https://www.semanticscholar.org/paper/a80f845796ebedc9b673177cbe99444a358beeba,CS,Qualitative
Technological Barriers and Challenges in the Use of ICT during the COVID-19 Emergency Remote Learning,"This study aimed to examine university students' insights and observations concerning the technological barriers and difficulties they encountered in the use of Information and Communication Technology (ICT) during the COVID-19 Emergency Remote Learning (ERL) This research employed a qualitative phenomenological approach as the method of inquiry The study included eighty university students, who studied at the Faculty of Education at a state university in Jakarta, Indonesia Data were collected through a two-week daily journal, the students' reflective essays and an online focus group discussion The study revealed the technology barriers and challenges in using ICT included: Device issues, internet connectivity, technology costs, and lack of technology skills Students also had problems with: Incompatible devices, sharing devices with other family members, unstable internet connection, restricted or unavailable internet access, data costs, purchasing new devices, new programs or apps, inexperience with ICT, lack of ICT skills, and inadequate learning platforms The findings of this research are useful for improving students' learning experience and access during these challenging times COVID-19 is a recent phenomenon;it is novel to research the case, and this research offers both practical and theoretical measures to help improve ERL in the future © 2020 by authors",https://www.semanticscholar.org/paper/aca56e35bf9fab524b7ea30e47ca2daa1bb4ac68,IS,Qualitative
Benefits And Challenges Of Cloud Computing Technology Adoption In Small And Medium Enterprises (SMEs),"In today's digital era and globalization, many small and medium business (SME) businesses are adopting cloud computing for the company's operations. Cloud computing is a growing data center technology in line with increas-ing traffic on the internet in the era of the Internet of Things (IoT). This technology overcomes the weaknesses of conven-tional servers for speed, scalability, and efficiency. However, there are still SMEs who are not sure of the benefits of cloud services. Therefore, this research is conducted to explore the opinions of SMEs about the benefits of cloud computing ser-vices in their business operations that encourage them to adopt this service. The qualitative research method is conducted by interviewing the top management of a number of SMEs engaged in the information and communication technology (ICT) industry. SMEs are domiciled in Jakarta and Bandung, Indonesia and have been using cloud computing platform services. The results found that the most perceived benefits of cloud computing are cost savings because it can reduce cap-ital expenditures, such as procurement of computers with high performance and purchase their own server by SMEs. On the other hand, the service enhances the company's internal organizational processes to accelerate decision making, expand markets, and speed up communication with customers. However, the security aspect and the limited provision of infrastructure remain a challenge for the adoption of cloud computing in SMEs in Indonesia. The company's effective strategy is considered to be able to minimize the negative impact of this challenge.",https://www.semanticscholar.org/paper/290448f568a9f83bcc16c6e7a799496ac1705c4a,IT,Qualitative
