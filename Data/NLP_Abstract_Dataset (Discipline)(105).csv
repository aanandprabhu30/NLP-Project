ID,Discipline,Abstract
1,CS,"Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity."
2,CS,"Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page."
3,CS,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed."
4,CS,"The goal of user experience design in industry is to improve customer satisfaction and loyalty through the utility, ease of use, and pleasure provided in the interaction with a product. So far, user experience studies have mostly focused on short-term evaluations and consequently on aspects relating to the initial adoption of new product designs. Nevertheless, the relationship between the user and the product evolves over long periods of time and the relevance of prolonged use for market success has been recently highlighted. In this paper, we argue for the cost-effective elicitation of longitudinal user experience data. We propose a method called the “UX Curve” which aims at assisting users in retrospectively reporting how and why their experience with a product has changed over time. The usefulness of the UX Curve method was assessed in a qualitative study with 20 mobile phone users. In particular, we investigated how users’ specific memories of their experiences with their mobile phones guide their behavior and their willingness to recommend the product to others. The results suggest that the UX Curve method enables users and researchers to determine the quality of long-term user experience and the influences that improve user experience over time or cause it to deteriorate. The method provided rich qualitative data and we found that an improving trend of perceived attractiveness of mobile phones was related to user satisfaction and willingness to recommend their phone to friends. This highlights that sustaining perceived attractiveness can be a differentiating factor in the user acceptance of personal interactive products such as mobile phones. The study suggests that the proposed method can be used as a straightforward tool for understanding the reasons why user experience improves or worsens in long-term product use and how these reasons relate to customer loyalty."
5,CS,"Elliptic curve cryptosystems are considered an efficient alternative to conventional systems such as DSA and RSA. Recently, Montgomery and Edwards elliptic curves have been used to implement cryptosystems. In particular, the elliptic curves Curve25519 and Curve448 were used for instantiating Diffie-Hellman protocols named X25519 and X448. Mapping these curves to twisted Edwards curves allowed deriving two new signature instances, called Ed25519 and Ed448, of the Edwards Digital Signature Algorithm. In this work, we focus on the secure and efficient software implementation of these algorithms using SIMD parallel processing. We present software techniques that target the Intel AVX2 vector instruction set for accelerating prime field arithmetic and elliptic curve operations. Our contributions result in a high-performance software library for AVX2-ready processors. For example, our library computes digital signatures 19% (for Ed25519) and 29% (for Ed448) faster than previous optimized implementations. Also, our library improves by 10% and 20% the execution time of X25519 and X448, respectively."
6,IS,"Enterprise Resource Planning (ERP) systems provide extensive benefits and facilities to the whole enterprise. ERP systems help the enterprise to share and transfer data and information across all functions units inside and outside the enterprise. Sharing data and information between enterprise departments helps in many aspects and aims to achieve different objectives. Cloud computing is a computing model which takes place over the internet and provides scalability, reliability, availability and low cost of computer reassures. Implementing and running ERP systems over the cloud offers great advantages and benefits, in spite of its many difficulties and challenges. In this paper, we follow the Systematic Literature Review (SLR) research method to explore the benefits and challenges of implementing ERP systems over a cloud environment. "
7,IS,"This research makes an attempt to understand various factors that influence the adoption of mobile applications. Within the context of the “Unified theory of acceptance and use of technology” (UTAUT) modified model, considering the upcoming demand and increase in demand for mobile-banking applications, the researcher tried to explore the theoretical concept between random people of various states in India. The primary data was collected by preparing a questionnaire and circulating it using Google Forms. The collected data was further coded into Smart PLS 4 to understand the model and structural equation with reference to mobile-banking technological adoption and factors that had a significant impact. The conclusions derived from the study is that social influence, “effort expectancy”, and “trust” factors had a very strong influence on the “purchase intention”, whereas “effort” and “risk” factors had a negligible impact on purchase intent. It was also found that the UTAUT model is appropriate for evaluating the technological adoption of mobile-banking applications. With the advent of many players in the market and their unique banking management applications on mobile platforms, consumers are moving towards different third-party app than their origin bank in which they hold account. This has forced banking institutions to up the pace in the competition, introducing a lot of new features. It is also important to understand that, as a customer, there are a lot of attributes that he would be looking into for adoption. This paper is an attempt to understand the advancements in various variables that consumers would look at in the area of mobile-banking applications."
8,IS,"Electronic learning (e-learning) has been widely used as a complement subject to traditional learning methods. Implementing e-learning is becoming one of many solutions in effecting the study process offered by higher educational institutions, such as Universitas Pembangunan Nasional “Veteran” Jakarta = UPNVJ and Sekolah Tinggi Teknologi Telematika Telkom (ST3 Telkom) Purwokerto, Central Java. The objective of this research is to measure the e-learning system success implemented by UPNVJ and ST3 Telkom Purwokerto and then compare the similarities and differences between of two. The modified DeLone & McLean Information Systems Success Model and part of Technology Acceptance Model are adopted in this study. The respondents are students who have been experience in using e-learning and are selected randomly from the Faculty of Computer Science of UPNVJ and from ST3 Telkom Purwokerto. The total respondents are 387, which consist of 215 students from UPNVJ and 172 from ST3 Telkom Purwokerto. For the results at UPNVJ, there are two hypotheses that were proven insignificant and at ST3 Telkom there are three hypotheses that were proven insignificant. Findings at UPNVJ show that the e-learning benefit can be explained by its independent variables by 53.8%%, and at ST3 Telkom by 60.6%. These percentages show that the predictors explained 53.8% for UPNVJ's e-learning benefit and 60.6% e-learning benefit for ST3 Telkom Purwokerto. These findings could be considered as novelties, because they are different from the original model."
9,IS,"The success rate of enterprise resource planning (ERP) implementations is not high, despite the significant investments made by organizations in these applications. It has often been suggested that a combination of inadequate preparedness and inappropriate project management is responsible for this low success rate. This paper presents a case study of a successful ERP implementation, specifically examining the rollout in the Irish subsidiary of a UK multinational. The study investigates the applicability of the Project Management Body of Knowledge (PMBOK) framework to ERP projects. By discussing each PMBOK category, the case illustrates where the framework aligns well and where it may require adaptation for ERP contexts. The findings suggest that while PMBOK is broadly useful, ERP projects have specific demands that necessitate a shift in emphasis within certain categories. The case also provides insights into how organizations evaluate the success of complex change initiatives. The research highlights the importance of context-specific project management strategies and emphasizes the value of understanding organizational nuances when planning ERP rollouts. The authors conclude with suggestions for future research involving additional ERP case studies in diverse industries and organizational settings."
10,IS,"Mobile-based advisory services have significant benefits, including access to agricultural information, knowledge sharing, meteorological information, marketing-related information, and financial services for smallholder farmers. This study aimed to assess farmers’ information behavior regarding mobile-based advisory services and how farmers with different characteristics and attitudes access and adopt information. Data were collected from 382 farmers in Dakhalia governorate, Egypt. The most frequently received information was related to best agricultural practices, weather forecasts, seed varieties and treatment, and water management. Cluster analysis revealed that 47% of the farmers had low information behavior. Seventy-one of the respondents had a favorable attitude toward information retrieval from mobile agricultural services. The information behavior groups of the farmers significantly differed in education, farm size, diversity of agricultural production, and attitude regarding trust and quality of the information provided. Information behavior among farmers has useful implications for policymakers in supporting the long-term benefits of mobile-based advisory services."
11,IT,"This article describes an approach for providing dynamic quality of service (QoS) support in a variable bandwidth network, which may include wireless links and mobile nodes. The dynamic QoS approach centers on the notion of providing QoS support at some point within a range requested by applications. To utilize dynamic QoS, applications must be capable of adapting to the level of QoS provided by the network, which may vary during the course of a connection. To demonstrate and evaluate the dynamic QoS concept, we have implemented a new protocol called dynamic resource reservation protocol (dRSVP) and a new QoS application program interface (API). The paper describes this new protocol and API and also discusses our experience with adaptive streaming video and audio applications that work with the new protocol in a testbed network, including wireless local area network connectivity and wireless link connectivity emulated over the wired Ethernet. Qualitative and quantitative assessments of the dynamic RSVP protocol are provided."
12,IT,"This paper presents a novel monitoring architecture addressed to the cloud provider and the cloud consumers. This architecture offers a monitoring platform-as-a-Service to each cloud consumer that allows to customize the monitoring metrics. The cloud provider sees a complete overview of the infrastructure whereas the cloud consumer sees automatically her cloud resources and can define other resources or services to be monitored. This is accomplished by means of an adaptive distributed monitoring architecture automatically deployed in the cloud infrastructure. This architecture has been implemented and released under GPL license to the community as “MonPaaS”, open source software for integrating Nagios and OpenStack. An intensive empirical evaluation of performance and scalability have been done using a real deployment of a cloud computing infrastructure in which more than 3,700 VMs have been executed."
13,IT,"Proper configuration of security technologies is critical to balance the needs for access and protection of information. The common practice of using a layered security architecture that has multiple technologies amplifies the need for proper configuration because the configuration decision about one security technology has ramifications for the configuration decisions about others. Furthermore, security technologies rely on each other for their operations, thereby affecting each other's contribution. In this paper we study configuration of and interaction between a firewall and intrusion detection systems (IDS). We show that deploying a technology, whether it is the firewall or the IDS, could hurt the firm if the configuration is not optimized for the firm's environment. A more serious consequence of deploying the two technologies with suboptimal configurations is that even if the firm could benefit when each is deployed alone, the firm could be hurt by deploying both. Configuring the IDS and the firewall optimally eliminates the conflict between them, ensuring that if the firm benefits from deploying each of these technologies when deployed alone, it will always benefit from deploying both. When optimally configured, we find that these technologies complement or substitute each other. Furthermore, we find that while the optimal configuration of an IDS does not change whether it is deployed alone or together with a firewall, the optimal configuration of a firewall has a lower detection rate (i.e., allowing more access) when it is deployed with an IDS than when deployed alone. Our results highlight the complex interactions between firewall and IDS technologies when they are used together in a security architecture, and, hence, the need for proper configuration to benefit from these technologies."
14,IT,"Cloud has become a dominant technology in todays information technology environment and the need arises to abide with the growing demand of clients. The pressure on information technology organizations is constantly increasing to deploy the customer's application over private cloud. This change has come into light since a large number of clients have directly started to contact the cloud venders for this support. Moreover, the DevOps teams are in much bigger focus now since they are responsible for the automation and provisioning of the whole environment along with the client application. This paper focuses upon the automation of customer application right from environment provisioning to application deployment. In this paper the whole architecture of automated application deployment assembled using amazon as IAAS provider and Ansible as the orchestration engine."
15,IT,"Current approaches to access control on the Web servers do not scale to enterprise-wide systems because they are mostly based on individual user identities. Hence we were motivated by the need to manage and enforce the strong and efficient RBAC access control technology in large-scale Web environments. To satisfy this requirement, we identify two different architectures for RBAC on the Web, called user-pull and server-pull. To demonstrate feasibility, we implement each architecture by integrating and extending well-known technologies such as cookies, X.509, SSL, and LDAP, providing compatibility with current web technologies. We describe the technologies we use to implement RBAC on the Web in different architectures. Based on our experience, we also compare the tradeoffs of the different approaches."
16,CS,"In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL."
17,CS,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work."
18,CS,"Social Network Analysis (SNA) has been a popular field of research since the early 1990s. Law enforcement agencies have been utilizing it as a tool for intelligence gathering and criminal investigation for decades. However, the graph nature of social networks makes it highly restricted to intelligence analysis tasks, such as role prediction (node classification), social relation inference (link prediction), and criminal group discovery (community detection), etc. In the past few years, many studies have focused on Graph Neural Network (GNN), which utilizes deep learning methods to solve graph-related problems. However, we have rarely seen GNNs tackle time-evolving social network problems, especially in the criminology field. The existing studies have commonly over-looked the temporal-evolution characteristics of social networks. In this paper, we propose a graph neural network framework, namely Spatial-Temporal Graph Social Network (STGSN), which models social networks from both spatial and temporal perspectives. Using a novel approach, we leverage the temporal attention mechanism to capture social networks’ temporal features. We design a method analyzing temporal attention distribution to improve the interpretation ability of our method. In the end, we conduct extensive experiments on six public datasets to prove our methods’ effectiveness."
19,CS,"Context: Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection.Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings.Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source programs.Results: Our experiment showed that 27% of C/C++ vulnerabilities and 11% of Java vulnerabilities were missed by all three tools. Some vulnerabilities were detected by only one or combination of two tools; 41% of C/C++ and 21% of Java vulnerabilities were detected by all three tools. More importantly, static code analysis tools did not show statistically significant difference in their ability to detect security vulnerabilities for both C/C++ and Java. Interestingly, all tools had median and mean of the per CWE recall values and overall recall across all CWEs close to or below 50%, which indicates comparable or worse performance than random guessing. While for C/C++ vulnerabilities one of the tools had better performance in terms of probability of false alarm than the other two tools, there was no statistically significant difference among tools’ probability of false alarm for Java test cases.Conclusions: Despite recent advances in methods for static code analysis, the state-of-the-art tools are not very effective in detecting security vulnerabilities."
20,CS,"Faster, ultra-reliable, low-power, and secure communications has always been high on the wireless evolutionary agenda. However, the appetite for faster, more reliable, greener, and more secure communications continues to grow. The state-of-the-art methods conceived for achieving the performance targets of the associated processes may be accompanied by an increase in computational complexity. Alternatively, a degraded performance may have to be accepted due to the lack of jointly optimized system components. In this survey we investigate the employment of quantum computing for solving problems in wireless communication systems. By exploiting the inherent parallelism of quantum computing, quantum algorithms may be invoked for approaching the optimal performance of classical wireless processes, despite their reduced number of cost-function evaluations. In this contribution we discuss the basics of quantum computing using linear algebra, before presenting the operation of the major quantum algorithms, which have been proposed in the literature for improving wireless communications systems. Furthermore, we investigate a number of optimization problems encountered both in the physical and network layer of wireless communications, while comparing their classical and quantum-assisted solutions. Finally, we state a number of open problems in wireless communications that may benefit from quantum computing."
21,IS,"While clinical DSS have many proven benefits, their uptake by GPs (general practitioners) is limited. The purpose of this research was to develop and explore a UTAUT (Unified Theory of Acceptance and Use of Technology) based model of how and why GPs accept DSS. Insight into the reasons why GPs do not use clinical DSS combined with knowledge of why GPs use DSS will allow the development of strategies to facilitate more widespread adoption with consequent improvements across many areas. Depth interviews were conducted with 37 GPs comprising a mix of education backgrounds, experience and gender. The developed model indicated that four main factors influence DSS acceptance and use including usefulness (incorporating consultation issue, professional development and patient presence), facilitating conditions (incorporating workflow, training and integration), ease of use and trust in the knowledge base."
22,IS,"ERP implementation is regarded as complex, cumbersome and costly, and, very often, it exceeds the initial estimated resources. The process involves a thorough examination of the business processes in the organisation; selection of the best available software solution that matches the requirements of the enterprise; configuration of the selected systems;, training of staff; and customisation of the selected software solutions including development of required interfaces. Finally, the existing MIS of the organisation is replaced totally or partially by the new system. All the implementation processes should be carried out without affecting the daily operations across the whole enterprise. This can only be achieved by having an understanding of the key elements forming the infrastructure of the organisation, an effective plan for the implementation and an effective procedure to measure and evaluate the project throughout the implementation process. This paper presents the results of a study to identify and analyse the interrelationships of the critical issues involved in the implementation of ERP in small and medium sized enterprises (SMEs). Three basic research questions were addressed. First, what are the main critical success factors? Second, how do these factors interact throughout the implementation process? Third, which factors have their highest impact and in what stages? In order to answer these questions, over 50 relevant papers were critically reviewed to identify the main critical success factors (CSFs) for ERP implementation in large organisations. Then, the applicability of the identified CSFs to SMEs was investigated. Next, an industrial survey was also undertaken to identify which CSF has highest impact in what stages. The findings on relationships of the critical success factors have been utilised to develop a tool to monitor, and eventually improve, ERP implementations for SMEs. In the development of the tool, eight people from industry and academia with experience of ERP implementations were interviewed with the aim of validating the model being developed. The overall results provide useful pointers to the interplay of organisational and operational factors for the successful implementation of ERP."
23,IS,"The coronavirus disease 2019 (COVID-19) pandemic caused a rapid shift to full-time remote work for many information workers. Viewing this shift as a natural experiment in which some workers were already working remotely before the pandemic enables us to separate the effects of firm-wide remote work from other pandemic-related confounding factors. Here, we use rich data on the emails, calendars, instant messages, video/audio calls and workweek hours of 61,182 US Microsoft employees over the first six months of 2020 to estimate the causal effects of firm-wide remote work on collaboration and communication. Our results show that firm-wide remote work caused the collaboration network of workers to become more static and siloed, with fewer bridges between disparate parts. Furthermore, there was a decrease in synchronous communication and an increase in asynchronous communication. Together, these effects may make it harder for employees to acquire and share new information across the network."
24,IS,"Blockchain overcomes numerous complicated problems related to confidentiality, integrity, availability of fast and secure distributed systems. Using data from a cross-sectoral survey of 449 industries, we investigate factors that hinder or facilitate blockchain adoption in supply chains. To capture the most vital aspects of blockchain adoption in supply chains, our conceptual model integrates the unified theory of acceptance and use of technology (UTAUT) model with the task-technology fit (TTF) and information system success (ISS) models, with trust-based information technology innovation adoption constructs. Using structural equation modelling, we find that the ISS, TTF, and UTAUT models positively influence the key factors affecting supply chain employees’ willingness to adopt blockchain. Our results show that the UTAUT’s social influence factor has no significant effect on the intention to adopt blockchain, while inter-organisational trust has a significant effect on the relationship between the UTAUT dimension and intention to adopt blockchain."
25,IS,"The information systems (IS) literature has long emphasized the positive impact of information provided by business intelligence systems (BIS) on decision-making, particularly when organizations operate in highly competitive environments. Evaluating the effectiveness of BIS is vital to our understanding of the value and efficacy of management actions and investments. Yet, while IS success has been well-researched, our understanding of how BIS dimensions are interrelated and how they affect BIS use is limited. In response, we conduct a quantitative survey-based study to examine the relationships between maturity, information quality, analytical decision-making culture, and the use of information for decision-making as significant elements of the success of BIS. Statistical analysis of data collected from 181 medium and large organizations is combined with the use of descriptive statistics and structural equation modeling. Empirical results link BIS maturity to two segments of information quality, namely content and access quality. We therefore propose a model that contributes to understanding of the interrelationships between BIS success dimensions. Specifically, we find that BIS maturity has a stronger impact on information access quality. In addition, only information content quality is relevant for the use of information while the impact of the information access quality is non-significant. We find that an analytical decision-making culture necessarily improves the use of information but it may suppress the direct impact of the quality of the information content."
26,IT,"Cloud Computing has become more and more prevalent over the past few years, and we have seen the emergence of Infrastructure-as-a-Service (IaaS) which is the most acceptable Cloud Computing service model. However, coupled with the opportunities and benefits brought by IaaS, the adoption of IaaS also faces management complexity in the hybrid cloud environment which enterprise users are mostly building up. A cloud management system, Monsoon proposed in this paper provides enterprise users an interface and portal to manage the cloud infrastructures from multiple public and private cloud service providers. To meet the requirements of the enterprise users, Monsoon has key components such as user management, access control, reporting and analytic tools, corporate account/role management, and policy implementation engine. Corporate Account module supports enterprise users' subscription and management of multi-level accounts in a hybrid cloud which may consist of multiple public cloud service providers and private cloud. Policy Implementation Engine module in Monsoon will allow users to define the geography-based requirements, security level, government regulations and corporate policies and enforce these policies to all the subscription and deployment of user's cloud infrastructure."
27,IT,"Effective information technology service provision is key to the success of organisations. This paper presents results from five Australian organisations that implemented the Information Technology Infrastructure Library (ITIL). As a consequence, all these organisations have transformed their IT service management to provide significant benefits to their organisations, such as more rigorous control of testing and system changes, more predictable infrastructure, improved consultation with IT groups within the organisation, reduced server faults, seamless end-to-end service, documented and consistent IT service management processes across the organisation, and consistent logging of incidents. Key success factors for ITIL implementation include effective engagement of the personnel affected, support from senior management and communication of results."
28,IT,"With the ongoing adoption of remotely communicating and interacting control systems harbored by critical infrastructures, the potential attack surface of such systems also increases drastically. Therefore, not only the need for standardized and manufacturer-agnostic control system communication protocols has grown, but also the requirement to protect those control systems' communication. There have already been numerous security analyses of different control system communication protocols; yet, these have not been combined with each other sufficiently, mainly due to three reasons: First, the life cycles of such protocols are usually much longer than those of other Internet and communication technologies, therefore legacy protocols are often not considered in current security analyses. Second, the usage of certain control system communication protocols is usually restricted to a particular infrastructure domain, which leads to an isolated view on them. Third, with the accelerating pace at which both control system communication protocols and threats against them develop, existing surveys are aging at an increased rate, making their re-investigation a necessity. In this paper, a comprehensive survey on the security of the most important control system communication protocols, namely Modbus, OPC UA, TASE.2, DNP3, IEC 60870-5-101, IEC 60870-5-104, and IEC 61850 is performed. To achieve comparability, a common test methodology based on attacks exploiting well-known control system protocol vulnerabilities is created for all protocols. In addition, the effectiveness of the related security standard IEC 62351 is analyzed by a pre- and post-IEC 62351 comparison."
29,IT,"Workflow scheduling is one of the most difficult tasks due to the variation in the traffic flows generated from diverse cloud applications. Hence, in this article, a container-based virtualization is used to design an energy-efficient workflow scheduling in software-defined data centers. The containers provide the flexibility to the applications to access the underlying resource as per their requirements. Moreover, a runtime scheduler is responsible to handle all the scheduling decisions in the proposed workflow scheduling scheme. Even more, a doubly linked list-based access mechanism is used to provide access to the servers and virtual machines by traversing both ways. Finally, a hashing scheme is used to select an ideal location for the allocation of the containers. The proposed scheme is evaluated with respect to different performance metrics (makespan, execution time, fault tolerance, energy consumption, etc.) on the real data traces. The results obtained depict the superiority of the proposed scheme in comparison to the other existing schemes of its category."
30,IT,"The relentless onslaught of computers and communications technologies has recently descended on the healthcare industry. Fortunately, however, the utilization of technologies in healthcare delivery and administration could not be timelier because of the need to control escalating health costs. While the proliferation of information and communication technologies in healthcare, referred to as health management information systems (HMIS), is certainly long overdue in healthcare organizations (HCOs), it is important to recognize and be prepared for the vulnerabilities of these technologies to natural, technological, and man‐made disasters. This paper describes how HCOs have justifiably become dependent on HMIS and how these organizations may proactively plan for disasters which can impact on HMIS. A phased approach, referred to as the disaster recovery and business continuity (DRBC) planning model, is presented in the paper as an approach to develop and implement business continuity plans in HCOs."
31,CS,"Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits."
32,CS,"In pattern recognition tasks, we usually do not pay much attention to the arbitrarily chosen training set of a pattern classifier beforehand. This correspondence proposes several methods for pruning data sets based upon graph theory in order to alleviate redundancy in the original data set while retaining the original data structure as far as possible. The proposed methods are applied to the training sets for pattern recognition by a multilayered perceptron neural network (MLP-NN) and the locations of the centroids of a radial basis function neural network (RBF-NN). The advantage of the proposed graph theoretic methods is that they do not require any calculation for the statistical distributions of the clusters. The experimental results in comparison both with the k-means clustering and with the learning vector quantization (LVQ) methods show that the proposed methods give encouraging performance in terms of computation for data classification tasks."
33,CS,"The growth of the Internet has triggered tremendous opportunities for cooperative computation, where people are jointly conducting computation tasks based on the private inputs they each supplies. These computations could occur between mutually untrusted parties, or even between competitors. For example, customers might send to a remote database queries that contain private information; two competing financial organizations might jointly invest in a project that must satisfy both organizations' private and valuable constraints, and so on. Today, to conduct such computations, one entity must usually know the inputs from all the participants; however if nobody can be trusted enough to know all the inputs, privacy will become a primary concern.This problem is referred to as Secure Multi-party Computation Problem (SMC) in the literature. Research in the SMC area has been focusing on only a limited set of specific SMC problems, while privacy concerned cooperative computations call for SMC studies in a variety of computation domains. Before we can study the problems, we need to identify and define the specific SMC problems for those computation domains. We have developed a framework to facilitate this problem-discovery task. Based on our framework, we have identified and defined a number of new SMC problems for a spectrum of computation domains. Those problems include privacy-preserving database query, privacy-preserving scientific computations, privacy-preserving intrusion detection, privacy-preserving statistical analysis, privacy-preserving geometric computations, and privacy-preserving data mining.The goal of this paper is not only to present our results, but also to serve as a guideline so other people can identify useful SMC problems in their own computation domains."
34,CS,"Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide."
35,CS,"We develop succinct data structures to represent (i) a sequence of values to support partial sum and select queries and update(changing values) and (ii) a dynamic array consisting of a sequence of elements which supports insertion, deletion and access of an element at any given index.For the partial sums problem on n non-negative integers of k bits each, we support update operations in O(b) time and sum in O(logb n) time, for any parameter b, lgn/lglgn ≤ b ≤ n∈ for any fixed positive ∈ < 1. The space used is kn+o(kn) bits and the time bounds are optimal. When b = lgn/lglgn or k = 1 (i.e., when we are dealing with a bit-vector), we can also support the select operation in the same time as the sum operation, but the update time becomes amortised. For the dynamic array problem, we give two structures both using o(n) bits of extra space where n is the number of elements in the array: one supports lookup in constant worst case time and updates in O(n ε) worst case time, and the other supports all operations in O(lgn/lglgn) amortized time. The time bound of both these structures are optimal."
36,IS,"Nonprofit organizations (NPOs) are critical to the quality of life in many communities not only due to the valuable services and social impact they create, but also because of the positive economic impact within local communities. However, NPOs, just as for-profits, need to innovate in response to changing customer demands and lifestyles and to capitalize on opportunities offered by technology and changing marketplaces, structures and dynamics. Digitalization is essential to fuel NPO's innovation in order to be a differentiator in the highly competitive environment. In this paper, we first develop a review to identify the challenges of digital transformation and then we examine some of the challenges that the nonprofit sector faces in undertaking digital transformation initiative"
37,IS,"As we live in the modern time and use social media, the amount of advertisements we get exposed to is increasing more and more daily (Andrews, Van Leeuwen, & Van Baaren, 2013, p. 8), the amount of mobile game ads are parallelly increasing in social media and digital landscape. These persuasive lines can cause UX (User Experience) to become a manipulative practice and turn into Dark UX, known as ”Dark Patterns” (Brignull, 2013). This thesis addresses the Dark UX patterns employed in mobile game advertising, including persuasive techniques and deception elements, and it finds out different types of Dark UX strategies in mobile game advertising. These include explicit psychological tricks and Dark UI methods incorporated within the in-game or in-app ad frameworks. These techniques are discussed concerning empiricism, with detailed descriptions of the complex maneuvers employed. The work analyzes the underlying patterns within which the dark UX elements are hidden for the content/narrative and the mobile game UI designs. The inquiry does not just identify problems but also provides critique and design ethics. By analyzing user experiences, interviews, surveys n = 125, and empirical data, the study sheds light on the immediate effectiveness of these tactics compared to their long-term effects: reduced consumer confidence, and more remarkable instances of advertising aversion. It shows a need to switch to honest, user-focused advertising approaches and friendlier UX and UI. This thesis also contributes significantly to the current discourse on digital ethics and consumer protection in an era dominated by online advertising. It underscores the necessity of perpetually updated surveillance mechanisms since digital environments and user behavior constantly change. Besides, this study calls out to the advertisers and developers of mobile games to reassess their ethics and reorganize their advertising strategies to enhance users’ rights and improve the user experience in the short and long term."
38,IS,"This article aims to find a chain of causal relations affecting the operating effectiveness of the implemented enterprise resource planning (ERP) system instead of focusing on either the evaluation of software/vendors/consultants or critical successful factors (CSF) identification for ERP implementation, a course followed by the dominant ERP literature. This article is a process‐oriented approach and aims to give a moving picture of how one step affects another step from pre‐implementation stage, to during‐implementation stage, and to post‐implementation stage. A significant insight learned from this study is that end‐users across the organization must be educated from the onset of ERP implementation. Although education is a corner‐stone of ERP implementation, the user training is usually only emphasized and the courses are centered on computer/system operation rather than on understanding the ERP concept and spirit. This article may be interesting to some academic researchers and practical managers, and hopefully can provide a link/step for advanced researches in exploring post‐implementation ERP."
39,IS,"There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action."
40,IS,"Given the newly established communication environment of social media and highly unpredictable crisis situations, this study questioned how tourists facing an unexpected crisis situation use social media to communicate and search for information. To this end, this study developed a multi-phased social media analytic framework (data crawling, data processing and text mining, social network analysis, semantic network analysis, and network visualization) to assess the structure of information exchanges between the members of a tourism organization’s social network community and identified influential actors and information content within the social network. This study’s findings suggest genuine ways of relating with and utilizing opinion leaders and influencers in social media marketing communication as well as crisis communication. The authors expect this proposed methodological framework of social media analytics to help other scholars scientifically identify and implement the proper methodologies for utilizing social media data."
41,IT,"Edge computing is an emerging technology that aims to include latency-sensitive and data-intensive applications such as mobile or IoT services, into the cloud ecosystem by placing computational resources at the edge of the network. Close proximity to producers and consumers of data brings significant benefits in latency and bandwidth. However, edge resources are, by definition, limited in comparison to cloud counterparts, thus, a trade-off exists between deploying a service closest to its users and avoiding resource overload. We propose a score-based edge service scheduling algorithm that evaluates both network and computational capabilities of edge nodes and outputs the maximum scoring mapping between services and resources. Our extensive simulation based on a live video streaming service, demonstrates significant improvements in both network delay and service time. Additionally, we compare edge computing technology with the state-of-the-art cloud computing and content deli very network solutions within the context of latency-sensitive and data-intensive applications. Our results show that edge computing enhanced with suggested scheduling algorithm is a viable solution for achieving high quality of service and responsivity in deploying such applications."
42,IT,"Besides conventional mobile broadband communication service, 5G is envisioned to support various new use cases from vertical industries. These new scenarios bring diverse and challenging requirements, such as a broader range of performance, cost, security protection, and mobility management. The one-size-fits-all design philosophy applied in existing networks is not viable any more. Slicing a single physical network into several logical networks customized to different unique requirements has emerged as a promising approach to fulfill such divergent requirements in a sustainable way. In this article, we provide a comprehensive survey of 5G network slicing. We first present the driving forces and the concept of network slicing. Then related key enabling technologies, including network function virtualization and modularization, dynamic service chaining, management and orchestration are discussed. The latest progress in 3GPP standardization and industry implementation on 5G network slicing is presented. Finally, the article identifies several important open issues and challenges to inspire further study toward a practical network slicing enabled 5G system."
43,IT,"Efficient load balancing stands out as a crucial challenge in multi-cloud environments, particularly for applications that demand ultra-reliable, low-latency communications (URLLC). This paper proposes a novel approach integrating Decision Functions with Normal Distributions (DFND) for precise probabilistic modeling of task-to-cloud compatibility. Multivariate normal distributions capture interdependencies between resource features such as CPU, memory, bandwidth, and latency, ensuring accurate resource compatibility evaluation. Additionally, the Tasmanian Devil Optimization (TDO) algorithm employs dynamic exploration and exploitation strategies inspired by natural behaviors, providing rigorous optimization to improve task assignment in dynamic, multi-cloud environments. It uses flexible methods to ensure the optimization process is both efficient and scalable. Simulation results using CloudSim demonstrate significant improvements over state-of-the-art methods in terms of makespan reduction, response time minimization, resource utilization, and cost efficiency. The proposed framework effectively supports latency-sensitive, large-scale applications in dynamic, heterogeneous multi-cloud environments."
44,IT,"Pervasive use of cloud computing and the resulting rise in the number of datacenters and hosting centers (that provide platform or software services to clients who do not have the means to set up and operate their own computing facilities) have brought forth many concerns, including the electrical energy cost, peak power dissipation, cooling, and carbon emission. With power consumption becoming an increasingly important issue for the operation and maintenance of the hosting centers, corporate and business owners are becoming increasingly concerned. Furthermore, provisioning resources in a cost-optimal manner so as to meet different performance criteria, such as throughput or response time, has become a critical challenge. The goal of this paper is to provide an introduction to resource provisioning and power or thermal management problems in datacenters, and to review strategies that maximize the datacenter energy efficiency subject to peak or total power consumption and thermal constraints, while meeting stipulated service level agreements in terms of task throughput and/or response time."
45,IT,"IoT (Internet of Things) based smart devices such as sensors have been actively used in edge clouds i.e., ‘fogs’ along with public clouds. They provide critical data during scenarios ranging from e.g., disaster response to in-home healthcare. However, for these devices to work effectively, end-to-end security schemes for the device communication protocols have to be flexible and should depend upon the application requirements as well as the resource constraints at the network-edge. In this paper, we present the design and implementation of a flexible IoT security middleware for end-to-end cloud–fog communications involving smart devices and cloud-hosted applications. The novel features of our middleware are in its ability to cope with intermittent network connectivity as well as device constraints in terms of computational power, memory, energy, and network bandwidth. To provide security during intermittent network conditions, we use a ‘Session Resumption’ algorithm in order for our middleware to reuse encrypted sessions from the recent past, if a recently disconnected device wants to resume a prior connection that was interrupted. In addition, we describe an ‘Optimal Scheme Decider’ algorithm that enables our middleware to select the best possible end-to-end security scheme option that matches with a given set of device constraints. Experiment results show how our middleware implementation also provides fast and resource-aware security by leveraging static properties i.e., static pre-shared keys (PSKs) for a variety of IoT-based application requirements that have trade-offs in higher security or faster data transfer rates."
46,CS,"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis."
47,CS,"Graph-based models have been widely used to fraud detection tasks. Owing to the development of Graph Neural Networks~(GNNs), recent works have proposed many GNN-based fraud detectors based on either homogeneous or heterogeneous graphs. These works leverage existing GNNs and aggregate the neighborhood information to learn the node embeddings, which relies on the assumption that the neighbors share similar context, features, and relations. However, the inconsistency problem incurred by fraudsters is hardly investigated, i.e., the context inconsistency, feature inconsistency, and relation inconsistency. In this paper, we introduce these inconsistencies and design a new GNN framework, GraphConsis, to tackle the inconsistency problem: (1) for the context inconsistency, we propose to combine the context embeddings with node features; (2) for the feature inconsistency, we design a consistency score to filter the inconsistent neighbors and generate corresponding sampling probability; (3) for the relation inconsistency, we learn the relation attention weights associated with the sampled nodes. Empirical analysis on four datasets demonstrates that the inconsistency problem is critical in fraud detection tasks. Extensive experiments show the effectiveness of GraphConsis. We also released a GNN-based fraud detection toolbox with implementations of SOTA models. The code is available at \urlhttps://github.com/safe-graph/DGFraud"
48,CS,"The paper provides an overview and analysis of the current state, problems, and prospects of post-quantum cryptography. Considered the status of the PostQuantum Cryptography Standardization Process. Organizations like the National Institute of Standards and Technology (NIST) are actively working on standardizing post-quantum cryptography. Evaluation rounds have been conducted, thoroughly analyzing numerous candidate post-quantum algorithms to select the most efficient and secure ones. Identified main categories of postquantum cryptographic algorithms. Described key size of post-quantum cryptographic algorithms. Open-source libraries like Open Quantum Safe (OQS) have been developed, offering implementations of various post-quantum algorithms. These libraries enable researchers, developers, and engineers to utilize and test post-quantum algorithms in various applications. There is growing awareness of the need to prepare for the post-quantum computing era. Many companies, organizations, and governments are exploring the implications of quantum computing for their infrastructure and data security and considering the adoption of post-quantum cryptographic solutions."
49,CS,"Multiple fault localization (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed. The aim of this study is to systematically review existing research on MFL in the software fault localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain. Consequently, using an evidence-based systematic methodology, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process. The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most. The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community."
50,CS,"The recent development of fast depth map fusion technique enables the realtime, detailed scene reconstruction using commodity depth camera, making the indoor scene understanding more possible than ever. To address the specific challenges in object analysis at subscene level, this work proposes a data-driven approach to modeling contextual information covering both intra-object part relations and inter-object object layouts. Our method combines the detection of individual objects and object groups within the same framework, enabling contextual analysis without knowing the objects in the scene a priori. The key idea is that while contextual information could benefit the detection of either individual objects or object groups, both can contribute to object extraction when objects are unknown.Our method starts with a robust segmentation and partitions a subscene into segments, each of which represents either an independent object or a part of some object. A set of classifiers are trained for both individual objects and object groups, using a database of 3D scene models. We employ the multiple kernel learning (MKL) to learn per-category optimized classifiers for objects and object groups. Finally, we perform a graph matching to extract objects using the classifiers, thus grouping the segments into either an object or an object group. The output is an object-level labeled segmentation of the input subscene. Experiments demonstrate that the unified contextual analysis framework achieves robust object detection and recognition over cluttered subscenes."
51,IS,"Employing the framework of the technology acceptance model (TAM), the present study investigates the factors that affect employees’ acceptance and use of teleconferencing systems for work-related meetings in business settings. Based on survey data of 155 working professionals, a path analysis confirmed the key propositions of TAM. Importantly, the results also showed that both individual factors such as anxiety and self-efficacy, and institutional factors such as institutional support and voluntariness were significantly related to perceived ease of use (PEOU), perceived usefulness (PU), and actual use of the systems. By examining teleconferencing that typically involves group communication within organizations, this study contributes to theoretical refinement of group-based technology use and adoption. Theoretical and practical implications are discussed."
52,IS,"The primary objective of an enterprise resource planning (ERP) system is to help integrate an organization's business operations and processes effectively and efficiently. Not all firms have been successful in their ERP implementations and to that end research has helped to identify many factors that might be critical to a successful implementation. Such factors as the use of business process reengineering (BPR), and establishing a total quality management (TQM) culture have all shown to play important roles in ERP implementation. The focus of this survey research on US electronic manufacturing firms is to identify successful integration sequences of TQM and BPR with ERP. The findings reveal that both the sequence of implementation and the strategies selected to initiate ERP systems can significantly impact business performance successfulness."
53,IS,"In this article, we explore the challenges of global governance and the particular challenge presented by global data governance. We discuss a range of challenges to developing meaningful global governance institutions for regulating how companies and governments around the world manage and utilize consumer data. These challenges are compounded by their global nature and the complexities of Internet-based technologies. We argue that the following gaps exist for effective global data governance: (a) there is no overarching global framework for protecting consumer data, and it is partial and incomplete; (b) there is a lack of data protection for international data transfers, as much of the regulation that is being developed is not global in scale; and (c) new areas of data collection and use compound concerns to effective data governance in a globalized digital world. Moreover, we highlight important needs in terms of both global governance and impending challenges related to current and new uses of data. Any global governance framework should recognize the need for an iterative process where communication is ongoing between the necessary stakeholders. Agreements should incorporate common goals to maximize the potential development of global data governance norms. However, goals must remain flexible to the different data environments across nation-states while maintaining a global scope to ensure data protection. In addition, any agreement should consider the emerging challenges in this area. These challenges include new methods of data collection and use, as well as protecting individuals from manipulation and undue influence based on how their data are being used, processed, and collected."
54,IS,"Limitations in healthcare funding require hospitals to find more effective ways to utilize resources. An effective patient management system is critically dependent on the accurate analysis of individual patient outcomes and resource utilization. In the current paper, a management-oriented decision support model is thus proposed to assist health system managers in improving the efficiency of their systems. In the first stage of the model, the key variables affecting system efficiency, as well as their causal relationships, are identified through causal maps. Efficiency is measured by the total time spent in the system. In the second stage, a Bayesian Belief Network (BBN) is employed to represent both the conditional dependencies and uncertainties of the key variables. In the third stage, a sensitivity analysis is performed using a BBN to determine the most critical variable(s) in terms of impact on the system. Finally, strategies to improve system efficiency are proposed. The suggested decision support system is applied to the tomography section in the radiology department of a private hospital in Turkey."
55,IS,"From a supply chain perspective, new technologies such as blockchain can improve the efficiency and competitiveness of logistics and increase customer satisfaction. Although blockchain technology has been lauded as a way for firms to build sustainable supply chain networks, the rate of acceptance of this technology remains low. Therefore, this study seeks to identify the factors that discourage firms from merging blockchain with the supply chain. Instead of providing further reasons for adopting blockchain technology, we try to understand what deters firms from adding blockchain to their operations. Following the deductive approach, a confirmatory factor analysis is conducted on pre-test questionnaires to test, improve, and verify the constructs (questions) to measure the hypothesized factors. A theoretical model is proposed based on the hypotheses, and structural equation modeling is applied. The results are estimated using the partial least squares approach and a sample of 83 respondents. Our findings based on our empirical data support most of our hypotheses. We find that various factors impede the adoption of blockchain technologies, including technological barriers, constraints rooted in organizations and the environment, and system-related governmental barriers. In addition, various factors are critical determinants of resistance to blockchain in the technological, organizational, and environmental dimensions."
56,IT,"The vast majority of Web services and sites are hosted in various kinds of cloud services, and ordering some level of quality of service (QoS) in such systems requires effective load-balancing policies that choose among multiple clouds. Recently, software-defined networking (SDN) is one of the most promising solutions for load balancing in cloud data center. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. By using these technologies, SDN and cloud computing can improve cloud reliability, manageability, scalability and controllability. SDN-based cloud is a new type cloud in which SDN technology is used to acquire control on network infrastructure and to provide networking-as-a-service (NaaS) in cloud computing environments. In this paper, we introduce an SDN-enhanced Inter cloud Manager (S-ICM) that allocates network flows in the cloud environment. S-ICM consists of two main parts, monitoring and decision making. For monitoring, S-ICM uses SDN control message that observes and collects data, and decision-making is based on the measured network delay of packets. Measurements are used to compare S-ICM with a round robin (RR) allocation of jobs between clouds which spreads the workload equitably, and with a honeybee foraging algorithm (HFA). We see that S-ICM is better at avoiding system saturation than HFA and RR under heavy load formula using RR job scheduler. Measurements are also used to evaluate whether a simple queueing formula can be used to predict system performance for several clouds being operated under an RR scheduling policy, and show the validity of the theoretical approximation."
57,IT,"In this study, an artificial intelligence (AI) intrusion detection system using a deep neural network (DNN) was investigated and tested with the KDD Cup 99 dataset in response to ever-evolving network attacks. First, the data were preprocessed through data transformation and normalization for input to the DNN model. The DNN algorithm was applied to the data refined through preprocessing to create a learning model, and the entire KDD Cup 99 dataset was used to verify it. Finally, the accuracy, detection rate, and false alarm rate were calculated to ascertain the detection efficacy of the DNN model, which was found to generate good results for intrusion detection."
58,IT,"Internet of Things (IoT) security and privacy remain a major challenge, mainly due to the massive scale and distributed nature of IoT networks. Blockchain-based approaches provide decentralized security and privacy, yet they involve significant energy, delay, and computational overhead that is not suitable for most resource-constrained IoT devices. In our previous work, we presented a lightweight instantiation of a BC particularly geared for use in IoT by eliminating the Proof of Work (POW) and the concept of coins. Our approach was exemplified in a smart home setting and consists of three main tiers namely: cloud storage, overlay, and smart home. In this paper we delve deeper and outline the various core components and functions of the smart home tier. Each smart home is equipped with an always online, high resource device, known as “miner” that is responsible for handling all communication within and external to the home. The miner also preserves a private and secure BC, used for controlling and auditing communications. We show that our proposed BC-based smart home framework is secure by thoroughly analysing its security with respect to the fundamental security goals of confidentiality, integrity, and availability. Finally, we present simulation results to highlight that the overheads (in terms of traffic, processing time and energy consumption) introduced by our approach are insignificant relative to its security and privacy gains."
59,IT,"The swift adoption of cloud services is accelerating the deployment of data centers. These data centers are consuming a large amount of energy, which is expected to grow dramatically under the existing technological trends. Therefore, research efforts are in great need to architect green data centers with better energy efficiency. The most prominent approach is the consolidation enabled by virtualization. However, little effort has been paid to the potential overhead in energy usage and the throughput reduction for virtualized servers. Clear understanding of energy usage on virtualized servers lays out a solid foundation for green data-center architecture. This paper investigates how virtualization affects the energy usage in servers under different task loads, aiming to understand a fundamental trade-off between the energy saving from consolidation and the detrimental effects from virtualization. We adopt an empirical approach to measure the server energy usage with different configurations, including a benchmark case and two alternative hypervisors. Based on the collected data, we report a few findings on the impact of virtualization on server energy usage and their implications to green data-center architecture. We envision that these technical insights would bring tremendous value propositions to green data-center architecture and operations."
60,IT,"This paper addresses how to construct an RBAC-compatible secure cloud storage service with a user-friendly and easy-to-manage attribute-based access control (ABAC) mechanism. Similar to role hierarchies in RBAC, attribute hierarchies (considered as partial ordering relations) are introduced into attribute-based encryption (ABE) in order to define a seniority relation among all values of an attribute, whereby a user holding senior attribute values acquires permissions of his/her juniors. Based on these notations, we present a new ABE scheme called attribute-based encryption with attribute hierarchies (ABE-AH) to provide an efficient approach to implement comparison operations between attribute values on a poset derived from an attribute lattice. By using bilinear groups of a composite order, we present a practical construction of ABE-AH based on forward and backward derivation functions. Compared with prior solutions, our scheme offers a compact policy representation approach that can significantly reduce the size of private-keys and ciphertexts. To demonstrate how to use the presented solution, we illustrate how to provide richer expressive access policies to facilitate flexible access control for data access services in clouds."
61,CS,"Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. Weshowthat our contributions lead to improved accuracy of the models, especially in cases where the training data is limited."
62,CS,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."
63,CS,"This paper connects multi-agent path planning on graphs (roadmaps) to network flow problems, showing that the former can be reduced to the latter, therefore enabling the application of combinatorial network flow algorithms, as well as general linear program techniques, tomulti-agent path planning problems on graphs. Exploiting this connection, we show that when the goals are permutation invariant, the problem always has a feasible solution path set with a longest finish time of no more than n + V - 1 steps, in which n is the number of agents and V is the number of vertices of the underlying graph.We then give a complete algorithm that finds such a solution in O(nVE) time, with E being the number of edges of the graph. Taking a further step, we study time and distance optimality of the feasible solutions, show that they have a pairwise Pareto optimal structure, and again provide efficient algorithms for optimizing two of these practical objectives."
64,CS,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction."
65,CS,"We present a three-stage quantum cryptographic protocol based on public key cryptography in which each party uses its own secret key. Unlike the BB84 protocol, where the qubits are transmitted in only one direction and classical information exchanged thereafter, the communication in the proposed protocol remains quantum in each stage. A related system of key distribution is also described."
66,IS,"The covid-19 pandemic revolutionises digital financial services, and hence digital financial inclusion is essential to ensure everyone can access digital financial services and thus promote sustainable economic growth. The development and activities promoting digital financial inclusion must align and help attain 2030 Sustainable Development Goals (SDGs). While the pandemic is anticipated to increase the usage of digital financial services, it has also created challenges for certain countries. Hence, a systematic literature review explores digital financial inclusion across countries. This research finds that developing countries, mainly Asian countries, embrace and improve digital financial inclusion to help reduce poverty. However, the results indicate that in developing countries, a persistent divide exists between gender, the wealthy and the poor, and urban and rural areas regarding access to and usage of digital financial services. At the end of the study, we propose a few recommendations, focusing on improving digital infrastructure, simplifying the complicated banking procedures, and stressing the importance of financial education, enabling the smooth implication of digital financial inclusion across countries."
67,IS,"Crisis management has gained importance in the policy agendas of many countries around the world due to the increases in the number of natural disasters and terrorist attacks. This paper illustrates how the Turkish Government’s Disaster and Crisis Management System has been developed and makes a qualitative evaluation of the current disaster and crisis management systems. A literature review shows that the disaster and crisis management system in Turkey has been developed after tragic events. The paper examines what kinds of initiatives were introduced and what trends in shift have occurred. After analyzing recent cases and exploring some government initiatives, alternative approaches and suggestions are included. Turkey has developed its disaster and crisis management system since 1930, which mostly depended on experiences. The current system is governed by a centralized structure that is the responsibility of different ministries. Nonetheless, the system is very weak at the local level. Furthermore, participation of non-profit organizations is very limited at both national and local levels. Thus, coordination and management of first-response operations during crises are problematic and ineffective. Particularly, the system is not designed for different types of crises such as terrorist attacks. Crisis management in Turkey needs a more unified and flexible structure to deal with current problems effectively. Further suggestions for better implementation are also provided. The effectiveness of the disaster and crisis management system is analyzed in natural and man-made disasters. Findings show that centralized and decentralized systems have different functions in different situations."
68,IS,"Since the advent of the embryonic model almost a century ago, R&D systems have gone through an evolutionary process of development that can be classified into three generations. Today, the fourth generation of R&D is emerging that emphasizes both strategic and operational importance of knowledge management (KM). Despite the importance of KM, the network between conceptual scheme of the fourth generation R&D and practical system of KM remains a missing link. In response, the main objective of this paper is to present a framework for designing and implementing knowledge management system (KMS) for the fourth generation R&D. The proposed system is named KNOWVATION, which combines the notions of knowledge and innovation. First, the evolutionary classification of the R&D generations and the corresponding characteristics of the respective generations are defined. Second, the organizational structure and knowledge functions of the fourth generation R&D are derived. Finally, the overall design framework and detailed sub-modules are presented."
69,IS,"The adoption of e-governing practices has revolutionised the administrative machinery of governments worldwide by improving efficiency, transparency, and accountability. Researchers and administrators often aim to identify emerging research fronts and the timeline of the evolution to forecast and implement technology. In this work, we systematically investigate the trajectory of the global evolution and emerging research fronts as well as the prospects for e-governance using citation network analysis. The growth curve fitted to the number of articles published per year shows that the research activities are still in the ascendant phase. We visualise the global main path of the citation network and investigate the patterns to trace the knowledge diffusion path, major milestones, and emerging research fronts. The cluster analysis identifies the major topics of research as administration and information system management, e-governance framework design, efficiency or quality evaluation, and the application of social networks and open data leading to e-democracy. The adoption of open data and social networking for user interactions with government that leads to participatory governance are the emerging research trends. We also identify research that can have a future impact based on network parameters. The results contribute to the literature by setting the focus of future research, and assisting administrators in selecting suitable models and methodologies, and manufacturers with the development of required technical devices suitable for the upcoming phase of symbiosis."
70,IS,"The paper discusses the bright and dark sides of the relationship between human judgment and AI-driven machine learning (ML) algorithms. While discussing important issues, such as algorithm aversion, automation bias, and trust, it probes into how AI improves decision-making efficiency through predictive accuracy, resource optimisation, and data-driven insights. Even as AI can revolutionise decision-making, its effective integration must balance algorithmic output and human judgment. The most critical challenges include automation bias resulting from over-reliance on advice given by AI and algorithm aversion driven by concerns related to AI failures. Open systems, explainable AI (XAI) frameworks, and user-centered design can help to engender confidence in AI systems and alleviate these issues. Accountability, equity, and prejudice concerns raise further ethical considerations with AI. The study proposed several tactics that might mitigate such challenges: audits of ethics, adherence to legal policy, and integration of the AI systems with the company’s values. It underlines the human-AI collaboration that will be increasingly necessary, as well as hybrid models for decision-making that bring algorithmic accuracy to human intuition. It follows the case study review and empirical findings with practical lessons for organisational leaders on ethics, best deployment practices for AI, and tactical ways to engender better collaboration and trust. The conclusion outlines the need to enhance the explainability features of AI, study cognitive dynamics in decision processes, and work out ethical schemata guiding leading positions for AI. Beyond providing a roadmap for organisations to leverage the interaction of human judgment and machine intelligence to drive and achieve more ethical and effective leadership outcomes, this paper tries to contribute to the ongoing debate on AI-augmented decision-making."
71,IT,"In the present era of smart cities and homes, the private information of the patients such as their names, address, and disease is being breached on a regular basis, which indirectly related to the security of electronic health records (EHRs). The existing state-of-the-art schemes handling the security of EHRs have resulted in data being generally inaccessible to patients. These schemes struggle in providing the efficient balance between the data privacy, need for patients, and providers to regularly interact with data. Blockchain technology resolves the aforementioned issues because it shares the data in a decentralized and transactional fashion. This can be leveraged in the healthcare sector to maintain the balance between privacy and accessibility of EHRs. In this paper, we propose a Blockchain-based framework for efficient storage and maintenance of EHRs. This further provides the secure and efficient access to medical data by patients, providers, and third parties, while preserving the patient private information. The goals of this paper are to analyze how our proposed framework fulfills the needs of patients, providers, and third parties, and to understand how the framework maintains the privacy and security concerns in the healthcare 4.0."
72,IT,"The increasing development of urban centers brings serious challenges for traffic management. In this paper, we introduce a smart visual sensor, developed for a pilot project taking place in the Australian city of Liverpool (NSW). The project’s aim was to design and evaluate an edge-computing device using computer vision and deep neural networks to track in real-time multi-modal transportation while ensuring citizens’ privacy. The performance of the sensor was evaluated on a town center dataset. We also introduce the interoperable Agnosticity framework designed to collect, store and access data from multiple sensors, with results from two real-world experiments."
73,IT,"Microservice architectures provide small services that may be deployed and scaled independently of each other, and may employ different middleware stacks for their implementation. Microservice architectures intend to overcome the shortcomings of monolithic architectures where all of the application's logic and data are managed in one deployable unit. We present how the properties of microservice architectures facilitate scalability, agility and reliability at otto.de, which is one of the biggest European e-commerce platforms. In particular, we discuss vertical decomposition into self contained systems and appropriate granularity of microservices as well as coupling, integration, scalability and monitoring of microservices at otto.de. While increasing agility to more than 500 live deployments per week, high reliability is achieved by means of automated quality assurance with continuous integration and deployment."
74,IT,"While the IoT deployments multiply in a wide variety of verticals, the most IoT devices lack a built-in secure firmware update mechanism. Without such a mechanism, however, critical security vulnerabilities cannot be fixed, and the IoT devices can become a permanent liability, as demonstrated by recent large-scale attacks. In this paper, we survey open standards and open source libraries that provide useful building blocks for secure firmware updates for the constrained IoT devices–by which we mean lowpower, microcontroller-based devices such as networked sensors/actuators with a small amount of memory, among other constraints. We design and implement a prototype that leverages these building blocks and assess the security properties of this prototype. We present experimental results including first experiments with SUIT, a new IETF standard for secure IoT firmware updates. We evaluate the performance of our implementation on a variety of commercial off-the-shelf constrained IoT devices. We conclude that it is possible to create a secure, standards-compliant firmware update solution that uses the state-of-the-art security for the IoT devices with less than 32 kB of RAM and 128 kB of flash memory."
75,IT,"Improvements in data storage and processing technologies have led many managers to change how they make decisions, relying less on intuition and more on data. This trend is especially notable for the manufacturing industry where Big Data applications, i.e. data analytics, are mentioned as an important enabler of value creation with the event of the fourth industrial revolution. Designing and building the entire data value chain that enables Big Data applications in manufacturing requires new knowledge about digital technologies combined with already established knowledge about the specific manufacturing processes. This paper focuses on the convergence of these different knowledge spaces applied to a specific case of implementing a Big Data application for predictive maintenance. Every step of building the data value chain from data acquisition to system feedback is presented and discussed in terms of the major challenges that were observed during the project. Results show that, just as the literature suggests, the knowledge gaps between different domains is a key component to manage for succeeding when building Big Data applications in the context of future manufacturing and maintenance."
76,CS,"In this paper we explore private computation built on vector addition and its applications in privacy-preserving data mining. Vector addition is a surprisingly general tool for implementing many algorithms prevalent in distributed data mining. Examples include linear algorithms like voting and summation, as well as non-linear algorithms such as SVD, PCA, k-means, ID3, machine learning algorithms based on Expectation Maximization (EM), etc., and all algorithms in the statistical query model [27]. The non-linear algorithms aggregate data only in certain steps, such as conjugate gradient, which are linear in the data. We introduce a new and highly efficient VSS (Verifiable Secret-Sharing) protocol in a special but widely-applicable model that allows secret-shared arithmetic operations in such aggregation steps to be done over small fields (e.g. 32 or 64 bits). There are two major advantages: (1) in this framework private arithmetic operations have the same cost as normal arithmetic and (2) the scheme admits extremely efficient zero-knowledge (ZK) protocols for verifying properties of user data. As a concrete example, we present a very efficient zero-knowledge method based on random projection for verification that uses a linear number of inexpensive small field operations, and only a logarithmic number of large-field (1024 bits or more) cryptographic operations. Our implementation shows that the approach can achieve orders of magnitude reduction in running time over standard techniques (from hours to seconds) for large scale problems. The ZK tools provide efficient mechanisms for dealing with actively cheating users, a realistic threat in distributed data mining which has been lacking practical solutions."
77,CS,"In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements."
78,CS,"Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER."
79,CS,"Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods."
80,CS,"User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution. This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test. We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines Reinforcement Learning (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant historical data, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the RL system can gain more insights into individual test cases. We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods. The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history."
81,IS,"Blockchain-based solutions have the potential to make government operations more efficient and improve the delivery of services in the public and private sectors. Identity verification and authentication technologies, as one of the applications of blockchainbased solutions – and the focus of our own efforts at SecureKey Technologies – have been critical components in service delivery in both sectors due to their power to increase trust between citizens and the services they access. To convert trust into solid value added, identities must be validated through highly-reliable technologies, such as blockchain, that have the capacity to reduce cost and fraud and to simplify the experience for customers while also keeping out the bad actors. With identities migrating to digital platforms, organizations and citizens need to be able to transact with reduced friction even as more counter-bound services move to online delivery. In this article, drawing on our own experiences with an ecosystem approach to digital identity, we describe the potential value of using blockchain technology to address the present and future challenges of identity verification and authentication within a Canadian context."
82,IS,"The objectives of this study are to identify and systematize scholarly articles on the use of information system to support sustainable supply chain management and to suggest future research opportunities. Therefore, a structured literature review was conducted. The most relevant studies identified were classified and categorized into seven dimensions: research context, research focus, research method, sector analyzed, information system (IS) beneficiaries, relationship between IS and green supply chain practices, and performance benefits. The main authors and articles on this particular topic were identified. In addition, it was concluded that IS is an important support tool for sustainable supply chain management practices since it brings benefits to the organization, suppliers, and customers. Furthermore, IS positively influences the operational, financial, and environmental performance of the organization. However, further advances in the literature are still needed. The major contribution of this research is related to the recommendations that provide opportunities for future research."
83,IS,"Remote work is a forced measure introduced by employers in order to prevent a viral infection. For employees, there are pluses in remote work - saving time and money on the road and high labour productivity because nothing distracts. There is a separate issue of information security for the employer when organising such a work regime for their employees. Any use of materials is allowed only with a hyperlink. Nowadays, in the realities of distance work, information security is coming first. Employees send all the information online; they use their data and send confidential information. Protection of personal data becomes a crucial point. The article deals with problems of ensuring information security in the field of remote work. The problems of information security during restrictive actions in connection with the coronavirus pandemic and the transfer of personnel to remote work are discussed. The threat of information leaks through remote workers is relatively high since the specialists responsible for the organisation's information security do not have the opportunity to apply the entire arsenal of technical means and policies, with the help of which security is ensured at workstations in the office. Information leakage will lead to severe problems, so it is essential to consider what means you can use to ensure the company's information security."
84,IS,"The role of Information Systems in Mergers and Acquisitions (M&A) becomes increasingly important as the need for speed of reaction and information is growing. A model for IS integration in M&A is presented; this includes the categories and strategic objectives of external growth as well as consideration of the possible choices for the hardware and software configuration after completion of the M&A. The basic variables of the model are defined and the possible IS integration strategies are classified in a matrix having, as dimensions, the software configuration and the computer architecture. The IS integration model resulting from these basic elements is divided into a descriptive model and a decision support model: the first one shows the relations among the involved variables, while the second provides a practical tool for decision-making. Both these models have been tested with a survey guided by a questionnaire; Factor Analysis was used to analyze the results."
85,IS,"Keeping employees aligned with modern trends and developments in their professional areas is the main focus of a lifelong lea rning approach. That becomes even more important for such dynamic industries like Information Technology. Therefore, it’s crucial to extend existing e-learning management system with an adaptive training component that enables the effective study of on-demand skills, leading to a broader range of candidates available for project management to select from and consequently improving the overall performance of an IT company. To improve the existing learning process according to company's and employee's needs the overview of a typical learning management system functionality is given in this paper. The main benefits from the adoption of a learning management system in small and medium-sized IT-companies were discussed, analysis of their features and problems was given. The adjustment plan for the typical learning management system to be suitable for the information technology domain including module of the adaptive learning content selection using the basic principles of graph theory was proposed to reduce the time of the learning process. LMS OpenOLAT was reworked according to the adjustment plan which is reflected by the number of diagrams such as sequence diagram, IDEF0 business process description, activity diagram that shows search algorithm steps and application component diagram. Also, GUI was adjusted to provide user with a good look and feel. The benefits of the proposed approach in business processes of IT-company are shown using “Academy – Smart”. To prove the efficiency of the proposed algorithm, real courses were used. Based on the learning material, provided by “Coursera”, a number of test cases was formed and analyzed. After applying adaptive content selection algorithm according to the models of “Academy – Smart” employees, learning time was reduced and optimized. This investigation has shown significant improvement in the resource management process and acceleration of the learning process for employees."
86,IT,"The implementation of the vision of Industry 4.0 towards the Smart Factory requires, in addition to data collection and constant networking of all devices and machines, the rapid processing of large amounts of data. The concept of Cloud Computing enables central processing, but the latency of the transmission to the cloud requires too much time for real time applications. Therefore, the concept of Edge or Fog Computing has arisen in the recent years. Thereby, the calculation is no longer central, but decentralized at the network's edge. This eliminates high transmission latency and offers high potential for calculations, especially analytical calculations, in real time. In the course of this paper we present the State-of-the-art of Edge Computing within the Smart Factory. Thereby, the results of this contribution depict the current thematic priorities of scientific discussion and derive the possibilities for the support of RTA by EC from it."
87,IT,"Federated learning, as a promising machine learning approach, has emerged to leverage a distributed personalized dataset from a number of nodes, for example, mobile devices, to improve performance while simultaneously providing privacy preservation for mobile users. In federated learning, training data is widely distributed and maintained on the mobile devices as workers. A central aggregator updates a global model by collecting local updates from mobile devices using their local training data to train the global model in each iteration. However, unreliable data may be uploaded by the mobile devices (i.e., workers), leading to frauds in tasks of federated learning. The workers may perform unreliable updates intentionally, for example, the data poisoning attack, or unintentionally, for example, low-quality data caused by energy constraints or high-speed mobility. Therefore, finding out trusted and reliable workers in federated learning tasks becomes critical. In this article, the concept of reputation is introduced as a metric. Based on this metric, a reliable worker selection scheme is proposed for federated learning tasks. Consortium blockchain is leveraged as a decentralized approach for achieving efficient reputation management of the workers without repudiation and tampering. By numerical analysis, the proposed approach is demonstrated to improve the reliability of federated learning tasks in mobile networks."
88,IT,"Cloud-native computing principles such as virtualization and orchestration are key to transferring to the promising paradigm of edge computing. Challenges of containerization, operative models and scarce availability of established tools make a thorough review indispensable. Therefore, the authors have described the practical methods and tools found in the literature as well as in current community-led development projects, and have thoroughly exposed the future directions of the field. Container virtualization and its orchestration through Kubernetes have dominated the cloud computing domain, while major efforts have been recently recorded focused on the adaptation of these technologies to the edge. Such initiatives have addressed either the reduction of container engines and the development of specific tailored operating systems or the development of smaller K8s distributions and edge-focused adaptations (such as KubeEdge). Finally, new workload virtualization approaches, such as WebAssembly modules together with the joint orchestration of these heterogeneous workloads, seem to be the topics to pay attention to in the short to medium term."
89,IT,"Addressing the recent trend of the massive demand for resources and ubiquitous use for all citizens has led to the conceptualization of technologies such as the Internet of Things (IoT) and smart cities. Ubiquitous IoT connectivity can be achieved to serve both urban and underserved remote areas such as rural communities by deploying 5G mobile networks with Low Power Wide Area Network (LPWAN). The current architectures will not offer flexible connectivity to many IoT applications due to high service demand, data exchange, emerging technologies, and security challenges. Hence, this paper explores various architectures that consider a hybrid 5G-LPWAN-IoT and Smart Cities. This includes security challenges as well as endogenous security and solutions in 5G and LPWAN-IoT. The slicing of virtual networks using software-defined network (SDN)/network function virtualization (NFV) based on the different quality of service (QoS) to satisfy different services and quality of experience (QoE) is presented. Also, a strategy that considers the implementation of 5G jointly with Weightless-N (TVWS) technologies to reduce the cell edge interference is considered. Discussions on the need for ubiquity connectivity leveraging 5G and LPWAN-IoT are presented. In addition, future research directions are presented, including a unified 5G network and LPWAN-IoT architecture that will holistically support integration with emerging technologies and endogenous security for improved/secured smart cities and remote areas IoT applications. Finally, the use of LPWAN jointly with low earth orbit (LEO) satellites for ubiquitous IoT connectivity is advocated in this paper."
90,IT,"Successful integration of deep neural networks (DNNs) or deep learning (DL) has resulted in breakthroughs in many areas. However, deploying these highly accurate models for data-driven, learned, automatic, and practical machine learning (ML) solutions to end-user applications remains challenging. DL algorithms are often computationally expensive, power-hungry, and require large memory to process complex and iterative operations of millions of parameters. Hence, training and inference of DL models are typically performed on high-performance computing (HPC) clusters in the cloud. Data transmission to the cloud results in high latency, round-trip delay, security and privacy concerns, and the inability of real-time decisions. Thus, processing on edge devices can significantly reduce cloud transmission cost. Edge devices are end devices closest to the user, such as mobile phones, cyber–physical systems (CPSs), wearables, the Internet of Things (IoT), embedded and autonomous systems, and intelligent sensors. These devices have limited memory, computing resources, and power-handling capability. Therefore, optimization techniques at both the hardware and software levels have been developed to handle the DL deployment efficiently on the edge. Understanding the existing research, challenges, and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence (AI) capability. Mainly, four research directions have been pursued for efficient DL inference on edge devices: 1) novel DL architecture and algorithm design; 2) optimization of existing DL methods; 3) development of algorithm–hardware codesign; and 4) efficient accelerator design for DL deployment. This article focuses on surveying each of the four research directions, providing a comprehensive review of the state-of-the-art tools and techniques for efficient edge inference."
91,CS,"Ethereum is a framework for cryptocurrencies which uses blockchain technology to provide an open global computing platform, called the Ethereum Virtual Machine (EVM). EVM executes bytecode on a simple stack machine. Programmers do not usually write EVM code; instead, they can program in a JavaScript-like language, called Solidity, that compiles to bytecode. Since the main purpose of EVM is to execute smart contracts that manage and transfer digital assets (called Ether), security is of paramount importance. However, writing secure smart contracts can be extremely difficult: due to the openness of Ethereum, both programs and pseudonymous users can call into the public methods of other programs, leading to potentially dangerous compositions of trusted and untrusted code. This risk was recently illustrated by an attack on TheDAO contract that exploited subtle details of the EVM semantics to transfer roughly $50M worth of Ether into the control of an attacker. In this paper, we outline a framework to analyze and verify both the runtime safety and the functional correctness of Ethereum contracts by translation to F*, a functional programming language aimed at program verification."
92,CS,"This paper presents FUNDED (Flow-sensitive vUl-Nerability coDE Detection), a novel learning framework for building vulnerability detection models. Funded leverages the advances in graph neural networks (GNNs) to develop a novel graph-based learning method to capture and reason about the program’s control, data, and call dependencies. Unlike prior work that treats the program as a sequential sequence or an untyped graph, Funded learns and operates on a graph representation of the program source code, in which individual statements are connected to other statements through relational edges. By capturing the program syntax, semantics and flows, Funded finds better code representation for the downstream software vulnerability detection task. To provide sufficient training data to build an effective deep learning model, we combine probabilistic learning and statistical assessments to automatically gather high-quality training samples from open-source projects. This provides many real-life vulnerable code training samples to complement the limited vulnerable code samples available in standard vulnerability databases. We apply Funded to identify software vulnerabilities at the function level from program source code. We evaluate Funded on large real-world datasets with programs written in C, Java, Swift and Php, and compare it against six state-of-the-art code vulnerability detection models. Experimental results show that Funded significantly outperforms alternative approaches across evaluation settings."
93,CS,"A system can accomplish an objective specified in temporal logic while interacting with an unknown, dynamic but rule-governed environment, by employing grammatical inference and adapting its plan of action on-line. The purposeful interaction of the system with its unknown environment can be described by a deterministic two-player zero-sum game. Using special new product operations, the whole game can be expressed with a factored, modular representation. This representation not only offers computational benefits but also isolates the unknown behavior of the dynamic environment in a particular subsystem, which then becomes the target of learning. As the fidelity of the identified environment model increases, the strategy synthesized based on the learned hypothesis converges in finite time to the one that satisfies the task specification."
94,CS,"We initiate the systematic study of the energy complexity of algorithms (in addition to time and space complexity) based on Landauer's Principle in physics, which gives a lower bound on the amount of energy a system must dissipate if it destroys information. We propose energy-aware variations of three standard models of computation: circuit RAM, word RAM, and transdichotomous RAM. On top of these models, we build familiar high-level primitives such as control logic, memory allocation, and garbage collection with zero energy complexity and only constant-factor overheads in space and time complexity, enabling simple expression of energy-efficient algorithms. We analyze several classic algorithms in our models and develop low-energy variations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays. We explore the time/space/energy trade-off and develop several general techniques for analyzing algorithms and reducing their energy complexity. These results lay a theoretical foundation for a new field of semi-reversible computing and provide a new framework for the investigation of algorithms."
95,CS,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation."
96,IS,"Organizations risk losing their competitive edge as they struggle to find and hire qualified talent. Hiring personnel turn to artificial intelligence (AI) tools to help acquire talent, increase efficiency, and reduce costs. Yet despite the best intentions for integrating fair and evidence-based systems, exacerbated levels of bias may occur from using these tools. Drawing from scholarship on process expertise and emerging practices of AI use at work, I provide a case study of 42 high-volume recruiters and uncover how hiring personnel enact and justify unsystematic sourcing practices within the confines of their held expertise, organizational demands, and technology choices. I explain how AI-based hiring decisions in organizations are context dependent and blend the capabilities of algorithmic-powered tools with choices and judgments made by process experts. I conclude by offering theoretical and practical considerations for expertise, hiring, and the integration of algorithms at work."
97,IS,"Industry 4.0-based manufacturing systems are equipped with Cyber-Physical Systems that are characterized by a strong interlinkage between the real world and the digital one: actions in one world have an impact on the other. In this paradigm, Digital Twins (DT) are defined as simulation models that are both getting data from the field and triggering actions on the physical equipment. However, most of the claimed DT in literature are only replicating the real system in a synchronized fashion, without feeding back actions on the control system of the equipment. In literature, these are referred to as Digital Shadows (DS). The paper proposes a way to integrate a DS simulation model with the Manufacturing Execution System (MES) in this way creating a DT. The MES-integrated DT is used for decision making thanks to the presence of an intelligence layer that hosts the rules and the knowledge to choose among alternatives. The paper proposes two frameworks based on the MES-integrated DT: one for managing error states and one for triggering disassembly processes as a consequence of low assembly quality. The DT simulation is developed and integrated with the MES of the Industry 4.0 Laboratory at the School of Management of Politecnico di Milano, where the proposed frameworks have been tested and validated."
98,IS,"We propose an organizational culture-based explanation of the level of difficulty of clinical information system (CIS) implementation and of the practices that can contribute to reduce the level of difficulty of this process. Adopting an analytic induction approach, we developed initial theoretical propositions based on a three-perspective conceptualization of organizational culture: integration, differentiation, and fragmentation. Using data from three cases of CIS implementation, we first performed a deductive analysis to test our propositions on the relationships between culture, CIS characteristics, implementation practices, and the level of implementation difficulty. Then, applying an inductive analysis strategy, we re-analyzed the data and developed new propositions. Our analysis shows that four values play a central role in CIS implementation. Two values, quality of care and efficiency of clinical practices, are key from an integration perspective; two others, professional status/autonomy and medical dominance, are paramount from a differentiation perspective. A fragmentation perspective analysis reveals that hospital users sometimes have ambiguous interpretations of some CIS characteristics and/or implementation practices in terms of their consistency with these four values. Overall, the proposed theory provides a rich explanation of the relationships between CIS characteristics, implementation practices, user values, and the level of difficulty of the implementation process."
99,IS,"Social media has become a part of life for a larger segment of the people globally. Businesses are also taking advantage of it, to instantly and easily create and share information with the customers and get benefited from this seamless communication. As a result of the increase in both social media platforms and users, the need to monitor, extract, analyse and report the data being created on these platforms has also increased for the Businesses. Present work identifies the social media analytics (SMA) process and online retail application; it identifies the different phases of the SMA process with their integration to the online retail strategy, challenges, and opportunities. SMA metrics for customer life-cycle (acquisition to enhancement) are discussed. A comparative analysis has been presented for different SMA tools to identify their utility to the online retail."
100,IS,"In Ghana, academic record-keeping and verification methods have become increasingly burdensome, error-prone, and susceptible to data security, privacy, and fraud prevention issues. Traditional systems often rely on paper-based records or fragmented digital databases, which can lead to inefficiencies, human errors, and difficulties in authenticating and validating academic credentials. These challenges can hinder students' and graduates' efforts to pursue further education or employment opportunities and create administrative burdens for educational institutions and employers. Blockchain technology, an emerging innovation, offers a promising solution to address these challenges. By digitizing and securely storing academic records on a blockchain, the system will facilitate efficient, accurate, and tamper-proof verification of credentials. This will reduce errors and fraud and improve academic qualifications' overall reliability and credibility. Furthermore, the system will enhance data privacy by allowing individuals to control who can access their records and under what conditions. This applied project creates a user-friendly platform that simplifies the verification process for students, graduates, educational institutions, and employers. By harnessing the power of blockchain technology, we can revolutionize academic record-keeping and verification in Ghana, ensuring a more secure, efficient, and trustworthy system for all stakeholders involved."
101,IT,"Currently, enterprises have to make quick and resilient responses to changing market requirements. In light of this, low-code development platforms provide the technology mechanisms to facilitate and automate the development of software applications to support current enterprise needs and promote digital transformation. Based on a theory-building research methodology through the literature and other information sources review, the main contribution of this paper is the current characterisation of the emerging low-code domain following the foundations of the computer-aided software engineering field. A context analysis, focused on the current status of research related to the low-code development platforms, is performed. Moreover, benchmarking among the existing low-code development platforms addressed to manufacturing industry is analysed to identify the current lacking features. As an illustrative example of the emerging low-code paradigm and respond to the identified uncovered features, the virtual factory open operating system (vf-OS) platform is described as an open multi-sided low-code framework able to manage the overall network of a collaborative manufacturing and logistics environment that enables humans, applications, and Internet of Things (IoT) devices to seamlessly communicate and interoperate in the interconnected environment, promoting resilient digital transformation."
102,IT,"IoT-enabled applications, such as cloud manufacturing, guided water quality or air quality analysis, energy-conscious societal applications, and smart agricultural economics, are designed using a blend of high-end computing technologies, such as cloud, edge, and fog. Smart cities and governmental authorities keep a keen eye out for implementing IoT applications in an automated/decentralized approach with enhanced security measures so that tens of thousands of users, including entrepreneurs, are benefited. Existing IoT architectures are prone to energy inefficiency or resource underutilization problems due to the avoidance of apt technologies, such as serverless computing. This article proposes to set forth a serverless blockchain-enabled IoT architecture for societal applications. It explores the existing IoT architectures and pinpoints the advantages of applying serverless blockchains on IoT architectures. In addition, the proposed IoT architecture is illustrated with a specific use case of IoT societal applications namely air quality monitoring for smart cities (AQMS). This article discloses how air quality sensor data from defective industries were securely transacted to blockchain networks surpassing from the three levels of computing namely edge, fog, and cloud while utilizing serverless and server-oriented functions. In addition, this article exposes a list of the most potent serverless functions that assist AQMS IoT societal applications in detail. The IoT architecture, discussed in this article, will enable innovations and research works for IoT developers and researchers."
103,IT,Cloud security is the primary need for the vital Information Technology industry. It adopts dynamic qualities and enhances various heterogeneous resources for its applications. Cloud environment enables virtual technologies using virtual machine placement method. Therefore any virtual machines can move between any physical devices for achieving cost optimization and network traffic minimization sake. Multi-tenancy means the use of multiple systems applications or data from various organizations residing on a single physical device. Here a single instance of the application software running on the service providers’ platform can be accessed by multiple clients simultaneously. Multi-tenancy concept refers to both public as well as private cloud model which relates to all the three layers in cloud computing system. Adaptive particle swarm optimization is proposed in this paper which also addresses the multi-tenancy process which enables high resource utilization service under cloud storage network.
104,IT,"Railways are considered to be an environmentally friendly and efficient means of transport for people and goods with increasing importance in the transport policies of many countries. However, the infrastructure and the substantial demand for maintenance create additional costs for railway operators. To overcome outdated maintenance modes, implementation of new solutions, optimization of maintenance activities, and resource utilization are required. Through a systematic literature review, this article evaluates new approaches toward implementing predictive maintenance in the railway domain. A comprehensive search, including the IEEE Xplore, Science Direct, and ACM Digital Library, has been conducted, focusing on papers related to predictive maintenance and railway systems, published in peer-reviewed journals since 2016. The selected papers were analyzed and grouped to allocate the research purposes as well as the considered assets, components, predicted defects, and maintenance conditions. Furthermore, the utilized predictive maintenance algorithms and their limitations are structured and evaluated. Analysis shows that a great variety of algorithms were used for either defect detection or the prediction of conditions of 20 different components, which are critical for the safety and availability of railway operations. The study shows that the proposed approaches were successfully tested and yielded great potential for predictive maintenance solutions. Researchers state to enhance proposed solutions within their future work, increasing accuracy and performance and widening the area of application in the railway domain."
105,IT,"In this paper, we propose a mobility-aware scheme, named MobiPlace, to address the controller placement problem (CPP) at the Road Side Units (RSUs) in Software-Defined Vehicular Networks (SDVNs). MobiPlace places local controllers at the selected RSUs to reduce the operational delay experienced in traditional SDVN architecture, where controllers are placed at the cloud. Additionally, we consider the effect of dynamic road traffic and propose dynamic adjustment of the placement of the controller with minimal changes. In contrast to the existing literature, we infuse traffic monitoring and traffic prediction strategies to ensure accurate delivery of control messages and data packets. We formulate an Integer Linear Program (ILP) and propose a solution approach consisting of three modules - mobility management, controller placement, and controller selection. The mobility management module uses Markov predictor to predict vehicle movement and reduce controller synchronization overhead when a vehicle moves to a different controller's coverage area. The controller placement module applies a simulated annealing-based algorithm to select potential RSUs that serve as local controllers. The controller selection module determines the preferable controller location for processing the service requests. Simulation results depict that MobiPlace reduces the average flow setup delay by 13.85% compared to the existing state-of-the-art."