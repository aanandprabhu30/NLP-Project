Title,Abstract,Subfield,Link
Considerations for Cloud Security Operations,"Information Security in Cloud Computing environments is explored. Cloud Computing is presented, security needs are discussed, and mitigation approaches are listed. Topics covered include Information Security, Cloud Computing, Private Cloud, Public Cloud, SaaS, PaaS, IaaS, ISO 27001, OWASP, Secure SDLC.",CLD,http://arxiv.org/abs/1601.06289v1
Security of Cloud FPGAs: A Survey,"Integrating Field Programmable Gate Arrays (FPGAs) with cloud computing instances is a rapidly emerging trend on commercial cloud computing platforms such as Amazon Web Services (AWS), Huawei cloud, and Alibaba cloud. Cloud FPGAs allow cloud users to build hardware accelerators to speed up the computation in the cloud. However, since the cloud FPGA technology is still in its infancy, the security implications of this integration of FPGAs in the cloud are not clear. In this paper, we survey the emerging field of cloud FPGA security, providing a comprehensive overview of the security issues related to cloud FPGAs, and highlighting future challenges in this research area.",CLD,http://arxiv.org/abs/2005.04867v1
Interoperability and Standardization of Intercloud Cloud Computing,"Cloud computing is getting mature, and the interoperability and standardization of the clouds is still waiting to be solved. This paper discussed the interoperability among clouds about message transmission, data transmission and virtual machine transfer. Starting from IEEE Pioneering Cloud Computing Initiative, this paper discussed about standardization of the cloud computing, especially intercloud cloud computing. This paper also discussed the standardization from the market-oriented view.",CLD,http://arxiv.org/abs/1212.5956v1
Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management,"In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.",CLD,http://arxiv.org/abs/2402.17216v1
Evolution of Cloud Storage as Cloud Computing Infrastructure Service,"Enterprises are driving towards less cost, more availability, agility, managed risk - all of which is accelerated towards Cloud Computing. Cloud is not a particular product, but a way of delivering IT services that are consumable on demand, elastic to scale up and down as needed, and follow a pay-for-usage model. Out of the three common types of cloud computing service models, Infrastructure as a Service (IaaS) is a service model that provides servers, computing power, network bandwidth and Storage capacity, as a service to their subscribers. Cloud can relate to many things but without the fundamental storage pieces, which is provided as a service namely Cloud Storage, none of the other applications is possible. This paper introduces Cloud Storage, which covers the key technologies in cloud computing and Cloud Storage, management insights about cloud computing, different types of cloud services, driving forces of cloud computing and cloud storage, advantages and challenges of cloud storage and concludes by pinpointing few challenges to be addressed by the cloud storage providers.",CLD,http://arxiv.org/abs/1308.1303v1
A Survey on Cloud Security Issues and Techniques,"Today, cloud computing is an emerging way of computing in computer science. Cloud computing is a set of resources and services that are offered by the network or internet. Cloud computing extends various computing techniques like grid computing, distributed computing. Today cloud computing is used in both industrial field and academic field. Cloud facilitates its users by providing virtual resources via internet. As the field of cloud computing is spreading the new techniques are developing. This increase in cloud computing environment also increases security challenges for cloud developers. Users of cloud save their data in the cloud hence the lack of security in cloud can lose the users trust. In this paper we will discuss some of the cloud security issues in various aspects like multi-tenancy, elasticity, availability etc. The paper also discuss existing security techniques and approaches for a secure cloud. This paper will enable researchers and professionals to know about different security threats and models and tools proposed.",CLD,http://arxiv.org/abs/1403.5627v1
Discussion of various models related to cloud performance,"This paper discusses the various models related to cloud computing. Knowing the metrics related to infrastructure is very critical to enhance the performance of cloud services. Various metrics related to clouds such as pageview response time, admission control and enforcing elasticity to cloud infrastructure are very crucial in analyzing the characteristics of the cloud to enhance the cloud performance.",CLD,http://arxiv.org/abs/1505.00236v1
A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment,Cloud Computing is a new trend emerging in IT environment with huge requirements of infrastructure and resources. Load Balancing is an important aspect of cloud computing environment. Efficient load balancing scheme ensures efficient resource utilization by provisioning of resources to cloud users on demand basis in pay as you say manner. Load Balancing may even support prioritizing users by applying appropriate scheduling criteria. This paper presents various load balancing schemes in different cloud environment based on requirements specified in Service Level Agreement (SLA).,CLD,http://arxiv.org/abs/1403.6918v1
Resource Management in Cloud Computing: Classification and Taxonomy,Cloud Computing is a new era of remote computing / Internet based computing where one can access their personal resources easily from any computer through Internet. Cloud delivers computing as a utility as it is available to the cloud consumers on demand. It is a simple pay-per-use consumer-provider service model. It contains large number of shared resources. So Resource Management is always a major issue in cloud computing like any other computing paradigm. Due to the availability of finite resources it is very challenging for cloud providers to provide all the requested resources. From the cloud providers perspective cloud resources must be allocated in a fair and efficient manner. Research Survey is not available from the perspective of resource management as a process in cloud computing. So this research paper provides a detailed sequential view / steps on resource management in cloud computing. Firstly this research paper classifies various resources in cloud computing. It also gives taxonomy on resource management in cloud computing through which one can do further research. Lastly comparisons on various resource management algorithms has been presented.,CLD,http://arxiv.org/abs/1703.00374v1
Securing the Data in Clouds with Hyperelliptic Curve Cryptography,"In todays world, Cloud computing has attracted research communities as it provides services in reduced cost due to virtualizing all the necessary resources. Even modern business architecture depends upon Cloud computing .As it is a internet based utility, which provides various services over a network, it is prone to network based attacks. Hence security in clouds is the most important in case of cloud computing. Cloud Security concerns the customer to fully rely on storing data on clouds. That is why Cloud security has attracted attention of the research community. This paper will discuss securing the data in clouds by implementing key agreement, encryption and signature verification/generation with hyperelliptic curve cryptography.",CLD,http://arxiv.org/abs/1411.6771v1
A Survey on Cloud Computing Security,"Computation encounter the new approach of cloud computing which maybe keeps the world and possibly can prepare all the human's necessities. In other words, cloud computing is the subsequent regular step in the evolution of on-demand information technology services and products. The Cloud is a metaphor for the Internet and is a concept for the covered complicated infrastructure; it also depends on sketching in computer network diagrams. In this paper we will focus on concept of cloud computing, cloud deployment models, cloud security challenges encryption and data protection, privacy and security and data management and movement from grid to cloud.",CLD,http://arxiv.org/abs/1206.5468v1
A Preliminary Study On Emerging Cloud Computing Security Challenges,"Cloud computing is the internet based provisioning of the computing resources, software, and information on demand. Cloud Computing is referred to as one of most recent emerging paradigms of computing utilities. Since Cloud computing is the dominant infrastructure of the shared services over the internet, it is important to be aware of the security risk and the challenges associated with this emerging computing paradigm. This survey provides a brief introduction to the cloud computing, its major characteristics, and service models. It also explores cloud security threats, lists a few security solutions , and proposes a promsing research direction to deal with the evolving security challenges in Cloud computing.",CLD,http://arxiv.org/abs/1808.04143v1
Framework for cloud computing adoption: A road map for Smes to cloud migration,"Small and Medium size Enterprises (SME) are considered as a backbone of many developing and developed economies of the world; they are the driving force to any major economy across the globe. Through Cloud Computing firms outsource their entire information technology (IT) process while concentrating more on their core business. It allows businesses to cut down heavy cost incurred over IT infrastructure without losing focus on customer needs. However, Cloud industry to an extent has struggled to grow among SMEs due to the reluctance and concerns expressed by them. Throughout the course of this study several interviews were conducted and the literature was reviewed to understand how cloud providers offer services and what challenges SMEs are facing. The study identified issues like cloud knowledge, interoperability, security and contractual concerns to be hindering SMEs adoption of cloud services. From the interviews common practices followed by cloud vendors and what concerns SMEs have were identified as a basis for a cloud framework which will bridge gaps between cloud vendors and SMEs. A stepwise framework for cloud adoption is formulated which identifies and provides recommendation to four most predominant challenges which are hurting cloud industry and taking SMEs away from cloud computing, as well as guide SMEs aiding in successful cloud adoption. Moreover, this framework streamlines the cloud adoption process for SMEs by removing ambiguity in regards to fundamentals associated with their organisation and cloud adoption process.",CLD,http://arxiv.org/abs/1601.01608v1
Usage of Cloud Computing Simulators and Future Systems For Computational Research,"Cloud Computing is an Internet based computing, whereby shared resources, software and information, are provided to computers and devices on demand, like the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service) are used as a business model for Cloud Computing. Nowadays, the adoption and deployment of Cloud Computing is increasing in various domains, forcing researchers to conduct research in the area of Cloud Computing globally. Setting up the research environment is critical for the researchers in the developing countries to evaluate the research outputs. Currently, modeling, simulation technology and access of resources from various university data centers has become a useful and powerful tool in cloud computing research. Several cloud simulators have been specifically developed by various universities to carry out Cloud Computing research, including CloudSim, SPECI, Green Cloud and Future Systems (the Indiana University machines India, Bravo, Delta, Echo and Foxtrot) supports leading edge data science research and a broad range of computing-enabled education as well as integration of ideas from cloud and HPC systems. In this paper, the features, suitability, adaptability and the learning curve of the existing Cloud Computing simulators and Future Systems are reviewed and analyzed.",CLD,http://arxiv.org/abs/1605.00085v1
Is Cloud Computing Steganography-proof?,The paper focuses on characterisation of information hiding possibilities in Cloud Computing. After general introduction to cloud computing and its security we move to brief description of steganography. In particular we introduce classification of steganographic communication scenarios in cloud computing which is based on location of the steganograms receiver. These scenarios as well as the threats that steganographic methods can cause must be taken into account when designing secure cloud computing services.,CLD,http://arxiv.org/abs/1107.4077v1
Surrogate cloud fields with measured cloud properties,"This paper describes two new methods to generate 2D and 3D cloud fields based on 1D and 2D ground based profiler meas-urements. These cloud fields share desired statistical properties with real cloud fields. As they, however, are similar but not the same as real clouds, we call them surrogate clouds. One important advantage of the new methods is that the amplitude distribution of cloud liquid water is also exactly determined by the measurement: The surrogate clouds made with the classi-cal methods such as the Fourier method and the Bounded Cascade method are Gaussian and 'log-normal-like', respectively. Our first new method iteratively creates a time series with a measured amplitude distribution and power spectrum. Our sec-ond method uses an evolutionary search algorithm to generate cloud fields with practically arbitrary constraints. These clouds will be used to study the relation between radiation and cloud structure.",CLD,http://arxiv.org/abs/physics/0306067v1
SecLaaS: Secure Logging-as-a-Service for Cloud Forensics,"Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. Analyzing various logs (e.g., process logs, network logs) plays a vital role in computer forensics. Unfortunately, collecting logs from a cloud is very hard given the black-box nature of clouds and the multi-tenant cloud models, where many users share the same processing and network resources. Researchers have proposed using log API or cloud management console to mitigate the challenges of collecting logs from cloud infrastructure. However, there has been no concrete work, which shows how to provide cloud logs to investigator while preserving users' privacy and integrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service (SecLaaS), which stores virtual machines' logs and provides access to forensic investigators ensuring the confidentiality of the cloud users. Additionally, SeclaaS preserves proofs of past log and thus protects the integrity of the logs from dishonest investigators or cloud providers. Finally, we evaluate the feasibility of the scheme by implementing SecLaaS for network access logs in OpenStack - a popular open source cloud platform.",CLD,http://arxiv.org/abs/1302.6267v1
"soCloud: A service-oriented component-based PaaS for managing portability, provisioning, elasticity, and high availability across multiple clouds","Multi-cloud computing is a promising paradigm to support very large scale world wide distributed applications. Multi-cloud computing is the usage of multiple, independent cloud environments, which assumed no priori agreement between cloud providers or third party. However, multi-cloud computing has to face several key challenges such as portability, provisioning, elasticity, and high availability. Developers will not only have to deploy applications to a specific cloud, but will also have to consider application portability from one cloud to another, and to deploy distributed applications spanning multiple clouds. This article presents soCloud a service-oriented component-based Platform as a Service (PaaS) for managing portability, elasticity, provisioning, and high availability across multiple clouds. soCloud is based on the OASIS Service Component Architecture (SCA) standard in order to address portability. soCloud provides services for managing provisioning, elasticity, and high availability across multiple clouds. soCloud has been deployed and evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE, Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an Eucalyptus private cloud.",CLD,http://arxiv.org/abs/1407.1963v1
Cloud Computing and Grid Computing 360-Degree Compared,"Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",CLD,http://arxiv.org/abs/0901.0131v1
Cloud Adoption A Modern Approach,"Todays Information Technology world is cloud-centric. Companies are intrigued to migrate their workload private cloud from on-premise Datacenter to Public cloud to take advantage of the latest innovations. It drives the business growth and competitiveness of the organization. At the same time, it is important for Enterprise Architects to understand the drawbacks and challenges to migrate the workload to Cloud. This paper aims to identify the key factors to migrate the workload to the cloud. It also helps an organization to identify the candidate for cloud migration. An impulsive decision to move to the Cloud may be detrimental to an organization. Also, I will discuss one case study to see the benefits and disadvantages of cloud migration. This will help the organization to maximize its ROI.",CLD,http://arxiv.org/abs/2305.18308v1
Cloud Security and Security Challenges Revisited,"In recent years, Cloud Computing has transformed local businesses and created new business models on the Internet- and Cloud services are still flourishing. But after the emphatic hype in the early years, a more realistic perception of Cloud services has emerged. One reason for this surely is that today, Cloud Computing is considered as an established and well-accepted technology and no longer as a technical novelty. But the second reason for this assessment might also be numerous security issues that Cloud Computing in general or specific Cloud services have experienced since then. In this paper, we revisit attacks on Cloud services and Cloud-related attack vectors that have been published in recent years. We then consider successful or proposed solutions to cope with these challenges. Based on these findings, we apply a security metric in order to rank all these Cloud-related security challenges concerning their severity. This should assist security professionals to prioritize their efforts toward addressing these issues.",CLD,http://arxiv.org/abs/2405.11350v1
"11-Year Warm Cloud Modification Experiment in Maharashtra State, India","A warm cloud modification experiment was carried out in an area of 4800 Sq.Km in the Pune region,India, during the 11-summer monsoon (June-September) seasons (1973-74, 1976, 1979-86). A double-area cross-over design with area randomization was adopted and an instrumented aircraft was used for seeding and cloud physical measurements. Finely pulverised salt (sodium chloride) particles were released into the monsoon clouds (cumulus and stratocumulus) during aircraft penetrations into the clouds at a height of 200-300 m above the cloud-base. The warm cloud responses to salt seeding are found to be critically dependent on the cloud physical characteristics e.g., vertical thickness and liquid water content. Clouds with vertical thickness greater than 1 km, LWC greater than 0.5 gm/cubic m when seeded with salt particles (modal diameter 10 micro m, concentration 1 per litre of cloud air) produced increase in rainfall of 24 per cent significant at 4 per cent level. Shallow clouds (vertical thickness less than 1 km, LWC less than 0.5 gm/cubic m) when seeded showed tendency for dissipation. The cloud physical observations made in not-seeded (control) and seeded (target) clouds have provided some useful evidence to test the applicability of the warm cloud modification hypothesis. The results of the cloud model computations suggested that moderate convergence at the cloud-base is essential for the cloud growth and development of precipitation in the real world. Hygroscopic particle seeding of warm clouds under favourable dynamical conditions (convergence at the cloud-base level) may result in the acceleration of the collision-coalescence process resulting in the enhancement of rainfall.",CLD,http://arxiv.org/abs/physics/9812046v1
Application of Ontologies in Cloud Computing: The State-Of-The-Art,"This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities.",CLD,http://arxiv.org/abs/1610.02333v1
A Slow Read attack Using Cloud,"Cloud computing relies on sharing computing resources rather than having local servers or personal devices to handle applications. Nowadays, cloud computing has become one of the fastest growing fields in information technology. However, several new security issues of cloud computing have emerged due to its service delivery models. In this paper, we discuss the case of distributed denial-of-service (DDoS) attack using Cloud resources. First, we show how such attack using a cloud platform could not be detected by previous techniques. Then we present a tricky solution based on the cloud as well.",CLD,http://arxiv.org/abs/1712.01939v1
KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge Collaboration Framework,"As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between cloud and edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework. The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading. Considering the scheduling optimization of cloud-edge workflows, this paper proposes a cloud-edge workflow scheduling model and cloud-edge node model and designs a cloud-edge workflow scheduling engine to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to design the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and vertical offloading strategies between the cloud and edges for workflow tasks are designed to realize the cloud-edge application collaboration. Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.",CLD,http://arxiv.org/abs/2401.01217v1
Research Challenges for Enterprise Cloud Computing,"Cloud computing represents a shift away from computing as a product that is purchased, to computing as a service that is delivered to consumers over the internet from large-scale data centers - or ""clouds"". This paper discusses some of the research challenges for cloud computing from an enterprise or organizational perspective, and puts them in context by reviewing the existing body of literature in cloud computing. Various research challenges relating to the following topics are discussed: the organizational changes brought about by cloud computing; the economic and organizational implications of its utility billing model; the security, legal and privacy issues that cloud computing raises. It is important to highlight these research challenges because cloud computing is not simply about a technological improvement of data centers but a fundamental change in how IT is provisioned and used. This type of research has the potential to influence wider adoption of cloud computing in enterprise, and in the consumer market too.",CLD,http://arxiv.org/abs/1001.3257v1
I Have the Proof: Providing Proofs of Past Data Possession in Cloud Forensics,"Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. A key task of digital forensics is to prove the presence of a particular file in a given storage system. Unfortunately, it is very hard to do so in a cloud given the black-box nature of clouds and the multi-tenant cloud models. In clouds, analyzing the data from a virtual machine instance or data stored in a cloud storage only allows us to investigate the current content of the cloud storage, but not the previous contents. In this paper, we introduce the idea of building proofs of past data possession in the context of a cloud storage service. We present a scheme for creating such proofs and evaluate its performance in a real cloud provider. We also discuss how this proof of past data possession can be used effectively in cloud forensics.",CLD,http://arxiv.org/abs/1211.4328v1
A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing,"Cloud computing is a recent paradigm based around the notion of delivery of resources via a service model over the Internet. Despite being a new paradigm of computation, cloud computing owes its origins to a number of previous paradigms. The term cloud computing is well defined and no longer merits rigorous taxonomies to furnish a definition. Instead this survey paper considers the past, present and future of cloud computing. As an evolution of previous paradigms, we consider the predecessors to cloud computing and what significance they still hold to cloud services. Additionally we examine the technologies which comprise cloud computing and how the challenges and future developments of these technologies will influence the field. Finally we examine the challenges that limit the growth, application and development of cloud computing and suggest directions required to overcome these challenges in order to further the success of cloud computing.",CLD,http://arxiv.org/abs/1306.1394v1
Dynamic Resource Allocation for Virtual Machine Migration Optimization using Machine Learning,"The paragraph is grammatically correct and logically coherent. It discusses the importance of mobile terminal cloud computing migration technology in meeting the demands of evolving computer and cloud computing technologies. It emphasizes the need for efficient data access and storage, as well as the utilization of cloud computing migration technology to prevent additional time delays. The paragraph also highlights the contributions of cloud computing migration technology to expanding cloud computing services. Additionally, it acknowledges the role of virtualization as a fundamental capability of cloud computing while emphasizing that cloud computing and virtualization are not inherently interconnected. Finally, it introduces machine learning-based virtual machine migration optimization and dynamic resource allocation as a critical research direction in cloud computing, citing the limitations of static rules or manual settings in traditional cloud computing environments. Overall, the paragraph effectively communicates the importance of machine learning technology in addressing resource allocation and virtual machine migration challenges in cloud computing.",CLD,http://arxiv.org/abs/2403.13619v1
Compute and Storage Clouds Using Wide Area High Performance Networks,"We describe a cloud based infrastructure that we have developed that is optimized for wide area, high performance networks and designed to support data mining applications. The infrastructure consists of a storage cloud called Sector and a compute cloud called Sphere. We describe two applications that we have built using the cloud and some experimental studies.",CLD,http://arxiv.org/abs/0808.1802v1
An Automated Implementation of Hybrid Cloud for Performance Evaluation of Distributed Databases,"A Hybrid cloud is an integration of resources between private and public clouds. It enables users to horizontally scale their on-premises infrastructure up to public clouds in order to improve performance and cut up-front investment cost. This model of applications deployment is called cloud bursting that allows data-intensive applications especially distributed database systems to have the benefit of both private and public clouds. In this work, we present an automated implementation of a hybrid cloud using (i) a robust and zero-cost Linux-based VPN to make a secure connection between private and public clouds, and (ii) Terraform as a software tool to deploy infrastructure resources based on the requirements of hybrid cloud. We also explore performance evaluation of cloud bursting for six modern and distributed database systems on the hybrid cloud spanning over local OpenStack and Microsoft Azure. Our results reveal that MongoDB and MySQL Cluster work efficient in terms of throughput and operations latency if they burst into a public cloud to supply their resources. In contrast, the performance of Cassandra, Riak, Redis, and Couchdb reduces if they significantly leverage their required resources via cloud bursting.",CLD,http://arxiv.org/abs/2006.02833v1
Characterizing User and Provider Reported Cloud Failures,"Cloud computing is the backbone of the digital society. Digital banking, media, communication, gaming, and many others depend on cloud services. Unfortunately, cloud services may fail, leading to damaged services, unhappy users, and perhaps millions of dollars lost for companies. Understanding a cloud service failure requires a detailed report on why and how the service failed. Previous work studies how cloud services fail using logs published by cloud operators. However, information is lacking on how users perceive and experience cloud failures. Therefore, we collect and characterize the data for user-reported cloud failures from Down Detector for three cloud service providers over three years. We count and analyze time patterns in the user reports, and derive failures from those user reports and characterize their duration and interarrival time. We characterize provider-reported cloud failures and compare the results with the characterization of user-reported failures. The comparison reveals the information of how users perceive failures and how much of the failures are reported by cloud service providers. Overall, this work provides a characterization of user- and provider-reported cloud failures and compares them with each other.",CLD,http://arxiv.org/abs/2110.12237v2
Platforms for Building and Deploying Applications for Cloud Computing,"Cloud computing is rapidly emerging as a new paradigm for delivering IT services as utlity-oriented services on subscription-basis. The rapid development of applications and their deployment in Cloud computing environments in efficient manner is a complex task. In this article, we give a brief introduction to Cloud computing technology and Platform as a Service, we examine the offerings in this category, and provide the basis for helping readers to understand basic application platform opportunities in Cloud by technologies such as Microsoft Azure, Sales Force, Google App, and Aneka for Cloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application Platform (CAP) leveraging these concepts and allowing an easy development of Cloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers facilities for quickly developing Cloud applications and a modular platform where additional services can be easily integrated to extend the system capabilities, thus being at pace with the rapidly evolution of Cloud computing.",CLD,http://arxiv.org/abs/1104.4379v1
Formal Specification Language Based IaaS Cloud Workload Regression Analysis,"Cloud Computing is an emerging area for accessing computing resources. In general, Cloud service providers offer services that can be clustered into three categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload analysis. The efficient Cloud workload resource mapping technique is proposed. This paper aims to provide a means of understanding and investigating IaaS Cloud workloads and the resources. In this paper, regression analysis is used to analyze the Cloud workloads and identifies the relationship between Cloud workloads and available resources. The effective organization of dynamic nature resources can be done with the help of Cloud workloads. Till Cloud workload is considered a vital talent, the Cloud resources cannot be consumed in an effective style. The proposed technique has been validated by Z Formal specification language. This approach is effective in minimizing the cost and submission burst time of Cloud workloads.",CLD,http://arxiv.org/abs/1402.3034v1
An Analysis of the Cloud Computing Security Problem,"Cloud computing is a new computational paradigm that offers an innovative business model for organizations to adopt IT without upfront investment. Despite the potential gains achieved from the cloud computing, the model security is still questionable which impacts the cloud model adoption. The security problem becomes more complicated under the cloud model as new dimensions have entered into the problem scope related to the model architecture, multi-tenancy, elasticity, and layers dependency stack. In this paper we introduce a detailed analysis of the cloud security problem. We investigated the problem from the cloud architecture perspective, the cloud offered characteristics perspective, the cloud stakeholders' perspective, and the cloud service delivery models perspective. Based on this analysis we derive a detailed specification of the cloud security problem and key features that should be covered by any proposed security solution.",CLD,http://arxiv.org/abs/1609.01107v1
Datacenter Changes vs. Employment Rates for Datacenter Managers In the Cloud Computing Era,"Due to the evolving Cloud Computing paradigm, there is a prevailing concern that in the near future data center managers may be in short supply. Cloud computing, as a whole, is becoming more prevalent into today s computing world. In fact, cloud computing has become so popular that some are now referring to data centers as cloud centers. How does this interest in cloud computing translate into employment rates for data center managers? The popularity of the public and private cloud models are the prevailing force behind answering this question. Therefore, the skill set of the datacenter manager has evolved to harness the on demand self-services, broad network access, resource pooling, rapid elasticity, measured service, and multi tenacity characteristics of cloud computing. Using diverse sources ranging from the Bureau of Labor and Statistics to trade articles, this manuscript takes an in-depth look at these employment rates related to the cloud and the determining factors behind them. Based on the information available, datacenter manager employment rates in the cloud computing era will continue to increase well into 2016.",CLD,http://arxiv.org/abs/1404.2151v1
A Novel Application Licensing Framework for Mobile Cloud Environment,"Mobile cloud computing is a new technology that enhances smartphone applications capabilities in terms of performance, energy efficiency, and execution support. These features are achieved via computation offloading technique that is supported by specialized mobile cloud application development models. However, the cloud-enabled applications are prone to application piracy issue for which the traditional licensing frameworks are of no use. Therefore, a new licensing framework is required to control application piracy in mobile cloud environment. This paper presents a preliminary design of a novel application licensing framework for mobile cloud environment that restricts execution of applications on unauthenticated smartphones and cloud resources.",CLD,http://arxiv.org/abs/1401.0034v1
Towards Constraint-based High Performance Cloud System in the Process of Cloud Computing Adoption in an Organization,"Cloud computing is penetrating into various domains and environments, from theoretical computer science to economy, from marketing hype to educational curriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently developing state of cloud computing leaves several issues to address and also affects cloud computing adoption by organizations. In this paper, we explain how the transition into the cloud can occur in an organization and describe the mechanism for transforming legacy infrastructure into a virtual infrastructure-based cloud. We describe the state of the art of infrastructural cloud, which is essential in the decision making on cloud adoption, and highlight the challenges that can limit the scale and speed of the adoption. We then suggest a strategic framework for designing a high performance cloud system. This framework is applicable when transformation cloudbased deployment model collides with some constraints. We give an example of the implementation of the framework in a design of a budget-constrained high availability cloud system.",CLD,http://arxiv.org/abs/1010.4952v1
Cloud Infrastructure Service Management - A Review,"The new era of computing called Cloud Computing allows the user to access the cloud services dynamically over the Internet wherever and whenever needed. Cloud consists of data and resources; and the cloud services include the delivery of software, infrastructure, applications, and storage over the Internet based on user demand through Internet. In short, cloud computing is a business and economic model allowing the users to utilize high-end computing and storage virtually with minimal infrastructure on their end. Cloud has three service models namely, Cloud Software-as-a-Service (SaaS), Cloud Platform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS). This paper talks in depth of cloud infrastructure service management.",CLD,http://arxiv.org/abs/1206.6016v1
Towards a Taxonomy of Performance Evaluation of Commercial Cloud Services,"Cloud Computing, as one of the most promising computing paradigms, has become increasingly accepted in industry. Numerous commercial providers have started to supply public Cloud services, and corresponding performance evaluation is then inevitably required for Cloud provider selection or cost-benefit analysis. Unfortunately, inaccurate and confusing evaluation implementations can be often seen in the context of commercial Cloud Computing, which could severely interfere and spoil evaluation-related comprehension and communication. This paper introduces a taxonomy to help profile and standardize the details of performance evaluation of commercial Cloud services. Through a systematic literature review, we constructed the taxonomy along two dimensions by arranging the atomic elements of Cloud-related performance evaluation. As such, this proposed taxonomy can be employed both to analyze existing evaluation practices through decomposition into elements and to design new experiments through composing elements for evaluating performance of commercial Cloud services. Moreover, through smooth expansion, we can continually adapt this taxonomy to the more general area of evaluation of Cloud Computing.",CLD,http://arxiv.org/abs/1302.1957v1
A Survey and Comparative Study on Multi-Cloud Architectures: Emerging Issues And Challenges For Cloud Federation,"Multi-cloud concept has broaden the world of cloud computing and has become a buzzword today. The word Multi-cloud envisions utilization of services from multiple heterogeneous cloud providers via a single architecture at customer premises. Though cloud computing has many issues and offers open research challenges, still the academics and industrial research has paved a pathway for multi-cloud environment. The concept of multi-cloud is in maturing phase, and many research projects are in progress to provide a multi-cloud architecture which is successfully enabled in all the respects like easy configuration, security, management etc. In this paper, concepts, challenges, requirement and future directions for multi-cloud environment are discussed. A survey of existing approaches and solutions provided by different multi-cloud architectures is entailed along with analysis of the pros and cons of different architectures while comparing the same.",CLD,http://arxiv.org/abs/2108.12831v1
Molecular Dynamics Simulations on Cloud Computing and Machine Learning Platforms,"Scientific computing applications have benefited greatly from high performance computing infrastructure such as supercomputers. However, we are seeing a paradigm shift in the computational structure, design, and requirements of these applications. Increasingly, data-driven and machine learning approaches are being used to support, speed-up, and enhance scientific computing applications, especially molecular dynamics simulations. Concurrently, cloud computing platforms are increasingly appealing for scientific computing, providing ""infinite"" computing powers, easier programming and deployment models, and access to computing accelerators such as TPUs (Tensor Processing Units). This confluence of machine learning (ML) and cloud computing represents exciting opportunities for cloud and systems researchers. ML-assisted molecular dynamics simulations are a new class of workload, and exhibit unique computational patterns. These simulations present new challenges for low-cost and high-performance execution. We argue that transient cloud resources, such as low-cost preemptible cloud VMs, can be a viable platform for this new workload. Finally, we present some low-hanging fruits and long-term challenges in cloud resource management, and the integration of molecular dynamics simulations into ML platforms (such as TensorFlow).",CLD,http://arxiv.org/abs/2111.06466v1
Model-Based Cloud Resource Management with TOSCA and OCCI,"With the advent of cloud computing, different cloud providers with heterogeneous cloud services (compute, storage, network, applications, etc.) and their related Application Programming Interfaces (APIs) have emerged. This heterogeneity complicates the implementation of an interoperable cloud system. Several standards have been proposed to address this challenge and provide a unified interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers while the Topology and Orchestration Specification for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper definition of the topology of cloud applications and their orchestrations on top of a cloud system. TOSCA thereby does not define how the application topologies are created on the cloud. Therefore, we analyse the conceptual similarities between the two approaches and we study how we can integrate them to obtain a complete standard-based approach to manage both cloud infrastructure and cloud application layers. We propose an automated extensive mapping between the concepts of the two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully designing and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier.",CLD,http://arxiv.org/abs/2001.07900v2
Defining Cross-Cloud Systems,"Recent years have seen an increasing number of cross-cloud architectures, i.e. systems that span across cloud provisioning boundaries. However, the cloud computing world still lacks any standards in terms of programming interfaces, which has a knock-on effect on the costs associated with interoperability and severely limits the flexibility and portability of applications and virtual infrastructures. This paper outlines the different types of cross-cloud systems, and the associated design decisions.",CLD,http://arxiv.org/abs/1602.02698v1
An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud Computing Platform,"Cloud Computing is becoming a viable computing solution for services oriented computing. Several open-source cloud solutions are available to these supports. Open-source software stacks offer a huge amount of customizability without huge licensing fees. As a result, open source software are widely used for designing cloud, and private clouds are being built increasingly in the open source way. Numerous contributions have been made by the open-source community related to private-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private cloud management software. However, little has been done to systematically investigate the performance evaluation of this open-source cloud solution in the existing literature. The performance evaluation aids new and existing research, industry and international projects when selecting OpenNebula software to their work. The objective of this paper is to evaluate the load-balancing performance of the OpenNebula cloud management software. For the performance evaluation, the OpenNebula cloud management software is installed and configured as a prototype implementation and tested on the DIU Cloud Lab. In this paper, two set of experiments are conducted to identify the load balancing performance of the OpenNebula cloud management platform- (1) Delete and Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping Physical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.",CLD,http://arxiv.org/abs/1406.5759v1
SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering,"Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner. Code is available at https://github.com/fpthink/SSPU-Net.",CLD,http://arxiv.org/abs/2108.00454v2
A Hybrid Cloud ERP Framework For Processing Purchasing Data,"Cloud-based enterprise resource planning (cloud ERP) systems have existed in the business market for around ten years. Cloud ERP supports enterprises' daily activities by integrating organizational back-end systems in the cloud environment. One of the critical functions that cloud ERP offers is the purchasing application. The purchasing function of cloud ERP enables enterprises to streamline all the online purchasing transactions in real-time automatically. Even cloud ERP is deployed quite often these days, organizations somehow still lack the knowledge of it; to be specific, there are many issues attached to cloud ERP implementation yet to be solved. Hence, this paper compares four leading cloud ERP platforms in Australia and proposes a hybrid cloud ERP framework to process online purchasing transactions. By adopting a case study approach, a purchasing web-based application is designed and presented in this paper. In general, the proposed hybrid cloud ERP framework and the integrated web-based purchasing application allow user companies to process online purchasing transactions with short operation time and increased business efficiency; in the meantime, the proposed framework also reduces security risks attached to the public cloud.",CLD,http://arxiv.org/abs/2202.10786v1
Configuration management in the distributed cloud,"Owing to their cost-effectiveness and flexibility, cloud services have been the default choice for the deployment of innumerable software systems over the years. However, novel paradigms are beginning to emerge, as the cloud can't meet the requirements of increasingly many latency- and privacy-sensitive applications. The distributed cloud model, being one of the attempts to overcome these challenges, places a distributed cloud layer between device and cloud layers, intending to bring resources closer to data sources. As application code should be kept separate from its configuration, especially in highly dynamic cloud environments, there is a need to incorporate configuration primitives in future distributed cloud platforms. In this paper, we present the design and implementation of a configuration management subsystem for an open-source distributed cloud platform. Our solution spreads across the cloud and distributed cloud layers and supports configuration versioning, selective dissemination to nodes in the distributed cloud layer, and logical isolation via namespaces. Our work serves as a demonstration of the feasibility and usability of the new cloud-extending models and provides valuable insight into one of the possible implementations.",CLD,http://arxiv.org/abs/2410.20276v1
A Cost-Effective Strategy for Storing Scientific Datasets with Multiple Service Providers in the Cloud,"Cloud computing provides scientists a platform that can deploy computation and data intensive applications without infrastructure investment. With excessive cloud resources and a decision support system, large generated data sets can be flexibly 1 stored locally in the current cloud, 2 deleted and regenerated whenever reused or 3 transferred to cheaper cloud service for storage. However, due to the pay for use model, the total application cost largely depends on the usage of computation, storage and bandwidth resources, hence cutting the cost of cloud based data storage becomes a big concern for deploying scientific applications in the cloud. In this paper, we propose a novel strategy that can cost effectively store large generated data sets with multiple cloud service providers. The strategy is based on a novel algorithm that finds the trade off among computation, storage and bandwidth costs in the cloud, which are three key factors for the cost of data storage. Both general (random) simulations conducted with popular cloud service providers pricing models and three specific case studies on real world scientific applications show that the proposed storage strategy is highly cost effective and practical for run time utilization in the cloud.",CLD,http://arxiv.org/abs/1601.07028v1
A Context Aware and Self Adaptation Strategy for Cloud Service Selection and Configuration in Run Time,"Day after day, the number of mobile applications deployed on cloud computing continues in increasing because o f smartphone capabilities improvement. Cloud computing has already succeeded in the web based application, for that reason, the demand for context aware services provided by cloud computing increases. To customize a cloud service that takes into account th e consumer requirements, which depend on information change, it brings to light many recent challenges to cloud computing about environment aware, location aware, time aware. The cloud provider, moreover, has to manage personalized applications and the con straints of mobile devices in matters of interaction abilities and communication restrictions. This paper proposes a strategy for selecting automatically an appropriate cloud environment that runs out whole requirements, defines a configuration for the ass ociated cloud environment and able to easily adapt to the change of the environment on either the user or the cloud side or both. This process builds on the principles of dynamic software product lines, Agent oriented software engineering, and the MAPE k m odel to select and configure cloud environments according to the consumer needs and the context change.",CLD,http://arxiv.org/abs/2104.00813v1
What is the next innovation after the internet of things?,"The world had witnessed several generations of the Internet. Starting with the Fixed Internet, then the Mobile Internet, scientists now focus on many types of research related to the ""Thing"" Internet (or Internet of Things). The question is ""what is the next Internet generation after the Thing Internet?"" This paper envisions about the Tactile Internet which could be the next Internet generation in the near future. The paper will introduce what is the tactile internet, why it could be the next future Internet, as well as the impact and its application in the future society. Furthermore, some challenges and the requirements are presented to guide further research in this near future field.",IOTNET,http://arxiv.org/abs/1708.07160v1
In Things We Trust? Towards trustability in the Internet of Things,"This essay discusses the main privacy, security and trustability issues with the Internet of Things.",IOTNET,http://arxiv.org/abs/1109.2637v1
Privacy in the Internet of Things: Threats and Challenges,"The Internet of Things paradigm envisions the pervasive interconnection and cooperation of smart things over the current and future Internet infrastructure. The Internet of Things is, thus, the evolution of the Internet to cover the real-world, enabling many new services that will improve people's everyday lives, spawn new businesses and make buildings, cities and transport smarter. Smart things allow indeed for ubiquitous data collection or tracking, but these useful features are also examples of privacy threats that are already now limiting the success of the Internet of Things vision when not implemented correctly. These threats involve new challenges such as the pervasive privacy-aware management of personal data or methods to control or avoid ubiquitous tracking and profiling. This paper analyzes the privacy issues in the Internet of Things in detail. To this end, we first discuss the evolving features and trends in the Internet of Things with the goal of scrutinizing their privacy implications. Second, we classify and examine privacy threats in this new setting, pointing out the challenges that need to be overcome to ensure that the Internet of Things becomes a reality.",SEC,http://arxiv.org/abs/1505.07683v1
Sensing as a Service (S2aaS): Buying and Selling IoT Data,"The Internet of Things (IoT) [1] envisions the creation of an environment where everyday objects (e.g. microwaves, fridges, cars, coffee machines, etc.) are connected to the internet and make users' lives more convenient. It will also lead users to consume resources more efficiently.",IOTNET,http://arxiv.org/abs/1702.02380v1
Rentable Internet of Things Infrastructure for Sensing as a Service (S2aaS),Sensing as a Service (S2aaS) model [1] [2] is inspired by the traditional Everything as a service (XaaS) approaches [3]. It aims to better utilize the existing Internet of Things (IoT) infrastructure. S2aaS vision aims to create 'rentable infrastructure' where interested parties can gather IoT data by paying a fee for the infrastructure owners.,IOTNET,http://arxiv.org/abs/1807.09680v1
User Empowerment in the Internet of Things,This paper focuses on the characteristics of two big triggers that facilitated wide user adoption of the Internet: Web 2.0 and online social networks. We detect brakes for reproduction of these events in Internet of things. To support our hypothesis we first compare the difference between the ways of use of the Internet with the future scenarios of Internet of things. We detect barriers that could slow down apparition of this kind of social events during user adoption of Internet of Things and we propose a conceptual framework to solve these problems.,IOTNET,http://arxiv.org/abs/1107.3759v1
Review of internet of things of security threats and Challenges,"The Internet of Things has received a lot of research attention. It is considered part of the Internet of the future and is made up of billions of intelligent communication. The future of the Internet will consist of heterogeneously connected devices that expand the world boundaries with physical entities and virtual components. It provides new functionality for related things. This study systematically examines the definition, architecture, essential technologies, and applications of the Internet of Things. We will introduce various definitions of the Internet of Things. Then, it will be discussed new techniques for implementing the Internet of Things and several open issues related to the Internet of Things applications will be investigated. Finally, the key challenges that need to be addressed by the research community and possible solutions to address them are investigated.",IOTNET,http://arxiv.org/abs/2107.10733v1
Challenges and Opportunities in Securing the Industrial Internet of Things,"Given the tremendous success of the Internet of Things in interconnecting consumer devices, we observe a natural trend to likewise interconnect devices in industrial settings, referred to as Industrial Internet of Things or Industry 4.0. While this coupling of industrial components provides many benefits, it also introduces serious security challenges. Although sharing many similarities with the consumer Internet of Things, securing the Industrial Internet of Things introduces its own challenges but also opportunities, mainly resulting from a longer lifetime of components and a larger scale of networks. In this paper, we identify the unique security goals and challenges of the Industrial Internet of Things, which, unlike consumer deployments, mainly follow from safety and productivity requirements. To address these security goals and challenges, we provide a comprehensive survey of research efforts to secure the Industrial Internet of Things, discuss their applicability, and analyze their security benefits.",IOTNET,http://arxiv.org/abs/2111.11714v1
A Study on Internet of Things based Applications,"This paper gives a detail analysis of various applications based on Internet of Thing (IoT)s. This explains about how internet of things evolved from mobile computing and ubiquitous computing. It emphasises the fact that objects are connected over the internet rather than people. The properties of Internet of Things (IOT) are product information, electronic tag, standard expressed and uploading information. It utilises the Radio Frequency Identification (RFID) technology and wireless sensor networks (WSN). IOT applications are used in domains such as healthcare, supply chain management, defence and agriculture. Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT faces certain crucial issues like privacy and security.",IOTNET,http://arxiv.org/abs/1206.3891v1
Human Resource Development and the Internet of Things,"The Internet of Things (IoT) is affecting national innovation ecosystems and the approach of organizations to innovation and how they create and capture value in everyday business activities. The Internet of Things (IoT), is disruptive, and it will change the manner in which human resources are developed and managed, calling for a new and adaptive human resource development approach. The Classical Internet communication form is human-human. The prospect of IoT is that every object will have a unique way of identification and can be addressed so that every object can be connected. The communication forms will expand from human-human to human-human, human-thing, and thing-thing. This will bring a new challenge to how Human Resource Development (HRD) is practiced. This paper provides an overview of the Internet of Things and conceptualizes the role of HRD in the age of the Internet of Things. Keywords:",IOTNET,http://arxiv.org/abs/2107.04003v1
6G Internet of Things: A Comprehensive Survey,"The sixth generation (6G) wireless communication networks are envisioned to revolutionize customer services and applications via the Internet of Things (IoT) towards a future of fully intelligent and autonomous systems. In this article, we explore the emerging opportunities brought by 6G technologies in IoT networks and applications, by conducting a holistic survey on the convergence of 6G and IoT. We first shed light on some of the most fundamental 6G technologies that are expected to empower future IoT networks, including edge intelligence, reconfigurable intelligent surfaces, space-air-ground-underwater communications, Terahertz communications, massive ultra-reliable and low-latency communications, and blockchain. Particularly, compared to the other related survey papers, we provide an in-depth discussion of the roles of 6G in a wide range of prospective IoT applications via five key domains, namely Healthcare Internet of Things, Vehicular Internet of Things and Autonomous Driving, Unmanned Aerial Vehicles, Satellite Internet of Things, and Industrial Internet of Things. Finally, we highlight interesting research challenges and point out potential directions to spur further research in this promising area.",IOTNET,http://arxiv.org/abs/2108.04973v1
On Web-based Domain-Specific Language for Internet of Things,"This paper discusses the challenges of the Internet of Things programming. Sensing and data gathering from the various sources are often the key elements of applications for Smart Cities. So, the effective programming models for them are very important. In this article, we discuss system software models and solutions, rather than network related aspects. In our paper, we present the web-based domain-specific language for Internet of Things applications. Our goal is to present the modern models for data processing in Internet of Things and Smart Cities applications. In our view, the use of this kind of tools should seriously reduce the time to develop new applications.",IOTNET,http://arxiv.org/abs/1505.06713v1
"Internet of Nano, Bio-Nano, Biodegradable and Ingestible Things: A Survey","In recent years, advances in biotechnology, nanotechnology and materials science have led to development of revolutionizing applications in Internet of Things (IoT). In particular, the interconnection of nanomaterials, nanoimplants and nanobiosensors with existing IoT networks have inspired the concepts of Internet of Nano Things (IoNT), Internet of Bio-Nano Things (IoBNT), Internet of Biodegradable Things (IoBDT) and Internet of Ingestible Things (IoIT). To date, although there are several survey papers that addressed these concepts separately, there is no current survey covering all studies in IoNT, IoBNT, IoBDT and IoIT. Therefore, in this paper, we provide a complete overview of all recent work in these four areas. Furthermore, we emphasize the research challenges, potential applications, and open research areas.",IOTNET,http://arxiv.org/abs/2202.12409v1
The Internet of Things: Perspectives on Security from RFID and WSN,"A massive current research effort focuses on combining pre-existing 'Intranets' of Things into one Internet of Things. However, this unification is not a panacea; it will expose new attack surfaces and vectors, just as it enables new applications. We therefore urgently need a model of security in the Internet of Things. In this regard, we note that IoT descends directly from pre-existing research (in embedded Internet and pervasive intelligence), so there exist several bodies of related work: security in RFID, sensor networks, cyber-physical systems, and so on. In this paper, we survey the existing literature on RFID and WSN security, as a step to compiling all known attacks and defenses relevant to the Internet of Things.",IOTNET,http://arxiv.org/abs/1604.00389v1
Turing Test for the Internet of Things,"How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices `smart' by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.",IOTNET,http://arxiv.org/abs/1412.3802v1
Privacy Preservation Technologies in Internet of Things,"Since the beginning of the Internet thirty years ago, we have witnessed a number of changes in the application of communication technologies. Today, the Internet can be described to a large extent as a ubiquitous infrastructure that is always accessible. After the era of connecting places and connecting people, the Internet of the future will also connect things. The idea behind the resulting Internet of Things is to seamlessly gather and use information about objects of the real world during their entire lifecycle. In this paper, we consider different approaches to technological protection of user data privacy in the world of Internet of Things. In particular,we consider what kind of security problems are being faced and what level of protection can be provided by applying approaches based on secure multi-party computations.",SEC,http://arxiv.org/abs/1012.2177v3
User-driven Privacy Enforcement for Cloud-based Services in the Internet of Things,"Internet of Things devices are envisioned to penetrate essentially all aspects of life, including homes and urbanspaces, in use cases such as health care, assisted living, and smart cities. One often proposed solution for dealing with the massive amount of data collected by these devices and offering services on top of them is the federation of the Internet of Things and cloud computing. However, user acceptance of such systems is a critical factor that hinders the adoption of this promising approach due to severe privacy concerns. We present UPECSI, an approach for user-driven privacy enforcement for cloud-based services in the Internet of Things to address this critical factor. UPECSI enables enforcement of all privacy requirements of the user once her sensitive data leaves the border of her network, provides a novel approach for the integration of privacy functionality into the development process of cloud-based services, and offers the user an adaptable and transparent configuration of her privacy requirements. Hence, UPECSI demonstrates an approach for realizing user-accepted cloud services in the Internet of Things.",CLD,http://arxiv.org/abs/1412.3325v1
Low power communication signal enhancement method of Internet of things based on nonlocal mean denoising,"In order to improve the transmission effect of low-power communication signal of Internet of things and compress the enhancement time of low-power communication signal, this paper designs a low-power communication signal enhancement method of Internet of things based on nonlocal mean denoising. Firstly, the residual of one-dimensional communication layer is pre processed by convolution core to obtain the residual of one-dimensional communication layer; Then, according to the two classification recognition method, the noise reduction signal feature recognition of the low-power communication signal of the Internet of things is realized, the non local mean noise reduction algorithm is used to remove the low-power communication signal of the Internet of things, and the weight value between similar blocks is calculated according to the European distance method. Finally, the low-power communication signal enhancement of the Internet of things is realized by the non local mean value denoising method. The experimental results show that the communication signal enhancement time overhead of this method is low, which is always less than 2.6s. The lowest bit error rate after signal enhancement is about 1%, and the signal-to-noise ratio is up to 18 dB, which shows that this method can achieve signal enhancement.",IOTNET,http://arxiv.org/abs/2205.10323v1
Cells in the Internet of Things,"The Internet of Things combines various earlier areas of research. As a result, research on the subject is still organized around these pre-existing areas: distributed computing with services and objects, networks (usually combining 6lowpan with Zigbee etc. for the last-hop), artificial intelligence and semantic web, and human-computer interaction. We are yet to create a unified model that covers all these perspectives - domain, device, service, agent, etc. In this paper, we propose the concept of cells as units of structure and context in the Internet of things. This allows us to have a unified vocabulary to refer to single entities (whether dumb motes, intelligent spimes, or virtual services), intranets of things, and finally the complete Internet of things. The question that naturally follows, is what criteria we choose to demarcate boundaries; we suggest various possible answers to this question. We also mention how this concept ties into the existing visions and protocols, and suggest how it may be used as the foundation of a formal model.",IOTNET,http://arxiv.org/abs/1510.07861v1
Economic viability and Future Impact of Internet of Things in India: An Inevitable wave,"The Internet of things , sometimes referred as Internet of objects can be stated as an environment in which any physical things or objects are assiThis paper studies the evolution of internet usage and classifies the impact areas where internet will go beyond personal communication or knowledge interface but it will provide communication and knowledge base support to numerous gadgets and systems around us",IOTNET,http://arxiv.org/abs/1601.04363v2
Wireless Sensors Networks for Internet of Things,"The Internet is smoothly migrating from an Internet of people towards an Internet of Things (IoT). By 2020, it is expected to have 50 billion things connected to the Internet. However, such a migration induces a strong level of complexity when handling interoperability between the heterogeneous Inter- net things, e.g., RFIDs (Radio Frequency Identification), mobile handheld devices, and wireless sensors. In this context, a couple of standards have been already set, e.g., IPv6, 6LoWPAN (IPv6 over Low power Wireless Personal Area Networks), and M2M (Machine to Machine communications). In this paper, we focus on the integration of wireless sensor networks into IoT, and shed further light on the subtleties of such integration. We present a real-world test bed deployment where wireless sensors are used to control electrical appliances in a smart building. Encountered problems are highlighted and suitable solutions are presented.",IOTNET,http://arxiv.org/abs/1606.08407v1
A Middleware for the Internet of Things,"The Internet of Things (IoT) connects everyday objects including a vast array of sensors, actuators, and smart devices, referred to as things to the Internet, in an intelligent and pervasive fashion. This connectivity gives rise to the possibility of using the tracking capabilities of things to impinge on the location privacy of users. Most of the existing management and location privacy protection solutions do not consider the low-cost and low-power requirements of things, or, they do not account for the heterogeneity, scalability, or autonomy of communications supported in the IoT. Moreover, these traditional solutions do not consider the case where a user wishes to control the granularity of the disclosed information based on the context of their use (e.g. based on the time or the current location of the user). To fill this gap, a middleware, referred to as the Internet of Things Management Platform (IoT-MP) is proposed in this paper.",IOTNET,http://arxiv.org/abs/1604.04823v1
A Review on Security and Privacy of Internet of Medical Things,"The Internet of Medical Things (IoMT) are increasing the accuracy, reliability, and the production capability of electronic devices by playing a very important part in the industry of healthcare. The available medical resources and services related to healthcare are working to get an interconnection with each other by the digital healthcare system by the contribution of the researchers. Sensors, wearable devices, medical devices, and clinical devices are all connected to form an ecosystem of the Internet of Medical Things. The different applications of healthcare are enabled by the Internet of Medical Things to reduce the healthcare costs, to attend the medical responses on time and it also helps in increasing the quality of the medical treatment. The healthcare industry is transformed by the Internet of Medical Things as it delivers targeted and personalized medical care and it also seamlessly enables the communication of medical data. Devices used in the medical field and their application are connected to the system of healthcare of Information technology with the help of the digital world.",SEC,http://arxiv.org/abs/2009.05394v1
Social Internet of Things: Architectural Approaches and Challenges,"Social Internet of Things (SIoT) takes a step forward over the traditional Internet of Things (IoT), introducing a new paradigm that combines the concepts of social networks with the IoT, to obtain the benefits of both worlds, as in the case of the Social Internet of Vehicles. With the emergence of the Social Internet of Things, new challenges also arise that need to be analyzed in depth. In this article, the key challenges around the software architecture of the various SIoT system described in the literature are analyzed. One of the conclusions is that SIoT is still at an early stage of development, and therefore, SIoT systems architecture will be concerned by this fact. Challenging quality attributes specific for SIoT include scalability, navigability and trust",IOTNET,http://arxiv.org/abs/2002.04566v1
Can Blockchain Protect Internet-of-Things?,"In the Internet-of-Things, the number of connected devices is expected to be extremely huge, i.e., more than a couple of ten billion. It is however well-known that the security for the Internet-of-Things is still open problem. In particular, it is difficult to certify the identification of connected devices and to prevent the illegal spoofing. It is because the conventional security technologies have advanced for mainly protecting logical network and not for physical network like the Internet-of-Things. In order to protect the Internet-of-Things with advanced security technologies, we propose a new concept (datachain layer) which is a well-designed combination of physical chip identification and blockchain. With a proposed solution of the physical chip identification, the physical addresses of connected devices are uniquely connected to the logical addresses to be protected by blockchain.",IOTNET,http://arxiv.org/abs/1807.06357v2
From Internet of Things to Internet of Data Apps,"We introduce the Internet of Data Apps (IoDA), representing the next natural progression of the Internet, Big Data, AI, and the Internet of Things. Despite advancements in these fields, the full potential of universal data access - the capability to seamlessly consume and contribute data via data applications - remains stifled by organizational and technological silos. To address these constraints, we propose the designs of an IoDA layer borrowing inspirations from the standard Internet protocols. This layer facilitates the interconnection of data applications across different devices and domains. This short paper serves as an invitation to dialogue over this proposal.",IOTNET,http://arxiv.org/abs/2309.04546v1
When Distributed Ledger Technology meets Internet of Things -- Benefits and Challenges,"There is a growing interest from both the academia and industry to employ distributed ledger technology in the Internet-of-Things domain for addressing security-related and performance challenges. Distributed ledger technology enables non-trusted entities to communicate and reach consensus in a fully distributed manner through a cryptographically secure and immutable ledger. However, significant challenges arise mainly related to transaction processing speed and user privacy. This work explores the interplay between Internet-of-Things and distributed ledger technology, analysing the fundamental characteristics of this technology and discussing the related benefits and challenges.",SEC,http://arxiv.org/abs/2008.12569v1
Wearable Internet of Things for Personalized Healthcare Study of Trends and Latent Research,"In this age of heterogeneous systems, diverse technologies are integrated to create application-specific solutions. The recent upsurge in acceptance of technologies such as cloud computing and ubiquitous Internet has cleared the path for Internet of Things (IoT). Moreover, the increasing Internet penetration with the rising use of mobile devices has inspired an era of technology that allows interfacing of physical objects and connecting them to Internet for developing applications serving a wide range of purposes. Recent developments in the area of wearable devices has led to the creation of another segment in IoT, which can be conveniently referred to as Wearable Internet of Things (WIoT). Research in this area promises to personalize healthcare in previously unimaginable ways by allowing individual tracking of wellness and health information. This chapter shall cover the different facets of Wearable Internet of Things (WIoT) and ways in which it is a key driving technology behind the concept of personalized healthcare. It shall discuss the theoretical aspects of WIoT, focusing on functionality, design and applicability. Moreover, it shall also elaborate on the role of wearable sensors, big data and cloud computing as enabling technologies for WIoT.",IOTNET,http://arxiv.org/abs/2005.06958v1
Context-aware Dynamic Discovery and Configuration of 'Things' in Smart Environments,"The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as RFIDs, sensors, actuators, as well as other instruments and smart appliances that are becoming an integral component of the future Internet. Currently, such Internet-connected objects or `things' outnumber both people and computers connected to the Internet and their population is expected to grow to 50 billion in the next 5 to 10 years. To be able to develop IoT applications, such `things' must become dynamically integrated into emerging information networks supported by architecturally scalable and economically feasible Internet service delivery models, such as cloud computing. Achieving such integration through discovery and configuration of `things' is a challenging task. Towards this end, we propose a Context-Aware Dynamic Discovery of {Things} (CADDOT) model. We have developed a tool SmartLink, that is capable of discovering sensors deployed in a particular location despite their heterogeneity. SmartLink helps to establish the direct communication between sensor hardware and cloud-based IoT middleware platforms. We address the challenge of heterogeneity using a plug in architecture. Our prototype tool is developed on an Android platform. Further, we employ the Global Sensor Network (GSN) as the IoT middleware for the proof of concept validation. The significance of the proposed solution is validated using a test-bed that comprises 52 Arduino-based Libelium sensors.",IOTNET,http://arxiv.org/abs/1311.2134v1
Achieving Ethical Algorithmic Behaviour in the Internet-of-Things: a Review,"The Internet-of-Things is emerging as a vast inter-connected space of devices and things surrounding people, many of which are increasingly capable of autonomous action, from automatically sending data to cloud servers for analysis, changing the behaviour of smart objects, to changing the physical environment. A wide range of ethical concerns has arisen in their usage and development in recent years. Such concerns are exacerbated by the increasing autonomy given to connected things. This paper reviews, via examples, the landscape of ethical issues, and some recent approaches to address these issues, concerning connected things behaving autonomously, as part of the Internet-of-Things. We consider ethical issues in relation to device operations and accompanying algorithms. Examples of concerns include unsecured consumer devices, data collection with health related Internet-of-Things, hackable vehicles and behaviour of autonomous vehicles in dilemma situations, accountability with Internet-of-Things systems, algorithmic bias, uncontrolled cooperation among things, and automation affecting user choice and control. Current ideas towards addressing a range of ethical concerns are reviewed and compared, including programming ethical behaviour, whitebox algorithms, blackbox validation, algorithmic social contracts, enveloping IoT systems, and guidelines and code of ethics for IoT developers - a suggestion from the analysis is that a multi-pronged approach could be useful, based on the context of operation and deployment.",IOTNET,http://arxiv.org/abs/1910.10241v1
"A Review on Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano Things (IoNT)","The current prominence and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano Things (IoNT) are extensively reviewed and a summary survey report is presented. The analysis clearly distinguishes between IoT and IoE which are wrongly considered to be the same by many people. Upon examining the current advancement in the fields of IoT, IoE and IoNT, the paper presents scenarios for the possible future expansion of their applications.",IOTNET,http://arxiv.org/abs/1709.10470v1
IoT Applications in Urban Sustainability,"Internet of Things is one of the driving technologies behind the concept of Smart Cities and is capable of playing a significant role in facilitating urban sustainable development. This chapter explores the relationship between three core concepts namely Smart Cities, Internet of Things and Sustainability; thereby identifying the challenges and opportunities that exist in the synergistic use of Internet of Things for sustainability, in the Smart Cities context. Moreover, this chapter also presents some of the existing use cases that apply Internet of Things for urban sustainable development, also presenting the vision for these applications as they continue to evolve in and adapt to the real world scenario. It is because of the interdisciplinary nature of these applications that a clear comprehension of the associated challenges becomes quintessential. Study of challenges and opportunities in this area shall facilitate collaboration between different sectors of urban planning and optimize the utilization of Internet of Things for sustainability.",IOTNET,http://arxiv.org/abs/2008.10656v1
Comment on Chen et al.'s Authentication Protocol for Internet of Health Things,"The Internet of Medical Things has revolutionized the healthcare industry, enabling the seamless integration of connected medical devices and wearable sensors to enhance patient care and optimize healthcare services. However, the rapid adoption of the Internet of Medical Things also introduces significant security challenges that must be effectively addressed to preserve patient privacy, protect sensitive medical data, and ensure the overall reliability and safety of Internet of Medical Things systems. In this context, a key agreement protocol is used to securely establish shared cryptographic keys between interconnected medical devices and the central system, ensuring confidential and authenticated communication. Recently Chen et al. proposed a lightweight authentication and key agreement protocol for the Internet of health things. In this article, we provide a descriptive analysis of their proposed scheme and prove that Chen et al.'s scheme is vulnerable to Known session-specific temporary information attacks and stolen verifier attacks.",SEC,http://arxiv.org/abs/2406.16804v1
Addressing Security and Privacy Challenges in Internet of Things,"Internet of Things (IoT), also referred to as the Internet of Objects, is envisioned as a holistic and transformative approach for providing numerous services. The rapid development of various communication protocols and miniaturization of transceivers along with recent advances in sensing technologies offer the opportunity to transform isolated devices into communicating smart things. Smart things, that can sense, store, and even process electrical, thermal, optical, chemical, and other signals to extract user-/environment-related information, have enabled services only limited by human imagination. Despite picturesque promises of IoT-enabled systems, the integration of smart things into the standard Internet introduces several security challenges because the majority of Internet technologies, communication protocols, and sensors were not designed to support IoT. Several recent research studies have demonstrated that launching security/privacy attacks against IoT-enabled systems, in particular wearable medical sensor (WMS)-based systems, may lead to catastrophic situations and life-threatening conditions. Therefore, security threats and privacy concerns in the IoT domain need to be proactively studied and aggressively addressed. In this thesis, we tackle several domain-specific security/privacy challenges associated with IoT-enabled systems.",SEC,http://arxiv.org/abs/1807.06724v1
Sense-Deliberate-Act Cognitive Agents for Sense-Compute-Control Applications in the Internet of Things & Services,"In this paper, we advocate Agent-Oriented Software Engi-neering (AOSE) through employing Belief-Desire-Intention (BDI) intel-ligent agents for developing Sense-Compute-Control (SCC) applications in the Internet of Things and Services (IoTS). We argue that not only the agent paradigm, in general, but also cognitive BDI agents with sense-deliberate-act cycle, in particular, fit very well to the nature of SCC applications in the IoTS. However, considering the highly constrained heterogeneous devices that are prevalent in the IoTS, existing BDI agent frameworks, even those especially created for Wireless Sensor Networks (WSNs), do not work. We elaborate on the challenges and propose pos-sible approaches to address them.",IOTNET,http://arxiv.org/abs/2009.10638v1
A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of Internet of Things (IoT) Services,"Service quality possesses the vital prominence in usability of innovative products and services. As technological innovation has made the life synchronized and effective, Internet of Things (IoT) is matter of discussion everywhere. From users' perspective, IoT services are always embraced by various system characteristics of security and performance. A service quality model can better present the preference of such technology customers. the study intends to project theoretical model of service quality for internet of things (IoT). Based on the existing models of service quality and the literature in internet of things, a framework is proposed to conceptualize and measure service quality for internet of things.This study established the IoT-Servqual model with four dimensions (i.e., Privacy, Functionality, Efficiency, and Tangibility) of multiple service quality models. These dimensions are essential and inclined towards the users' leaning of IoT Services. This paper contributes to research on internet of things services by development of a comprehensive framework for customers' quality apprehension. This model will previse the expression of information secrecy of users related with internet of things (IoT). This research will advance understanding of service quality in modern day technology and assist firms to devise the fruitful service structure.",IOTNET,http://arxiv.org/abs/2001.01840v1
Semantic Reasoning for Context-aware Internet of Things Applications,"Advances in ICT are bringing into reality the vision of a large number of uniquely identifiable, interconnected objects and things that gather information from diverse physical environments and deliver the information to a variety of innovative applications and services. These sensing objects and things form the Internet of Things (IoT) that can improve energy and cost efficiency and automation in many different industry fields such as transportation and logistics, health care and manufacturing, and facilitate our everyday lives as well. IoT applications rely on real-time context data and allow sending information for driving the behaviors of users in intelligent environments.",IOTNET,http://arxiv.org/abs/1604.08340v1
HMIoT: A New Healthcare Model Based on Internet of Things,"In recent century, with developing of equipment, using of the internet and things connected to the internet is growing. Therefore, the need for informing in the process of expanding the scope of its application is very necessary and important. These days, using intelligent and autonomous devices in our daily lives has become commonplace and the Internet is the most important part of the relationship between these tools and even at close distances also. Things connected to the Internet that are currently in use and can be inclusive of all the sciences as a step to develop and coordinate of them. In this paper we investigate application and using of Internet of things from the perspective of various sciences. We show that how this phenomenon can influence on future health of people.",IOTNET,http://arxiv.org/abs/1507.08569v1
"Internet of Nano-Things, Things and Everything: Future Growth Trends","The current statuses and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano-Things (IoNT) are extensively reviewed and a summarized survey is presented. The analysis clearly distinguishes between IoT and IoE, which are wrongly considered to be the same by many commentators. After evaluating the current trends of advancement in the fields of IoT, IoE and IoNT, this paper identifies the 21 most significant current and future challenges as well as scenarios for the possible future expansion of their applications. Despite possible negative aspects of these developments, there are grounds for general optimism about the coming technologies. Certainly, many tedious tasks can be taken over by IoT devices. However, the dangers of criminal and other nefarious activities, plus those of hardware and software errors, pose major challenges that are a priority for further research. Major specific priority issues for research are identified.",IOTNET,http://arxiv.org/abs/1808.09869v1
Survey of Security and Privacy Issues of Internet of Things,This paper is a general survey of all the security issues existing in the Internet of Things (IoT) along with an analysis of the privacy issues that an end-user may face as a consequence of the spread of IoT. The majority of the survey is focused on the security loopholes arising out of the information exchange technologies used in Internet of Things. No countermeasure to the security drawbacks has been analyzed in the paper.,SEC,http://arxiv.org/abs/1501.02211v1
"Internet of Things: Concept, Building blocks, Applications and Challenges","Internet of things (IoT) constitutes one of the most important technology that has the potential to affect deeply our way of life, after mobile phones and Internet. The basic idea is that every objet that is around us will be part of the network (Internet), interacting to reach a common goal. In another word, the Internet of Things concept aims to link the physical world to the digital one. Technology advances along with popular demand will foster the wide spread deployement of IoT's services, it would radically transform our corporations, communities, and personal spheres. In this survey, we aim to provide the reader with a broad overview of the Internet of things concept, its building blocks, its applications along with its challenges.",IOTNET,http://arxiv.org/abs/1401.6877v1
"Development of Internet of Things, Augmented Reality and 5G technologies (review)","Just as the emergence of personal computers and smartphones has changed the life of modern society, the Internet of Things, augmented reality and ultra-fast and reliable telecommunications networks of the new generation, by combining the physical objects of the real world with the ever-increasing computing power and intelligence of cyberspace, will make the next big revolution in all spheres of human activity. Keywords: Internet of Things, 5G, augmented reality.",IOTNET,http://arxiv.org/abs/1902.08008v1
Improving the quality of healthcare through Internet of Things,"This paper attempts to outline how the adoption of Internet of Things (IoT) in healthcare can create real economic value and improve the patient experience. Thus, getting the maximum benefits requires understanding both the IoT paradigm and the enabling technologies, and how IoT can be applied in the field of healthcare. We will mention some open challenging issues to be addressed by the research community, and not only. Besides the real barriers in adopting the Internet of Things, there are some advantages regard collecting and processing patient data, and monitoring the daily health states of individuals, just to name a few. These aspects could revolutionize the healthcare industry.",IOTNET,http://arxiv.org/abs/1903.05221v1
A Novel Method for Developing Robotics via Artificial Intelligence and Internet of Things,"This paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things. Now a day, we can say Artificial Intelligence take the world into robotics. Almost all industries use robots for lot of works. They are use co-operative robots to make different kind of works. But there was some problem to make robot for multi tasks. So there was a necessary new methodology to made multi tasking robots. It will be done only by artificial intelligence and internet of things.",IOTNET,http://arxiv.org/abs/1405.3939v1
Towards a Practical Architecture for India Centric Internet of Things,"An effective architecture for the Internet of Things (IoT), particularly for an emerging nation like India with limited technology penetration at the national scale, should be based on tangible technology advances in the present, practical application scenarios of social and entrepreneurial value, and ubiquitous capabilities that make the realization of IoT affordable and sustainable. Humans, data, communication and devices play key roles in the IoT ecosystem that we perceive. In a push towards this sustainable and practical IoT Architecture for India, we synthesize ten design paradigms to consider.",IOTNET,http://arxiv.org/abs/1407.0434v2
Unveiling Contextual Similarity of Things via Mining Human-Thing Interactions in the Internet of Things,"With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations of ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based method for discovering underlying connections of things via mining the rich content embodied in human-thing interactions in terms of user, temporal and spatial information. We model these various information using two graphs, namely spatio-temporal graph and social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management. To demonstrate the utility, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach.",IOTNET,http://arxiv.org/abs/1512.08493v3
KubeAdaptor: A Docking Framework for Workflow Containerization on Kubernetes,"As Kubernetes becomes the infrastructure of the cloud-native era, the integration of workflow systems with Kubernetes is gaining more and more popularity. To our knowledge, workflow systems employ scheduling algorithms that optimize task execution order of workflow to improve performance and execution efficiency. However, due to its inherent scheduling mechanism, Kubernetes does not execute containerized scheduling following the optimized task execution order of workflow amid migrating workflow systems to the Kubernetes platform. This inconsistency in task scheduling order seriously degrades the efficiency of workflow execution and brings numerous challenges to the containerized process of workflow systems on Kubernetes. In this paper, we propose a cloud-native workflow engine, also known as KubeAdaptor, a docking framework able to implement workflow containerization on Kubernetes, integrate workflow systems with Kubernetes, ensuring the consistency of task scheduling order. We introduce the design and architecture of the KubeAdaptor, elaborate on the functionality implementation and the event-trigger mechanism within the KubeAdaptor. Experimental results about four real-world workflows show that the KubeAdaptor ensures the consistency of the workflow systems and Kubernetes in the task scheduling order. Compared with the baseline Argo workflow engine, the KubeAdaptor achieves better performance in terms of the average execution time of task pod, average workflow lifecycle, and resource usage rate.",CLD,http://arxiv.org/abs/2207.01222v1
Multi-objective Optimization of Clustering-based Scheduling for Multi-workflow On Clouds Considering Fairness,"Distributed computing, such as cloud computing, provides promising platforms to execute multiple workflows. Workflow scheduling plays an important role in multi-workflow execution with multi-objective requirements. Although there exist many multi-objective scheduling algorithms, they focus mainly on optimizing makespan and cost for a single workflow. There is a limited research on multi-objective optimization for multi-workflow scheduling. Considering multi-workflow scheduling, there is an additional key objective to maintain the fairness of workflows using the resources. To address such issues, this paper first defines a new multi-objective optimization model based on makespan, cost, and fairness, and then proposes a global clustering-based multi-workflow scheduling strategy for resource allocation. Experimental results show that the proposed approach performs better than the compared algorithms without significant compromise of the overall makespan and cost as well as individual fairness, which can guide the simulation workflow scheduling on clouds.",OPS,http://arxiv.org/abs/2205.11173v1
Data-Aware Approximate Workflow Scheduling,"Optimization of data placement in complex scientific workflows has become very crucial since the large amounts of data generated by these workflows significantly increases the turnaround time of the end-to-end application. It is almost impossible to make an optimal scheduling for the end-to-end workflow without considering the intermediate data movement. In order to reduce the complexity of the workflow-scheduling problem, most of the existing work constrains the problem space by some unrealistic assumptions, which result in non-optimal scheduling in practice. In this study, we propose a genetic data-aware algorithm for the end-to-end workflow scheduling problem. Distinct from the past research, we develop a novel data-aware evaluation function for each chromosome, a common augmenting crossover operator and a simple but effective mutation operator. Our experiments on different workflow structures show that the proposed GA based approach gives a scheduling close to the optimal one.",OPS,http://arxiv.org/abs/1805.10499v1
Energy-efficient workflow scheduling based on workflow structures under deadline and budget constraints in the cloud,"The utilization of cloud environments to deploy scientific workflow applications is an emerging trend in scientific community. In this area, the main issue is the scheduling of workflows, which is known as an NP-complete problem. Apart from respecting user-defined deadline and budget, energy consumption is a major concern for cloud providers in implementing the scheduling strategy. The types and the number of virtual machines (VMs) used are determinant to handle those issues, and their determination is highly influenced by the structure of the workflow. In this paper, we propose two workflow scheduling algorithms that take advantage of the structural properties of the workflows. The first algorithm is called Structure-based Multi-objective Workflow Scheduling with an Optimal instance type (SMWSO). It introduces a new approach to determine the optimal instance type along with the optimal number of VMs to be provisioned. We also consider the use of heterogeneous VMs in the Structure-based Multi-objective Workflow Scheduling with Heterogeneous instance types (SMWSH), to highlight the algorithm's strength within the heterogeneous environment. The simulation results show that our proposal produces better energy-efficiency in 80% of workflow/workload scenarios, and save more than 50% overall energy compared to a recent state-of-the-art algorithm.",OPS,http://arxiv.org/abs/2201.05429v1
WOW: Workflow-Aware Data Movement and Task Scheduling for Dynamic Scientific Workflows,"Scientific workflows process extensive data sets over clusters of independent nodes, which requires a complex stack of infrastructure components, especially a resource manager (RM) for task-to-node assignment, a distributed file system (DFS) for data exchange between tasks, and a workflow engine to control task dependencies. To enable a decoupled development and installation of these components, current architectures place intermediate data files during workflow execution independently of the future workload. In data-intensive applications, this separation results in suboptimal schedules, as tasks are often assigned to nodes lacking input data, causing network traffic and bottlenecks. This paper presents WOW, a new scheduling approach for dynamic scientific workflow systems that steers both data movement and task scheduling to reduce network congestion and overall runtime. For this, WOW creates speculative copies of intermediate files to prepare the execution of subsequently scheduled tasks. WOW supports modern workflow systems that gain flexibility through the dynamic construction of execution plans. We prototypically implemented WOW for the popular workflow engine Nextflow using Kubernetes as a resource manager. In experiments with 16 synthetic and real workflows, WOW reduced makespan in all cases, with improvement of up to 94.5% for workflow patterns and up to 53.2% for real workflows, at a moderate increase of temporary storage space. It also has favorable effects on CPU allocation and scales well with increasing cluster size.",CLD,http://arxiv.org/abs/2503.13072v2
A Cost-Driven Fuzzy Scheduling Strategy for Intelligent Workflow Decision Making Systems in Uncertain Edge-Cloud Environments,"Workflow decision making is critical to performing many practical workflow applications. Scheduling in edge-cloud environments can address the high complexity problem of workflow applications, while decreasing the data transmission delay between the cloud and end devices. However, because of the heterogeneous resources in edge-cloud environments and the complicated data dependencies among the tasks in a workflow, significant challenges for workflow scheduling remain, including the selection of an optimal tasks-servers solution from the possible numerous combinations. The existing studies have been mainly done subject to rigorous conditions without fluctuations, ignoring the fact that workflow scheduling is typically present in uncertain environments. In this study, we focus on reducing the execution cost of workflow applications mainly caused by task computation and data transmission, while satisfying the workflow deadline in uncertain edge-cloud environments. The Triangular Fuzzy Numbers (TFNs) are adopted to represent the task processing time and data transferring time. A cost-driven fuzzy scheduling strategy based on an Adaptive Discrete Particle Swarm Optimization (ADPSO) algorithm is proposed, which employs the operators of Genetic Algorithm (GA). This strategy introduces the randomly two-point crossover operator, neighborhood mutation operator, and adaptive multipoint mutation operator of GA to effectively avoid converging on local optima. The experimental results show that our strategy can effectively reduce the workflow execution cost in uncertain edge-cloud environments, compared with other benchmark solutions.",CLD,http://arxiv.org/abs/2107.01405v4
Analysis of Workflow Schedulers in Simulated Distributed Environments,"Task graphs provide a simple way to describe scientific workflows (sets of tasks with dependencies) that can be executed on both HPC clusters and in the cloud. An important aspect of executing such graphs is the used scheduling algorithm. Many scheduling heuristics have been proposed in existing works; nevertheless, they are often tested in oversimplified environments. We provide an extensible simulation environment designed for prototyping and benchmarking task schedulers, which contains implementations of various scheduling algorithms and is open-sourced, in order to be fully reproducible. We use this environment to perform a comprehensive analysis of workflow scheduling algorithms with a focus on quantifying the effect of scheduling challenges that have so far been mostly neglected, such as delays between scheduler invocations or partially unknown task durations. Our results indicate that network models used by many previous works might produce results that are off by an order of magnitude in comparison to a more realistic model. Additionally, we show that certain implementation details of scheduling algorithms which are often neglected can have a large effect on the scheduler's performance, and they should thus be described in great detail to enable proper evaluation.",CLD,http://arxiv.org/abs/2204.07211v1
Efficient Orchestrated AI Workflows Execution on Scale-out Spatial Architecture,"Given the increasing complexity of AI applications, traditional spatial architectures frequently fall short. Our analysis identifies a pattern of interconnected, multi-faceted tasks encompassing both AI and general computational processes. In response, we have conceptualized ""Orchestrated AI Workflows,"" an approach that integrates various tasks with logic-driven decisions into dynamic, sophisticated workflows. Specifically, we find that the intrinsic Dual Dynamicity of Orchestrated AI Workflows, namely dynamic execution times and frequencies of Task Blocks, can be effectively represented using the Orchestrated Workflow Graph. Furthermore, the intrinsic Dual Dynamicity poses challenges to existing spatial architecture, namely Indiscriminate Resource Allocation, Reactive Load Rebalancing, and Contagious PEA Idleness. To overcome these challenges, we present Octopus, a scale-out spatial architecture and a suite of advanced scheduling strategies optimized for executing Orchestrated AI Workflows, such as the Discriminate Dual-Scheduling Mechanism, Adaptive TBU Scheduling Strategy, and Proactive Cluster Scheduling Strategy. Our evaluations demonstrate that Octopus significantly outperforms traditional architectures in handling the dynamic demands of Orchestrated AI Workflows, and possesses robust scalability in large scale hardware such as wafer-scale chip.",OPS,http://arxiv.org/abs/2405.17221v1
Resource Provisioning and Scheduling Algorithm for Meeting Cost and Deadline-Constraints of Scientific Workflows in IaaS Clouds,"Infrastructure as a Service model of cloud computing is a desirable platform for the execution of cost and deadline constrained workflow applications as the elasticity of cloud computing allows large-scale complex scientific workflow applications to scale dynamically according to their deadline requirements. However, scheduling of these multitask workflow jobs in a distributed computing environment is a computationally hard multi-objective combinatorial optimization problem. The critical challenge is to schedule the workflow tasks whilst meeting user quality of service (QoS) requirements and the application's deadline. The existing research work not only fails to address this challenge but also do not incorporate the basic principles of elasticity and heterogeneity of computing resources in cloud environment. In this paper, we propose a resource provisioning and scheduling algorithm to schedule the workflow applications on IaaS clouds to meet application deadline constraints while optimizing the execution cost. The proposed algorithm is based on the nature-inspired population based Intelligent Water Drop (IWD) optimization algorithm. The experimental results in the simulated environment of CloudSim with four real-world workflow applications demonstrates that IWD algorithm schedules workflow tasks with optimized cost within the specified deadlines. Moreover, the IWD algorithm converges fast to near optimal solution.",CLD,http://arxiv.org/abs/1806.02397v1
An Efficient Fault Tolerant Workflow Scheduling Approach using Replication Heuristics and Checkpointing in the Cloud,"Scientific workflows have been predominantly used for complex and large scale data analysis and scientific computation/automation and the need for robust workflow scheduling techniques has grown considerably. But, most of the existing workflow scheduling algorithms do not provide the required reliability and robustness. In this paper, a new fault tolerant workflow scheduling algorithm that learns replication heuristics in an unsupervised manner has been proposed. Furthermore, the use of light weight synchronized checkpointing enables efficient resubmission of failed tasks and ensures workflow completion even in precarious environments. The proposed technique improves upon metrics like Resource Wastage and Resource Usage in comparison to the Replicate-All algorithm, while maintaining an acceptable increase in Makespan as compared to the vanilla Heterogeneous Earliest Finish Time (HEFT).",OPS,http://arxiv.org/abs/1810.06361v2
Reinforcement Learning-driven Data-intensive Workflow Scheduling for Volunteer Edge-Cloud,"In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a cost-effective, community computing paradigm to support data-intensive scientific workflows. However, due to the highly distributed and heterogeneous nature of VEC resources, centralized workflow task scheduling remains a challenge. In this paper, we propose a Reinforcement Learning (RL)-driven data-intensive scientific workflow scheduling approach that takes into consideration: i) workflow requirements, ii) VEC resources' preference on workflows, and iii) diverse VEC resource policies, to ensure robust resource allocation. We formulate the long-term average performance optimization problem as a Markov Decision Process, which is solved using an event-based Asynchronous Advantage Actor-Critic RL approach. Our extensive simulations and testbed implementations demonstrate our approach's benefits over popular baseline strategies in terms of workflow requirement satisfaction, VEC preference satisfaction, and available VEC resource utilization.",OPS,http://arxiv.org/abs/2407.01428v1
Workflow-as-a-Service Cloud Platform and Deployment of Bioinformatics Workflow Applications,"Workflow management systems (WMS) support the composition and deployment of workflow-oriented applications in distributed computing environments. They hide the complexity of managing large-scale applications, which includes the controlling data pipelining between tasks, ensuring the application's execution, and orchestrating the distributed computational resources to get a reasonable processing time. With the increasing trends of scientific workflow adoption, the demand to deploy them using a third-party service begins to increase. Workflow-as-a-service (WaaS) is a term representing the platform that serves the users who require to deploy their workflow applications on third-party cloud-managed services. This concept drives the existing WMS technology to evolve towards the development of the WaaS cloud platform. Based on this requirement, we extend CloudBus WMS functionality to handle the workload of multiple workflows and develop the WaaS cloud platform prototype. We implemented the Elastic Budget-constrained resource Provisioning and Scheduling algorithm for Multiple workflows (EBPSM) algorithm that is capable of scheduling multiple workflows and evaluated the platform using two bioinformatics workflows. Our experimental results show that the platform is capable of efficiently handling multiple workflows execution and gaining its purpose to minimize the makespan while meeting the budget.",CLD,http://arxiv.org/abs/2006.01957v1
Adaptive Scheduling for Efficient Execution of Dynamic Stream Workflows,"Stream workflow application such as online anomaly detection or online traffic monitoring, integrates multiple streaming big data applications into data analysis pipeline. This application can be highly dynamic in nature, where the data velocity may change at runtime and therefore the resources should be managed overtime. To manage these changes, the orchestration of this application requires a dynamic execution environment and dynamic scheduling technique. For the former requirement, Multicloud environment is a visible solution to cope with the dynamic aspects of this workflow application. While for the latter requirement, dynamic scheduling technique not only need to adhere to end user's requirements in terms of data processing and deadline for decision making, and data stream sources location constraints, but also adjust provisioning and scheduling plan at runtime to cope with dynamic variations of stream data rates. Therefore, we propose a two-phase adaptive scheduling technique to efficiently schedule dynamic workflow application in Multicloud environment that can respond to changes in the velocity of data at runtime. The experimental results showed that the proposed technique is close to the lower bound and effective for different experiment scenarios.",CLD,http://arxiv.org/abs/1912.08397v1
BeeFlow: Behavior Tree-based Serverless Workflow Modeling and Scheduling for Resource-Constrained Edge Clusters,"Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce BeeFlow, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that BeeFlow achieves up to 3.2X speedup in a high-density, resource-constrained edge testbed and 2.5X speedup in a high-profile cloud testbed, compared with the state-of-the-art.",OPS,http://arxiv.org/abs/2308.16517v1
RIOT: a Stochastic-based Method for Workflow Scheduling in the Cloud,"Cloud computing provides engineers or scientists a place to run complex computing tasks. Finding a workflow's deployment configuration in a cloud environment is not easy. Traditional workflow scheduling algorithms were based on some heuristics, e.g. reliability greedy, cost greedy, cost-time balancing, etc., or more recently, the meta-heuristic methods, such as genetic algorithms. These methods are very slow and not suitable for rescheduling in the dynamic cloud environment. This paper introduces RIOT (Randomized Instance Order Types), a stochastic based method for workflow scheduling. RIOT groups the tasks in the workflow into virtual machines via a probability model and then uses an effective surrogate-based method to assess a large amount of potential scheduling. Experiments in dozens of study cases showed that RIOT executes tens of times faster than traditional methods while generating comparable results to other methods.",CLD,http://arxiv.org/abs/1708.08127v2
Scheduling Algorithms for Efficient Execution of Stream Workflow Applications in Multicloud Environments,"Big data processing applications are becoming more and more complex. They are no more monolithic in nature but instead they are composed of decoupled analytical processes in the form of a workflow. One type of such workflow applications is stream workflow application, which integrates multiple streaming big data applications to support decision making. Each analytical component of these applications runs continuously and processes data streams whose velocity will depend on several factors such as network bandwidth and processing rate of parent analytical component. As a consequence, the execution of these applications on cloud environments requires advanced scheduling techniques that adhere to end user's requirements in terms of data processing and deadline for decision making. In this paper, we propose two Multicloud scheduling and resource allocation techniques for efficient execution of stream workflow applications on Multicloud environments while adhering to workflow application and user performance requirements and reducing execution cost. Results showed that the proposed genetic algorithm is an adequate and effective for all experiments.",CLD,http://arxiv.org/abs/1912.08392v1
Efficient Probabilistic Workflow Scheduling for IaaS Clouds,"The flexibility and the variety of computing resources offered by the cloud make it particularly attractive for executing user workloads. However, IaaS cloud environments pose non-trivial challenges in the case of workflow scheduling under deadlines and monetary cost constraints. Indeed, given the typical uncertain performance behavior of cloud resources, scheduling algorithms that assume deterministic execution times may fail, thus requiring probabilistic approaches. However, existing probabilistic algorithms are computationally expensive, mainly due to the greater complexity of the workflow scheduling problem in its probabilistic form, and they hardily scale with the size of the problem instance. In this article, we propose EPOSS, a novel workflow scheduling algorithm for IaaS cloud environments based on a probabilistic formulation. Our solution blends together the low execution latency of state-of-the-art scheduling algorithms designed for the case of deterministic execution times and the capability to enforce probabilistic constraints.Designed with computational efficiency in mind, EPOSS achieves one to two orders lower execution times in comparison with existing probabilistic schedulers. Furthermore, it ensures good scaling with respect to workflow size and number of heterogeneous virtual machine types offered by the IaaS cloud environment. We evaluated the benefits of our algorithm via an experimental comparison over a variety of workloads and characteristics of IaaS cloud environments.",CLD,http://arxiv.org/abs/2412.06073v1
Solving workflow scheduling problems with QUBO modeling,"In this paper we investigate the workflow scheduling problem, a known NP-hard class of scheduling problems. We derive problem instances from an industrial use case and compare against several quantum, classical, and hybrid quantum-classical algorithms. We develop a novel QUBO to represent our scheduling problem and show how the QUBO complexity depends on the input problem. We derive and present a decomposition method for this specific application to mitigate this complexity and demonstrate the effectiveness of the approach.",OPS,http://arxiv.org/abs/2205.04844v1
Resource-sharing Policy in Multi-tenant Scientific Workflow-as-a-Service Cloud Platform,"Increased adoption of scientific workflows in the community has urged for the development of multi-tenant platforms that provide these workflow executions as a service. As a result, Workflow-as-a-Service (WaaS) concept has been created by researchers to address the future design of Workflow Management Systems (WMS) that can serve a large number of users from a single point of service. These platforms differ from traditional WMS in that they handle a workload of workflows at runtime. A traditional WMS is usually designed to execute a single workflow in a dedicated process while WaaS cloud platforms enhance the process by exploiting multiple workflows execution in a multi-tenant environment model. In this paper, we explore a novel resource-sharing policy to improve system utilization and to fulfil various Quality of Service (QoS) requirements from multiple users in WaaS cloud platforms. We propose an Elastic Budget-constrained resource Provisioning and Scheduling algorithm for Multiple workflows that can reduce the computational overhead by encouraging resource sharing to minimize workflows' makespan while meeting a user-defined budget. Our experiments show that the EBPSM algorithm can utilize the resource-sharing policy to achieve higher performance in terms of minimizing the makespan compared to the state-of-the-art budget-constraint scheduling algorithm.",CLD,http://arxiv.org/abs/1903.01113v3
Workflow Scheduling in the Cloud with Weighted Upward-rank Priority Scheme Using Random Walk and Uniform Spare Budget Splitting,"We study a difficult problem of how to schedule complex workflows with precedence constraints under a limited budget in the cloud environment. We first formulate the scheduling problem as an integer programming problem, which can be optimized and used as the baseline of performance. We then consider the traditional approach of scheduling jobs in a prioritized order based on the upward-rank of each job. For those jobs with no precedence constraints among themselves, the plain upward-rank priority scheme assigns priorities in an arbitrary way. We propose a job prioritization scheme that uses Markovian chain stationary probabilities as a measure of importance of jobs. The scheme keeps the precedence order for the jobs that have precedence constraints between each other, and assigns priorities according to the jobs' importance for the jobs without precedence constraints. We finally design a uniform spare budget splitting strategy, which splits the spare budget uniformly across all the jobs. We test our algorithms on a variety of workflows, including FFT, Gaussian elimination, typical scientific workflows, randomly generated workflows and workflows from an in-production cluster of an online streaming service company. We compare our algorithms with the-state-of-art algorithms. The empirical results show that the uniform spare budget splitting scheme outperforms the splitting scheme in proportion to extra demand in average for most cases, and the Markovian based prioritization further improves the workflow makespan.",CLD,http://arxiv.org/abs/1903.01154v1
Minimizing Energy in Reliability and Deadline-Ensured Workflow Scheduling in Cloud,"With the increasing prevalence of computationally intensive workflows in cloud environments, it has become crucial for cloud platforms to optimize energy consumption while ensuring the feasibility of user workflow schedules with respect to strict deadlines and reliability constraints. The key challenges faced when cloud systems provide virtual machines of varying levels of reliability, energy consumption, processing frequencies, and computing capabilities to execute tasks of these workflows. To address these issues, we propose an adaptive strategy based on maximum fan-out ratio considering the slack of tasks and deadline distribution for scheduling workflows in a single cloud platform, intending to minimise energy consumption while ensuring strict reliability and deadline constraints. We also propose an approach for dynamic scheduling of workflow using the rolling horizon concept to consider the dynamic execution time of tasks of the workflow where the actual task execution time at run time is shorter than worst-case execution time in most of the cases. Our proposed static approach outperforms the state-of-the-art (SOTA) by up to 70% on average in scenarios without deadline constraints, and achieves an improvement of approximately 2% in deadline-constrained cases. The dynamic variant of our approach demonstrates even stronger performance, surpassing SOTA by 82% in non-deadline scenarios and by up to 27% on average when deadline constraints are enforced. Furthermore, in comparison with the static optimal solution, our static approach yields results within a factor of 1.1, while the dynamic approach surpasses the optimal baseline by an average of 25%.",CLD,http://arxiv.org/abs/2505.16496v1
A Deep Reinforcement Learning Approach for Cost Optimized Workflow Scheduling in Cloud Computing Environments,"Cost optimization is a common goal of workflow schedulers operating in cloud computing environments. The use of spot instances is a potential means of achieving this goal, as they are offered by cloud providers at discounted prices compared to their on-demand counterparts in exchange for reduced reliability. This is due to the fact that spot instances are subjected to interruptions when spare computing capacity used for provisioning them is needed back owing to demand variations. Also, the prices of spot instances are not fixed as pricing is dependent on long term supply and demand. The possibility of interruptions and pricing variations associated with spot instances adds a layer of uncertainty to the general problem of workflow scheduling across cloud computing environments. These challenges need to be efficiently addressed for enjoying the cost savings achievable with the use of spot instances without compromising the underlying business requirements. To this end, in this paper we use Deep Reinforcement Learning for developing an autonomous agent capable of scheduling workflows in a cost efficient manner by using an intelligent mix of spot and on-demand instances. The proposed solution is implemented in the open source container native Argo workflow engine that is widely used for executing industrial workflows. The results of the experiments demonstrate that the proposed scheduling method is capable of outperforming the current benchmarks.",CLD,http://arxiv.org/abs/2408.02926v1
A Comparative Evaluation of Population-based Optimization Algorithms for Workflow Scheduling in Cloud-Fog Environments,"This work presents a comparative evaluation of four population-based optimization algorithms for workflow scheduling in cloud-fog environments. These algorithms are as follows: Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Differential Evolution (DE) and GA-PSO. This work also provides the motivational groundwork for the weighted sum objective function for the workflow scheduling problem and develops this function based on three objectives: makespan, cost and energy. The recently proposed FogWorkflowSim is used as the simulation environment with the aforementioned objectives serving performance metrics. Results show that hybrid combination of the GA-PSO algorithm exhibits slightly better than the standard algorithms. Future work will include expansion of the workflows used by increasing the number of tasks as well as adding some more workflows. The addition of some more objectives to the weighted objective function will also be pursued",OPS,http://arxiv.org/abs/2012.00176v2
MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems,"Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user's service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively.",CLD,http://arxiv.org/abs/2112.07269v1
A Survey on Deadline Constrained Workflow Scheduling Algorithms in Cloud Environment,"Cloud Computing is the latest blooming technology in the era of Computer Science and Information Technology domain. There is an enormous pool of data centres, which are termed as Clouds where the services and associated data are being deployed and users need a constant Internet connection to access them. One of the highlights in Cloud is the delivering of applications or services in an on-demand environment. One of the most promising areas in Cloud scheduling is Scheduling of workflows which is intended to match the request of the user to the appropriate resources. There are several algorithms to automate the workflows in a way to satisfy the Quality of service (QoS) of the user among which deadline is considered as a major criterion, i.e. Satisfying the needs of the user with minimized cost and within the minimum stipulated time. This paper surveys various workflow scheduling algorithms that have a deadline as its criterion.",CLD,http://arxiv.org/abs/1409.7916v1
"Reinforcement Learning based Workflow Scheduling in Cloud and Edge Computing Environments: A Taxonomy, Review and Future Directions","Deep Reinforcement Learning (DRL) techniques have been successfully applied for solving complex decision-making and control tasks in multiple fields including robotics, autonomous driving, healthcare and natural language processing. The ability of DRL agents to learn from experience and utilize real-time data for making decisions makes it an ideal candidate for dealing with the complexities associated with the problem of workflow scheduling in highly dynamic cloud and edge computing environments. Despite the benefits of DRL, there are multiple challenges associated with the application of DRL techniques including multi-objectivity, curse of dimensionality, partial observability and multi-agent coordination. In this paper, we comprehensively analyze the challenges and opportunities associated with the design and implementation of DRL oriented solutions for workflow scheduling in cloud and edge computing environments. Based on the identified characteristics, we propose a taxonomy of workflow scheduling with DRL. We map reviewed works with respect to the taxonomy to identify their strengths and weaknesses. Based on taxonomy driven analysis, we propose novel future research directions for the field.",OPS,http://arxiv.org/abs/2408.02938v1
Portability of Scientific Workflows in NGS Data Analysis: A Case Study,"The analysis of next-generation sequencing (NGS) data requires complex computational workflows consisting of dozens of autonomously developed yet interdependent processing steps. Whenever large amounts of data need to be processed, these workflows must be executed on a parallel and/or distributed systems to ensure reasonable runtime. Porting a workflow developed for a particular system on a particular hardware infrastructure to another system or to another infrastructure is non-trivial, which poses a major impediment to the scientific necessities of workflow reproducibility and workflow reusability. In this work, we describe our efforts to port a state-of-the-art workflow for the detection of specific variants in whole-exome sequencing of mice. The workflow originally was developed in the scientific workflow system snakemake for execution on a high-performance cluster controlled by Sun Grid Engine. In the project, we ported it to the scientific workflow system SaasFee that can execute workflows on (multi-core) stand-alone servers or on clusters of arbitrary sizes using the Hadoop. The purpose of this port was that also owners of low-cost hardware infrastructures, for which Hadoop was made for, become able to use the workflow. Although both the source and the target system are called scientific workflow systems, they differ in numerous aspects, ranging from the workflow languages to the scheduling mechanisms and the file access interfaces. These differences resulted in various problems, some expected and more unexpected, that had to be resolved before the workflow could be run with equal semantics. As a side-effect, we also report cost/runtime ratios for a state-of-the-art NGS workflow on very different hardware platforms: A comparably cheap stand-alone server (80 threads), a mid-cost, mid-sized cluster (552 threads), and a high-end HPC system (3784 threads).",CLD,http://arxiv.org/abs/2006.03104v1
Lotaru: Locally Estimating Runtimes of Scientific Workflow Tasks in Heterogeneous Clusters,"Many scientific workflow scheduling algorithms need to be informed about task runtimes a-priori to conduct efficient scheduling. In heterogeneous cluster infrastructures, this problem becomes aggravated because these runtimes are required for each task-node pair. Using historical data is often not feasible as logs are typically not retained indefinitely and workloads as well as infrastructure changes. In contrast, online methods, which predict task runtimes on specific nodes while the workflow is running, have to cope with the lack of example runs, especially during the start-up. In this paper, we present Lotaru, a novel online method for locally estimating task runtimes in scientific workflows on heterogeneous clusters. Lotaru first profiles all nodes of a cluster with a set of short-running and uniform microbenchmarks. Next, it runs the workflow to be scheduled on the user's local machine with drastically reduced data to determine important task characteristics. Based on these measurements, Lotaru learns a Bayesian linear regression model to predict a task's runtime given the input size and finally adjusts the predicted runtime specifically for each task-node pair in the cluster based on the micro-benchmark results. Due to its Bayesian approach, Lotaru can also compute robust uncertainty estimates and provides them as an input for advanced scheduling methods. Our evaluation with five real-world scientific workflows and different datasets shows that Lotaru significantly outperforms the baselines in terms of prediction errors for homogeneous and heterogeneous clusters.",OPS,http://arxiv.org/abs/2205.11181v1
A Makespan and Energy-Aware Scheduling Algorithm for Workflows under Reliability Constraint on a Multiprocessor Platform,"Many scientific workflows can be modeled as a Directed Acyclic Graph (henceforth mentioned as DAG) where the nodes represent individual tasks, and the directed edges represent data and control flow dependency between two tasks. Due to the large volume of data, multiprocessor systems are often used to execute these workflows. Hence, scheduling the tasks of a workflow to achieve certain goals (such as minimizing the makespan, energy, or maximizing reliability, processor utilization, etc.) remains an active area of research in embedded systems. In this paper, we propose a workflow scheduling algorithm to minimize the makespan and energy for a given reliability constraint. If the reliability constraint is higher, we further propose Energy Aware Fault Tolerant Scheduling (henceforth mentioned as EAFTS) based on active replication. Additionally, given that the allocation of task nodes to processors is known, we develop a frequency allocation algorithm that assigns frequencies to the processors. Mathematically we show that our algorithms can work for any satisfiable reliability constraint. We analyze the proposed solution approaches to understand their time requirements. Experiments with real-world Workflows show that our algorithms, MERT and EAFTS, outperform the state-of-art approaches. In particular, we observe that MERT gives 3.12% lesser energy consumption and 14.14% lesser makespan on average. In the fault-tolerant setting, our method EAFTS gives 11.11% lesser energy consumption on average when compared with the state-of-art approaches.",OPS,http://arxiv.org/abs/2212.09274v1
Cost-Aware Dynamic Cloud Workflow Scheduling using Self-Attention and Evolutionary Reinforcement Learning,"The Cost-aware Dynamic Multi-Workflow Scheduling (CDMWS) in the cloud is a kind of cloud workflow management problem, which aims to assign virtual machine (VM) instances to execute tasks in workflows so as to minimize the total costs, including both the penalties for violating Service Level Agreement (SLA) and the VM rental fees. Powered by deep neural networks, Reinforcement Learning (RL) methods can construct effective scheduling policies for solving CDMWS problems. Traditional policy networks in RL often use basic feedforward architectures to separately determine the suitability of assigning any VM instances, without considering all VMs simultaneously to learn their global information. This paper proposes a novel self-attention policy network for cloud workflow scheduling (SPN-CWS) that captures global information from all VMs. We also develop an Evolution Strategy-based RL (ERL) system to train SPN-CWS reliably and effectively. The trained SPN-CWS can effectively process all candidate VM instances simultaneously to identify the most suitable VM instance to execute every workflow task. Comprehensive experiments show that our method can noticeably outperform several state-of-the-art algorithms on multiple benchmark CDMWS problems.",CLD,http://arxiv.org/abs/2409.18444v2
A Reference Architecture for Datacenter Scheduling: Extended Technical Report,"Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.",CLD,http://arxiv.org/abs/1808.04224v1
"A Survey and Annotated Bibliography of Workflow Scheduling in Computing Infrastructures: Community, Keyword, and Article Reviews -- Extended Technical Report","Workflows are prevalent in today's computing infrastructures. The workflow model support various different domains, from machine learning to finance and from astronomy to chemistry. Different Quality-of-Service (QoS) requirements and other desires of both users and providers makes workflow scheduling a tough problem, especially since resource providers need to be as efficient as possible with their resources to be competitive. To a newcomer or even an experienced researcher, sifting through the vast amount of articles can be a daunting task. Questions regarding the difference techniques, policies, emerging areas, and opportunities arise. Surveys are an excellent way to cover these questions, yet surveys rarely publish their tools and data on which it is based. Moreover, the communities that are behind these articles are rarely studied. We attempt to address these shortcomings in this work. We focus on four areas within workflow scheduling: 1) the workflow formalism, 2) workflow allocation, 3) resource provisioning, and 4) applications and services. Each part features one or more taxonomies, a view of the community, important and emerging keywords, and directions for future work. We introduce and make open-source an instrument we used to combine and store article meta-data. Using this meta-data, we 1) obtain important keywords overall and per year, per community, 2) identify keywords growing in importance, 3) get insight into the structure and relations within each community, and 4) perform a systematic literature survey per part to validate and complement our taxonomies.",OPS,http://arxiv.org/abs/2004.10077v1
A Cost Effective Reliability Aware Scheduler for Task Graphs in Multi-Cloud System,"Many scientific workflows can be represented by a Directed Acyclic Graph (DAG) where each node represents a task, and there will be a directed edge between two tasks if and only if there is a dependency relationship between the two i.e. the second one can not be started unless the first one is finished. Due to the increasing computational requirements of these workflows, they are deployed on cloud computing systems. Scheduling of workflows on such systems to achieve certain goals(e.g. minimization of makespan, cost, or maximization of reliability, etc.) remains an active area of research. In this paper, we propose a scheduling algorithm for allocating the nodes of our task graph in a heterogeneous multi-cloud system. The proposed scheduler considers many practical concerns such as pricing mechanisms, discounting schemes, and reliability analysis for task execution. This is a list-based heuristic that allocates tasks based on the expected times for which VMs need to be rented for them. We have analyzed the proposed approach to understand its time requirement. We perform a large number of experiments with real-world workflows: FFT, Ligo, Epigenomics, and Random workflows and observe that the proposed scheduler outperforms the state-of-art approaches up to 12%, 11%, and 1.1% in terms of cost, makespan, and reliability, respectively.",CLD,http://arxiv.org/abs/2212.09166v1
MARS: Malleable Actor-Critic Reinforcement Learning Scheduler,"In this paper, we introduce MARS, a new scheduling system for HPC-cloud infrastructures based on a cost-aware, flexible reinforcement learning approach, which serves as an intermediate layer for next generation HPC-cloud resource manager. MARS ensembles the pre-trained models from heuristic workloads and decides on the most cost-effective strategy for optimization. A whole workflow application would be split into several optimizable dependent sub-tasks, then based on the pre-defined resource management plan, a reward will be generated after executing a scheduled task. Lastly, MARS updates the Deep Neural Network (DNN) model based on the reward. MARS is designed to optimize the existing models through reinforcement mechanisms. MARS adapts to the dynamics of workflow applications, selects the most cost-effective scheduling solution among pre-built scheduling strategies (backfilling, SJF, etc.) and self-learning deep neural network model at run-time. We evaluate MARS with different real-world workflow traces. MARS can achieve 5%-60% increased performance compared to the state-of-the-art approaches.",CLD,http://arxiv.org/abs/2005.01584v3
Cybersecurity Dynamics,"We explore the emerging field of {\em Cybersecurity Dynamics}, a candidate foundation for the Science of Cybersecurity.",SEC,http://arxiv.org/abs/1502.05100v1
Novel Approach for Cybersecurity Workforce Development: A Course in Secure Design,"Training the future cybersecurity workforce to respond to emerging threats requires introduction of novel educational interventions into the cybersecurity curriculum. To be effective, these interventions have to incorporate trending knowledge from cybersecurity and other related domains while allowing for experiential learning through hands-on experimentation. To date, the traditional interdisciplinary approach for cybersecurity training has infused political science, law, economics or linguistics knowledge into the cybersecurity curriculum, allowing for limited experimentation. Cybersecurity students were left with little opportunity to acquire knowledge, skills, and abilities in domains outside of these. Also, students in outside majors had no options to get into cybersecurity. With this in mind, we developed an interdisciplinary course for experiential learning in the fields of cybersecurity and interaction design. The inaugural course teaches students from cybersecurity, user interaction design, and visual design the principles of designing for secure use - or secure design - and allows them to apply them for prototyping of Internet-of-Things (IoT) products for smart homes. This paper elaborates on the concepts of secure design and how our approach enhances the training of the future cybersecurity workforce.",SEC,http://arxiv.org/abs/1806.01198v1
"Building a Resilient Cybersecurity Posture: A Framework for Leveraging Prevent, Detect and Respond Functions and Law Enforcement Collaboration","This research paper proposes a framework for building a resilient cybersecurity posture that leverages prevent, detect, and respond functions and law enforcement collaboration. The Cybersecurity Resilience and Law Enforcement Collaboration (CyRLEC) Framework is designed to provide a comprehensive and integrated approach to cybersecurity that emphasizes collaboration with law enforcement agencies to mitigate cyber threats. The paper compares and contrasts the CyRLEC Framework with the NIST Cybersecurity Framework and highlights the critical differences between the two frameworks. While the NIST framework focuses on managing cybersecurity risk, the CyRLEC Framework takes a broader view of cybersecurity, including proactive prevention, early detection, rapid response to cyber-attacks, and close collaboration with law enforcement agencies to investigate and prosecute cybercriminals. The paper also provides a case study of a simulated real-world implementation of the CyRLEC Framework and evaluates its effectiveness in improving an organization's cybersecurity posture. The research findings demonstrate the value of the CyRLEC Framework in enhancing cybersecurity resilience and promoting effective collaboration with law enforcement agencies. Overall, this research paper contributes to the growing knowledge of cybersecurity frameworks and provides practical insights for organizations seeking to improve their cybersecurity posture.",SEC,http://arxiv.org/abs/2303.10874v1
Cybersecurity Dynamics: A Foundation for the Science of Cybersecurity,"Cybersecurity Dynamics is new concept that aims to achieve the modeling, analysis, quantification, and management of cybersecurity from a holistic perspective, rather than from a building-blocks perspective. It is centered at modeling and analyzing the attack-defense interactions in cyberspace, which cause a ``natural'' phenomenon -- the evolution of the global cybersecurity state. In this Chapter, we systematically introduce and review the Cybersecurity Dynamics foundation for the Science of Cybersecurity. We review the core concepts, technical approaches, research axes, and results that have been obtained in this endeavor. We outline a research roadmap towards the ultimate research goal, including a systematic set of technical barriers.",SEC,http://arxiv.org/abs/2010.05683v1
Adopting the Cybersecurity Concepts into Curriculum The Potential Effects on Students Cybersecurity Knowledge,"This study examines the effect of adopting cybersecurity concepts on the IT curriculum and determines the potential effect on students' knowledge of cybersecurity practices and level of awareness. To this end, a pilot study was first conducted to measure the current level of cybersecurity awareness. The results revealed that students do not have much knowledge of Cybersecurity. Thus, a four-step approach was proposed to infuse the relevant cybersecurity topics in five matched courses based on the latest Cybersecurity curricular guidelines (CSEC2017). A sample of 42 students was selected purposively without prior knowledge of Cybersecurity and divided identically into experimental and control groups. Students in the experimental group were asked to take five consecutive courses over five semesters. In each course, groups went through a pre-test for the infused topics. Then, the experimental group taught the corresponding infused topics. A post-test was administered to both groups at the end of each course, and the t-test was conducted. The results found significant differences between marks of prior and post-tests for 11 out of 14 infused topics. These satisfactory results would encourage universities to infuse cybersecurity concepts into their curriculum",SEC,http://arxiv.org/abs/2209.10407v1
Practical Cybersecurity Ethics: Mapping CyBOK to Ethical Concerns,"Research into the ethics of cybersecurity is an established and growing topic of investigation, however the translation of this research into practice is lacking: there exists a small number of professional codes of ethics or codes of practice in cybersecurity, however these are very broad and do not offer much insight into the ethical dilemmas that can be faced while performing specific cybersecurity activities. In order to address this gap, we leverage ongoing work on the Cyber Security Body of Knowledge (CyBOK) to help elicit and document the responsibilities and ethics of the profession. Based on a literature review of the ethics of cybersecurity, we use CyBOK to frame the exploration of ethical challenges in the cybersecurity profession through a series of 15 interviews with cybersecurity experts. Our approach is qualitative and exploratory, aiming to answer the research question ""What ethical challenges, insights, and solutions arise in different areas of cybersecurity?"". Our findings indicate that there are broad ethical challenges across the whole of cybersecurity, but also that different areas of cybersecurity can face specific ethical considerations for which more detailed guidance can help professionals in those areas. In particular, our findings indicate that security decision-making is expected of all security professionals, but that this requires them to balance a complex mix of technical, objective and subjective points of view, and that resolving conflicts raises challenging ethical dilemmas. We conclude that more work is needed to explore, map, and integrate ethical considerations into cybersecurity practice; the urgent need to conduct further research into the ethics of cybersecurity AI; and highlight the importance of this work for individuals and professional bodies who seek to develop and mature the cybersecurity profession in a responsible manner.",SEC,http://arxiv.org/abs/2311.10165v1
Toward a Blockchain-based Platform to Manage Cybersecurity Certification of IoT devices,"The goal of this paper is to propose a blockchain-based platform to enhance transparency and traceability of cybersecurity certification information motivated by the recently adopted EU Cybersecurity Act. The proposed platform is generic and intended to support the trusted exchange of cybersecurity certification information for any electronic product, service, or process. However, for the purposes of this paper, we focus on the case study of the cybersecurity certification of IoT devices, which are explicitly referenced in the recently adopted Cybersecurity Act as one of the main domains where it is highlighted the need for an increased level of trust.",SEC,http://arxiv.org/abs/1909.07039v1
Assessing and Improving Cybersecurity Maturity for SMEs: Standardization aspects,"SMEs constitute a very large part of the economy in every country and they play an important role in economic growth and social development. SMEs are frequent targets of cybersecurity attacks similar to large enterprises. However, unlike large enterprises, SMEs mostly have limited capabilities regarding cybersecurity practices. Given the increasing cybersecurity risks and the large impact that the risks may bring to the SMEs, assessing and improving the cybersecurity capabilities is crucial for SMEs for sustainability. This research aims to provide an approach for SMEs for assessing and improving their cybersecurity capabilities by integrating key elements from existing industry standards.",SEC,http://arxiv.org/abs/2007.01751v1
"ICAR, a categorical framework to connect vulnerability, threat and asset managements","We present ICAR, a mathematical framework derived from category theory for representing cybersecurity NIST and MITRE's ontologies. Designed for cybersecurity, ICAR is a category whose objects are cybersecurity knowledge (weakness, vulnerability, impacted product, attack technique, etc.) and whose morphisms are relations between this knowledge, that make sense for cybersecurity. Within this rigorous and unified framework, we obtain a knowledge graph capable of identifying the attack and weakness structures of an IS, at the interface between description logics, database theory and cybersecurity. We then define ten cybersecurity queries to help understand the risks incurred by IS and organise their defence.",SEC,http://arxiv.org/abs/2306.12240v1
Collaborative Approaches to Enhancing Smart Vehicle Cybersecurity by AI-Driven Threat Detection,"The introduction sets the stage for exploring collaborative approaches to bolstering smart vehicle cybersecurity through AI-driven threat detection. As the automotive industry increasingly adopts connected and automated vehicles (CAVs), the need for robust cybersecurity measures becomes paramount. With the emergence of new vulnerabilities and security requirements, the integration of advanced technologies such as 5G networks, blockchain, and quantum computing presents promising avenues for enhancing CAV cybersecurity . Additionally, the roadmap for cybersecurity in autonomous vehicles emphasizes the importance of efficient intrusion detection systems and AI-based techniques, along with the integration of secure hardware, software stacks, and advanced threat intelligence to address cybersecurity challenges in future autonomous vehicles.",SEC,http://arxiv.org/abs/2501.00261v1
Cybersecurity in the AWS Cloud,"This paper re-examines the content of a standard advanced course in Cybersecurity from the perspective of Cloud Computing. More precisely, we review the core concepts of Cybersecurity, as presented in a senior undergraduate or graduate class, in light of the Amazon Web Services (AWS) cloud.",SEC,http://arxiv.org/abs/2003.12905v1
Ten AI Stepping Stones for Cybersecurity,"With the turmoil in cybersecurity and the mind-blowing advances in AI, it is only natural that cybersecurity practitioners consider further employing learning techniques to help secure their organizations and improve the efficiency of their security operation centers. But with great fears come great opportunities for both the good and the evil, and a myriad of bad deals. This paper discusses ten issues in cybersecurity that hopefully will make it easier for practitioners to ask detailed questions about what they want from an AI system in their cybersecurity operations. We draw on the state of the art to provide factual arguments for a discussion on well-established AI in cybersecurity issues, including the current scope of AI and its application to cybersecurity, the impact of privacy concerns on the cybersecurity data that can be collected and shared externally to the organization, how an AI decision can be explained to the person running the operations center, and the implications of the adversarial nature of cybersecurity in the learning techniques. We then discuss the use of AI by attackers on a level playing field including several issues in an AI battlefield, and an AI perspective on the old cat-and-mouse game including how the adversary may assess your AI power.",SEC,http://arxiv.org/abs/1912.06817v1
A Systems Thinking for Cybersecurity Modeling,"Solving cybersecurity issues requires a holistic understanding of components, factors, structures and their interactions in cyberspace, but conventional modeling approaches view the field of cybersecurity by their boundaries so that we are still not clear to cybersecurity and its changes. In this paper, we attempt to discuss the application of systems thinking approaches to cybersecurity modeling. This paper reviews the systems thinking approaches and provides the systems theories and methods for tackling cybersecurity challenges, regarding relevant fields, associated impact factors and their interactions. Moreover, an illustrative example of systems thinking frameworks for cybersecurity modeling is developed to help broaden the mind in methodology, theory, technology and practice. This article concludes that systems thinking can be considered as one of the powerful tools of cybersecurity modeling to find, characterize, understand, evaluate and predict cybersecurity.",SEC,http://arxiv.org/abs/2001.05734v1
Classifying SMEs for Approaching Cybersecurity Competence and Awareness,"Cybersecurity is increasingly a concern for small and medium-sized enterprises (SMEs), and there exist many awareness training programs and tools for them. The literature mainly studies SMEs as a unitary type of company and provides one-size-fits-all recommendations and solutions. However, SMEs are not homogeneous. They are diverse with different vulnerabilities, cybersecurity needs, and competencies. Few studies considered such differences in standards and certificates for security tools adoption and cybersecurity tailoring for these SMEs. This study proposes a classification framework with an outline of cybersecurity improvement needs for each class. The framework suggests five SME types based on their characteristics and specific security needs: cybersecurity abandoned SME, unskilled SME, expert-connected SME, capable SME, and cybersecurity provider SME. In addition to describing the five classes, the study explains the framework's usage in sampled SMEs. The framework proposes solutions for each class to approach cybersecurity awareness and competence more consistent with SME needs. The final publication is available at ACM Digital Library via this https URL https://doi.org/10.1145/3465481.3469200",SEC,http://arxiv.org/abs/2110.05370v1
"A Review of Quantum Cybersecurity: Threats, Risks and Opportunities","The promise of quantum computing is not speeding up conventional computing rather delivering an exponential advantage for certain classes of problems, with profound implications for cybersecurity for instance. With the advent and development of quantum computers, cyberspace security can surely become the most critical problem for the Internet in near future. On contrary, prosaic quantum technology can be promising to transform cybersecurity. This research aims to synthesize basic and fundamental studies concerning quantum cybersecurity that can be emerged both as a threat and solution to critical cybersecurity issues based on a systematic study. We provide a comprehensive, illustrative description of the current state-of-the-art quantum computing and cybersecurity and present the proposed approaches to date. Findings in quantum computing cybersecurity suggest that quantum computing can be adopted for the betterment of cybersecurity threats while it poses the most unexpected threats to cybersecurity. The focus and depth of this systematic survey not only provide quantum and cybersecurity practitioners and researchers with a consolidated body of knowledge about current trends in this area but also underpins a starting point for further research in this field.",SEC,http://arxiv.org/abs/2207.03534v1
Ontological Approach toward Cybersecurity in Cloud Computing,"Widespread deployment of the Internet enabled building of an emerging IT delivery model, i.e., cloud computing. Albeit cloud computing-based services have rapidly developed, their security aspects are still at the initial stage of development. In order to preserve cybersecurity in cloud computing, cybersecurity information that will be exchanged within it needs to be identified and discussed. For this purpose, we propose an ontological approach to cybersecurity in cloud computing. We build an ontology for cybersecurity operational information based on actual cybersecurity operations mainly focused on non-cloud computing. In order to discuss necessary cybersecurity information in cloud computing, we apply the ontology to cloud computing. Through the discussion, we identify essential changes in cloud computing such as data-asset decoupling and clarify the cybersecurity information required by the changes such as data provenance and resource dependency information.",CLD,http://arxiv.org/abs/1405.6169v1
Emergent Behavior in Cybersecurity,We argue that emergent behavior is inherent to cybersecurity.,SEC,http://arxiv.org/abs/1502.05102v1
Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk Management,"There is no standard yet for measuring and controlling the costs associated with implementing cybersecurity programs. To advance research and practice towards this end, we develop a mapping using the well-known concept of quality costs and the Framework Core within the Cybersecurity Framework produced by the National Institute of Standards and Technology (NIST) in response to the Cybersecurity Enhancement Act of 2014. This mapping can be easily adopted by organizations that are already using the NIST CSF for cybersecurity risk management to plan, manage, and continually improve cybersecurity operations. If an organization is not using the NIST CSF, this mapping may still be useful for linking elements in accounting systems that are associated with cybersecurity operations and risk management to a quality cost model.",SEC,http://arxiv.org/abs/1707.02653v1
Explaining Cybersecurity with Films and the Arts (Extended Abstract),Explaining Cybersecurity with Films and the Arts,SEC,http://arxiv.org/abs/1905.01730v1
A Value Driven Framework for Cybersecurity Innovation in Transportation & Infrastructure,"This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field. Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals. This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives. We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner. The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity. Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden.",SEC,http://arxiv.org/abs/2405.07358v1
Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education,"Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.",SEC,http://arxiv.org/abs/2411.03219v1
A Review of Topological Data Analysis for Cybersecurity,"In cybersecurity it is often the case that malicious or anomalous activity can only be detected by combining many weak indicators of compromise, any one of which may not raise suspicion when taken alone. The path that such indicators take can also be critical. This makes the problem of analysing cybersecurity data particularly well suited to Topological Data Analysis (TDA), a field that studies the high level structure of data using techniques from algebraic topology, both for exploratory analysis and as part of a machine learning workflow. By introducing TDA and reviewing the work done on its application to cybersecurity, we hope to highlight to researchers a promising new area with strong potential to improve cybersecurity data science.",SEC,http://arxiv.org/abs/2202.08037v1
Cybersecurity Entity Alignment via Masked Graph Attention Networks,"Cybersecurity vulnerability information is often recorded by multiple channels, including government vulnerability repositories, individual-maintained vulnerability-gathering platforms, or vulnerability-disclosure email lists and forums. Integrating vulnerability information from different channels enables comprehensive threat assessment and quick deployment to various security mechanisms. Efforts to automatically gather such information, however, are impeded by the limitations of today's entity alignment techniques. In our study, we annotate the first cybersecurity-domain entity alignment dataset and reveal the unique characteristics of security entities. Based on these observations, we propose the first cybersecurity entity alignment model, CEAM, which equips GNN-based entity alignment with two mechanisms: asymmetric masked aggregation and partitioned attention. Experimental results on cybersecurity-domain entity alignment datasets demonstrate that CEAM significantly outperforms state-of-the-art entity alignment methods.",SEC,http://arxiv.org/abs/2207.01434v1
Elicitation of SME Requirements for Cybersecurity Solutions by Studying Adherence to Recommendations,"Small and medium-sized enterprises (SME) have become the weak spot of our economy for cyber attacks. These companies are large in number and often do not have the controls in place to prevent successful attacks, respectively are not prepared to systematically manage their cybersecurity capabilities. One of the reasons for why many SME do not adopt cybersecurity is that developers of cybersecurity solutions understand little the SME context and the requirements for successful use of these solutions. We elicit requirements by studying how cybersecurity experts provide advice to SME. The experts recommendations offer insights into what important capabilities of the solution are and how these capabilities ought to be used for mitigating cybersecurity threats. The adoption of a recommendation hints at a correct match of the solution, hence successful consideration of requirements. Abandoned recommendations point to a misalignment that can be used as a source to inquire missed requirements. Re-occurrence of adoption or abandonment decisions corroborate the presence of requirements. This poster describes the challenges of SME regarding cybersecurity and introduces our proposed approach to elicit requirements for cybersecurity solutions. The poster describes CYSEC, our tool used to capture cybersecurity advice and help to scale cybersecurity requirements elicitation to a large number of participating SME. We conclude by outlining the planned research to develop and validate CYSEC.",SEC,http://arxiv.org/abs/2007.08177v1
An Assessment Methodology and Instrument for Cybersecurity: The Ireland Use Case,"Governments around the world are required to strengthen their national cybersecurity capabilities to respond effectively to the growing, changing, and sophisticated cyber threats and attacks, thus protecting society and the way of life as a whole. Responsible government institutions need to revise, evaluate, and bolster their national cybersecurity capabilities to fulfill the new requirements, for example regarding new trends affecting cybersecurity, key supporting laws and regulations, and implementations risk and challenges. This report presents a comprehensive assessment instrument for cybersecurity at the national level in order to help countries to ensure optimum response capability and more effective use of critical resources of each state. More precisely, the report - builds a common understanding of the critical cybersecurity capabilities and competence to be assessed at the national level, - adds value to national strategic planning and implementation which impact the development and adaptation of national cybersecurity strategies, - provides an overview of the assessment approaches at the national level, including capabilities, frameworks, and controls, - introduces a comprehensive cybersecurity instrument for countries to determine areas of improvement and develop enduring national capabilities, - describes how to apply the proposed national cybersecurity assessment framework in a real-world case, and - presents the results and lessons learned of the application of the assessment framework at the national level to assist governments in further building cybersecurity capabilities.",SEC,http://arxiv.org/abs/2302.05166v1
Towards a Systematic View on Cybersecurity Ecology,"Current network security systems are progressively showing their limitations. One credible estimate is that only about 45% of new threats are detected. Therefore it is vital to find a new direction that cybersecurity development should follow. We argue that the next generation of cybersecurity systems should seek inspiration in nature. This approach has been used before in the first generation of cybersecurity systems; however, since then cyber threats and environment have evolved significantly, and accordingly the first-generation systems have lost their effectiveness. A next generation of bio-inspired cybersecurity research is emerging, but progress is hindered by the lack of a framework for mapping biological security systems to their cyber analogies. In this paper, using terminology and concepts from biology, we describe a cybersecurity ecology and a framework that may be used to systematically research and develop bio-inspired cybersecurity.",SEC,http://arxiv.org/abs/1505.04207v2
Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,"Indicators of Compromise (IOCs) are artifacts observed on a network or in an operating system that can be utilized to indicate a computer intrusion and detect cyber-attacks in an early stage. Thus, they exert an important role in the field of cybersecurity. However, state-of-the-art IOCs detection systems rely heavily on hand-crafted features with expert knowledge of cybersecurity, and require large-scale manually annotated corpora to train an IOC classifier. In this paper, we propose using an end-to-end neural-based sequence labelling model to identify IOCs automatically from cybersecurity articles without expert knowledge of cybersecurity. By using a multi-head self-attention module and contextual features, we find that the proposed model is capable of gathering contextual information from texts of cybersecurity articles and performs better in the task of IOC identification. Experiments show that the proposed model outperforms other sequence labelling models, achieving the average F1-score of 89.0% on English cybersecurity article test set, and approximately the average F1-score of 81.8% on Chinese test set.",SEC,http://arxiv.org/abs/1907.02636v2
Cybersecurity and Sustainable Development,"Growing interdependencies between organizations lead them towards the creation of inter-organizational networks where cybersecurity and sustainable development have become one of the most important issues. The Environmental Goods and Services Sector (EGSS) is one of the fastest developing sectors of the economy fueled by the growing relationships between network entities based on ICT usage. In this sector, Green Cybersecurity is an emerging issue because it secures processes related directly and indirectly to environmental management and protection. In the future, the multidimensional development of the EGSS can help European Union to overcome the upcoming crises. At the same time, computer technologies and cybersecurity can contribute to the implementation of the concept of sustainable development. The development of environmental technologies along with their cybersecurity is one of the aims of the realization of sustainable production and domestic security concepts among the EU countries. Hence, the aim of this article is a theoretical discussion and research on the relationships between cybersecurity and sustainable development in inter-organizational networks. Therefore, the article is an attempt to give an answer to the question about the current state of the implementation of cybersecurity in relation to the EGSS part of the economy in different EU countries.",SEC,http://arxiv.org/abs/2105.13652v1
A Model-Driven Methodology for Automotive Cybersecurity Test Case Generation,"Through international regulations (most prominently the latest UNECE regulation) and standards, the already widely perceived higher need for cybersecurity in automotive systems has been recognized and will mandate higher efforts for cybersecurity engineering. T he UNECE also demands the effectiveness of these engineering to be verified and validated through testing. T his requires both a significantly higher rate and more comprehensiveness of cybersecurity testing that is not effectively to cope with using current, predominantly manual, automotive cybersecurity testing techniques. To allow for comprehensive and efficient testing at all stages of the automotive life cycle, including supply chain parts not at band, and to facilitate efficient third party testing, as well as to test under real-world conditions, also methodologies for testing the cybersecurity of vehicular systems as a black box are necessary. T his paper therefore presents a model and attack tree-based approach to (semi-)automate automotive cybersecurity testing, as well as considerations for automatically black box-deriving models for the use in attack modeling.",SEC,http://arxiv.org/abs/2107.06024v1
Understanding parents' perceptions of children's cybersecurity awareness in Norway,"Children are increasingly using the internet nowadays. While internet use exposes children to various privacy and security risks, few studies have examined how parents perceive and address their children's cybersecurity risks. To address this gap, we conducted a qualitative study with 25 parents living in Norway with children aged between 10 to 15. We conducted semi-structured interviews with the parents and performed a thematic analysis of the interview data. The results of this paper include a list of cybersecurity awareness needs for children from a parental perspective, a list of learning resources for children, and a list of challenges for parents to ensure cybersecurity at home. Our results are useful for developers and educators in developing cybersecurity solutions for children. Future research should focus on defining cybersecurity theories and practices that contribute to children's and parents' awareness about cybersecurity risks, needs, and solutions.",SEC,http://arxiv.org/abs/2108.02512v1
Multidimensional Cybersecurity Framework for Strategic Foresight,"Cybersecurity is now at the forefront of most organisational digital transformative agendas and National economic, social and political programmes. Hence its impact to society can no longer be seen to be one dimensional. The rise in National cybersecurity laws and regulations is a good indicator of its perceived importance to nations. And the recent awakening for social and ethical transparency in society and coupled with sustainability issues demonstrate the need for a paradigm shift in how cybersecurity discourses can now happen. In response to this shift, a multidimensional cybersecurity framework for strategic foresight underpinned on situational awareness is proposed. The conceptual cybersecurity framework comprising six domains such as Physical, Cultural, Economic, Social, Political and Cyber, is discussed. The guiding principles underpinning the framework are outlined, followed by in-depth reflection on the Business, Operational, Technological and Human (BOTH) factors and their implications for strategic foresight for cybersecurity.",SEC,http://arxiv.org/abs/2202.02537v1
The Future of Cybersecurity in Southeast Asia along the Maritime Silk Road,"This paper proposes an analysis of the prospects of the cyber security industry and educational ecosystems in four Southeast Asian countries, namely Vietnam, Singapore, Malaysia, and Indonesia, which are along the Maritime Silk Road, by using two novel metrics: the ""Cybersecurity Education Prospects Index"" (CEPI) and the ""Cybersecurity Industry Prospects Index"" (CIPI). The CEPI evaluates the state of cybersecurity education by assessing the availability and quality of cybersecurity degrees together with their ability to attract new students. On the other hand, the CIPI measures the potential for the cybersecurity industry's growth and development by assessing the talent pool needed to build and sustain its growth. Ultimately, this study emphasizes the vital importance of a healthy cybersecurity ecosystem where education is responsible for supporting the industry to ensure the security and reliability of commercial operations in these countries against a complex and evolving cyber threat landscape.",SEC,http://arxiv.org/abs/2308.06963v1
"Unaware, Unfunded and Uneducated: A Systematic Review of SME Cybersecurity","Small and Medium Enterprises (SMEs) are pivotal in the global economy, accounting for over 90% of businesses and 60% of employment worldwide. Despite their significance, SMEs have been disregarded from cybersecurity initiatives, rendering them ill-equipped to deal with the growing frequency, sophistication, and destructiveness of cyber-attacks. We systematically reviewed the cybersecurity literature on SMEs published between 2017 and 2023. We focus on research discussing cyber threats, adopted controls, challenges, and constraints SMEs face in pursuing cybersecurity resilience. Our search yielded 916 studies that we narrowed to 77 relevant papers. We identified 44 unique themes and categorised them as novel findings or established knowledge. This distinction revealed that research on SMEs is shallow and has made little progress in understanding SMEs' roles, threats, and needs. Studies often repeated early discoveries without replicating or offering new insights. The existing research indicates that the main challenges to attaining cybersecurity resilience of SMEs are a lack of awareness of the cybersecurity risks, limited cybersecurity literacy and constrained financial resources. However, resource availability varied between developed and developing countries. Our analysis indicated a relationship among these themes, suggesting that limited literacy is the root cause of awareness and resource constraint issues.",SEC,http://arxiv.org/abs/2309.17186v1
Data Driven Approaches to Cybersecurity Governance for Board Decision-Making -- A Systematic Review,"Cybersecurity governance influences the quality of strategic decision-making to ensure cyber risks are managed effectively. Board of Directors are the decisions-makers held accountable for managing this risk; however, they lack adequate and efficient information necessary for making such decisions. In addition to the myriad of challenges they face, they are often insufficiently versed in the technology or cybersecurity terminology or not provided with the correct tools to support them to make sound decisions to govern cybersecurity effectively. A different approach is needed to ensure BoDs are clear on the approach the business is taking to build a cyber resilient organization. This systematic literature review investigates the existing risk measurement instruments, cybersecurity metrics, and associated models for supporting BoDs. We identified seven conceptual themes through literature analysis that form the basis of this study's main contribution. The findings showed that, although sophisticated cybersecurity tools exist and are developing, there is limited information for Board of Directors to support them in terms of metrics and models to govern cybersecurity in a language they understand. The review also provides some recommendations on theories and models that can be further investigated to provide support to Board of Directors.",SEC,http://arxiv.org/abs/2311.17578v1
Designing Cybersecurity Awareness Solutions for the Young People in Rural Developing Countries: The Need for Diversity and Inclusion,"Cybersecurity challenges and the need for awareness are well-recognized in developed countries, but this still needs attention in less-developed countries. With the expansion of technology, security concerns are also becoming more prevalent worldwide. This paper presents a design and creation research study exploring which factors we should consider when designing cybersecurity awareness solutions for young people in developing countries. We have developed prototypes of mini-cybersecurity awareness applications and conducted a pilot study with eight participants (aged 16-30) from Gambia, Eritrea, and Syria. Our findings show that factors like the influence of culture and social constructs, literacy, and language competence, the way of introducing cybersecurity terms and concepts, and the need for reflection are essential to consider when designing and developing cybersecurity awareness solutions for target users in developing countries. The findings of this study will guide future researchers to design more inclusive cybersecurity awareness solutions for users in developing countries.",SEC,http://arxiv.org/abs/2312.12073v1
Toward a Quantum Information System Cybersecurity Taxonomy and Testbed: Exploiting a Unique Opportunity for Early Impact,"Any human-designed system can potentially be exploited in ways that its designers did not envision, and information systems or networks using quantum components do not escape this reality. We are presented with a unique but quickly waning opportunity to bring cybersecurity concerns to the forefront for quantum information systems before they become widely deployed. The resources and knowledge required to do so, however, may not be common in the cybersecurity community. Yet, a nexus exist. Cybersecurity starts with risk, and there are good taxonomies for security vulnerabilities and impacts in classical systems. In this paper, we propose a preliminary taxonomy for quantum cybersecurity vulnerabilities that accounts for the latest advances in quantum information systems, and must evolve to incorporate well-established cybersecurity principles and methodologies. We envision a testbed environment designed and instrumented with the specific purpose of enabling a broad collaborative community of cybersecurity and quantum information system experts to conduct experimental evaluation of software and hardware security including both physical and virtual quantum components. Furthermore, we envision that such a resource may be available as a user facility to the open science research community.",SEC,http://arxiv.org/abs/2404.12465v1
When LLMs Meet Cybersecurity: A Systematic Literature Review,"The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",SEC,http://arxiv.org/abs/2405.03644v2
Gender of Recruiter Makes a Difference: A study into Cybersecurity Graduate Recruitment,"An ever-widening workforce gap exists in the global cybersecurity industry but diverse talent is underutilized. The global cybersecurity workforce is only 25% female. Much research exists on the effect of gender bias on the hiring of women into the technical workforce, but little on how the gender of the recruiter (gender difference) affects recruitment decisions. This research reveals differences between the non-technical skills sought by female vs non-female cybersecurity recruiters. The former look for recruits with people-focused skills while the latter look for task-focused skills, highlighting the need for gender diversity in recruitment panels. Recruiters are increasingly seeking non-technical (soft) skills in technical graduate recruits. This requires STEM curriculum in Universities to adapt to match. Designing an industry-ready cybersecurity curriculum requires knowledge of these non-technical skills. An online survey of cybersecurity professionals was used to determine the most sought after non-technical skills in the field. Analysis of the data reveals distinct gender differences in the non-technical skills most valued in a recruit, based on the gender of the recruiter (not the recruited). The gender differences discovered do not correspond to the higher proportion of women employed in non-technical cybersecurity roles.",SEC,http://arxiv.org/abs/2408.05895v1
SoK: Identifying Limitations and Bridging Gaps of Cybersecurity Capability Maturity Models (CCMMs),"In the rapidly evolving digital landscape, where organisations are increasingly vulnerable to cybersecurity threats, Cybersecurity Capability Maturity Models (CCMMs) emerge as pivotal tools in enhancing organisational cybersecurity posture. CCMMs provide a structured framework to guide organisations in assessing their current cybersecurity capabilities, identifying critical gaps, and prioritising improvements. However, the full potential of CCMMs is often not realised due to inherent limitations within the models and challenges encountered during their implementation and adoption processes. These limitations and challenges can significantly hamper the efficacy of CCMMs in improving cybersecurity. As a result, organisations remain vulnerable to cyber threats as they may fail to identify and address critical security gaps, implement necessary improvements or allocate resources effectively. To address these limitations and challenges, conducting a thorough investigation into existing models is essential. Therefore, we conducted a Systematic Literature Review (SLR) analysing 43 publications to identify existing CCMMs, their limitations, and the challenges organisations face when implementing and adopting them. By understanding these barriers, we aim to explore avenues for enhancing the efficacy of CCMMs, ensuring they more effectively meet the cybersecurity needs of organisational entities.",SEC,http://arxiv.org/abs/2408.16140v1
A Survey-Based Quantitative Analysis of Stress Factors and Their Impacts Among Cybersecurity Professionals,"This study investigates the prevalence and underlying causes of work-related stress and burnout among cybersecurity professionals using a quantitative survey approach guided by the Job Demands-Resources model. Analysis of responses from 50 cybersecurity practitioners reveals an alarming reality: 44% report experiencing severe work-related stress and burnout, while an additional 28% are uncertain about their condition. The demanding nature of cybersecurity roles, unrealistic expectations, and unsupportive organizational cultures emerge as primary factors fueling this crisis. Notably, 66% of respondents perceive cybersecurity jobs as more stressful than other IT positions, with 84% facing additional challenges due to the pandemic and recent high-profile breaches. The study finds that most cybersecurity experts are reluctant to report their struggles to management, perpetuating a cycle of silence and neglect. To address this critical issue, the paper recommends that organizations foster supportive work environments, implement mindfulness programs, and address systemic challenges. By prioritizing the mental health of cybersecurity professionals, organizations can cultivate a more resilient and effective workforce to protect against an ever-evolving threat landscape.",SEC,http://arxiv.org/abs/2409.12047v1
Cybersecurity Study Programs: What's in a Name?,"Improving cybersecurity education has become a priority for many countries and organizations worldwide. Computing societies and professional associations have recognized cybersecurity as a distinctive computing discipline and created specialized cybersecurity curricular guidelines. Higher education institutions are introducing new cybersecurity programs, attracting students to this expanding field. In this paper, we examined 101 study programs across 24 countries. Based on their analysis, we argue that top-ranked universities have not yet fully implemented the guidelines and offer programs that have ""cyber"" in their name but lack some essential elements of a cybersecurity program. In particular, most programs do not sufficiently cover non-technical components, such as law, policies, or risk management. Also, most programs teach knowledge and skills but do not expose students to experiential learning outside the traditional classroom (such as internships) to develop their competencies. As a result, graduates of these programs may not meet employer expectations and may require additional training. To help program directors and educators improve their programs and courses, this paper offers examples of effective practices from cybersecurity programs around the world and our teaching practice.",SEC,http://arxiv.org/abs/2411.09240v1
"Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice","Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",SEC,http://arxiv.org/abs/2503.00070v1
Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report,"As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",SEC,http://arxiv.org/abs/2504.21039v1
SecureBERT: A Domain-Specific Language Model for Cybersecurity,"Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures. This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.",SEC,http://arxiv.org/abs/2204.02685v3
"Systemization of Knowledge (SoK)- Cross Impact of Transfer Learning in Cybersecurity: Offensive, Defensive and Threat Intelligence Perspectives","Recent literature highlights a significant cross-impact between transfer learning and cybersecurity. Many studies have been conducted on using transfer learning to enhance security, leading to various applications in different cybersecurity tasks. However, previous research is focused on specific areas of cybersecurity. This paper presents a comprehensive survey of transfer learning applications in cybersecurity by covering a wide range of domains, identifying current trends, and shedding light on under-explored areas. The survey highlights the significance of transfer learning in addressing critical issues in cybersecurity, such as improving detection accuracy, reducing training time, handling data imbalance, and enhancing privacy preservation. Additional insights are provided on the common problems solved using transfer learning, such as the lack of labeled data, different data distributions, and privacy concerns. The paper identifies future research directions and challenges that require community attention, including the need for privacy-preserving models, automatic tools for knowledge transfer, metrics for measuring domain relatedness, and enhanced privacy preservation mechanisms. The insights and roadmap presented in this paper will guide researchers in further advancing transfer learning in cybersecurity, fostering the development of robust and efficient cybersecurity systems to counter emerging threats and protect sensitive information. To the best of our knowledge, this paper is the first of its kind to present a comprehensive taxonomy of all areas of cybersecurity that benefited from transfer learning and propose a detailed future roadmap to shape the possible research direction in this area.",SEC,http://arxiv.org/abs/2309.05889v1
A Transdisciplinary Approach to Cybersecurity: A Framework for Encouraging Transdisciplinary Thinking,"Classical cybersecurity is often perceived as a rigid science discipline filled with computer scientists and mathematicians. However, due to the rapid pace of technology development and integration, new criminal enterprises, new defense tactics, and the understanding of the human element, cybersecurity is quickly beginning to encompass more than just computers. Cybersecurity experts must broaden their perspectives beyond traditional disciplinary boundaries to provide the best protection possible. They must start to practice transdisciplinary cybersecurity. Taking influence from the Stakeholder Theory in business ethics, this paper presents a framework to encourage transdisciplinary thinking and assist experts in tackling the new challenges of the modern day. The framework uses the simple Think, Plan, Do approach to enable experts to develop their transdisciplinary thinking. The framework is intended to be used as an evaluation tool for existing cybersecurity practices or postures, as a development tool to engage with other disciplines to foster learning and create new methods, and as a guidance tool to encourage new ways of thinking about, perceiving, and executing cybersecurity practices. For each of those intended uses, a use case is presented as an example to showcase how the framework might be used. The ultimate goal of this paper is not the framework but transdisciplinary thinking. By using the tool presented here and developing their own transdisciplinary thinking, cybersecurity experts can be better prepared to face cybersecurity's unique and complex challenges.",SEC,http://arxiv.org/abs/2405.10373v1
Navigating the road to automotive cybersecurity compliance,"The automotive industry has evolved significantly since the introduction of the Ford Model T in 1908. Today's vehicles are not merely mechanical constructs; they are integral components of a complex digital ecosystem, equipped with advanced connectivity features powered by Artificial Intelligence and cloud computing technologies. This evolution has enhanced vehicle safety, efficiency, and the overall driving experience. However, it also introduces new challenges, notably in cybersecurity. With the increasing integration of digital technologies, vehicles have become more susceptible to cyber-attacks, prompting significant cybersecurity concerns. These concerns include securing sensitive data, protecting vehicles from unauthorized access, and ensuring user privacy. In response, the automotive industry is compelled to adopt robust cybersecurity measures to safeguard both vehicles and data against potential threats. Legislative frameworks such as UNR155 and UNR156 by the United Nations, along with other international regulations, aim to establish stringent cybersecurity mandates. These regulations require compliance with comprehensive cybersecurity management systems and necessitate regular updates and testing to cope with the evolving nature of cyber threats. The introduction of such regulations highlights the growing recognition of cybersecurity as a critical component of automotive safety and functionality. The future of automotive cybersecurity lies in the continuous development of advanced protective measures and collaborative efforts among all stakeholders, including manufacturers, policymakers, and cybersecurity professionals. Only through such concerted efforts can the industry hope to address the dual goals of innovation in vehicle functionality and stringent security measures against the backdrop of an increasingly interconnected digital landscape.",SEC,http://arxiv.org/abs/2407.00483v1
Assessing the Maturity of Cybersecurity Education in Virginia and the Impact of State Level Investment,"With a global shortage of cybersecurity students with the education and experience necessary to fill more than 3 million jobs, cybersecurity education is an international problem. Significant research within this field has explored this problem in depth, identifying a variety of shortcomings in the cybersecurity educational pipeline including lack of certifications, security clearances, and appropriate educational opportunities within institutions of higher education. Additional research has built on this, exploring specific gaps within what cybersecurity opportunities are provided within institutions of higher education. We build an ordinal scale for assessing this, the cybersecurity education maturity model scale (CEMMs), and provide evidence of reliability and validity. We then calculate the CEMMs score for all public four-year universities in the state of Virginia between 2017 and 2025, with 2017 marking a year in which the state started the Commonwealth Cyber Initiative (CCI). We find that the scale proposed provides a consistent and reliable way to compare the cybersecurity offerings available between universities. When comparing year to year average CEMMs score, we find that public four year universities in Virginia are increasing their program offerings in the area of cybersecurity, with potential to make an impact on the cybersecurity jobs gap.",SEC,http://arxiv.org/abs/2502.18456v1
Cybersecurity Awareness,"Cybersecurity awareness can be viewed as the level of appreciation, understanding or knowledge of cybersecurity or information security aspects. Such aspects include cognizance of cyber risks and threats, but also appropriate protection measures.",SEC,http://arxiv.org/abs/2103.00474v1
A systematic literature review on Ransomware attacks,"In the area of information technology, cybersecurity is critical. Information security is one of todays highest priorities. Cyber attacks, which are on the rise and include Ransomware, are the first thing that springs to mind when we think about cybersecurity. To counteract cybercrime, several governments and companies employ a range of strategies. Despite several cybersecurity measures, ransomware continues to terrify people.",SEC,http://arxiv.org/abs/2212.04063v1
The NIST Definition of Cloud Computing,"Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model promotes availability and is composed of five essential characteristics, three service models, and four deployment models.",CLD,https://www.semanticscholar.org/paper/487b787e6ca2368aff7941c86e39941db83c5087
Cloud computing: state-of-the-art and research challenges,"Cloud computing has recently emerged as a new paradigm for hosting and delivering services over the Internet. Cloud computing is attractive to business owners as it eliminates the requirement for users to plan ahead for provisioning, and allows enterprises to start from the small and increase resources only when there is a rise in service demand. However, despite the fact that cloud computing offers huge opportunities to the IT industry, the development of cloud computing technology is currently at its infancy, with many issues still to be addressed. In this paper, we present a survey of cloud computing, highlighting its key concepts, architectural principles, state-of-the-art implementation as well as research challenges. The aim of this paper is to provide a better understanding of the design challenges of cloud computing and identify important research directions in this increasingly important area.",CLD,https://www.semanticscholar.org/paper/7ef08f1fa127af817cdfd9d3bd00bdf60e32143b
Energy efficiency in cloud computing data centers: a survey on software technologies,"Cloud computing is a commercial and economic paradigm that has gained traction since 2006 and is presently the most significant technology in IT sector. From the notion of cloud computing to its energy efficiency, cloud has been the subject of much discussion. The energy consumption of data centres alone will rise from 200 TWh in 2016 to 2967 TWh in 2030. The data centres require a lot of power to provide services, which increases CO2 emissions. In this survey paper, software-based technologies that can be used for building green data centers and include power management at individual software level has been discussed. The paper discusses the energy efficiency in containers and problem-solving approaches used for reducing power consumption in data centers. Further, the paper also gives details about the impact of data centers on environment that includes the e-waste and the various standards opted by different countries for giving rating to the data centers. This article goes beyond just demonstrating new green cloud computing possibilities. Instead, it focuses the attention and resources of academia and society on a critical issue: long-term technological advancement. The article covers the new technologies that can be applied at the individual software level that includes techniques applied at virtualization level, operating system level and application level. It clearly defines different measures at each level to reduce the energy consumption that clearly adds value to the current environmental problem of pollution reduction. This article also addresses the difficulties, concerns, and needs that cloud data centres and cloud organisations must grasp, as well as some of the factors and case studies that influence green cloud usage.",CLD,https://www.semanticscholar.org/paper/1e58d26463223d6f4bfb12a23ba5f040013bae1f
Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing,"Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases.",CLD,https://www.semanticscholar.org/paper/0d88252e3a8777618d680fbb7fe64f8c1bdd1483
"IoT and Cloud Computing Issues, Challenges and Opportunities: A Review","With the exponential growth of the Industrial Internet of Things (IIoT), multiple outlets are constantly producing a vast volume of data. It is unwise to locally store all the raw data in the IIoT devices since the energy and storage spaces of the end devices are strictly constrained. self-organization and short-range Internet of Things (IoT) networking also support outsourced data and cloud computing, independent of the distinctive resource constraint properties. For the remainder of the findings, there is a sequence of unfamiliar safeguards for IoT and cloud integration problems. The delivery of cloud computing is highly efficient, storage is becoming more and more current, and some groups are now altering their data from in house records Cloud Computing Vendors' hubs. Intensive IoT applications for workloads and data are subject to challenges while utilizing cloud computing tools. In this report, we research IoT and cloud computing and address cloud-compatible problems and computing techniques to promote the stable transition of IoT programs to the cloud.",CLD,https://www.semanticscholar.org/paper/20f846fc514c4b2ef83a2e0764dce29f4ea8a925
"Edge Computing and Sensor-Cloud: Overview, Solutions, and Directions","Sensor-cloud originates from extensive recent applications of wireless sensor networks and cloud computing. To draw a roadmap of the current research activities of the sensor-cloud community, we first investigate the state-of-the-art sensor-cloud literature reviews published since the late 2010s and discovered that these surveys have primarily studied the sensor-cloud in specific aspects, security-enabled solutions, efficient management mechanisms, and architectural challenges. While the existing surveys have reviewed the sensor-cloud from various perspectives, they are inadequate for the three key issues that require urgent attention in the sensor-cloud: reliability, energy, and heterogeneity. To fill this gap, we perform a thorough survey by examining the origins of the sensor-cloud and providing an in-depth and comprehensive discussion of these three key challenges. We summarize initial designs of the new edge-based schemes to address these challenges and identify several open issues and promising future research directions.",CLD,https://www.semanticscholar.org/paper/03e945552e6e45bac5bc0b4a35edd834d04a9924
CLOUD COMPUTING,"This journal review provides an overview of the current state of cloud computing, including its definition, benefits, and challenges. It examines the various cloud computing models and architectures, and discusses the security and privacy issues associated with the cloud. It also looks at the potential of cloud computing to transform the IT industry and provide new opportunities for businesses. Finally, the review looks at the future of cloud computing, including potential use cases and applications Cloud computing is a paradigm that has revolutionized the way we store, process and access data and applications. At its core, it involves delivering computing resources such as servers, storage, and applications over the internet, allowing users to access them on-demand from anywhere and at any time. This technology has numerous benefits, including cost-effectiveness, scalability, flexibility, and ease of maintenance. It has become a crucial enabler for many businesses and organizations, offering them the ability to streamline their operations and remain competitive in a constantly changing market. In this abstract, we explore the fundamentals of cloud computing, its key features, and its potential impact on the future of technology.. KEYWORDS Cloud computing, technology, advantages, disadvantages, security, challenges, opportunities.",CLD,https://www.semanticscholar.org/paper/63384bda9e7e530f754574a9e6b9c7d218019bfc
"A survey of mobile cloud computing: architecture, applications, and approaches","Together with an explosive growth of the mobile applications and emerging of cloud computing concept, mobile cloud computing (MCC) has been introduced to be a potential technology for mobile services. MCC integrates the cloud computing into the mobile environment and overcomes obstacles related to the performance (e.g., battery life, storage, and bandwidth), environment (e.g., heterogeneity, scalability, and availability), and security (e.g., reliability and privacy) discussed in mobile computing. This paper gives a survey of MCC, which helps general readers have an overview of the MCC including the definition, architecture, and applications. The issues, existing solutions, and approaches are presented. In addition, the future research directions of MCC are discussed. Copyright © 2011 John Wiley & Sons, Ltd.",CLD,https://www.semanticscholar.org/paper/6da3d71dc601fd9cd6b4e84bc947de5474c5873b
A Systematic Literature Review on Cloud Computing Security: Threats and Mitigation Strategies,"Cloud computing has become a widely exploited research area in academia and industry. Cloud computing benefits both cloud services providers (CSPs) and consumers. The security challenges associated with cloud computing have been widely studied in the literature. This systematic literature review (SLR) is aimed to review the existing research studies on cloud computing security, threats, and challenges. This SLR examined the research studies published between 2010 and 2020 within the popular digital libraries. We selected 80 papers after a meticulous screening of published works to answer the proposed research questions. The outcomes of this SLR reported seven major security threats to cloud computing services. The results showed that data tampering and leakage were among the highly discussed topics in the chosen literature. Other identified security risks were associated with the data intrusion and data storage in the cloud computing environment. This SLR’s results also indicated that consumers’ data outsourcing remains a challenge for both CSPs and cloud users. Our survey paper identified the blockchain as a partnering technology to alleviate security concerns. The SLR findings reveal some suggestions to be carried out in future works to bring data confidentiality, data integrity, and availability.",CLD,https://www.semanticscholar.org/paper/dea1f5e230ac344950d6fd53b46c5fdc73b12a95
Resource Allocation in 5G IoV Architecture Based on SDN and Fog-Cloud Computing,"In the traditional cloud-based Internet of Vehicles (IoV) architecture, it is difficult to guarantee the low latency requirements of the current intelligent transportation system (ITS). As a supplement to cloud computing, fog computing can effectively alleviate the bottlenecks of cloud computing bandwidth and computing resources and improve the quality of service (QoS) of the IoV. However, as a distributed system that operates near users, fog computing has a complicated network structure. In the complex and dynamic IoV environment, to effectively manage these computing resources with different attributes and provide high-quality services, it is necessary to design an efficient architecture and a resource allocation algorithm. Therefore, on the basis of fog-cloud computing and software-defined networking (SDN), a novel 5G IoV architecture is designed. In addition, after fully considering the service requirements of the IoV, a model of four objectives is constructed, and a many-objective optimization algorithm is proposed. The experiment results show that the proposed algorithm outperforms the other state-of-the-art algorithms.",CLD,https://www.semanticscholar.org/paper/7c99f048618a7e3a6e31dfb62e5e913ba62550d6
Cyber Security in IoT-Based Cloud Computing: A Comprehensive Survey,"Cloud computing provides the flexible architecture where data and resources are dispersed at various locations and are accessible from various industrial environments. Cloud computing has changed the using, storing, and sharing of resources such as data, services, and applications for industrial applications. During the last decade, industries have rapidly switched to cloud computing for having more comprehensive access, reduced cost, and increased performance. In addition, significant improvement has been observed in the internet of things (IoT) with the integration of cloud computing. However, this rapid transition into the cloud raised various security issues and concerns. Traditional security solutions are not directly applicable and sometimes ineffective for cloud-based systems. Cloud platforms’ challenges and security concerns have been addressed during the last three years, despite the successive use and proliferation of multifaceted cyber weapons. The rapid evolution of deep learning (DL) in the artificial intelligence (AI) domain has brought many benefits that can be utilized to address industrial security issues in the cloud. The findings of the proposed research include the following: we present a comprehensive survey of enabling cloud-based IoT architecture, services, configurations, and security models; the classification of cloud security concerns in IoT into four major categories (data, network and service, applications, and people-related security issues), which are discussed in detail; we identify and inspect the latest advancements in cloud-based IoT attacks; we identify, discuss, and analyze significant security issues in each category and present the limitations from a general, artificial intelligence and deep learning perspective; we provide the technological challenges identified in the literature and then identify significant research gaps in the IoT-based cloud infrastructure to highlight future research directions to blend cybersecurity in cloud.",CLD,https://www.semanticscholar.org/paper/507eaac51213225b741bd1f1a507991b316a1a1f
Blockchain Meets Cloud Computing: A Survey,"Blockchain technology has been deemed to be an ideal choice for strengthening existing computing systems in varied manners. As one of the network-enabled technologies, cloud computing has been broadly adopted in the industry through numerous cloud service models. Fusing blockchain technology with existing cloud systems has a great potential in both functionality/performance enhancement and security/privacy improvement. The question remains on how blockchain technology inserts into current deployed cloud solutions and enables the reengineering of cloud datacenter. This survey addresses this issue and investigates recent efforts in the technical fusion of blockchain and clouds. Three technical dimensions roughly are covered in this work. First, we concern the service model and review an emerging cloud-relevant blockchain service model, Blockchain-as-a-Service (BaaS); second, security is considered a key technical dimension in this work and both access control and searchable encryption schemes are assessed; finally, we examine the performance of cloud datacenter with supports/participance of blockchain from hardware and software perspectives. Main findings of this survey will be theoretical supports for future reference of blockchain-enabled reengineering of cloud datacenter.",CLD,https://www.semanticscholar.org/paper/1870ebd563fcc8cd2b4b0c66ceb17c80a852dbcc
Blockchain-based Database to Ensure Data Integrity in Cloud Computing Environments,"Data is nowadays an invaluable resource; indeed, it guides all business decisions in most of the computer-aided human activities. Threats to data integrity are thus of paramount relevance, as tampering with data may maliciously affect crucial business decisions. This issue is especially true in cloud computing environments, where data owners cannot control fundamental data aspects, like the physical storage of data and the control of its accesses. Blockchain has recently emerged as a fascinating technology which, among others, provides compelling properties about data integrity. Using the Blockchain to face data integrity threats seems to be a natural choice, but its current limitations of low throughput, high latency, and weak stability hinder the practical feasibility of any Blockchain-based solutions. In this paper, by focusing on a case study from the European SUNFISH project, which concerns the design of a secure by-design cloud federation platform for the public sector, we precisely delineate the actual data integrity needs of cloud computing environments and the research questions to be tackled to adopt Blockchain-based databases. First, we detail the open research questions and the difficulties inherent in addressing them. Then, we outline a preliminary design of an effective Blockchain-based database for cloud computing environments.",CLD,https://www.semanticscholar.org/paper/dc03d098ec890f3b5df438a37e33a147ecbf790d
Collaborate Edge and Cloud Computing With Distributed Deep Learning for Smart City Internet of Things,"City Internet-of-Things (IoT) applications are becoming increasingly complicated and thus require large amounts of computational resources and strict latency requirements. Mobile cloud computing (MCC) is an effective way to alleviate the limitation of computation capacity by offloading complex tasks from mobile devices (MDs) to central clouds. Besides, mobile-edge computing (MEC) is a promising technology to reduce latency during data transmission and save energy by providing services in a timely manner. However, it is still difficult to solve the task offloading challenges in heterogeneous cloud computing environments, where edge clouds and central clouds work collaboratively to satisfy the requirements of city IoT applications. In this article, we consider the heterogeneity of edge and central cloud servers in the offloading destination selection. To jointly optimize the system utility and the bandwidth allocation for each MD, we establish a hybrid offloading model, including the collaboration of MCC and MEC. A distributed deep learning-driven task offloading (DDTO) algorithm is proposed to generate near-optimal offloading decisions over the MDs, edge cloud server, and central cloud server. Experimental results demonstrate the accuracy of the DDTO algorithm, which can effectively and efficiently generate near-optimal offloading decisions in the edge and cloud computing environments. Furthermore, it achieves high performance and greatly reduces the computational complexity when compared with other offloading schemes that neglect the collaboration of heterogeneous clouds. More precisely, the DDTO scheme can improve computational performance by 63%, compared with the local-only scheme.",CLD,https://www.semanticscholar.org/paper/4ad9b582450d564be2a69f2f812b5c44306846ae
"Blockchain-based trust management in cloud computing systems: a taxonomy, review and future directions","Through virtualization and resource integration, cloud computing has expanded its service area and offers a better user experience than the traditional platforms, along with its business operation model bringing huge economic and social benefits. However, a large amount of evidence shows that cloud computing is facing with serious security and trust crisis, and building a trust-enabled transaction environment has become its key factor. The traditional cloud trust model usually adopts a centralized architecture, which causes large management overhead, network congestion and even single point of failure. Furthermore, due to a lack of transparency and traceability, trust evaluation results cannot be fully recognized by all participants. Blockchain is a new and promising decentralized framework and distributed computing paradigm. Its unique features in operating rules and traceability of records ensure the integrity, undeniability and security of the transaction data. Therefore, blockchain is very suitable for constructing a distributed and decentralized trust architecture. This paper carries out a comprehensive survey on blockchain-based trust approaches in cloud computing systems. Based on a novel cloud-edge trust management framework and a double-blockchain structure based cloud transaction model, it identifies the open challenges and gives directions for future research in this field.",CLD,https://www.semanticscholar.org/paper/9cac2e2820955f745a53b800a5503a2cab086275
A Survey on Internet of Things and Cloud Computing for Healthcare,"The fast development of the Internet of Things (IoT) technology in recent years has supported connections of numerous smart things along with sensors and established seamless data exchange between them, so it leads to a stringy requirement for data analysis and data storage platform such as cloud computing and fog computing. Healthcare is one of the application domains in IoT that draws enormous interest from industry, the research community, and the public sector. The development of IoT and cloud computing is improving patient safety, staff satisfaction, and operational efficiency in the medical industry. This survey is conducted to analyze the latest IoT components, applications, and market trends of IoT in healthcare, as well as study current development in IoT and cloud computing-based healthcare applications since 2015. We also consider how promising technologies such as cloud computing, ambient assisted living, big data, and wearables are being applied in the healthcare industry and discover various IoT, e-health regulations and policies worldwide to determine how they assist the sustainable development of IoT and cloud computing in the healthcare industry. Moreover, an in-depth review of IoT privacy and security issues, including potential threats, attack types, and security setups from a healthcare viewpoint is conducted. Finally, this paper analyzes previous well-known security models to deal with security risks and provides trends, highlighted opportunities, and challenges for the IoT-based healthcare future development.",IOTNET,https://www.semanticscholar.org/paper/49403326211c20b3c1ac7f6b6ac2471a11af8f07
A Study of Moving from Cloud Computing to Fog Computing,"The exponential growth of the Internet of Things (IoT) technology poses various challenges to the classic centralized cloud computing paradigm, including high latency, limited capacity, and network failure. Cloud computing and Fog computing carry the cloud closer to IoT computers in order to overcome these problems. Cloud and Fog provide IoT processing and storage of IoT items locally instead of sending them to the cloud. Cloud and Fog provide quicker reactions and better efficiency in conjunction with the cloud. Cloud and fog computing should also be viewed as the safest approach to ensure that IoT delivers reliable and stable resources to multiple IoT customers. This article discusses the latest in cloud and Fog computing and their convergence with IoT by stressing deployment's advantages and complexities. It also concentrates on cloud and Fog design and new IoT technologies, enhanced by utilizing the cloud and Fog model. Finally, transparent topics are addressed, along with potential testing recommendations for cloud storage and Fog computing, and IoT.",CLD,https://www.semanticscholar.org/paper/10d455ccda2e7f7211424ab61ca122793ab0073c
Cloud Computing and Its Role in the Information Technology,"The concept of Cloud Computing has been distinguished as one of the major computing models in recent years. Cloud computing has become a great innovation that has important consequences not just for services on the internet but also for the entire Information technology (IT) market. Its emergence aims to optimize on-demand technology, hardware and information provisioning as a service, reaching the economy of scale in the distribution and operation of IT strategies. A great deal of cloud computing research has been concerned over some obstacles and challenges that rely upon behind the lure of cloud computing. Security has been always raised as one of the most critical issues of cloud computing where resolving such an issue would result in constant growth in the use and popularity of the cloud. Security requirements represent a major issue that has to be met in order of easing some of these obstacles. This article presents the role of cloud computing in the IT sectors.",CLD,https://www.semanticscholar.org/paper/948d5832def9dc8e1efa691abab61d08dafd1756
Efficient Computing Resource Sharing for Mobile Edge-Cloud Computing Networks,"Both the edge and the cloud can provide computing services for mobile devices to enhance their performance. The edge can reduce the conveying delay by providing local computing services while the cloud can support enormous computing requirements. Their cooperation can improve the utilization of computing resources and ensure the QoS, and thus is critical to edge-cloud computing business models. This paper proposes an efficient framework for mobile edge-cloud computing networks, which enables the edge and the cloud to share their computing resources in the form of wholesale and buyback. To optimize the computing resource sharing process, we formulate the computing resource management problems for the edge servers to manage their wholesale and buyback scheme and the cloud to determine the wholesale price and its local computing resources. Then, we solve these problems from two perspectives: i) social welfare maximization and ii) profit maximization for the edge and the cloud. For i), we have proved the concavity of the social welfare and proposed an optimal cloud computing resource management to maximize the social welfare. For ii), since it is difficult to directly prove the convexity of the primal problem, we first proved the concavity of the wholesaled computing resources with respect to the wholesale price and designed an optimal pricing and cloud computing resource management to maximize their profits. Numerical evaluations show that the total profit can be maximized by social welfare maximization while the respective profits can be maximized by the optimal pricing and cloud computing resource management.",CLD,https://www.semanticscholar.org/paper/e2f2c10591eb41c1ca9d0abae507be0d02a3ef73
What Is Cloud Computing?,"Cloud computing is a distributed environment for multiple organizations to use remotely and get high scalability, reliability on anytime, anywhere, and pay-as-you-go concepts. An organization has to create data centres to store, manage, and process the information to achieve benefits from data and make decisions. Cloud gives organizations a successful approach that leads to profit without maintaining the cost of data centres and technical staff to manage the services. Cloud has different types of architectures, types of clouds, and cost packages for using the cloud. These services can be scaled up or down when required by an organization. Cloud has unbeatable future because IT world is acquiring it and giving a boost to their businesses. Many cloud providers are using it and the remaining are moving to cloud. Cloud computing also gives birth to edge computing, fog computing, and many more zero downtime solutions.",CLD,https://www.semanticscholar.org/paper/805f2ab1c5c6035744d647744af58a1359df12c1
Cost-Aware Multimedia Data Allocation for Heterogeneous Memory Using Genetic Algorithm in Cloud Computing,"Recent expansions of Internet-of-Things (IoT) applying cloud computing have been growing at a phenomenal rate. As one of the developments, heterogeneous cloud computing has enabled a variety of cloud-based infrastructure solutions, such as multimedia big data. Numerous prior researches have explored the optimizations of on-premise heterogeneous memories. However, the heterogeneous cloud memories are facing constraints due to the performance limitations and cost concerns caused by the hardware distributions and manipulative mechanisms. Assigning data tasks to distributed memories with various capacities is a combinatorial NP-hard problem. This paper focuses on this issue and proposes a novel approach, Cost-Aware Heterogeneous Cloud Memory Model (CAHCM), aiming to provision a high performance cloud-based heterogeneous memory service offerings. The main algorithm supporting CAHCM is Dynamic Data Allocation Advance (2DA) Algorithm that uses genetic programming to determine the data allocations on the cloud-based memories. In our proposed approach, we consider a set of crucial factors impacting the performance of the cloud memories, such as communication costs, data move operating costs, energy performance, and time constraints. Finally, we implement experimental evaluations to examine our proposed model. The experimental results have shown that our approach is adoptable and feasible for being a cost-aware cloud-based solution.",CLD,https://www.semanticscholar.org/paper/8e999d58b26d608ffbf8cd09e329084f8f12fa85
Dynamic Computation Offloading for Mobile Cloud Computing: A Stochastic Game-Theoretic Approach,"Driven by the growing popularity of mobile applications, mobile cloud computing has been envisioned as a promising approach to enhance computation capability of mobile devices and reduce the energy consumptions. In this paper, we investigate the problem of multi-user computation offloading for mobile cloud computing under dynamic environment, wherein mobile users become active or inactive dynamically, and the wireless channels for mobile users to offload computation vary randomly. As mobile users are self-interested and selfish in offloading computation tasks to the mobile cloud, we formulate the mobile users’ offloading decision process under dynamic environment as a stochastic game. We prove that the formulated stochastic game is equivalent to a weighted potential game which has at least one Nash Equilibrium (NE). We quantify the efficiency of the NE, and further propose a multi-agent stochastic learning algorithm to reach the NE with a guaranteed convergence rate (which is also analytically derived). Finally, we conduct simulations to validate the effectiveness of the proposed algorithm and evaluate its performance under dynamic environment.",CLD,https://www.semanticscholar.org/paper/b86bf3a69dc7a56b4b7e58a1e431eb747f0de344
Energy-Efficient Dynamic Computation Offloading and Cooperative Task Scheduling in Mobile Cloud Computing,"Mobile cloud computing (MCC) as an emerging and prospective computing paradigm, can significantly enhance computation capability and save energy for smart mobile devices (SMDs) by offloading computation-intensive tasks from resource-constrained SMDs onto resource-rich cloud. However, how to achieve energy-efficient computation offloading under hard constraint for application completion time remains a challenge. To address such a challenge, in this paper, we provide an energy-efficient dynamic offloading and resource scheduling (eDors) policy to reduce energy consumption and shorten application completion time. We first formulate the eDors problem into an energy-efficiency cost (EEC) minimization problem while satisfying task-dependency requirement and completion time deadline constraint. We then propose a distributed eDors algorithm consisting of three subalgorithms of computation offloading selection, clock frequency control, and transmission power allocation. Next, we show that computation offloading selection depends on not only the computing workload of a task, but also the maximum completion time of its immediate predecessors and the clock frequency and transmission power of the mobile device. Finally, we provide experimental results in a real testbed and demonstrate that the eDors algorithm can effectively reduce EEC by optimally adjusting CPU clock frequency of SMDs in local computing, and adapting the transmission power for wireless channel conditions in cloud computing.",CLD,https://www.semanticscholar.org/paper/d9a4301be953abb3e706f6933c6c289f9dca1230
"A Smart Manufacturing Service System Based on Edge Computing, Fog Computing, and Cloud Computing","The state-of-the-art technologies in new generation information technologies (New IT) greatly stimulate the development of smart manufacturing. In a smart manufacturing environment, more and more devices would be connected to the Internet so that a large volume of data can be obtained during all phases of the product lifecycle. Cloud-based smart manufacturing paradigm facilitates a new variety of applications and services to analyze a large volume of data and enable large-scale manufacturing collaboration. However, different factors, such as the network unavailability, overfull bandwidth, and latency time, restrict its availability for high-speed and low-latency real-time applications. Fog computing and edge computing extended the compute, storage, and networking capabilities of the cloud to the edge, which will respond to the above-mentioned issues. Based on cloud computing, fog computing, and edge computing, in this paper, a hierarchy reference architecture is introduced for smart manufacturing. The architecture is expected to be applied in the digital twin shop floor, which opens a bright perspective of new applications within the field of manufacturing.",CLD,https://www.semanticscholar.org/paper/44c7532f7115a1ce1c836eb72a831533dbc3ac0a
A Survey of Communication Protocols for Internet of Things and Related Challenges of Fog and Cloud Computing Integration,"The fast increment in the number of IoT (Internet of Things) devices is accelerating the research on new solutions to make cloud services scalable. In this context, the novel concept of fog computing as well as the combined fog-to-cloud computing paradigm is becoming essential to decentralize the cloud, while bringing the services closer to the end-system. This article surveys e application layer communication protocols to fulfill the IoT communication requirements, and their potential for implementation in fog- and cloud-based IoT systems. To this end, the article first briefly presents potential protocol candidates, including request-reply and publish-subscribe protocols. After that, the article surveys these protocols based on their main characteristics, as well as the main performance issues, including latency, energy consumption, and network throughput. These findings are thereafter used to place the protocols in each segment of the system (IoT, fog, cloud), and thus opens up the discussion on their choice, interoperability, and wider system integration. The survey is expected to be useful to system architects and protocol designers when choosing the communication protocols in an integrated IoT-to-fog-to-cloud system architecture.",CLD,https://www.semanticscholar.org/paper/8d0efc6da17d5bf01446ce48ed9e9ac7e32d6565
Multi-User Multi-Task Computation Offloading in Green Mobile Edge Cloud Computing,"Mobile Edge Cloud Computing (MECC) has becoming an attractive solution for augmenting the computing and storage capacity of Mobile Devices (MDs) by exploiting the available resources at the network edge. In this work, we consider computation offloading at the mobile edge cloud that is composed of a set of Wireless Devices (WDs), and each WD has an energy harvesting equipment to collect renewable energy from the environment. Moreover, multiple MDs intend to offload their tasks to the mobile edge cloud simultaneously. We first formulate the multi-user multi-task computation offloading problem for green MECC, and use Lyaponuv Optimization Approach to determine the energy harvesting policy: how much energy to be harvested at each WD; and the task offloading schedule: the set of computation offloading requests to be admitted into the mobile edge cloud, the set of WDs assigned to each admitted offloading request, and how much workload to be processed at the assigned WDs. We then prove that the task offloading scheduling problem is NP-hard, and introduce centralized and distributed Greedy Maximal Scheduling algorithms to resolve the problem efficiently. Performance bounds of the proposed schemes are also discussed. Extensive evaluations are conducted to test the performance of the proposed algorithms.",CLD,https://www.semanticscholar.org/paper/dadf729bdd6f2d1a9f3be6fa0e0c5ef917342740
Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",CLD,https://www.semanticscholar.org/paper/116927fbe4c9732fd1e392035a100c33b14e9d59
A Survey on Distributed Denial of Service (DDoS) Attacks in SDN and Cloud Computing Environments,"Recently, software defined networks (SDNs) and cloud computing have been widely adopted by researchers and industry. However, widespread acceptance of these novel networking paradigms has been hampered by the security threats. Advances in the processing technologies have helped attackers in increasing the attacks too, for instance, the development of Denial of Service (DoS) attacks to distributed DoS (DDoS) attacks which are seldom identified by conventional firewalls. In this paper, we present the state of art of the DDoS attacks in SDN and cloud computing scenarios. Especially, we focus on the analysis of SDN and cloud computing architecture. Besides, we also overview the research works and open problems in identifying and tackling the DDoS attacks.",CLD,https://www.semanticscholar.org/paper/fec133d3d5f7e736eb8ea7bbbd7587f033a4dac4
Load balancing in cloud computing – A hierarchical taxonomical classification,"Load unbalancing problem is a multi-variant, multi-constraint problem that degrades performance and efficiency of computing resources. Load balancing techniques cater the solution for load unbalancing situation for two undesirable facets- overloading and under-loading. In contempt of the importance of load balancing techniques to the best of our knowledge, there is no comprehensive, extensive, systematic and hierarchical classification about the existing load balancing techniques. Further, the factors that cause load unbalancing problem are neither studied nor considered in the literature. This paper presents a detailed encyclopedic review about the load balancing techniques. The advantages and limitations of existing methods are highlighted with crucial challenges being addressed so as to develop efficient load balancing algorithms in future. The paper also suggests new insights towards load balancing in cloud computing.",CLD,https://www.semanticscholar.org/paper/11e50be223fb811367df87c5d3c56ccda676abfb
Workflow Scheduling Using Hybrid GA-PSO Algorithm in Cloud Computing,"Cloud computing environment provides several on-demand services and resource sharing for clients. Business processes are managed using the workflow technology over the cloud, which represents one of the challenges in using the resources in an efficient manner due to the dependencies between the tasks. In this paper, a Hybrid GA-PSO algorithm is proposed to allocate tasks to the resources efficiently. The Hybrid GA-PSO algorithm aims to reduce the makespan and the cost and balance the load of the dependent tasks over the heterogonous resources in cloud computing environments. The experiment results show that the GA-PSO algorithm decreases the total execution time of the workflow tasks, in comparison with GA, PSO, HSGA, WSGA, and MTCT algorithms. Furthermore, it reduces the execution cost. In addition, it improves the load balancing of the workflow application over the available resources. Finally, the obtained results also proved that the proposed algorithm converges to optimal solutions faster and with higher quality compared to other algorithms.",CLD,https://www.semanticscholar.org/paper/4f37a0fac9478623c6a7c61ec507d5a60647280d
Issues and Challenges of Load Balancing Techniques in Cloud Computing,"With the growth in computing technologies, cloud computing has added a new paradigm to user services that allows accessing Information Technology services on the basis of pay-per-use at any time and any location. Owing to flexibility in cloud services, numerous organizations are shifting their business to the cloud and service providers are establishing more data centers to provide services to users. However, it is essential to provide cost-effective execution of tasks and proper utilization of resources. Several techniques have been reported in the literature to improve performance and resource use based on load balancing, task scheduling, resource management, quality of service, and workload management. Load balancing in the cloud allows data centers to avoid overloading/underloading in virtual machines, which itself is a challenge in the field of cloud computing. Therefore, it becomes a necessity for developers and researchers to design and implement a suitable load balancer for parallel and distributed cloud environments. This survey presents a state-of-the-art review of issues and challenges associated with existing load-balancing techniques for researchers to develop more effective algorithms.",CLD,https://www.semanticscholar.org/paper/a6a7448014dc6db37d70342ba5b5c98b7d1a843f
Block Design-Based Key Agreement for Group Data Sharing in Cloud Computing,"Data sharing in cloud computing enables multiple participants to freely share the group data, which improves the efficiency of work in cooperative environments and has widespread potential applications. However, how to ensure the security of data sharing within a group and how to efficiently share the outsourced data in a group manner are formidable challenges. Note that key agreement protocols have played a very important role in secure and efficient group data sharing in cloud computing. In this paper, by taking advantage of the symmetric balanced incomplete block design (SBIBD), we present a novel block design-based key agreement protocol that supports multiple participants, which can flexibly extend the number of participants in a cloud environment according to the structure of the block design. Based on the proposed group data sharing model, we present general formulas for generating the common conference key <inline-formula><tex-math notation=""LaTeX"">$\mathcal {K}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""script"">K</mml:mi></mml:math><inline-graphic xlink:href=""xiang-ieq1-2725953.gif""/></alternatives></inline-formula> for multiple participants. Note that by benefiting from the <inline-formula><tex-math notation=""LaTeX"">$(v,k + 1,1)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""xiang-ieq2-2725953.gif""/></alternatives></inline-formula>-block design, the computational complexity of the proposed protocol linearly increases with the number of participants and the communication complexity is greatly reduced. In addition, the fault tolerance property of our protocol enables the group data sharing in cloud computing to withstand different key attacks, which is similar to Yi's protocol.",CLD,https://www.semanticscholar.org/paper/1add7229b226e148a4eb1ffc3d917dda2b022182
Secure Data Storage and Searching for Industrial IoT by Integrating Fog Computing and Cloud Computing,"With the fast development of industrial Internet of things (IIoT), a large amount of data is being generated continuously by different sources. Storing all the raw data in the IIoT devices locally is unwise considering that the end devices’ energy and storage spaces are strictly limited. In addition, the devices are unreliable and vulnerable to many threats because the networks may be deployed in remote and unattended areas. In this paper, we discuss the emerging challenges in the aspects of data processing, secure data storage, efficient data retrieval and dynamic data collection in IIoT. Then, we design a flexible and economical framework to solve the problems above by integrating the fog computing and cloud computing. Based on the time latency requirements, the collected data are processed and stored by the edge server or the cloud server. Specifically, all the raw data are first preprocessed by the edge server and then the time-sensitive data (e.g., control information) are used and stored locally. The non-time-sensitive data (e.g., monitored data) are transmitted to the cloud server to support data retrieval and mining in the future. A series of experiments and simulation are conducted to evaluate the performance of our scheme. The results illustrate that the proposed framework can greatly improve the efficiency and security of data storage and retrieval in IIoT.",IOTNET,https://www.semanticscholar.org/paper/11c3154d709c74dbbe702e7f7c46a37224f9cc36
Comparison of Fog Computing & Cloud Computing,"Fog computing is extending cloud computing by transferring computation on the edge of networks such as mobile collaborative devices or fixed nodes with built-in data storage, computing, and communication devices. Fog gives focal points of enhanced proficiency, better security, organize data transfer capacity sparing and versatility. With a specific end goal to give imperative subtle elements of Fog registering, we propose attributes of this region and separate from cloud computing research. Cloud computing is developing innovation which gives figuring assets to a specific assignment on pay per utilize. Cloud computing gives benefit three unique models and the cloud gives shoddy; midway oversaw assets for dependable registering for performing required errands. This paper gives correlation and attributes both Fog and cloud computing differs by outline, arrangement, administrations and devices for associations and clients. This comparison shows that Fog provides more flexible infrastructure and better service of data processing by consuming low network bandwidth instead of shifting whole data to the cloud.",CLD,https://www.semanticscholar.org/paper/c79d2859e6dc65404209fa3e8425278bee16d4da
Cloud Programming Simplified: A Berkeley View on Serverless Computing,"Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.",CLD,https://www.semanticscholar.org/paper/05a2f1fe94ac485d9adf9a5bce131b66c56b47c4
Task scheduling and resource allocation in cloud computing using a heuristic approach,"Cloud computing is required by modern technology. Task scheduling and resource allocation are important aspects of cloud computing. This paper proposes a heuristic approach that combines the modified analytic hierarchy process (MAHP), bandwidth aware divisible scheduling (BATS) + BAR optimization, longest expected processing time preemption (LEPT), and divide-and-conquer methods to perform task scheduling and resource allocation. In this approach, each task is processed before its actual allocation to cloud resources using a MAHP process. The resources are allocated using the combined BATS + BAR optimization method, which considers the bandwidth and load of the cloud resources as constraints. In addition, the proposed system preempts resource intensive tasks using LEPT preemption. The divide-and-conquer approach improves the proposed system, as is proven experimentally through comparison with the existing BATS and improved differential evolution algorithm (IDEA) frameworks when turnaround time and response time are used as performance metrics.",CLD,https://www.semanticscholar.org/paper/e7668a9082680cae8add5c1c73d3d60efb52fa5b
JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services,"Deep learning models are being deployed in many mobile intelligent applications. End-side services, such as intelligent personal assistants, autonomous cars, and smart home services often employ either simple local models on the mobile or complex remote models on the cloud. However, recent studies have shown that partitioning the DNN computations between the mobile and cloud can increase the latency and energy efficiencies. In this paper, we propose an efficient, adaptive, and practical engine, JointDNN, for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase. JointDNN not only provides an energy and performance efficient method of querying DNNs for the mobile side but also benefits the cloud server by reducing the amount of its workload and communications compared to the cloud-only approach. Given the DNN architecture, we investigate the efficiency of processing some layers on the mobile device and some layers on the cloud server. We provide optimization formulations at layer granularity for forward- and backward-propagations in DNNs, which can adapt to mobile battery limitations and cloud server load constraints and quality of service. JointDNN achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively.",CLD,https://www.semanticscholar.org/paper/cbab055052c6098b21c210e945ca17df143273da
Reliability and high availability in cloud computing environments: a reference roadmap,"Reliability and high availability have always been a major concern in distributed systems. Providing highly available and reliable services in cloud computing is essential for maintaining customer confidence and satisfaction and preventing revenue losses. Although various solutions have been proposed for cloud availability and reliability, but there are no comprehensive studies that completely cover all different aspects in the problem. This paper presented a ‘Reference Roadmap’ of reliability and high availability in cloud computing environments. A big picture was proposed which was divided into four steps specifying through four pivotal questions starting with ‘Where?’, ‘Which?’, ‘When?’ and ‘How?’ keywords. The desirable result of having a highly available and reliable cloud system could be gained by answering these questions. Each step of this reference roadmap proposed a specific concern of a special portion of the issue. Two main research gaps were proposed by this reference roadmap.",CLD,https://www.semanticscholar.org/paper/deca1de0fb33839389c726d2b235fbab71a2b08f
Understanding determinants of cloud computing adoption using an integrated TAM-TOE model,"– The purpose of this paper is to integrate TAM model and TOE framework for cloud computing adoption at organizational level. , – A conceptual framework was developed using technological and organizational variables of TOE framework as external variables of TAM model while environmental variables were proposed to have direct impact on cloud computing adoption. A questionnaire was used to collect the data from 280 companies in IT, manufacturing and finance sectors in India. The data were analyzed using exploratory and confirmatory factor analyses. Further, structural equation modeling was used to test the proposed model. , – The study identified relative advantage, compatibility, complexity, organizational readiness, top management commitment, and training and education as important variables for affecting cloud computing adoption using perceived ease of use (PEOU) and perceived usefulness (PU) as mediating variables. Also, competitive pressure and trading partner support were found directly affecting cloud computing adoption intentions. The model explained 62 percent of cloud computing adoption. , – The model can be used as a guideline to ensure a positive outcome of the cloud computing adoption in organizations. It also provides relevant recommendations to achieve conducive implementation environment for cloud computing adoption. , – This study integrates two of the information technology adoption models to improve predictive power of resulting model.",CLD,https://www.semanticscholar.org/paper/7cf026a27f60fdce71e1bed0ce3808e5c149e319
A Modified Hierarchical Attribute-Based Encryption Access Control Method for Mobile Cloud Computing,"Abstract—Cloud computing is an Internet-based computing pattern through which shared resources are provided to devices on-demand. It is an emerging but promising paradigm to integrating mobile devices into cloud computing, and the integration performs in the cloud based hierarchical multi-user data-shared environment. With integrating into cloud computing, security issues such as data confidentiality and user authority may arise in the mobile cloud computing system, and it is concerned as the main constraints to the developments of mobile cloud computing. In order to provide safe and secure operation, a hierarchical access control method using modified hierarchical attribute-based encryption (M-HABE) and a modified three-layer structure is proposed in this paper. In a specific mobile cloud computing model, enormous data which may be from all kinds of mobile devices, such as smart phones, functioned phones and PDAs and so on can be controlled and monitored by the system, and the data can be sensitive to unauthorized third party and constraint to legal users as well. The novel scheme mainly focuses on the data processing, storing and accessing, which is designed to ensure the users with legal authorities to get corresponding classified data and to restrict illegal users and unauthorized legal users get access to the data, which makes it extremely suitable for the mobile cloud computing paradigms.",CLD,https://www.semanticscholar.org/paper/d855743da29c14df045b38cc45120596267849fe
Cloud Computing: History and Overview,"Cloud computing has emerged as a new technology and business paradigm in the last couple of years. Cloud computing platforms provide easy access, scalability, reliability, reconfigurability, and high performance from its resources over the Internet without complex infrastructure management by customers. This article presents, a brief history of cloud computing from 1961 when McCarthy at MIT introduced about cloud computing, the evolution of cloud computing from its predecessors such as Utility computing and Grid computing, and development of cloud computing. We have also presented various directions in cloud computing along with advantages, cloud-centric design, mobile cloud, and security. It covers the characteristics, service models, and deployment models of cloud computing. We have presented the applications and security aspects associated with cloud computing.",CLD,https://www.semanticscholar.org/paper/a44daae7ba0bc7f8f8030cc8ff5286d9c99e40c2
The Security Risks of Cloud Computing,"Cloud computing is rapidly increasing for achieving comfortable computing. Cloud computing has essentially security vulnerability of software and hardware. For achieving secure cloud computing, the vulnerabilities of cloud computing could be analyzed in a various and systematic approach from perspective of the service designer, service operator, the designer of cloud security and certifiers of cloud systems. The paper investigates the vulnerabilities and security controls in terms of secure cryptography. For achieving the research aims, this paper analyzes technological security vulnerability, operational weakness and suggests secure ways for cloud systems.",CLD,https://www.semanticscholar.org/paper/397ec1f31e8ca7e91560d01230961f5772a61290
Collaborative Cloud and Edge Computing for Latency Minimization,"By performing data processing at the network edge, mobile edge computing can effectively overcome the deficiencies of network congestion and long latency in cloud computing systems. To improve edge cloud efficiency with limited communication and computation capacities, we investigate the collaboration between cloud computing and edge computing, where the tasks of mobile devices can be partially processed at the edge node and at the cloud server. First, a joint communication and computation resource allocation problem is formulated to minimize the weighted-sum latency of all mobile devices. Then, the closed-form optimal task splitting strategy is derived as a function of the normalized backhaul communication capacity and the normalized cloud computation capacity. Some interesting and useful insights for the optimal task splitting strategy are also highlighted by analyzing four special scenarios. Based on this, we further transform the original joint communication and computation resource allocation problem into an equivalent convex optimization problem and obtain the closed-form computation resource allocation strategy by leveraging the convex optimization theory. Moreover, a necessary condition is also developed to judge whether a task should be processed at the corresponding edge node only, without offloading to the cloud server. Finally, simulation results confirm our theoretical analysis and demonstrate that the proposed collaborative cloud and edge computing scheme can evidently achieve a better delay performance than the conventional schemes.",CLD,https://www.semanticscholar.org/paper/af2ad9cad35e99a9076b37176b92398a487e057d
UAV-Assisted Task Offloading in Vehicular Edge Computing Networks,"Vehicular edge computing (VEC) provides an effective task offloading paradigm by pushing cloud resources to the vehicular network edges, e.g., road side units (RSUs). However, overloaded RSUs are likely to occur especially in urban aggregation areas, possibly leading to greatly compromised offloading performance. Inspired by this, this article explores this situation by introducing an unmanned aerial vehicle (UAV) to address the VEC overload problem. Specifically, we formulate a novel online UAV-assisted vehicular task offloading problem to minimize vehicular task delay under the long-term UAV energy constraint. To solve the formulated problem, we first decouple the long-term energy constraint based on the Lyapunov optimization technique. In this way, the problem can be solved in a real-time manner without requiring future information. Then, we construct a Markov chain based on Markov approximation optimization to find out the close-to-optimal UAV-assisted offloading strategies. Furthermore, we derive a mathematical analysis to rigorously demonstrate the offloading performance of the proposed algorithm. Additionally, the simulation results show that the proposed method outperforms the baselines by significantly reducing the vehicular task delay constrained by the long-term UAV energy budget under various system parameters, such as the energy budget and computation workloads.",IOTNET,https://www.semanticscholar.org/paper/bd04f581fcba24749aa8ba35c5ccaf29b3ac43d0
REVIEWING THE TRANSFORMATIONAL IMPACT OF EDGE COMPUTING ON REAL-TIME DATA PROCESSING AND ANALYTICS,"Edge computing has emerged as a pivotal paradigm shift in the realm of data processing and analytics, revolutionizing the way organizations handle real-time data. This review presents a comprehensive review of the transformational impact of edge computing on real-time data processing and analytics. Firstly, the review delves into the fundamental concepts of edge computing, elucidating its architectural framework and highlighting its distinct advantages over traditional cloud-centric approaches. By distributing computational resources closer to data sources, edge computing mitigates latency issues and enhances responsiveness, thereby enabling real-time data processing at the edge. Furthermore, this review explores how edge computing facilitates the seamless integration of analytics capabilities into edge devices, empowering organizations to derive actionable insights at the source of data generation. Leveraging advanced analytics algorithms, such as machine learning and artificial intelligence, edge computing enables autonomous decision-making and predictive analytics in real time, fostering innovation across diverse industry verticals. Moreover, the review examines the transformative implications of edge computing on various sectors, including healthcare, manufacturing, transportation, and smart cities. By enabling localized data processing and analytics, edge computing enhances operational efficiency, ensures data privacy and security, and unlocks new opportunities for business optimization and value creation. This review underscores the profound impact of edge computing on real-time data processing and analytics, revolutionizing the way organizations harness data to drive informed decision-making and gain competitive advantage in today's dynamic business landscape. As edge computing continues to evolve, its transformative potential is poised to redefine the future of data-driven innovation and digital transformation. Keywords: Edge, Computing, Analytics, Data, Impact, Review.",IOTNET,https://www.semanticscholar.org/paper/fcd93b5c5dc9628777976ce7f3ac0b27d9d3ee5c
Mobile Edge Computing: A Survey on Architecture and Computation Offloading,"Technological evolution of mobile user equipment (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. A suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC.",CLD,https://www.semanticscholar.org/paper/afd9dadac8d3354615e26b2038887ecdbc9e33e5
Intelligent Delay-Aware Partial Computing Task Offloading for Multiuser Industrial Internet of Things Through Edge Computing,"The development of Industrial Internet of Things (IIoT) and Industry 4.0 has completely changed the traditional manufacturing industry. Intelligent IIoT technology usually involves a large number of intensive computing tasks. Resource-constrained IIoT devices often cannot meet the real-time requirements of these tasks. As a promising paradigm, the mobile-edge computing (MEC) system migrates the computation intensive tasks from resource-constrained IIoT devices to nearby MEC servers, thereby obtaining lower delay and energy consumption. However, considering the varying channel conditions as well as the distinct delay requirements for various computing tasks, it is challenging to coordinate the computing task offloading among multiple users. In this article, we propose an autonomous partial offloading system for delay-sensitive computation tasks in multiuser IIoT MEC systems. Our goal is to provide offloading services with minimum delay for better Quality of Service (QoS). Enlighten by the recent advancement of reinforcement learning (RL), we propose two RL-based offloading strategies to automatically optimize the delay performance. Specifically, we first implement the $Q$ -learning algorithm to provide a discrete partial offloading decision. Then, to further optimize the system performance with more flexible task offloading, the offloading decisions are given as continuous based on deep deterministic policy gradient (DDPG). The simulation results show that the $Q$ -learning scheme reduces the delay by 23%, and the DDPG scheme reduces the delay by 30%.",IOTNET,https://www.semanticscholar.org/paper/2c32ec19926c3caad396ae93dd41c81228f7cfc8
Edge Computing on IoT for Machine Signal Processing and Fault Diagnosis: A Review,"Edge computing is an emerging paradigm that offloads the computations and analytics workloads onto the Internet of Things (IoT) edge devices to accelerate the computation efficiency, reduce the channel occupation of signal transmission, and reduce the storage and computation workloads on the cloud servers. These distinct merits make it a promising tool for IoT-based machine signal processing and fault diagnosis. This article reviews the edge computing methods in signal processing-based machine fault diagnosis from the aspects of concepts, state-of-the-art methods, case studies, and research prospects. In particular, the lightweight designed algorithms and application-specific hardware platforms of edge computing in the typical fault diagnosis procedures, including signal acquisition, signal preprocessing, feature extraction, and pattern recognition, are reviewed and discussed in detail. The review provides an insight into the edge computing framework, methods, and applications, so as to meet the requirements of IoT-based machine real-time signal processing, low-latency fault diagnosis, and high-efficient predictive maintenance.",IOTNET,https://www.semanticscholar.org/paper/e097ebf454256a0d0082fcdd27eb24fa11ee4179
At the Confluence of Artificial Intelligence and Edge Computing in IoT-Based Applications: A Review and New Perspectives,"Given its advantages in low latency, fast response, context-aware services, mobility, and privacy preservation, edge computing has emerged as the key support for intelligent applications and 5G/6G Internet of things (IoT) networks. This technology extends the cloud by providing intermediate services at the edge of the network and improving the quality of service for latency-sensitive applications. Many AI-based solutions with machine learning, deep learning, and swarm intelligence have exhibited the high potential to perform intelligent cognitive sensing, intelligent network management, big data analytics, and security enhancement for edge-based smart applications. Despite its many benefits, there are still concerns about the required capabilities of intelligent edge computing to deal with the computational complexity of machine learning techniques for big IoT data analytics. Resource constraints of edge computing, distributed computing, efficient orchestration, and synchronization of resources are all factors that require attention for quality of service improvement and cost-effective development of edge-based smart applications. In this context, this paper aims to explore the confluence of AI and edge in many application domains in order to leverage the potential of the existing research around these factors and identify new perspectives. The confluence of edge computing and AI improves the quality of user experience in emergency situations, such as in the Internet of vehicles, where critical inaccuracies or delays can lead to damage and accidents. These are the same factors that most studies have used to evaluate the success of an edge-based application. In this review, we first provide an in-depth analysis of the state of the art of AI in edge-based applications with a focus on eight application areas: smart agriculture, smart environment, smart grid, smart healthcare, smart industry, smart education, smart transportation, and security and privacy. Then, we present a qualitative comparison that emphasizes the main objective of the confluence, the roles and the use of artificial intelligence at the network edge, and the key enabling technologies for edge analytics. Then, open challenges, future research directions, and perspectives are identified and discussed. Finally, some conclusions are drawn.",IOTNET,https://www.semanticscholar.org/paper/00044a527a0642b08f385cda845331f0fc8cbbfd
Joint Optimization of Computing Offloading and Service Caching in Edge Computing-Based Smart Grid,"With the continuous expansion of the power Internet of Things (IoT) and the rapid increase in the number of Smart Devices (SDs), the data generated by SDs has exponentially increased. The traditional cloud-based smart grid cannot meet the low latency and high reliability requirements of emerging applications. By moving computing, data, and services from the centralized cloud to Edge Servers (ESs), edge computing exhibits excellent performance in communication delay and traffic reduction. Simultaneously, service caching also shows attractive advantages in handling the surge in data traffic. In this paper, we consider the joint optimization of computing offloading and service caching in edge computing-based smart grid, and formulate the problem as a Mixed-Integer Non-Linear Program (MINLP), aiming to minimize the task cost of the system. The original problem is decomposed into an equivalent master problem and sub-problem, and a Collaborative Computing Offloading and Resource Allocation Method (CCORAM) is proposed to solve the optimization problem, which includes two low-complexity algorithms. Specifically, a gradient descent allocation algorithm is first proposed to determine the computing resource allocation strategy, and then a game theory-based algorithm is proposed to determine the computing strategy. Simulation results show that CCORAM with low time complexity is very close to the optimal method, and performs much better than other benchmark methods.",IOTNET,https://www.semanticscholar.org/paper/7196f7830cb05b789e34cb5ea6f58ae4a4671dd3
Task Partitioning and Offloading in DNN-Task Enabled Mobile Edge Computing Networks,"Deep neural network (DNN)-task enabled mobile edge computing (MEC) is gaining ubiquity due to outstanding performance of artificial intelligence. By virtue of characteristics of DNN, this paper develops a joint design of task partitioning and offloading for a DNN-task enabled MEC network that consists of a single server and multiple mobile devices (MDs), where the server and each MD employ the well-trained DNNs for task computation. The main contributions of this paper are as follows: First, we propose a layer-level computation partitioning strategy for DNN to partition each MD's task into the subtasks that are either locally computed at the MD or offloaded to the server. Second, we develop a delay prediction model for DNN to characterize the computation delay of each subtask at the MD and the server. Third, we design a slot model and a dynamic pricing strategy for the server to efficiently schedule the offloaded subtasks. Fourth, we jointly optimize the design of task partitioning and offloading to minimize each MD's cost that includes the computation delay, the energy consumption, and the price paid to the server. In particular, we propose two distributed algorithms based on the aggregative game theory to solve the optimization problem. Finally, numerical results demonstrate that the proposed scheme is scalable to different types of DNNs and shows the superiority over the baseline schemes in terms of processing delay and energy consumption.",IOTNET,https://www.semanticscholar.org/paper/f00721dc5ce97c210cd882339f5dc0adeebc2dde
Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing,"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.",IOTNET,https://www.semanticscholar.org/paper/928cd808aba140ec298508df87c5579811ff2f41
AI-Enabled Secure Microservices in Edge Computing: Opportunities and Challenges,"The paradigm of edge computing has formed an innovative scope within the domain of the Internet of Things (IoT) through expanding the services of the cloud to the network edge to design distributed architectures and securely enhance decision-making applications. Due to the heterogeneous, distributed and resource-constrained essence of edge Computing, edge applications are required to be developed as a set of lightweight and interdependent modules. As this concept aligns with the objectives of microservice architecture, effective implementation of microservices-based edge applications within IoT networks has the prospective of fully leveraging edge nodes capabilities. Deploying microservices at IoT edge faces plenty of challenges associated with security and privacy. Advances in Artificial Intelligence (AI) (especially Machine Learning), and the easy access to resources with powerful computing providing opportunities for deriving precise models and developing different intelligent applications at the edge of network. In this study, an extensive survey is presented for securing edge computing-based AI Microservices to elucidate the challenges of IoT management and enable secure decision-making systems at the edge. We present recent research studies on edge AI and microservices orchestration and highlight key requirements as well as challenges of securing Microservices at IoT edge. We also propose a Microservices-based edge computing framework that provides secure edge AI algorithms as Microservices utilizing the containerization technology to offer automated and secure AI-based applications at the network edge.",IOTNET,https://www.semanticscholar.org/paper/514a96af287ab19a399479f91ae91a28b1593c5c
Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection,"With fast increase in volume of mobile multimedia data, how to apply powerful deep learning methods to process data with real-time response becomes a major issue. Meanwhile, edge computing structure helps improve response time and user experience by bringing flexible computation and storage capabilities. Considering both technologies for successful AI-based applications, we propose an edge-computing driven and end-to-end framework to perform tasks of image enhancement and object detection under low-light conditions. The framework consists of a cloud-based enhancement and an edge-based detection stage. In the first stage, we establish connections between edge devices and cloud servers to input re-scaled illumination parts of low-light images, where enhancement subnetworks are dynamically and parallel coupled to compute enhanced illumination parts based on low-light context. During the edge-based detection stage, edge devices could accurately and rapidly detect objects based on cloud-computed informative feature map. Experimental results show the proposed method significantly improves detection performance in low-light conditions with low latency running on edge devices.",CLD,https://www.semanticscholar.org/paper/a0c703c255ca554508487b902bb4233f71f8738b
Joint Task Offloading and Resource Allocation for Energy-Constrained Mobile Edge Computing,"We consider the problem of task offloading and resource allocation in mobile edge computing (MEC). To maintain satisfactory quality of experience (QoE) of end-users, mobile devices (MDs) may offload their tasks to edge servers based on the allocated computation (e.g., CPU/GPU cycles and storage) and wireless resources (e.g., bandwidth). However, these resources could not be effectively utilized unless an encouraging resource allocation scheme can be proposed. What’s worse, task offloading incurs additional MEC energy consumption, which inevitably violate the long-term MEC energy budget. Considering these two challenges, we propose an online joint offloading and resource allocation (JORA) framework under the long-term MEC energy constraint, aiming at guaranteeing the end-users’ QoE. To achieve this, we leverage Lyapunov optimization to exploit the optimality of the long-term QoE maximization problem. By constructing an energy deficit queue to guide energy consumption, the problem can be solved in a real-time manner. On this basis, we propose online JORA methods in both centralized and distributed manners. Furthermore, we prove that our proposed methods enable the achievement of the close-to-optimal performance while satisfying the long-term MEC energy constraint. In addition, we conduct extensive simulations and the results show superiority in performance over other methods.",OPS,https://www.semanticscholar.org/paper/b52c106cb6f91f1f52b94c9ac883a17f7625bbc9
Accelerating Decentralized Federated Learning in Heterogeneous Edge Computing,"In edge computing (EC), federated learning (FL) enables massive devices to collaboratively train AI models without exposing local data. In order to avoid the possible bottleneck of the parameter server (PS) architecture, we concentrate on the decentralized federated learning (DFL), which adopts peer-to-peer (P2P) communication without maintaining a global model. However, due to the intrinsic features of EC, e.g., resource limitation and heterogeneity, network dynamics and non-IID data, DFL with a fixed P2P topology and/or an identical model compression ratio for all workers results in a slow convergence rate. In this paper, we propose an efficient algorithm (termed <italic>CoCo</italic>) to accelerate DFL by integrating optimization of topology <bold>Co</bold>nstruction and model <bold>Co</bold>mpression. Concretely, we adaptively construct P2P topology and determine specific compression ratios for each worker to conquer the system dynamics and heterogeneity under bandwidth constraints. To reflect how the non-IID data influence the consistency of local models in DFL, we introduce the <italic>consensus distance</italic>, i.e., the discrepancy between local models, as the quantitative metric to guide the fine-grained operations of the joint optimization. Extensive simulations and testbed experiments show that <italic>CoCo</italic> achieves 10× speedup, and reduces the communication cost by about <inline-formula><tex-math notation=""LaTeX"">$50\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""wang-ieq1-3178378.gif""/></alternatives></inline-formula> on average, compared with the existing DFL baselines.",IOTNET,https://www.semanticscholar.org/paper/36d93586d3054b2590e068262f880e9d107d86fc
Wireless Powered Mobile Edge Computing Networks: A Survey,"Wireless Powered Mobile Edge Computing (WPMEC) is an integration of Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies, to both improve computing capabilities of mobile devices and energy compensation for their limited battery capabilities. Generally, energy transmitters, mobile devices, and edge servers form a WPMEC system that realizes a closed loop of sending and collecting energy as well as offloading and receiving task data. Due to constraints of time-varying network environments, time-coupled battery levels, and the half-duplex character of mobile devices, the joint design of computation offloading and resource allocation solutions in WPMEC systems has become extremely challenging, and a great number of studies have been devoted to it in recent years. In this article, we first introduce the basic model of the WPMEC system. Then, we present key issues and techniques related to WPMEC. In addition, we summarize solutions for computation offloading and resource allocation to solve critical issues in WPMEC networks. Finally, we discuss some research challenges and open issues.",IOTNET,https://www.semanticscholar.org/paper/be07013b8e1a3648f49495c63558ae9b5eb7301f
Schema Design with Intelligent Multi Modelling Edge Computing Techniques for Industrial Applications,"The Internet has become one of the most essential sources of information and knowledge for consumers. It is important to figure out how to get users' specific needs from the large number of network document resources in a precise and efficient way. Sharing information on activities that are of interest to all members of a group may be done via group suggestion. Too much data and resources are no longer a concern with this system. There has been a comprehensive testing of how big data can be used in the packaging industry. Edge computing with knowledge graphs and LSTMs shows that the proposed system provides more accurate and better suggestions for network document resources. Consequently, it is capable of meeting the individual resource requirements of each user in a more effective manner.",IOTNET,https://www.semanticscholar.org/paper/7f5c04ec6f8936e49db82fcd3c90155bab505b14
Reverse Auction-Based Computation Offloading and Resource Allocation in Mobile Cloud-Edge Computing,"This article proposes a novel Reverse Auction-based Computation Offloading and Resource Allocation Mechanism, named RACORAM for the mobile Cloud-Edge computing. The basic idea is that the Cloud Service Center (CSC) recruits edge server owners to replace it to accommodate offloaded computation from nearby resource-constraint Mobile Devices (MDs). In RACORAM, the reverse auction is used to stimulate edge server owners to participate in the offloading process, and the reverse auction-based computation offloading and resource allocation problem is formulated as a Mixed Integer Nonlinear Programming (MINLP) problem, aiming to minimize the cost of the CSC. The original problem is decomposed into an equivalent master problem and subproblem, and low-complexity algorithms are proposed to solve the related optimization problems. Specifically, a Constrained Gradient Descent Allocation Method (CGDAM) is first proposed to determine the computation resource allocation strategy, and then a Greedy Randomized Adaptive Search Procedure based Winning Bid Scheduling Method (GWBSM) is proposed to determine the computation offloading strategy. Meanwhile, the CSC's payment determination for the winning edge server owners is also presented. Simulations are conducted to evaluate the performance of RACORAM, and the results show that RACORAM is very close to the optimal method with significantly reduced computational complexity, and greatly outperforms the other baseline methods in terms of the CSC's cost under different scenarios.",CLD,https://www.semanticscholar.org/paper/e75b174f2ed5e2ea321b1879f300d7acd70e5f51
Aerial Edge Computing on Orbit: A Task Offloading and Allocation Scheme,"As the communication mode with the greatest attention and global network coverage, the Low Earth Orbit (LEO) satellite network has the characteristics of low propagation delay, low link loss and handheld terminal. However, with the increasing requirements of user terminals for network latency and bandwidth, the traditional central cloud computing mode no longer has advantages. Therefore, borrowing from the idea of edge computing in terrestrial networks, Orbital Edge Computing (OEC) technology deploys Multi-access Edge Computing (MEC) servers on LEO satellite constellations to meet the growing demand for real-time and reliability of various applications. Based on the above motivations, this paper proposes an OEC Task Allocation (OEC-TA) algorithm based on the greedy strategy in LEO satellite networks for the Walker Delta satellite constellation, which fully utilizes satellite computing resources to provide services to ground users. Then, we analyze the performance of our proposed algorithm in terms of computational cost. Finally, experimental results show that OEC-TA is better than DEC (Double Edge Computing) and random allocation model on average delay and energy consumption reduction. Compared with DEC, our OEC-TA can reduce the average delay and energy consumption up to 10% and 16.5%, respectively.",IOTNET,https://www.semanticscholar.org/paper/2ee30f619c115318e4649c88fe61703e5be20795
Privacy-Preserving Federated Learning for Industrial Edge Computing via Hybrid Differential Privacy and Adaptive Compression,"With the continuous improvement of hardware computing power, edge computing of industrial data has been gradually applied. In the past decade, the promotion of edge computing has also greatly improved the efficiency of industrial production. Compared with the conventional cloud computing, it not only saves the bandwidth consumption of data transmission, but also ensures the terminal data security to a certain extent. However, the continuous update of attack types also put forward new requirements for the privacy protection of industrial edge computing. So it should fundamentally solve the risk of industrial data leakage in the process of deep model training in edge terminal. In this article, we propose a new federated edge learning framework based on hybrid differential privacy and adaptive compression for industrial data processing. Specifically, it first completes the adaptive gradient compression preparation, then constructs the industrial federated learning model, and finally makes use of adaptive differential privacy model to optimize, so as to complete the privacy protection towards the transmission of gradient parameters in industrial environment. By optimizing the hybrid differential privacy and adaptive compression, we can better prevent the terminal data privacy against inference attacks. The experimental results show that this method is very effective in the industrial edge computing situation, and it also opens up a new direction for the effect of differential privacy in federated learning.",SEC,https://www.semanticscholar.org/paper/ebc9ab9cf2ce9047649b6e58612be3ae5c769486
"Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions","Full leverage of the huge volume of data generated on a large number of user devices for providing intelligent services in the 6G network calls for Ubiquitous Intelligence (UI). A key to developing UI lies in the involvement of the large number of network devices, which contribute their data to collaborative Machine Learning (ML) and provide their computational resources to support the learning process. Federated Learning (FL) is a new ML method that enables data owners to collaborate in model training without exposing private data, which allows user devices to contribute their data to developing UI. Edge computing deploys cloud-like capabilities at the network edge, which enables network devices to offer their computational resources for supporting FL. Therefore, a combination of FL and edge computing may greatly facilitate the development of ubiquitous intelligence in the 6G network. In this article, we present a comprehensive survey of the recent developments in technologies for combining FL and edge computing with a holistic vision across the fields of FL and edge computing. We conduct our survey from both the perspective of an FL framework deployed in an edge computing environment (FL in Edge) and the perspective of an edge computing system providing a platform for FL (Edge for FL). From the FL in Edge perspective, we first identify the main challenges to FL in edge computing and then survey the representative technical strategies for addressing the challenges. From the Edge for FL perspective, we first analyze the key requirements for edge computing to support FL and then review the recent advances in edge computing technologies that may be exploited to meet the requirements. Then we discuss open problems and identify some possible directions for future research on combining FL and edge computing, with the hope of arousing the research community’s interest in this emerging and exciting interdisciplinary field.",IOTNET,https://www.semanticscholar.org/paper/1c665b2949c4d8c120049d13222157c4203d0a15
Topology-aware Federated Learning in Edge Computing: A Comprehensive Survey,"The ultra-low latency requirements of 5G/6G applications and privacy constraints call for distributed machine learning systems to be deployed at the edge. With its simple yet effective approach, federated learning (FL) is a natural solution for massive user-owned devices in edge computing with distributed and private training data. FL methods based on FedAvg typically follow a naive star topology, ignoring the heterogeneity and hierarchy of the volatile edge computing architectures and topologies in reality. Several other network topologies exist and can address the limitations and bottlenecks of the star topology. This motivates us to survey network topology-related FL solutions. In this paper, we conduct a comprehensive survey of the existing FL works focusing on network topologies. After a brief overview of FL and edge computing networks, we discuss various edge network topologies as well as their advantages and disadvantages. Lastly, we discuss the remaining challenges and future works for applying FL to topology-specific edge networks.",IOTNET,https://www.semanticscholar.org/paper/746f05d19de735e78632babfbcd23ea713228af9
"Resource Scheduling in Edge Computing: Architecture, Taxonomy, Open Issues and Future Research Directions","An inflection point in the computing industry is occurring with the implementation of the Internet of Things and 5G communications, which has pushed centralized cloud computing toward edge computing resulting in a paradigm shift in computing. The purpose of edge computing is to provide computing, network control, and storage to the network edge to accommodate computationally intense and latency-critical applications at resource-limited endpoints. Edge computing allows edge devices to offload their overflowing computing tasks to edge servers. This procedure may completely exploit the edge server’s computational and storage capabilities and efficiently execute computing operations. However, transferring all the overflowing computing tasks to an edge server leads to long processing delays and surprisingly high energy consumption for numerous computing tasks. Aside from this, unused edge devices and powerful cloud centers may lead to resource waste. Thus, hiring a collaborative scheduling approach based on task properties, optimization targets, and system status with edge servers, cloud centers, and edge devices is critical for the successful operation of edge computing. This paper briefly summarizes the edge computing architecture for information and task processing. Meanwhile, the collaborative scheduling scenarios are examined. Resource scheduling techniques are then discussed and compared based on four collaboration modes. As part of our survey, we present a thorough overview of the various task offloading schemes proposed by researchers for edge computing. Additionally, according to the literature surveyed, we briefly looked at the fairness and load balancing indicators in scheduling. Finally, edge computing resource scheduling issues, challenges, and future directions have discussed.",IOTNET,https://www.semanticscholar.org/paper/4a6d415b9ac71997a7d201d1adc46d2dd63d62c2
Adaptive Federated Learning in Resource Constrained Edge Computing Systems,"Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.",IOTNET,https://www.semanticscholar.org/paper/e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da
Serverless Edge Computing—Where We Are and What Lies Ahead,"The edge–cloud continuum combines heterogeneous resources, which are complex to manage. Serverless edge computing is a suitable candidate to manage the continuum by abstracting away the underlying infrastructure, improving developers’ experiences, and optimizing overall resource utilization. However, understanding and overcoming programming support, reliability, and performance engineering challenges are essential for the success of serverless edge computing. In this article, we review and evaluate the maturity of serverless approaches for the edge–cloud continuum. Our review includes commercial, community-driven offerings and approaches from academia. We identify several maturity levels of serverless edge computing and use them as criteria to evaluate the maturity of current state-of-the-art serverless approaches with a special focus on the programming, reliability, and performance challenges. Finally, we lay a road map toward the next generation of serverless edge computing systems.",IOTNET,https://www.semanticscholar.org/paper/eaa75704962e4d6f72a513598e78c7edd2d3e8cc
Edge Computing with Artificial Intelligence: A Machine Learning Perspective,"Recent years have witnessed the widespread popularity of Internet of things (IoT). By providing sufficient data for model training and inference, IoT has promoted the development of artificial intelligence (AI) to a great extent. Under this background and trend, the traditional cloud computing model may nevertheless encounter many problems in independently tackling the massive data generated by IoT and meeting corresponding practical needs. In response, a new computing model called edge computing (EC) has drawn extensive attention from both industry and academia. With the continuous deepening of the research on EC, however, scholars have found that traditional (non-AI) methods have their limitations in enhancing the performance of EC. Seeing the successful application of AI in various fields, EC researchers start to set their sights on AI, especially from a perspective of machine learning, a branch of AI that has gained increased popularity in the past decades. In this article, we first explain the formal definition of EC and the reasons why EC has become a favorable computing model. Then, we discuss the problems of interest in EC. We summarize the traditional solutions and hightlight their limitations. By explaining the research results of using AI to optimize EC and applying AI to other fields under the EC architecture, this article can serve as a guide to explore new research ideas in these two aspects while enjoying the mutually beneficial relationship between AI and EC.",IOTNET,https://www.semanticscholar.org/paper/3592f77e5fa3f4cd860a747ea8ef7fa627bea3f2
"Edge Learning for B5G Networks With Distributed Signal Processing: Semantic Communication, Edge Computing, and Wireless Sensing","To process and transfer large amounts of data in emerging wireless services, it has become increasingly appealing to exploit distributed data communication and learning. Specifically, edge learning (EL) enables local model training on geographically disperse edge nodes and minimizes the need for frequent data exchange. However, the current design of separating EL deployment and communication optimization does not yet reap the promised benefits of distributed signal processing, and sometimes suffers from excessive signalling overhead, long processing delay, and unstable learning convergence. In this paper, we provide an overview on practical distributed EL techniques and their interplay with advanced communication optimization designs. In particular, typical performance metrics for dual-functional learning and communication networks are discussed. Also, recent achievements of enabling techniques for the dual-functional design are surveyed with exemplifications from the mutual perspectives of “communications for learning” and “learning for communications.” The application of EL techniques within a variety of future communication systems are also envisioned for beyond 5G (B5G) wireless networks. For the application in goal-oriented semantic communication, we present a first mathematical model of the goal-oriented source entropy as an optimization problem. In addition, from the viewpoint of information theory, we identify fundamental open problems of characterizing rate regions for communication networks supporting distributed learning-and-computing tasks. We also present technical challenges as well as emerging application opportunities in this field, with the aim of inspiring future research and promoting widespread developments of EL in B5G.",IOTNET,https://www.semanticscholar.org/paper/928b9ba41c90e9228d9ea8d25f229c5119f23e23
A Survey on the Convergence of Edge Computing and AI for UAVs: Opportunities and Challenges,"The latest 5G mobile networks have enabled many exciting Internet of Things (IoT) applications that employ unmanned aerial vehicles (UAVs/drones). The success of most UAV-based IoT applications is heavily dependent on artificial intelligence (AI) technologies, for instance, computer vision and path planning. These AI methods must process data and provide decisions while ensuring low latency and low energy consumption. However, the existing cloud-based AI paradigm finds it difficult to meet these strict UAV requirements. Edge AI, which runs AI on-device or on edge servers close to users, can be suitable for improving UAV-based IoT services. This article provides a comprehensive analysis of the impact of edge AI on key UAV technical aspects (i.e., autonomous navigation, formation control, power management, security and privacy, computer vision, and communication) and applications (i.e., delivery systems, civil infrastructure inspection, precision agriculture, search and rescue (SAR) operations, acting as aerial wireless base stations (BSs), and drone light shows). As guidance for researchers and practitioners, this article also explores UAV-based edge AI implementation challenges, lessons learned, and future research directions.",IOTNET,https://www.semanticscholar.org/paper/0c6c9c901d9c10639a12067804dc90c282fc0b92
Federated Learning in Edge Computing: A Systematic Survey,"Edge Computing (EC) is a new architecture that extends Cloud Computing (CC) services closer to data sources. EC combined with Deep Learning (DL) is a promising technology and is widely used in several applications. However, in conventional DL architectures with EC enabled, data producers must frequently send and share data with third parties, edge or cloud servers, to train their models. This architecture is often impractical due to the high bandwidth requirements, legalization, and privacy vulnerabilities. The Federated Learning (FL) concept has recently emerged as a promising solution for mitigating the problems of unwanted bandwidth loss, data privacy, and legalization. FL can co-train models across distributed clients, such as mobile phones, automobiles, hospitals, and more, through a centralized server, while maintaining data localization. FL can therefore be viewed as a stimulating factor in the EC paradigm as it enables collaborative learning and model optimization. Although the existing surveys have taken into account applications of FL in EC environments, there has not been any systematic survey discussing FL implementation and challenges in the EC paradigm. This paper aims to provide a systematic survey of the literature on the implementation of FL in EC environments with a taxonomy to identify advanced solutions and other open problems. In this survey, we review the fundamentals of EC and FL, then we review the existing related works in FL in EC. Furthermore, we describe the protocols, architecture, framework, and hardware requirements for FL implementation in the EC environment. Moreover, we discuss the applications, challenges, and related existing solutions in the edge FL. Finally, we detail two relevant case studies of applying FL in EC, and we identify open issues and potential directions for future research. We believe this survey will help researchers better understand the connection between FL and EC enabling technologies and concepts.",CLD,https://www.semanticscholar.org/paper/b587165627a09a90781ff0f816a2fc0b82a6b688
Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges,"Mobile-edge computing (MEC) has been envisioned as a promising paradigm to handle the massive volume of data generated from ubiquitous mobile devices for enabling intelligent services with the help of artificial intelligence (AI). Traditionally, AI techniques often require centralized data collection and training in a single entity, e.g., an MEC server, which is now becoming a weak point due to data privacy concerns and high overhead of raw data communications. In this context, federated learning (FL) has been proposed to provide collaborative data training solutions, by coordinating multiple mobile devices to train a shared AI model without directly exposing their underlying data, which enjoys considerable privacy enhancement. To improve the security and scalability of FL implementation, blockchain as a ledger technology is attractive for realizing decentralized FL training without the need for any central server. Particularly, the integration of FL and blockchain leads to a new paradigm, called FLchain, which potentially transforms intelligent MEC networks into decentralized, secure, and privacy-enhancing systems. This article presents an overview of the fundamental concepts and explores the opportunities of FLchain in MEC networks. We identify several main issues in FLchain design, including communication cost, resource allocation, incentive mechanism, security and privacy protection. The key solutions and the lessons learned along with the outlooks are also discussed. Then, we investigate the applications of FLchain in popular MEC domains, such as edge data sharing, edge content caching and edge crowdsensing. Finally, important research challenges and future directions are also highlighted.",SEC,https://www.semanticscholar.org/paper/a025ad05916ddf3281a2bca0cdffbaed99a2c9dd
"A Survey on Mobile Augmented Reality With 5G Mobile Edge Computing: Architectures, Applications, and Technical Aspects","The Augmented Reality (AR) technology enhances the human perception of the world by combining the real environment with the virtual space. With the explosive growth of powerful, less expensive mobile devices, and the emergence of sophisticated communication infrastructure, Mobile Augmented Reality (MAR) applications are gaining increased popularity. MAR allows users to run AR applications on mobile devices with greater mobility and at a lower cost. The emerging 5G communication technologies act as critical enablers for future MAR applications to achieve ultra-low latency and extremely high data rates while Multi-access Edge Computing (MEC) brings enhanced computational power closer to the users to complement MAR. This paper extensively discusses the landscape of MAR through the past and its future prospects with respect to the 5G systems and complementary technology MEC. The paper especially provides an informative analysis of the network formation of current and future MAR systems in terms of cloud, edge, localized, and hybrid architectural options. The paper discusses key application areas for MAR and their future with the advent of 5G technologies. The paper also discusses the requirements and limitations of MAR technical aspects such as communication, mobility management, energy management, service offloading and migration, security, and privacy and analyzes the role of 5G technologies.",IOTNET,https://www.semanticscholar.org/paper/107b09b10b352154c51af2f9e4e2e8ab3e0c0531
An Overview on Edge Computing Research,"With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today’s intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.",CLD,https://www.semanticscholar.org/paper/98b53b924793dc57a0b0e2f5fe6d546760786854
Resource Scheduling in Edge Computing: A Survey,"With the proliferation of the Internet of Things (IoT) and the wide penetration of wireless networks, the surging demand for data communications and computing calls for the emerging edge computing paradigm. By moving the services and functions located in the cloud to the proximity of users, edge computing can provide powerful communication, storage, networking, and communication capacity. The resource scheduling in edge computing, which is the key to the success of edge computing systems, has attracted increasing research interests. In this paper, we survey the state-of-the-art research findings to know the research progress in this field. Specifically, we present the architecture of edge computing, under which different collaborative manners for resource scheduling are discussed. Particularly, we introduce a unified model before summarizing the current works on resource scheduling from three research issues, including computation offloading, resource allocation, and resource provisioning. Based on two modes of operation, i.e., centralized and distributed modes, different techniques for resource scheduling are discussed and compared. Also, we summarize the main performance indicators based on the surveyed literature. To shed light on the significance of resource scheduling in real-world scenarios, we discuss several typical application scenarios involved in the research of resource scheduling in edge computing. Finally, we highlight some open research challenges yet to be addressed and outline several open issues as the future research direction.",IOTNET,https://www.semanticscholar.org/paper/79131641a44c6ec4e65c2385ff07f0aef586025c
Imitation Learning Enabled Task Scheduling for Online Vehicular Edge Computing,"Vehicular edge computing (VEC) is a promising paradigm based on the Internet of vehicles to provide computing resources for end users and relieve heavy traffic burden for cellular networks. In this paper, we consider a VEC network with dynamic topologies, unstable connections and unpredictable movements. Vehicles inside can offload computation tasks to available neighboring VEC clusters formed by onboard resources, with the purpose of both minimizing system energy consumption and satisfying task latency constraints. For online task scheduling, existing researches either design heuristic algorithms or leverage machine learning, e.g., deep reinforcement learning (DRL). However, these algorithms are not efficient enough because of their low searching efficiency and slow convergence speeds for large-scale networks. Instead, we propose an imitation learning enabled online task scheduling algorithm with near-optimal performance from the initial stage. Specially, an expert can obtain the optimal scheduling policy by solving the formulated optimization problem with a few samples offline. For online learning, we train agent policies by following the expert’s demonstration with an acceptable performance gap in theory. Performance results show that our solution has a significant advantage with more than 50 percent improvement compared with the benchmark.",IOTNET,https://www.semanticscholar.org/paper/ac227e930ee64b138843f6caa95bc70a7c282125
Service Offloading With Deep Q-Network for Digital Twinning-Empowered Internet of Vehicles in Edge Computing,"With the potential of implementing computing-intensive applications, edge computing is combined with digital twinning (DT)-empowered Internet of vehicles (IoV) to enhance intelligent transportation capabilities. By updating digital twins of vehicles and offloading services to edge computing devices (ECDs), the insufficiency in vehicles’ computational resources can be complemented. However, owing to the computational intensity of DT-empowered IoV, ECD would overload under excessive service requests, which deteriorates the quality of service (QoS). To address this problem, in this article, a multiuser offloading system is analyzed, where the QoS is reflected through the response time of services. Then, a service offloading (SOL) method with deep reinforcement learning, is proposed for DT-empowered IoV in edge computing. To obtain optimized offloading decisions, SOL leverages deep Q-network (DQN), which combines the value function approximation of deep learning and reinforcement learning. Eventually, experiments with comparative methods indicate that SOL is effective and adaptable in diverse environments.",IOTNET,https://www.semanticscholar.org/paper/4b6f86a369d0fe3df92903573ff531cfaf0a6ff6
Multi-Agent Deep Reinforcement Learning for Task Offloading in UAV-Assisted Mobile Edge Computing,"Mobile edge computing can effectively reduce service latency and improve service quality by offloading computation-intensive tasks to the edges of wireless networks. Due to the characteristic of flexible deployment, wide coverage and reliable wireless communication, unmanned aerial vehicles (UAVs) have been employed as assisted edge clouds (ECs) for large-scale sparely-distributed user equipment. Considering the limited computation and energy capacities of UAVs, a collaborative mobile edge computing system with multiple UAVs and multiple ECs is investigated in this paper. The task offloading issue is addressed to minimize the sum of execution delays and energy consumptions by jointly designing the trajectories, computation task allocation, and communication resource management of UAVs. Moreover, to solve the above non-convex optimization problem, a Markov decision process is formulated for the multi-UAV assisted mobile edge computing system. To obtain the joint strategy of trajectory design, task allocation, and power management, a cooperative multi-agent deep reinforcement learning framework is investigated. Considering the high-dimensional continuous action space, the twin delayed deep deterministic policy gradient algorithm is exploited. The evaluation results demonstrate that our multi-UAV multi-EC task offloading method can achieve better performance compared with the other optimization approaches.",IOTNET,https://www.semanticscholar.org/paper/407ea590dd355d56e7825c170d21dc26c6a32416
"Edge Computing in Industrial Internet of Things: Architecture, Advances and Challenges","The Industrial Internet of Things (IIoT) is a crucial research field spawned by the Internet of Things (IoT). IIoT links all types of industrial equipment through the network; establishes data acquisition, exchange, and analysis systems; and optimizes processes and services, so as to reduce cost and enhance productivity. The introduction of edge computing in IIoT can significantly reduce the decision-making latency, save bandwidth resources, and to some extent, protect privacy. This paper outlines the research progress concerning edge computing in IIoT. First, the concepts of IIoT and edge computing are discussed, and subsequently, the research progress of edge computing is discussed and summarized in detail. Next, the future architecture from the perspective of edge computing in IIoT is proposed, and its technical progress in routing, task scheduling, data storage and analytics, security, and standardization is analyzed. Furthermore, we discuss the opportunities and challenges of edge computing in IIoT in terms of 5G-based edge communication, load balancing and data offloading, edge intelligence, as well as data sharing security. Finally, we introduce some typical application scenarios of edge computing in IIoT, such as prognostics and health management (PHM), smart grids, manufacturing coordination, intelligent connected vehicles (ICV), and smart logistics.",IOTNET,https://www.semanticscholar.org/paper/6058eb8a9282592eb8322e81cddaa1e379bb347e
Edge Computing for Internet of Everything: A Survey,"In this era of the Internet of Everything (IoE), edge computing has emerged as the critical enabling technology to solve a series of issues caused by an increasing amount of interconnected devices and large-scale data transmission. However, the deficiencies of edge computing paradigm are gradually being magnified in the context of IoE, especially in terms of service migration, security and privacy preservation, and deployment issues of edge node. These issues can not be well addressed by conventional approaches. Thanks to the rapid development of upcoming technologies, such as artificial intelligence (AI), blockchain, and microservices, novel and more effective solutions have emerged and been applied to solve existing challenges. In addition, edge computing can be deeply integrated with technologies in other domains (e.g., AI, blockchain, 6G, and digital twin) through interdisciplinary intersection and practice, releasing the potential for mutual benefit. These promising integrations need to be further explored and researched. In addition, edge computing provides strong support in applications scenarios, such as remote working, new physical retail industries, and digital advertising, which has greatly changed the way we live, work, and study. In this article, we present an up-to-date survey of the edge computing research. In addition to introducing the definition, model, and characteristics of edge computing, we discuss a set of key issues in edge computing and novel solutions supported by emerging technologies in IoE era. Furthermore, we explore the potential and promising trends from the perspective of technology integration. Finally, new application scenarios and the final form of edge computing are discussed.",IOTNET,https://www.semanticscholar.org/paper/423bc3c4cd825e1320557bf1de874254297ef2c2
Edge-computing-driven Internet of Things: A Survey,"The Internet of Things (IoT) is impacting the world’s connectivity landscape. More and more IoT devices are connected, bringing many benefits to our daily lives. However, the influx of IoT devices poses non-trivial challenges for the existing cloud-based computing paradigm. In the cloud-based architecture, a large amount of IoT data is transferred to the cloud for data management, analysis, and decision making. It could not only cause a heavy workload on the cloud but also result in unacceptable network latency, ultimately undermining the benefits of cloud-based computing. To address these challenges, researchers are looking for new computing models for the IoT. Edge computing, a new decentralized computing model, is valued by more and more researchers in academia and industry. The main idea of edge computing is placing data processing in near-edge devices instead of remote cloud servers. It is promising to build more scalable, low-latency IoT systems. Many studies have been proposed on edge computing and IoT, but a comprehensive survey of this crossover area is still lacking. In this survey, we first introduce the impact of edge computing on the development of IoT and point out why edge computing is more suitable for IoT than other computing paradigms. Then, we analyze the necessity of systematical investigation on the edge-computing-driven IoT (ECDriven-IoT) and summarize new challenges occurring in ECDriven-IoT. We categorize recent advances from bottom to top, covering six aspects of ECDriven-IoT. Finally, we conclude lessons learned and propose some challenging",IOTNET,https://www.semanticscholar.org/paper/fc3a6dcf60df73ec735dd0723b39875d3a6bb07e
Energy-Efficient UAV-Assisted Mobile Edge Computing: Resource Allocation and Trajectory Optimization,"In this paper, we study unmanned aerial vehicle (UAV) assisted mobile edge computing (MEC) with the objective to optimize computation offloading with minimum UAV energy consumption. In the considered scenario, a UAV plays the role of an aerial cloudlet to collect and process the computation tasks offloaded by ground users. Given the service requirements of users, we aim to maximize UAV energy efficiency by jointly optimizing the UAV trajectory, the user transmit power, and computation load allocation. The resulting optimization problem corresponds to nonconvex fractional programming, and the Dinkelbach algorithm and the successive convex approximation (SCA) technique are adopted to solve it. Furthermore, we decompose the problem into multiple subproblems for distributed and parallel problem solving. To cope with the case when the knowledge of user mobility is limited, we adopt a spatial distribution estimation technique to predict the location of ground users so that the proposed approach can still be applied. Simulation results demonstrate the effectiveness of the proposed approach for maximizing the energy efficiency of UAV.",IOTNET,https://www.semanticscholar.org/paper/61b559cb6d4c843d19e7f5340b88d018bb8594cc
TCDA: Truthful Combinatorial Double Auctions for Mobile Edge Computing in Industrial Internet of Things,"Mobile edge computing (MEC) emerges as an appealing paradigm to provide time-sensitive computing services for industrial Internet of Things (IIoT) applications. How to guarantee truthfulness and budget-balance under locality constraints is an important issue to the allocation and pricing design of the MEC system. In this paper, we propose a truthful combinatorial double auction mechanism, which integrates the padding concept and the efficient pricing strategy to guarantee desirable properties in constrained MEC environments. This mechanism takes into account the locality characteristics of the MEC systems, where mobile devices (MDs) only offload tasks to edge servers (ESs) in the proximity with various requirements, and ESs only serve their neighboring MDs with limited resources. To be specific, for allocation, a linear programming (LP)-based padding method is used to obtain the near-optimal solution in the polynomial time. For pricing, a critical-value-based pricing strategy and a VCG-based pricing strategy are designed for MDs and ESs to achieve truthfulness and budget-balance. Our theoretical analysis confirms that TCDA is able to hold a set of desirable economic properties, including truthfulness, individual rationality, and budget-balance. Furthermore, simulation results validate the theoretical analysis, and verify the effectiveness and efficiency of TCDA.",IOTNET,https://www.semanticscholar.org/paper/f351672725b443dc707e26a092435937de0be235
Toward Edge Intelligence: Multiaccess Edge Computing for 5G and Internet of Things,"To satisfy the increasing demand of mobile data traffic and meet the stringent requirements of the emerging Internet-of-Things (IoT) applications such as smart city, healthcare, and augmented/virtual reality (AR/VR), the fifth-generation (5G) enabling technologies are proposed and utilized in networks. As an emerging key technology of 5G and a key enabler of IoT, multiaccess edge computing (MEC), which integrates telecommunication and IT services, offers cloud computing capabilities at the edge of the radio access network (RAN). By providing computational and storage resources at the edge, MEC can reduce latency for end users. Hence, this article investigates MEC for 5G and IoT comprehensively. It analyzes the main features of MEC in the context of 5G and IoT and presents several fundamental key technologies which enable MEC to be applied in 5G and IoT, such as cloud computing, software-defined networking/network function virtualization, information-centric networks, virtual machine (VM) and containers, smart devices, network slicing, and computation offloading. In addition, this article provides an overview of the role of MEC in 5G and IoT, bringing light into the different MEC-enabled 5G and IoT applications as well as the promising future directions of integrating MEC with 5G and IoT. Moreover, this article further elaborates research challenges and open issues of MEC for 5G and IoT. Last but not least, we propose a use case that utilizes MEC to achieve edge intelligence in IoT scenarios.",IOTNET,https://www.semanticscholar.org/paper/4df6e4c2f0a05699b0952ded71315c394933b349
Digital Twin-Aided Intelligent Offloading With Edge Selection in Mobile Edge Computing,"In this letter, we study a mobile edge computing (MEC) architecture with the assistance of digital twin (DT) applied for industrial automation where multiple Internet-of-Things (IoT) devices intelligently offload computing tasks to multiple MEC servers to reduce end-to-end latency. To do so, first we propose and formulate a practical end-to-end latency minimization problem in the DT-assisted MEC model subject to the constraints of quality-of-services and computation resource at the IoT devices and MEC servers in industrial IoT networks. Then, we solve the proposed latency minimization problem by iteratively optimizing the transmit power of IoT devices, user association, intelligent task offloading, and estimated CPU processing rate of the devices. Finally, simulation results are conducted to prove the effectiveness of the proposed method in terms of the latency performance compared with some conventional methods.",IOTNET,https://www.semanticscholar.org/paper/7c3c5c9ff38a00436a141e118c5f694b9a137304
Federated Learning in Vehicular Edge Computing: A Selective Model Aggregation Approach,"Federated learning is a newly emerged distributed machine learning paradigm, where the clients are allowed to individually train local deep neural network (DNN) models with local data and then jointly aggregate a global DNN model at the central server. Vehicular edge computing (VEC) aims at exploiting the computation and communication resources at the edge of vehicular networks. Federated learning in VEC is promising to meet the ever-increasing demands of artificial intelligence (AI) applications in intelligent connected vehicles (ICV). Considering image classification as a typical AI application in VEC, the diversity of image quality and computation capability in vehicular clients potentially affects the accuracy and efficiency of federated learning. Accordingly, we propose a selective model aggregation approach, where “fine” local DNN models are selected and sent to the central server by evaluating the local image quality and computation capability. Regarding the implementation of model selection, the central server is not aware of the image quality and computation capability in the vehicular clients, whose privacy is protected under such a federated learning framework. To overcome this information asymmetry, we employ two-dimension contract theory as a distributed framework to facilitate the interactions between the central server and vehicular clients. The formulated problem is then transformed into a tractable problem through successively relaxing and simplifying the constraints, and eventually solved by a greedy algorithm. Using two datasets, i.e., MNIST and BelgiumTSC, our selective model aggregation approach is demonstrated to outperform the original federated averaging (FedAvg) approach in terms of accuracy and efficiency. Meanwhile, our approach also achieves higher utility at the central server compared with the baseline approaches.",IOTNET,https://www.semanticscholar.org/paper/8d5ed7038d9b08ad49559d7ef0abe90949e51391
Convergence of Edge Computing and Deep Learning: A Comprehensive Survey,"Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people’s lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.",IOTNET,https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d
Dependency-Aware Computation Offloading for Mobile Edge Computing With Edge-Cloud Cooperation,"Most of existing Multi-access edge computing (MEC) studies consider the remote cloud server as a special edge server, the opportunity of edge-cloud collaboration has not been well exploited. We propose a dependency-aware offloading scheme in MEC with edge-cloud cooperation under task dependency constraints. Each mobile device has a limited budget and has to determine which sub-task should be computed locally or should be sent to the edge or remote cloud. To address this issue, we divide the offloading problem into two application finishing time minimization sub-problems with two different cooperation modes, both of which are proved to be NP-hard. We then devise one greedy algorithm with approximation ratio of <inline-formula><tex-math notation=""LaTeX"">$1+\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq1-3037306.gif""/></alternatives></inline-formula> for the first mode with edge-cloud cooperation but no edge-edge cooperation. Then we design an efficient greedy algorithm for the second mode, considering both edge-cloud and edge-edge co-operations. Extensive simulation results show that for the first mode, the proposed greedy algorithm achieves near optimal performance for typical task topologies. On average, it outperforms the modified Hermes benchmark algorithm by about <inline-formula><tex-math notation=""LaTeX"">$23\%\sim 43.6\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>23</mml:mn><mml:mo>%</mml:mo><mml:mo>∼</mml:mo><mml:mn>43</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq2-3037306.gif""/></alternatives></inline-formula> in terms of application finishing time with given budgets. By further exploiting collaborations among edge servers in the second cooperation mode, the proposed algorithm helps to achieve over 20.3 percent average performance gain on the application finishing time over the first mode under various scenarios. Real-world experiments comply with simulation results.",CLD,https://www.semanticscholar.org/paper/826eef1019226873443e6ad488a4795e74ea2c63
Mobile Edge Computing,"Mobile edge computing is a promising paradigm that brings computing resources to mobile users at the network edge, allowing computing-intensive and delay-sensitive applications to be quickly processed by edge servers to satisfy the requirements of mobile users. In this chapter, we first introduce a hierarchical architecture of mobile edge computing that consists of a cloud plane, an edge plane, and a user plane. We then introduce three typical computation offloading decisions. Finally, we review state-of-the-art works on computation offloading and present the use case of joint computation offloading.",IOTNET,https://www.semanticscholar.org/paper/288062000d7bd7825533693d557de2ca8628dc56
Integration of Blockchain and Edge Computing in Internet of Things: A Survey,"As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the integration of blockchain and edge computing. In particular, we first give an overview of blockchain and edge computing. We then present a general architecture of an integration of blockchain and edge computing system. We next study how to utilize blockchain to benefit edge computing, as well as how to use edge computing to benefit blockchain. We also discuss the issues brought by the integration of blockchain and edge computing system and solutions from perspectives of resource management, joint optimization, data management, computation offloading and security mechanism. Finally, we analyze and summarize the existing challenges posed by the integration of blockchain and edge computing system and the potential solutions in the future.",IOTNET,https://www.semanticscholar.org/paper/2767b3704d1de9a6cfc100ef208f3ff2089bed9c
Edge computing,"IoT edge computing is a new computing paradigm “in the IoT domain” for performing calculations and processing at the edge of the network, closer to the user and the source of the data. This paradigm is relatively recent, and, together with cloud and fog computing, there may be some confusion about its meaning and implications. This paper aims to help practitioners and researchers better understand what the industry thinks about what IoT edge computing is, and the expected benefits and challenges associated with this paradigm. We conducted a survey using a semi-structured in-depth questionnaire to collect qualitative data from relevant stakeholders from 29 multinational companies and qualitatively analyzed these data using the Constructivist Grounded Theory (Charmaz) method. Several researchers participated in the coding process (collaborative coding). To ensure consensus on the constructs that support the theory and thus improve the rigor of qualitative research, we conducted an intercoder agreement analysis. From the analysis, we have derived a substantive and analytic theory of what companies perceive about IoT edge computing, its benefits and challenges. The theory is substantive in that the scope of validity refers to the 29 surveys processed and analytic in that it analyzes “what is” rather than explaining causality or attempting predictive generalizations. A public repository with all the data related to the information capture process and the products resulting from the analysis of this information is publicly available. This study aims to strengthen the evidence and support practitioners in making better informed decisions about why companies are adopting edge computing and the current challenges they face. Additionally, the testing theory phase shows that the results are aligned with the ISO/IEC TR 30164 standard.",IOTNET,https://www.semanticscholar.org/paper/e23e9925db038da63bb761fbada4a5fc1ff59874
Profit Maximization Incentive Mechanism for Resource Providers in Mobile Edge Computing,"Mobile edge computing (MEC) has become a promising technique to accommodate demands of resource-constrained mobile devices by offloading the task onto edge clouds nearby. However, most existing works only focus on whether to offload or where to offload the task but ignore the motivations of edge clouds to offer service. To stimulate service provisioning by edge clouds, it is essential to design an incentive mechanism that charges mobile devices and rewards edge clouds. In this paper, we first propose an incentive mechanism in a non-competitive environment. We utilize market-based profit maximization pricing model to establish the relationship between the resources provided by edge clouds and the price charged to mobile devices. By solving the optimization problem, we provide a reasonable pricing strategy to not only ensure the profit of resource providers but guarantee the quality of experience (QoE) of mobile devices. Furthermore, we design an online profit maximization multi-round auction (PMMRA) mechanism for the resource trading between edge clouds as sellers and mobile devices as buyers in a competitive environment. The mechanism can effectively determine the price paid by buyers to use the resources provided by sellers and make the corresponding match between edge clouds and mobile devices. Finally, numerical results show that proposed mechanism outperforms other existing algorithms in maximizing the profit of edge clouds.",CLD,https://www.semanticscholar.org/paper/74932f94437281d8700e64c1d6378826085430a9
"Computation Offloading in Mobile Cloud Computing and Mobile Edge Computing: Survey, Taxonomy, and Open Issues","Cloud and mobile edge computing (MEC) provides a wide range of computing services for mobile applications. In particular, mobile edge computing enables a computing and storage infrastructure provisioned closely to the end-users at the edge of a cellular network. The small base stations are deployed to establish a mobile edge network that can be coined with cloud infrastructure. A large number of enterprises and individuals rely on services offered by mobile edge and clouds to meet their computational and storage demands. Based on user behavior and demand, the computational tasks are first offloaded from mobile users to the mobile edge network and then executed at one or several specific base stations in the mobile edge network. The MEC architecture has the capability to handle a large number of devices that in turn generate high volumes of traffic. In this work, we first provide a holistic overview of MCC/MEC technology that includes the background and evolution of remote computation technologies. Then, the main part of this paper surveys up-to-date research on the concepts of offloading mechanisms, offloading granularities, and computational offloading techniques. Furthermore, we discuss the offloading mechanism in the static and dynamic environment along with optimization techniques. We further discuss the challenges and potential future directions for MEC research.",CLD,https://www.semanticscholar.org/paper/4dea3b773a82b00bc758701f0e3da62ce7b5a85b
Cost-Effective App User Allocation in an Edge Computing Environment,"Edge computing is a new distributed computing paradigm extending the cloud computing paradigm, offering much lower end-to-end latency, as real-time, latency-sensitive applications can now be deployed on edge servers that are much closer to end-users than distant cloud servers. In edge computing, edge user allocation (EUA) is a critical problem for any app vendors, who need to determine which edge servers will serve which users. This is to satisfy application-specific optimization objectives, e.g., maximizing users’ overall quality of experience, minimizing system costs, and so on. In this article, we focus on the cost-effectiveness of user allocation solutions with two optimization objectives. The primary one is to maximize the number of users allocated to edge servers. The secondary one is to minimize the number of required edge servers, which subsequently reduces the operating costs for app vendors. We first model this problem as a bin packing problem and introduce an approach for finding optimal solutions. However, finding optimal solutions to the <inline-formula><tex-math notation=""LaTeX"">$\mathcal {NP}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""script"">NP</mml:mi></mml:math><inline-graphic xlink:href=""lai-ieq1-3001570.gif""/></alternatives></inline-formula>-hard EUA problem in large-scale scenarios is intractable. Thus, we propose a heuristic to efficiently find sub-optimal solutions to large-scale EUA problems. Extensive experiments conducted on real-world data demonstrate that our heuristic can solve the EUA problem effectively and efficiently, outperforming the state-of-the-art and baseline approaches.",CLD,https://www.semanticscholar.org/paper/ff651010ce5dc9d64262fd552f937a92f883f3b8
Adaptive Digital Twin and Multiagent Deep Reinforcement Learning for Vehicular Edge Computing and Networks,"Technological advancements of urban informatics and vehicular intelligence have enabled connected smart vehicles as pervasive edge computing platforms for a plethora of powerful applications. However, varies types of smart vehicles with distinct capacities, diverse applications with different resource demands as well as unpredictive vehicular topology, pose significant challenges on realizing efficient edge computing services. To cope with these challenges, we incorporate digital twin technology and artificial intelligence into the design of a vehicular edge computing network. It centrally exploits potential edge service matching through evaluating cooperation gains in a mirrored edge computing system, while distributively scheduling computation task offloading and edge resource allocation in an multiagent deep reinforcement learning approach. We further propose a coordination graph driven vehicular task offloading scheme, which minimizes offloading costs through efficiently integrating service matching exploitation and intelligent offloading scheduling in both digital twin and physical networks. Numerical results based on real urban traffic datasets demonstrate the efficiency of our proposed schemes.",IOTNET,https://www.semanticscholar.org/paper/7fc6d13b415d48207cd94c608bdc1b377048080b
Retention-Aware Container Caching for Serverless Edge Computing,"Serverless edge computing adopts an event-based model where Internet-of-Things (IoT) services are executed in lightweight containers only when requested, leading to significantly improved edge resource utilization. Unfortunately, the startup latency of containers degrades the responsiveness of IoT services dramatically. Container caching, while masking this latency, requires retaining resources thus compromising resource efficiency. In this paper, we study the retention-aware container caching problem in serverless edge computing. We leverage the distributed and heterogeneous nature of edge platforms and propose to optimize container caching jointly with request distribution. We reveal step by step that this joint optimization problem can be mapped to the classic ski-rental problem. We first present an online competitive algorithm for a special case where request distribution and container caching are based on a set of carefully designed probability distribution functions. Based on this algorithm, we propose an online algorithm called O-RDC for the general case, which incorporates the resource capacity and network latency by opportunistically distributing requests. We conduct extensive experiments to examine the performance of the proposed algorithms with both synthetic and real-world serverless computing traces. Our results show that ORDC outperforms existing caching strategies of current serverless computing platforms by up to 94.5% in terms of the overall system cost.",CLD,https://www.semanticscholar.org/paper/ba17c5c3967e697602d89f7f54c4fd3eb86d3e17
Service Coverage for Satellite Edge Computing,"Recently, increasing investments in satellite-related technologies make the low earth orbit (LEO) satellite constellation a strong complement to terrestrial networks. To mitigate the limitations of the traditional satellite constellation “bent-pipe” architecture, satellite edge computing (SEC) has been proposed by placing computing resources at the LEO satellite constellation. Most existing works focus on space-air-ground integrated network architecture and SEC computing framework. Beyond these works, we are the first to investigate how to efficiently deploy services on the SEC nodes to realize robustness aware service coverage with constrained resources. Facing the challenges of spatial-temporal system dynamics and service coverage-robustness conflict, we propose a novel online service placement algorithm with a theoretical performance guarantee by leveraging Lyapunov optimization and Gibbs sampling. Extensive simulation results show that our algorithm can improve the service coverage by $4.3\times $ compared with the baseline.",IOTNET,https://www.semanticscholar.org/paper/017f5d664915080d088159b6b9a7e2a8d90ae0cd
Convergence of Blockchain and Edge Computing for Secure and Scalable IIoT Critical Infrastructures in Industry 4.0,"Critical infrastructure systems are vital to underpin the functioning of a society and economy. Due to the ever-increasing number of Internet-connected Internet-of-Things (IoT)/Industrial IoT (IIoT), and the high volume of data generated and collected, security and scalability are becoming burning concerns for critical infrastructures in industry 4.0. The blockchain technology is essentially a distributed and secure ledger that records all the transactions into a hierarchically expanding chain of blocks. Edge computing brings the cloud capabilities closer to the computation tasks. The convergence of blockchain and edge computing paradigms can overcome the existing security and scalability issues. In this article, we first introduce the IoT/IIoT critical infrastructure in industry 4.0, and then we briefly present the blockchain and edge computing paradigms. After that, we show how the convergence of these two paradigms can enable secure and scalable critical infrastructures. Then, we provide a survey on the state of the art for security and privacy and scalability of IoT/IIoT critical infrastructures. A list of potential research challenges and open issues in this area is also provided, which can be used as useful resources to guide future research.",IOTNET,https://www.semanticscholar.org/paper/7acae7d7462c4ee78e26e71553cd56897d012b6c
Computation Offloading in LEO Satellite Networks With Hybrid Cloud and Edge Computing,"Low earth orbit (LEO) satellite networks can break through geographical restrictions and achieve global wireless coverage, which is an indispensable choice for future mobile communication systems. In this article, we present a hybrid cloud and edge computing LEO satellite (CECLS) network with a three-tier computation architecture, which can provide ground users with heterogeneous computation resources and enable ground users to obtain computation services around the world. With the CECLS architecture, we investigate the computation offloading decisions to minimize the sum energy consumption of ground users, while satisfying the constraints in terms of the coverage time and the computation capability of each LEO satellite. The considered problem leads to a discrete and nonconvex since the objective function and constraints contain binary variables, which makes it difficult to solve. To address this challenging problem, we convert the original nonconvex problem into a linear programming problem by using the binary variables relaxation method. Then, we propose a distributed algorithm by leveraging the alternating direction method of multipliers (ADMMs) to approximate the optimal solution with low computational complexity. Simulation results show that the proposed algorithm can effectively reduce the total energy consumption of ground users.",CLD,https://www.semanticscholar.org/paper/bbb315f7ac94dd538799a1fe4e894def8fc498aa
An Online Framework for Joint Network Selection and Service Placement in Mobile Edge Computing,"With the rapid development and deployment of 5G wireless technology, mobile edge computing (MEC) has emerged as a new computing paradigm to facilitate a large variety of infrastructures at the network edge to reduce user-perceived communication delay. One of the fundamental problems in this new paradigm is to preserve satisfactory quality-of-service (QoS) for mobile users in light of densely dispersed wireless communication environment and often capacity-constrained MEC nodes. Such user-perceived QoS, typically in terms of the end-to-end delay, is highly vulnerable to both access network bottleneck and communication delay. Previous works have primarily focused on optimizing the communication delay through dynamic service placement, while ignoring the critical effect of access network selection on the access delay. In this work, we study the problem of jointly optimizing the access network selection and service placement for MEC, with the objective of improving the QoS in a cost-efficient manner by judiciously balancing the access delay, communication delay, and service switching cost. Specifically, we propose an efficient online framework to decompose a long-term time-varying optimization problem into a series of one-shot subproblems. To address the NP-hardness of the one-shot problem, we design a computationally-efficient two-phase algorithm based on matching and game theory, which achieves a near-optimal solution. Both rigorous theoretical analysis on the optimality gap and extensive trace-driven simulations are conducted to validate the efficacy of our proposed solution.",IOTNET,https://www.semanticscholar.org/paper/15d9679e489dec8e8f0d720b1d2a18a8be503294
Deep Reinforcement Learning-Based Workload Scheduling for Edge Computing,"Edge computing is a new paradigm for providing cloud computing capacities at the edge of network near mobile users. It offers an effective solution to help mobile devices with computation-intensive and delay-sensitive tasks. However, the edge of network presents a dynamic environment with large number of devices, high mobility of users, heterogeneous applications and intermittent traffic. In such environment, edge computing often suffers from unbalance resource allocation, which leads to task failure and affects system performance. To tackle this problem, we proposed a deep reinforcement learning(DRL)-based workload scheduling approach with the goal of balancing the workload, reducing the service time and the failed task rate. Meanwhile, We adopt Deep-Q-Network(DQN) algorithms to solve the complexity and high dimension of workload scheduling problem. Simulation results show that our proposed approach achieves the best performance in aspects of service time, virtual machine(VM) utilization, and failed tasks rate compared with other approaches. Our DRL-based approach can provide an efficient solution to the workload scheduling problem in edge computing.",IOTNET,https://www.semanticscholar.org/paper/0ff5f23f28933a05e1896c08dc4ee6b445b71e98
Pyramid: Enabling Hierarchical Neural Networks with Edge Computing,"Machine learning (ML) is powering a rapidly-increasing number of web applications. As a crucial part of 5G, edge computing facilitates edge artificial intelligence (AI) by ML model training and inference at the network edge on edge servers. Compared with centralized cloud AI, edge AI enables low-latency ML inference which is critical to many delay-sensitive web applications, e.g., web AR/VR, web gaming and Web-of-Things applications. Existing studies of edge AI focused on resource and performance optimization in training and inference, leveraging edge computing merely as a tool to accelerate training and inference processes. However, the unique ability of edge computing to process data with context awareness, a powerful feature for building the web-of-things for smart cities, has not been properly explored. In this paper, we propose a novel framework named Pyramid that unleashes the potential of edge AI by facilitating homogeneous and heterogeneous hierarchical ML inferences. We motivate and present Pyramid with traffic prediction as an illustrative example, and evaluate it through extensive experiments conducted on two real-world datasets. The results demonstrate the superior performance of Pyramid neural networks in hierarchical traffic prediction and weather analysis.",IOTNET,https://www.semanticscholar.org/paper/a5e537fe6d9d9a15dc7e1e9d723de10a66f1cff8
Deep Learning With Edge Computing: A Review,"Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices, such as smartphones and Internet-of-Things sensors, are generating data that need to be analyzed in real time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a fine mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low-latency requirements of deep learning on edge devices and also provides additional benefits in terms of privacy, bandwidth efficiency, and scalability. This paper aims to provide a comprehensive review of the current state of the art at the intersection of deep learning and edge computing. Specifically, it will provide an overview of applications where deep learning is used at the network edge, discuss various approaches for quickly executing deep learning inference across a combination of end devices, edge servers, and the cloud, and describe the methods for training deep learning models across multiple edge devices. It will also discuss open challenges in terms of systems performance, network technologies and management, benchmarks, and privacy. The reader will take away the following concepts from this paper: understanding scenarios where deep learning at the network edge can be useful, understanding common techniques for speeding up deep learning inference and performing distributed training on edge devices, and understanding recent trends and opportunities.",IOTNET,https://www.semanticscholar.org/paper/4f2d4e821dd03ac5df7d5448948bc738aefdd6db
Orbital Edge Computing: Nanosatellite Constellations as a New Class of Computer System,"Advances in nanosatellite technology and a declining cost of access to space have fostered an emergence of large constellations of sensor-equipped satellites in low-Earth orbit. Many of these satellite systems operate under a ""bent-pipe"" architecture, in which ground stations send commands to orbit and satellites reply with raw data. In this work, we observe that a bent-pipe architecture for Earth-observing satellites breaks down as constellation population increases. Communication is limited by the physical configuration and constraints of the system over time, such as ground station location, nanosatellite antenna size, and energy harvested on orbit. We show quantitatively that nanosatellite constellation capabilities are determined by physical system constraints. We propose an Orbital Edge Computing (OEC) architecture to address the limitations of a bent-pipe architecture. OEC supports edge computing at each camera-equipped nanosatellite so that sensed data may be processed locally when downlinking is not possible. In order to address edge processing latencies, OEC systems organize satellite constellations into computational pipelines. These pipelines parallelize both data collection and data processing based on geographic location and without the need for cross-link coordination. OEC satellites explicitly model constraints of the physical environment via a runtime service. This service uses orbit parameters, physical models, and ground station positions to trigger data collection, predict energy availability, and prepare for communication. We show that an OEC architecture can reduce ground infrastructure over 24x compared to a bent-pipe architecture, and we show that pipelines can reduce system edge processing latency over 617x.",IOTNET,https://www.semanticscholar.org/paper/89ee7b075f148f172bfab9b2563a9cf6a5bdfc65
Dynamic Computation Offloading for Mobile-Edge Computing With Energy Harvesting Devices,"Mobile-edge computing (MEC) is an emerging paradigm to meet the ever-increasing computation demands from mobile applications. By offloading the computationally intensive workloads to the MEC server, the quality of computation experience, e.g., the execution latency, could be greatly improved. Nevertheless, as the on-device battery capacities are limited, computation would be interrupted when the battery energy runs out. To provide satisfactory computation performance as well as achieving green computing, it is of significant importance to seek renewable energy sources to power mobile devices via energy harvesting (EH) technologies. In this paper, we will investigate a green MEC system with EH devices and develop an effective computation offloading strategy. The execution cost, which addresses both the execution latency and task failure, is adopted as the performance metric. A low-complexity online algorithm is proposed, namely, the Lyapunov optimization-based dynamic computation offloading algorithm, which jointly decides the offloading decision, the CPU-cycle frequencies for mobile execution, and the transmit power for computation offloading. A unique advantage of this algorithm is that the decisions depend only on the current system state without requiring distribution information of the computation task request, wireless channel, and EH processes. The implementation of the algorithm only requires to solve a deterministic problem in each time slot, for which the optimal solution can be obtained either in closed form or by bisection search. Moreover, the proposed algorithm is shown to be asymptotically optimal via rigorous analysis. Sample simulation results shall be presented to corroborate the theoretical analysis as well as validate the effectiveness of the proposed algorithm.",IOTNET,https://www.semanticscholar.org/paper/ba2c3ee8422f7cb28d184d0abd7ca6c27669ce8a
A Survey on the Edge Computing for the Internet of Things,"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge,” near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.",IOTNET,https://www.semanticscholar.org/paper/f863f0aaea94656807d21b66ba786137f5e28e1f
Survey on Multi-Access Edge Computing Security and Privacy,"The European Telecommunications Standards Institute (ETSI) has introduced the paradigm of Multi-Access Edge Computing (MEC) to enable efficient and fast data processing in mobile networks. Among other technological requirements, security and privacy are significant factors in the realization of MEC deployments. In this paper, we analyse the security and privacy of the MEC system. We introduce a thorough investigation of the identification and the analysis of threat vectors in the ETSI standardized MEC architecture. Furthermore, we analyse the vulnerabilities leading to the identified threat vectors and propose potential security solutions to overcome these vulnerabilities. The privacy issues of MEC are also highlighted, and clear objectives for preserving privacy are defined. Finally, we present future directives to enhance the security and privacy of MEC services.",SEC,https://www.semanticscholar.org/paper/b9c8611f41a11e95903c6e46b8468d915c89e6a1
A Survey of Recent Advances in Edge-Computing-Powered Artificial Intelligence of Things,"The Internet of Things (IoT) has created a ubiquitously connected world powered by a multitude of wired and wireless sensors generating a variety of heterogeneous data over time in a myriad of fields and applications. To extract complete information from these data, advanced artificial intelligence (AI) technology, especially deep learning (DL), has proved successful in facilitating data analytics, future prediction and decision making. The collective integration of AI and the IoT has greatly promoted the rapid development of AI-of-Things (AIoT) systems that analyze and respond to external stimuli more intelligently without involvement by humans. However, it is challenging or infeasible to process massive amounts of data in the cloud due to the destructive impact of the volume, velocity, and veracity of data and fatal transmission latency on networking infrastructures. These critical challenges can be adequately addressed by introducing edge computing. This article conducts an extensive survey of an end-edge-cloud orchestrated architecture for flexible AIoT systems. Specifically, it begins with articulating fundamental concepts including the IoT, AI and edge computing. Guided by these concepts, it explores the general AIoT architecture, presents a practical AIoT example to illustrate how AI can be applied in real-world applications and summarizes promising AIoT applications. Then, the emerging technologies for AI models regarding inference and training at the edge of the network are reviewed. Finally, the open challenges and future directions in this promising area are outlined.",IOTNET,https://www.semanticscholar.org/paper/32ed6e8770f643251aa93b8e457b22773221a666
Collaborative Cloud-Edge-End Task Offloading in Mobile-Edge Computing Networks With Limited Communication Capability,"Mobile edge computing (MEC) is an emerging computing paradigm for enabling low-latency, high-bandwidth and agile mobile services by deploying computing platform at the edge of network. In order to improve the cloud-edge-end processing efficiency of the tasks within the limited computation and communication capabilities, in this article, we investigate the collaborative computation offloading, computation and communication resource allocation scheme, and develop a collaborative computing framework that the tasks of mobile devices (MDs) can be partially processed at the terminals, edge nodes (EN) and cloud center (CC). Then, we propose the pipeline-based offloading scheme, where both MDs and ENs can offload computation-intensive tasks to a particular EN and CC, according to their computation and communication capacities, respectively. Based on the proposed pipeline offloading strategy, a sum latency of all MDs minimization problem is formulated with the consideration of the offloading strategy, computation resource, delivery rate and power allocation, which is a non-convex problem and difficult to deal with. To solve the optimization problem, by using the classic successive convex approximation (SCA) approach, we transform the non-convex optimization problem into the convex one. Finally, simulation results indicate that the proposed collaboration offloading scheme with the pipeline strategy is efficient and outperforms other offloading schemes.",CLD,https://www.semanticscholar.org/paper/2b41c459fe7041d4de00b66f9fa27dfca142a9ae
PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems,"Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker’s well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.",IOTNET,https://www.semanticscholar.org/paper/6e6e0979c07d3bc686af12b015f428790e05a6f7
Multi Objective Prioritized Workflow Scheduling Using Deep Reinforcement Based Learning in Cloud Computing,"Workflow Scheduling is a huge challenge in cloud paradigm as many number of workflows dynamically generated from various heterogeneous resources and task dependencies in each workflow varies from each other. Therefore, if a workflow with more number of dependencies is not scheduled onto an appropriate Virtual Machine i.e. with low processing capacity which leads to delay in executing workflows and it results in increase of makespan, cost, energy consumption. In order to effectively schedule complex workflows i.e. with more task dependencies, we propose a novel multi objective workflow scheduling algorithm using Deep reinforcement Learning. Initially, priorities of all workflows calculated based on their dependencies and then calculated priorities of VMs based on electricity cost at datacenters to map workflows onto precise VMs. These priorities are fed to scheduler which uses Deep Q-Network model to dynamically schedule tasks by considering both priorities of tasks and VMs. Extensive simulations carried out on workflowsim by considering realtime scientific workflows (Montage, cybershake, Epigenomics, LIGO). Our proposed MOPWSDRL compared against existing state of art approaches i.e. Heterogeneous Earliest First Deadline, Cat Swarm Optimization, Ant Colony Optimization. Results revealed that our proposed MOPDSWRL outperforms existing state of art algorithms by minimizing makespan, energy consumption.",CLD,https://www.semanticscholar.org/paper/bfd5f4bc11603321a46cf2bf25f752207c700940
Reliability Enhancement Strategies for Workflow Scheduling Under Energy Consumption Constraints in Clouds,"As the demand for Big Data analysis and artificial intelligence technology continues to surge, a significant amount of research has been conducted on cloud computing services. An effective workflow scheduling strategy stands as the pivotal factor in ensuring the quality of cloud services. Dynamic voltage and frequency scaling (DVFS) is an effective energy-saving technology that is extensively used in the development of workflow scheduling algorithms. However, DVFS reduces the processor's running frequency, which increases the possibility of soft errors in workflow execution, thereby lowering the workflow execution reliability. This study proposes an energy-aware reliability enhancement scheduling (EARES) method with a checkpoint mechanism to improve system reliability while meeting the workflow deadline and the energy consumption constraints. The proposed EARES algorithm consists of three phases, namely, workflow application initialization, deadline partitioning, and energy partitioning and virtual machine selection. Numerous experiments are conducted to assess the performance of the EARES algorithm using three real-world scientific workflows. Experimental results demonstrate that the EARES algorithm remarkably improves reliability in comparison with other state-of-the-art algorithms while meeting the deadline and satisfying the energy consumption requirement.",CLD,https://www.semanticscholar.org/paper/a6de2f7118839bdd97ca3f6d9cc42585faa48c89
DB-ACO: A Deadline-Budget Constrained Ant Colony Optimization for Workflow Scheduling in Clouds,"With the development of cloud computing, a growing number of workflows are deployed in cloud platform that can dynamically provide cloud resources on demand for users. In clouds, one basic problem is how to schedule workflow under the deadline constraint and minimize the execution cost. As the capability of cloud resources getting higher, the required cost is also rising. Capability of some resources exceeds the need of users, which leads to higher cost, and the budget of users should be considered. In this paper, a novel scheduling algorithm, named DB-ACO, is proposed to minimize the execution cost for the workflow with deadline and budget constraints. DB-ACO is verified on four typical scientific workflows, and the experiments results show it outperforms four state-of-the-art methods, especially for CyberShake.Note to Practitioners—Budget and deadline are important requirements for users in cloud computing, which are used as constraints. Extensive works have been devoted to minimize the cost of workflows execution with different scheduling strategies. However, most of them only consider one single constraint and assume the constraint is simple and loose, which is impractical in actual scenarios due to higher requirement of users. This paper investigates a novel scheduling algorithm DB-ACO to optimize cost under budget and deadline. DB-ACO combines heuristic and meta-heuristic, it uses ant colony optimization to optimize the execution cost under the deadline and budget constraints: each ant sorts tasks on the basis of the combination of the pheromone trail and heuristic information, the deadline and budget are distributed fairly to each task by a novel distribution method, then the service selection rules are introduced to build solution.",CLD,https://www.semanticscholar.org/paper/aa33d835e00d76e66f9769fba69839b4ab906bdf
A Two-stage Multi-population Genetic Algorithm with Heuristics for Workflow Scheduling in Heterogeneous Distributed Computing Environments,"Workflow scheduling in Heterogeneous Distributed Computing Environments (HDCEs) is a NP-hard problem. Although a number of scheduling approaches have been proposed for workflow scheduling in HDCEs, there is still a room and need for improvement. To fill the gaps, this study formulates workflow scheduling problem in HDCEs as a complete, solvable and extensible integer programming mathematical model with precedence and resource constraints that provides a theoretical foundation for developing workflow scheduling strategy. Then, this study develops a novel two-stage multi-population genetic algorithm with heuristics for workflow scheduling. In particular, two-stage multi-population coevolution strategy is employed with designed novel methods for population initialization, genetic operation, individual decoding and improvement. To estimate the validity, extensive experiments are designed and conducted on various scenarios based on real and random workflow applications. The results have shown the practical viability of the proposed algorithm outperforming conventional approaches.",OPS,https://www.semanticscholar.org/paper/df1f1fe84ef007f4c7a1f3d4177c1dd0f2a576fb
Workflow Scheduling in Serverless Edge Computing for the Industrial Internet of Things: A Learning Approach,"Serverless edge computing is seen as a promising enabler to execute differentiated Industrial Internet of Things (IIoT) applications without managing the underlying servers and clusters. In IIoT serverless edge computing, IIoT workflow scheduling for cloud-edge collaborative processing is closely related to the service quality of users. However, serverless functions decomposed by IIoT applications are limited in their deployment at the edge due to the resource-constrained nature of edge infrastructures. In addition, the scheduling of complex IIoT applications supported by serverless computing is more challenging. Therefore, considering the limited function deployment and the complex dependencies of serverless workflows, we model the workflow application as directed acyclic graph and formulate the scheduling problem as a multiobjective optimization problem. A dueling double deep Q-network-based solution is proposed to make scheduling decisions under dynamically changing systems. Extensive simulation experiments are conducted to validate the superiority of the proposed scheme.",IOTNET,https://www.semanticscholar.org/paper/befd85d5a242a5354e451114f0b126529b5c1d2b
ET2FA: A Hybrid Heuristic Algorithm for Deadline-Constrained Workflow Scheduling in Cloud,"Cloud computing is an emerging computational infrastructure for cost-efficient workflow execution that provides flexible and dynamically scalable computing resources at pay-as-you-go pricing. Workflow scheduling, as a typical NP-Complete problem, is one of the major issues in cloud computing. However, in the cloud scenario with unlimited resources, how to generate an efficient and economical workflow scheduling scheme under the deadline constraint is still an extraordinary challenge. In this article, we propose a hybrid heuristic algorithm called enhanced task type first algorithm (ET2FA) to solve deadline-constrained workflow scheduling in cloud with new features such as hibernation and per-second billing. The objectives to be minimized include the total cost and total idle rate. ET2FA involves three phases: 1) Task type first algorithm, which schedules tasks based on topological level and task types, and utilizes a compact-scheduling-condition based VM selection method to assign each task. 2) Delay operation based on block structure, which further optimizes total cost and total idle rate based on block structure properties. 3) Instance hibernate scheduling heuristic, which sets an instance to hibernate if idle for a duration. Extensive simulation experiments based on seven well-known real-world workflow applications show that ET2FA delivers better performance in comparison to the state-of-the-art algorithms.",CLD,https://www.semanticscholar.org/paper/13de2f6f59c34428fd71878ef09e99473aba14fb
Reliability-Aware Multi-Objective Memetic Algorithm for Workflow Scheduling Problem in Multi-Cloud System,"With the development of cloud computing, multi-cloud systems have become common platforms for hosting and executing workflow applications in recent years. However, the complexity of workflow scheduling increases exponentially because of the diversified billing mechanisms, heterogeneous virtual machines, and reliability of multi-cloud systems. This article focuses on a multi-objective workflow scheduling problem in multi-cloud systems (MOWSP-MCS). The makespan, cost, and reliability are considered the optimization objectives from the perspective of users. Compared with the classical multi-objective workflow scheduling in the cloud environment, MOWSP-MCS allows users to apply the backup technique to improve reliability. To solve the MOWSP-MCS, this article proposes a reliability-aware multi-objective memetic algorithm (RA-MOMA) containing a diversification strategy and intensification strategy. In the diversification strategy, several problem-specific genetic operators are introduced to construct the diversified offspring individuals. In the intensification strategy, four problem-specific neighborhood operators are designed based on the critical path and resource utilization rate to improve the quality of the individuals in the archive set. A comprehensive numerical experiment is conducted to evaluate the effectiveness of RA-MOMA. The comparisons with several related algorithms demonstrate the superiority of RA-MOMA for solving the MOWSP-MCS.",CLD,https://www.semanticscholar.org/paper/54b50783b84694a7407bc770adc5b99f73b44603
A Two-Stage Estimation of Distribution Algorithm With Heuristics for Energy-Aware Cloud Workflow Scheduling,"With the enormous increase in energy usage by cloud data centers for handling various workflow applications, the energy-aware cloud workflow scheduling has become a hot issue. However, there is still a need and room for improvement in both the model for estimating workflow energy consumption and the algorithm for energy-aware cloud workflow scheduling. To fill these gaps, a new model for estimating the energy consumption of the cloud workflow execution and a novel Two-Stage Estimation of Distribution Algorithm with heuristics (TSEDA) for energy-aware cloud workflow scheduling are proposed based on the relationships among scheduling scheme, host load and power. In particular, in the proposed TSEDA, a new probability model and its updating mechanism are presented, and a two-stage coevolution strategy with some novel heuristic methods for individual generation, decoding and improvement is designed. Extensive experiments are conducted on workflow applications with various sizes and types, and the results show that the proposed TSEDA outperforms conventional algorithms.",CLD,https://www.semanticscholar.org/paper/707c6a09ec44a327fa7ba84cd6b5a03886fe4933
Genetic Programming for Dynamic Workflow Scheduling in Fog Computing,"Dynamic Workflow Scheduling in Fog Computing (DWSFC) is an important optimisation problem with many real-world applications. The current workflow scheduling problems only consider cloud servers but ignore the roles of mobile devices and edge servers. Some applications need to consider the mobile devices, edge, and cloud servers simultaneously, making them work together to generate an effective schedule. In this article, a new problem model for DWSFC is considered and a new simulator is designed for the new DWSFC problem model. The designed simulator takes the mobile devices, edge, and cloud servers as a whole system, where they all can execute tasks. In the designed simulator, two kinds of decision points are considered, which are the routing decision points and the sequencing decision points. To solve this problem, a new Multi-Tree Genetic Programming (MTGP) method is developed to automatically evolve scheduling heuristics that can make effective real-time decisions on these decision points. The proposed MTGP method with a multi-tree representation can handle the routing decision points and sequencing decision points simultaneously. The experimental results show that the proposed MTGP can achieve significantly better test performance (reduce the makespan by up to 50%) on all the tested scenarios than existing state-of-the-art methods.",CLD,https://www.semanticscholar.org/paper/f81d9057a18c1ed64bf2d51f598317b424d58619
Reliability-Aware and Energy-Efficient Workflow Scheduling in IaaS Clouds,"Nowadays, more and more workflow applications with different computing requirements are migrated to clouds and executed with cloud resources. Workflow scheduling becomes a critical problem in the cloud environment, which focuses on meeting various quality of service (QoS) constraints. Workflow reliability and energy consumption are two essential parts in clouds and minimizing energy consumption for scheduling workflow with the reliability constraint is a challenging issue. In response to the challenge, we propose a workflow scheduling algorithm named REWS to reduce energy consumption and satisfy workflow reliability constraints. In REWS, a new sub-reliability constraint prediction strategy is adopted to break down the workflow reliability constraint to task sub-reliability constraints and the effectiveness of this strategy is proved. Moreover, an update method is adopted to adjust the task sub-reliability constraint for reducing energy consumption. In addition, a brief system framework which consists of five parts: workflow analyzer, reliability decomposer, resource manager, workflow scheduler and feedback processer is built to support the algorithm implementation of REWS. We conduct the experiments using both synthetic data and real-world data to evaluate the proposed REWS approach. The results demonstrate the superiority of REWS as compared with the state-of-the-art algorithms. Note to Practitioners—Workflow scheduling is a challenging issue in emerging trends of the cloud environment that focuses on satisfying various QoS constraints. In this paper, we investigate a reliability-aware and energy-efficient workflow scheduling problem in cloud computing. A novel workflow scheduling algorithm called REWS, is designed to reduce the energy consumption and meet the workfolw reliability constraint. The basic idea of REWS is to divide the workflow reliability constraint into task sub-reliability constraints and schedule tasks with an energy-efficient scheduling strategy. We conduct the experiments to evaluate the proposed REWS and the results demonstrate that REWS outperforms the state-of-the-art algorithms.",CLD,https://www.semanticscholar.org/paper/785d8835ea5a5b99850004af44ea1c0ccf14af07
MONWS: Multi-Objective Normalization Workflow Scheduling for Cloud Computing,"Cloud computing is a prominent approach for complex scientific and business workflow applications in the pay-as-you-go model. Workflow scheduling poses a challenge in cloud computing due to its widespread applications in physics, astronomy, bioinformatics, and healthcare, etc. Resource allocation for workflow scheduling is problematic due to the computationally intensive nature of the workflow, the interdependence of tasks, and the heterogeneity of cloud resources. During resource allocation, the time and cost of execution are significant issues in the cloud-computing environment, which can potentially degrade the service quality that is provided to end users. This study proposes a method focusing on makespan, average utilization, and cost. The authors propose a task’s dynamic priority for workflow scheduling using MONWS, which uses the min-max algorithm to minimize the finish time and maximize resource utilization by calculating the dynamic threshold value for scheduling tasks on virtual machines. When the experimental results were compared to existing algorithms, MONWS achieved a 35% improvement in makespan, an 8% increase in maximum average cloud utilization, and a 4% decrease in cost.",CLD,https://www.semanticscholar.org/paper/e3994f0561bf5463fa6a7150fbe6e45ac886b8b8
Adaptive and Convex Optimization-Inspired Workflow Scheduling for Cloud Environment,"Scheduling large-scale and resource-intensive workflows in cloud infrastructure is one of the main challenges for cloud service providers (CSPs). Cloud infrastructure is more efficient when virtual machines and other resources work up to their full potential. The main factor that influences the quality of cloud services is the distribution of workflow on virtual machines (VMs). Scheduling tasks to VMs depends on the type of workflow and mechanism of resource allocation. Scientific workflows include large-scale data transfer and consume intensive resources of cloud infrastructures. Therefore, scheduling of tasks from scientific workflows on VMs requires efficient and optimized workflow scheduling techniques. This paper proposes an optimised workflow scheduling approach that aims to improve the utilization of cloud resources without increasing execution time and execution cost.",CLD,https://www.semanticscholar.org/paper/a5020d99a7836a7c253b0c4bc1088610af77e524
A Workflow Scheduling Approach With Modified Fuzzy Adaptive Genetic Algorithm in IaaS Clouds,"The emergence of the cloud platform with substantial resources to offer on-demand instigated the researchers to migrate the scientific workflows to the cloud environment. The scheduling of workflows with diverse QoS parameters is not a trivial task, but an NP-Complete problem. Several heuristics for QoS constrained workflows have been investigated. However, most of them focus only on time and cost and do not guarantee high resource utilization. The scheduling of the workflow tasks over the minimum cloud resources under the defined time limit is a grave concern. In this article, an algorithm named MFGA (Modified Fuzzy Adaptive Genetic Algorithm) has been formulated to minimize the makespan and improve resource utilization under both deadline and budget constraints. A fuzzy logic controller has also been devised to control the crossover and mutation rates that prevent MFGA from getting stuck in a local optimum. MFGA has a novel crossover technique that adds the fittest solutions in the population. Additionally, a new mutation technique has also been introduced, which minimizes the makespan and increases the reusability of the resources. The simulation experiments with the real workflows show that the proposed MFGA outperforms other state-of-the-art algorithms.",CLD,https://www.semanticscholar.org/paper/82275a5bbd3012032e2336f48078aa68653e291d
Failure-Aware Elastic Cloud Workflow Scheduling,"With an increasing complexity and functionality in cloud data centers, fault tolerance becomes an essential requirement for tasks executed in clouds, especially for workflows with task precedences. Hosts and network devices are the main physical components in a cloud data center. The PB (Primary-Backup) model is a desirable approach to fault tolerance. Many PB-based workflow scheduling algorithms have been proposed for host faults. However, only a few studies focus on cloud workflow scheduling considering network device faults. This paper analyzes the fault-tolerant properties for scheduling dependent tasks and migrating VMs based on the PB model, considering both host and network device faults in a cloud data center. A failure-aware elastic cloud workflow scheduling algorithm is designed for both host and network device fault tolerance. Additionally, an elastic resource provisioning mechanism is proposed and incorporated into the proposed algorithm to improve resource utilization. Performance evaluations on both randomly generated and real-world workflows show that the proposal effectively improves resource utilization while guaranteeing fault tolerance.",CLD,https://www.semanticscholar.org/paper/1f79d49bb37c313afce3b952ec8138c38ad1660b
A hybrid algorithm for workflow scheduling in cloud environment,": The advances in cloud computing promote the problem in processing speed. Computing resources in cloud play a vital role in solving user demands, which can be regarded as workflows. Efficient workflow scheduling is a challenge in reducing the task execution time and cost. In recent years, deep reinforcement learning algorithm has been used to solve various combinatorial optimisation problems. However, the trained models often have volatility and can not be applied in real situation. In addition, evolutionary algorithm with a complete framework is a popular method to tackle the scheduling problem. But, it has a poor convergence speed. In this paper, we propose a hybrid algorithm to address the workflow scheduling problem, which combines deep reinforcement algorithm and evolutionary algorithm. The solutions generated by deep reinforcement learning are the initial population in the evolutionary algorithm. Results show that the proposed algorithm is effective.",CLD,https://www.semanticscholar.org/paper/a598ad86a1b270e328d4b41f16a40c89e01a34e0
Modified firefly algorithm for workflow scheduling in cloud-edge environment,"Edge computing is a novel technology, which is closely related to the concept of Internet of Things. This technology brings computing resources closer to the location where they are consumed by end-users—to the edge of the cloud. In this way, response time is shortened and lower network bandwidth is utilized. Workflow scheduling must be addressed to accomplish these goals. In this paper, we propose an enhanced firefly algorithm adapted for tackling workflow scheduling challenges in a cloud-edge environment. Our proposed approach overcomes observed deficiencies of original firefly metaheuristics by incorporating genetic operators and quasi-reflection-based learning procedure. First, we have validated the proposed improved algorithm on 10 modern standard benchmark instances and compared its performance with original and other improved state-of-the-art metaheuristics. Secondly, we have performed simulations for a workflow scheduling problem with two objectives—cost and makespan. We performed comparative analysis with other state-of-the-art approaches that were tested under the same experimental conditions. Algorithm proposed in this paper exhibits significant enhancements over the original firefly algorithm and other outstanding metaheuristics in terms of convergence speed and results’ quality. Based on the output of conducted simulations, the proposed improved firefly algorithm obtains prominent results and managed to establish improvement in solving workflow scheduling in cloud-edge by reducing makespan and cost compared to other approaches.",OPS,https://www.semanticscholar.org/paper/8f441e9715695c15b6aa48e7afcd2d5926a0f42d
Scoring and Dynamic Hierarchy-Based NSGA-II for Multiobjective Workflow Scheduling in the Cloud,"Cloud computing becomes a promising technology to reduce computation cost by providing users with elastic resources and application-deploying environments as a pay-per-use model. More scientific workflow applications have been moved or are being migrated to the cloud. Scheduling workflows turns to the main bottleneck for increasing resource utilization and quality of service (QoS) for users. This work formulates workflow scheduling as multiobjective optimization problems and proposes a Scoring and Dynamic Hierarchy-based NSGA-II (Nondominated Sorting Genetic Algorithm II), called SDHN for short, to minimize both makespan and cost of workflow execution. First, a scoring criterion is developed to calculate the total score for each individual during population updating, which is used as a quantitative index to evaluate the dominance degree of individuals among the whole population. Hence, SDHN can distinguish individuals within the same dominance level and target its search toward the directions of elite solutions as their different dominance degrees and accordingly improve search efficiency. Second, a population-based dynamic hierarchical structure (HS) and its evolutionary rules are presented to update HS by comparing each child with all parental individuals from bottom to up until finding a proper dominant level. Since traversing all HS levels is not needed in most cases, the number of individual comparisons is reduced and SDHN’s updating efficiency is greatly improved, especially for large-scale and complex applications. Third, to guarantee its converging to the near-optimal solutions, adaptive adjustment strategies (AASs) are designed to prevent the search from falling into local optima or diverging by checking the number of individuals at the highest HS level and then modifying the relevant genetic operations to guide the evolutionary process to approach the global Pareto Front. Extensive experiments are conducted to verify SDHN, and the results show that it outperforms the existing algorithms in the quality and diversity of resulting solutions as well as convergence time. Note to Practitioners—Most scientific applications are computation and/or data-intensive and need large-scale or high-performance resources for their execution. More and more scientists use workflows to manage their applications, but how to efficiently run them in the cloud is a big challenge due to their large scale as well as the dynamic characteristics of the elastic and heterogeneous cloud resources. In this article, we develop a novel multiobjective optimization technique for workflow scheduling such that the makespan and cost can be minimized simultaneously. A scoring criterion, dynamic hierarchical structure and its evolutionary rules, and adaptive adjustment strategies are designed to cooperate with each other and increase the search ability and efficiency of the original and widely used NSGA-II. Adequate experiments are conducted to verify the proposed method’s performance, and the experimental results show that it can provide more near-optimal solutions than the existing methods. It can be readily applied for implementing more efficient and effective cloud data centers to execute large-scale scientific workflows.",CLD,https://www.semanticscholar.org/paper/a8f8a94349fee205a4a2d79c2723430106dd759f
Evolutionary Multi-Objective Workflow Scheduling for Volatile Resources in the Cloud,"The cloud has been widely used as a distributed computing platform for running scientific workflow applications. Most of the cloud providers encourage the use of their underutilized resources as spot instances for much cheaper prices compared with common resources as on-demand instances, however, the promise of lower costs for resources results in the volatility such that spot instances can be interrupted at any time by cloud providers. Many workflow scheduling algorithms have been proposed to deal with volatile resources. In this article, we consider the two most important features of the volatile resources namely fulfillment and interruption rates to fully model the instability of the cloud infrastructure. Subsequently, we propose a novel evolutionary multi-objective workflow scheduling approach to generate a set of trade-off solutions that outperform state-of-the-art algorithms in both makespan and economic costs. In addition, we explore the fluctuation of makespan and costs for our obtained schedules under different levels of fulfillment and interruption rates. Experimental results with the five well-known real-world workflows demonstrate that our evolutionary multi-objective workflow scheduling algorithm is competitive in terms of makespan and cost compared with state-of-the-art on-demand scheduling techniques.",CLD,https://www.semanticscholar.org/paper/706ee4d79d9fd4bf628f0aab2af6695e043f0105
Cost-Efficient Workflow Scheduling Algorithm for Applications With Deadline Constraint on Heterogeneous Clouds,"In recent years, more and more large-scale data processing and computing workflow applications run on heterogeneous clouds. Such cloud applications with precedence-constrained tasks are usually deadline-constrained and their scheduling is an essential problem faced by cloud providers. Moreover, minimizing the workflow execution cost based on cloud billing periods is also a complex and challenging problem for clouds. In realizing this, we first model the workflow applications as I/O Data-aware Directed Acyclic Graph (DDAG), according to clouds with global storage systems. Then, we mathematically state this deadline-constrained workflow scheduling problem with the goal of minimum execution financial cost. We also prove that the time complexity of this problem is NP-hard by deducing from a multidimensional multiple-choice knapsack problem. Third, we propose a heuristic cost-efficient task scheduling strategy called CETSS, which includes workflow DDAG model building, task subdeadline initialization, greedy workflow scheduling algorithm, and task adjusting method. The greedy workflow scheduling algorithm mainly consists of dynamical task renting billing period sharing method and unscheduled task subdeadline relax technique. We perform rigorous simulations on some synthetic randomly generated applications and real-world applications, such as Epigenomics, CyberShake, and LIGO. The experimental results clearly demonstrate that our proposed heuristic CETSS outperforms the existing algorithms and can effective save the total workflow execution cost. In particular, CETSS is very suitable for large workflow applications.",CLD,https://www.semanticscholar.org/paper/6d2d362c6c43e2831ea8046c68edd0e5b06be50c
Multi-Swarm Co-Evolution Based Hybrid Intelligent Optimization for Bi-Objective Multi-Workflow Scheduling in the Cloud,"Many scientific applications can be well modelled as large-scale workflows. Cloud computing has become a suitable platform for hosting and executing them. Workflow scheduling has gained much attention in recent years. However, since cloud service providers must offer services for multiple users with various QoS demands, scheduling multiple applications with different QoS requirements is highly challenging. This work proposes a Multi-swarm Co-evolution-based Hybrid Intelligent Optimization (MCHO) algorithm for multiple-workflow scheduling to minimize total makespan and cost while meeting the deadline constraint of each workflow. First, we design a multi-swarm co-evolutionary mechanism where three swarms are adopted to sufficiently search for various elite solutions. Second, to improve global search and convergence performance, we embed local and global guiding information into the updating process of a Particle Swarm Optimizer, and develop a swarm cooperation technique. Third, we propose a Genetic Algorithm-based elite enhancement strategy to exploit more non-dominated individuals, and apply the Metropolis Acceptance rule of Simulated Annealing to update the local guiding solution for each swarm so as to prevent it from being stuck into a local optimum at an early stage. Extensive experimental results demonstrate that MCHO outperforms the state-of-art scheduling algorithms with better distributed non-dominated solutions.",CLD,https://www.semanticscholar.org/paper/704f152e638e033a33fe7fa7a8b831ef2eddb73b
A Cooperative Coevolution Hyper-Heuristic Framework for Workflow Scheduling Problem,"Workflow scheduling problem (WSP) is a well-known combinatorial optimization problem, which is defined to assign a series of interconnected tasks to the available resources to meet user defined Quality of Service (QoS). The guided random search methods and heuristic based methods are two most common methods for solving WSP. However, these methods either require expensive computational cost or heavily rely on human's empirical knowledge, which makes them inconvenient for practical applications. Keeping this in mind, this paper proposes a cooperative coevolution hyper-heuristic framework to solve WSP with an objective of minimizing the completed time of workflow. In particular, in the proposed framework, two heuristic rules, namely, the task selection rule (TSR) and the resource selection rule (RSR), are learned automatically by a cooperative coevolution genetic programming (CCGP) algorithm. The TSR is used to select a ready task for scheduling, while the RSR is used to allocate resources to perform the selected task. To improve the search efficiency, a set of low-level heuristics are defined and used as building blocks to construct the TSR and RSR. Further, to validate the effectiveness of the proposed framework, randomly generated workflow instances and four real-world workflows are used as test cases in the experimental study. Compared with several state-of-the-art methods, e.g., the Heterogeneous Earliest Finish Time (HEFT) and the Predict Earliest Finish Time (PEFT), the high-level heuristics found by our proposed framework demonstrate superior performance on all the test cases in terms of several metrics including the schedule length ratio, speedup and efficiency.",OPS,https://www.semanticscholar.org/paper/df24a45d468abc73a281a64aefefbca26598be44
"Load and Cost-Aware Min-Min Workflow Scheduling Algorithm for Heterogeneous Resources in Fog, Cloud, and Edge Scenarios","Fog computing and Edge computing are few of the latest technologies which are offered as solution to challenges faced in Cloud Computing. Instead of offloading of all the tasks to centralized cloud servers, some of the tasks can be scheduled at intermediate Fog servers or Edge devices. Though this solves most of the problems faced in cloud but also encounter other traditional problems due to resource-related constraints like load balancing, scheduling, etc. In order to address task scheduling and load balancing in Cloud-fog-edge collaboration among servers, we have proposed an improved version of min-min algorithm for workflow scheduling which considers cost, makespan, energy and load balancing in heterogeneous environment. This algorithm is implemented and tested in different offloading scenarios- Cloud only, Fog only, Cloud-fog and Cloud-Fog-Edge collaboration. This approach performed better and the result gives minimum makespan, less energy consumption along with load balancing and marginally less cost when compared to min-min and ELBMM algorithms",CLD,https://www.semanticscholar.org/paper/db4fc1f79e93f6fc5ae5d81edca50371191a68ec
EDQWS: an enhanced divide and conquer algorithm for workflow scheduling in cloud,"A workflow is an effective way for modeling complex applications and serves as a means for scientists and researchers to better understand the details of applications. Cloud computing enables the running of workflow applications on many types of computational resources which become available on-demand. As one of the most important aspects of cloud computing, workflow scheduling needs to be performed efficiently to optimize resources. Due to the existence of various resource types at different prices, workflow scheduling has evolved into an even more challenging problem on cloud computing. The present paper proposes a workflow scheduling algorithm in the cloud to minimize the execution cost of the deadline-constrained workflow. The proposed method, EDQWS, extends the current authors’ previous study (DQWS) and is a two-step scheduler based on divide and conquer. In the first step, the workflow is divided into sub-workflows by defining, scheduling, and removing a critical path from the workflow, similar to DQWS. The process continues until only chain-structured sub-workflows, called linear graphs, remain. In the second step which is linear graph scheduling, a new merging algorithm is proposed that combines the resulting linear graphs so as to reduce the number of used instances and minimize the overall execution cost. In addition, the current work introduces a scoring function to select the most efficient instances for scheduling the linear graphs. Experiments show that EDQWS outperforms its competitors, both in terms of minimizing the monetary costs of executing scheduled workflows and meeting user-defined deadlines. Furthermore, in more than 50% of the examined workflow samples, EDQWS succeeds in reducing the number of resource instances compared to the previously introduced DQWS method.",CLD,https://www.semanticscholar.org/paper/6aebf86b49a5194349e420a3f9d47e6fe23cdaca
Endpoint Communication Contention-Aware Cloud Workflow Scheduling,"Cloud platforms have recently become a popular target execution environment for numerous workflow applications. Hence, effective workflow scheduling strategies in cloud environments are in high demand. However, existing scheduling algorithms are grounded on an idealized target platform model where virtual machines are fully connected, and all communications can be performed concurrently. A significant aspect neglected by them is endpoint communication contention when executing workflows, which has a large impact on workflow makespan. This article investigates how to incorporate contention awareness into cloud workflow scheduling and proposes a new practical scheduling model. Endpoint communication contention-aware List Scheduling Heuristic (ELSH) is designed to minimize workflow makespan. It uses a novel task ranking property and schedules data communications to communication resources besides scheduling tasks to computing resources. Moreover, a rescheduling technique is employed to improve the schedule. In experiments, ELSH is evaluated against the traditional contention-oblivious list scheduling algorithm, which is adapted to address contention during execution in practice. The experimental results reveal that ELSH performs more efficaciously compared with the adapted traditional ones. Note to Practitioners—This article aims to advance the state of the art for workflow scheduling in clouds by taking into account endpoint communication contention that can occur in practice but has largely been neglected in existing investigations. A scheduling method called Endpoint communication contention-aware List Scheduling Heuristic (ELSH) is then proposed to optimize workflow makespan. Experimental results based on synthetic and realistic workflows show that ELSH performs better than traditional scheduling algorithms that fail to consider endpoint communication contention, especially for the workflow with a large communication-to-computation-cost ratio. The proposed approach can be readily put into use and help cloud service providers to offer their customers high-quality services when executing the latter’s workflows.",CLD,https://www.semanticscholar.org/paper/7ad33a467b4a5189092a853d3fd262b6029334a7
Multi-Swarm PSO Algorithm for Static Workflow Scheduling in Cloud-Fog Environments,"Scientific workflow scheduling involves the allocation of workflow tasks to particular computational resources. The generation of optimal solutions to reduce run-time, cost, and energy consumption, as well as ensuring proper load balancing, remains a major challenge. Therefore, this work presents a Multi-Swarm Particle Swarm Optimization (MS-PSO) algorithm to improve the scheduling of scientific workflows in cloud-fog environments. MS-PSO seeks to address the canonical PSO’s problem of premature convergence, which leads it to suboptimal solutions. In MS-PSO, particles are divided into several swarms, with each swarm having its own cognitive and social learning coefficients. This work also develops a weighted sum objective function for the workflow scheduling problem, based on four objectives: makespan, cost, energy and load balancing for cloud and fog tiers. The FogWorkflowSim Toolkit is used in the evaluation process, with the objectives serving as performance metrics. The MS-PSO approach is compared with the canonical PSO, Genetic Algorithm (GA), Differential Evolution (DE) and GA-PSO. The following scientific workflows are used in the simulations: Montage, Cybershake, Epigenomics, LIGO and SIPHT. MS-PSO outperforms the canonical PSO on all scientific workflows and under all performance metrics. It competes fairly well against the other approaches and it is more stable and reliable. It only ranks second to PSO, in terms of execution time. In future, multiple species, incorporating population update mechanisms from several algorithmic frameworks (MS-PSO, DE, GA), will be used for scientific workflow scheduling. Hybdridization of the realized algorithm with dynamic approaches will also be investigated.",OPS,https://www.semanticscholar.org/paper/927dcc1555fc2c71acc9af2435244ea08171c96e
Chaotic-Nondominated-Sorting Owl Search Algorithm for Energy-Aware Multi-Workflow Scheduling in Hybrid Clouds,"Since a single private cloud cannot satisfy the increasing computational requirements for executing multiple workflows simultaneously, hybrid clouds are often adopted to perform such execution. Multi-workflow scheduling is challenging as users may request various applications with different QoS requirements. This work proposes a Chaotic-nondominated-sorting Owl Search Algorithm (COSA) by combining an Owl Search Algorithm (OSA) with a Nondominated Sorting Genetic Algorithm II (NSGA-II) to schedule resource-constrained multiple workflows in hybrid clouds with makespan, cost and energy consumption minimized under the given deadline and budget constraints. First, a hierarchical evolving mechanism is designed to update the better half and worse half of population by NSGA-II and OSA, respectively to guarantee a good trade-off between exploration and exploitation. Second, a chaotic sequence is introduced to adaptively adjust OSA's step size during population evolution for better exploration. Third, we adopt a chaotic operator for searching around the resulting Non-Dominated Solutions (NDS) to improve COSA's local search ability. Experiments are conducted to compare COSA with four peers and the results show its superiority in the number of obtained NDS, diversity preservation and convergence towards the near optimal Pareto set. In particular, it can find at least 19% more NDS than its peers.",CLD,https://www.semanticscholar.org/paper/c7ef3d923aa0dae0278901b4ca419d27ad31f2be
An Effective Cloud Workflow Scheduling Approach Combining PSO and Idle Time Slot-Aware Rules,"Workflow scheduling is a key issue and remains a challenging problem in cloud computing. Faced with the large number of virtual machine (VM) types offered by cloud providers, cloud users need to choose the most appropriate VM type for each task. Multiple task scheduling sequences exist in a workflow application. Different task scheduling sequences have a significant impact on the scheduling performance. It is not easy to determine the most appropriate set of VM types for tasks and the best task scheduling sequence. Besides, the idle time slots on VM instances should be used fully to increase resources' utilization and save the execution cost of a workflow. This paper considers these three aspects simultaneously and proposes a cloud workflow scheduling approach which combines particle swarm optimization (PSO) and idle time slot-aware rules, to minimize the execution cost of a workflow application under a deadline constraint. A new particle encoding is devised to represent the VM type required by each task and the scheduling sequence of tasks. An idle time slot-aware decoding procedure is proposed to decode a particle into a scheduling solution. To handle tasks' invalid priorities caused by the randomness of PSO, a repair method is used to repair those priorities to produce valid task scheduling sequences. The proposed approach is compared with state-of-the-art cloud workflow scheduling algorithms. Experiments show that the proposed approach outperforms the comparative algorithms in terms of both of the execution cost and the success rate in meeting the deadline.",CLD,https://www.semanticscholar.org/paper/a72afb8ad60e6e9166c42a306dbea64acd333893
Real-Time Multiple-Workflow Scheduling in Cloud Environments,"With the development of cloud computing, an increasing number of applications in different fields have been deployed to the cloud. In this process, the real-time scheduling of multiple workflows composed of tasks from these different applications must consider various influencing factors that strongly affect scheduling performance. This paper proposes a real-time multiple-workflow scheduling (RMWS) scheme to schedule workflows dynamically with minimum cost under different deadline constraints. Due to the uncertainty of workflow arrival time and specification, RMWS dynamically allocates tasks and divides the scheduling process into three stages. First, when a new workflow arrives, the latest start time and the latest finish time of each task are calculated according to the deadline, and the subdeadline of each task is obtained by probabilistic upward ranking. Then, each ready task is allocated according to its subdeadline and the increased cost of the virtual machine (VM). Meanwhile, only one waiting task can be assigned to each VM to reduce delay fluctuations. Finally, when the task is completed on the assigned VM, all the parameters of the relevant tasks are updated before allocating them to appropriate VMs. The experimental results based on four real-world workflow traces show that the proposed algorithm is superior to two state-of-the-art algorithms in terms of total rental cost, resource utilization, success rate and deadline deviation under different conditions.",CLD,https://www.semanticscholar.org/paper/1bd63aca0461496c7a30e5bf3a0abe41beb9f540
Dynamic Group Learning Distributed Particle Swarm Optimization for Large-Scale Optimization and Its Application in Cloud Workflow Scheduling,"Cloud workflow scheduling is a significant topic in both commercial and industrial applications. However, the growing scale of workflow has made such a scheduling problem increasingly challenging. Many current algorithms often deal with small- or medium-scale problems (e.g., less than 1000 tasks) and face difficulties in providing satisfactory solutions when dealing with the large-scale problems, due to the curse of dimensionality. To this aim, this article proposes a dynamic group learning distributed particle swarm optimization (DGLDPSO) for large-scale optimization and extends it for the large-scale cloud workflow scheduling. DGLDPSO is efficient for large-scale optimization due to its following two advantages. First, the entire population is divided into many groups, and these groups are coevolved by using the master-slave multigroup distributed model, forming a distributed PSO (DPSO) to enhance the algorithm diversity. Second, a dynamic group learning (DGL) strategy is adopted for DPSO to balance diversity and convergence. When applied DGLDPSO into the large-scale cloud workflow scheduling, an adaptive renumber strategy (ARS) is further developed to make solutions relate to the resource characteristic and to make the searching behavior meaningful rather than aimless. Experiments are conducted on the large-scale benchmark functions set and the large-scale cloud workflow scheduling instances to further investigate the performance of DGLDPSO. The comparison results show that DGLDPSO is better than or at least comparable to other state-of-the-art large-scale optimization algorithms and workflow scheduling algorithms.",OPS,https://www.semanticscholar.org/paper/dde2907096369a8cc19a2af84d0c2f621b4ad12d
GRP-HEFT: A Budget-Constrained Resource Provisioning Scheme for Workflow Scheduling in IaaS Clouds,"In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (Multi-Objective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS.",CLD,https://www.semanticscholar.org/paper/2bcdf2144492e1eadf6563f76452d071c0ea06c8
An Intelligent Cloud Workflow Scheduling System With Time Estimation and Adaptive Ant Colony Optimization,"The introduction of workflow in cloud computing has afforded a new and efficient way to tackle large-scale applications. As an NP-hard problem, how to schedule cloud workflows effectively and economically with deadline constraints and different kinds of tasks and resources is extraordinarily challenging. To solve this constrained problem, this paper intends to develop an intelligent scheduling system from the perspective of users to reduce expenditure of workflow, subject to the deadline and other execution constraints. A new estimation model of the task execution time is designed according to virtual machine settings in real public clouds and execution data from practical workflows. Based on the new model, an adaptive ant colony optimization algorithm is proposed to meet the quality of service and orchestrate tasks. The adaptiveness of the algorithm is embodied in two aspects. First, an adaptive solution construction method is designed that each solution is built with a dynamically changing resource pool, thus the search space of the algorithm is narrowed down and the execution time is decreased. Second, two heuristics with self-adaptive weight are introduced to adaptively meet different deadline settings. Simulating results on four types of workflows show that the proposed approach is effective and competitive.",CLD,https://www.semanticscholar.org/paper/55968724c41ea9f3d9ebf6d7791df22dec88b978
IPPTS: An Efficient Algorithm for Scientific Workflow Scheduling in Heterogeneous Computing Systems,"Efficient scheduling algorithms are key for attaining high performance in heterogeneous computing systems. In this article, we propose a new list scheduling algorithm for assigning task graphs to fully connected heterogeneous processors with an aim to minimize the scheduling length. The proposed algorithm, called Improved Predict Priority Task Scheduling (IPPTS) algorithm has two phases: task prioritization phase, which gives priority to tasks, and processor selection phase, which selects a processor for a task. The IPPTS algorithm has a quadratic time complexity as the related algorithms for the same goal, that is <inline-formula><tex-math notation=""LaTeX"">$O(t^{2} \times p)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>×</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""djigal-ieq1-3041829.gif""/></alternatives></inline-formula>, for <inline-formula><tex-math notation=""LaTeX"">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=""djigal-ieq2-3041829.gif""/></alternatives></inline-formula> tasks and <inline-formula><tex-math notation=""LaTeX"">$p$</tex-math><alternatives><mml:math><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href=""djigal-ieq3-3041829.gif""/></alternatives></inline-formula> processors. Our algorithm reduces the scheduling length significantly by looking ahead in both task prioritization phase and processor selection phase. In this way, the algorithm is looking ahead to schedule a task and its heaviest successor task to the optimistic processor, i.e., the processor that minimizes their computation and communication costs. The experiments based on both randomly generated graphs and graphs of real-world applications show that the IPPTS algorithm significantly outperforms previous list scheduling algorithms in terms of makespan, speedup, makespan standard deviation, efficiency, and frequency of best results.",OPS,https://www.semanticscholar.org/paper/e37ccb1e82c7bb84b592eada25fffab8611e07c2
Bio-Inspired Workflow Scheduling on HPC Platforms,"Efficient scheduling of tasks in workflows of cloud or grid applications is a key to achieving better utilization of resources as well as timely completion of the user jobs. Many scientific applications comprise several tasks that are dependent in nature and are specified by workflow graphs. The aim of the cloud meta-scheduler is to schedule the user application tasks (and the applications) so as to optimize the resource utilization and to execute the user applications in minimum amount of time. During the past decade, there have been several attempts to use bio-inspired scheduling algorithms to obtain an optimal or near optimal schedule in order to minimize the overall schedule length and to optimize the use of resources. However, as the number of tasks increases, the solution space comprising different tasks-resource mapping sequences increases exponentially. Hence, there is a need to devise mechanisms to improvise the search strategies of the bio-inspired scheduling algorithms for better scheduling solutions in lesser number of iterations/time. The objective of the research work in this paper is to use bio-inspired bacteria foraging optimization algorithm (BFOA) along with other heuristics algorithms for better search of the scheduling solution space for multiple workflows. The idea is to first find a schedule by the heuristic algorithms such as MaxMin, MinMin, and Myopic, and use these as initial solutions (along with other randomly generated solutions) in the search space to get better solutions using BFOA. The performance of our approach with the existing approaches is compared for quality of the scheduling solutions. The results demonstrate that our hybrid approach (MinMin/Myopic with BFOA) outperforms other approaches.",CLD,https://www.semanticscholar.org/paper/fa0863e6203b214843572fff86b8f6e5579ad1c2
IoT Workflow Scheduling Using Intelligent Arithmetic Optimization Algorithm in Fog Computing,"Instead of the cloud, the Internet of things (IoT) activities are offloaded into fog computing to boost the quality of services (QoSs) needed by many applications. However, the availability of continuous computing resources on fog computing servers is one of the restrictions for IoT applications since transmitting the large amount of data generated using IoT devices would create network traffic and cause an increase in computational overhead. Therefore, task scheduling is the main problem that needs to be solved efficiently. This study proposes an energy-aware model using an enhanced arithmetic optimization algorithm (AOA) method called AOAM, which addresses fog computing's job scheduling problem to maximize users' QoSs by maximizing the makespan measure. In the proposed AOAM, we enhanced the conventional AOA searchability using the marine predators algorithm (MPA) search operators to address the diversity of the used solutions and local optimum problems. The proposed AOAM is validated using several parameters, including various clients, data centers, hosts, virtual machines, tasks, and standard evaluation measures, including the energy and makespan. The obtained results are compared with other state-of-the-art methods; it showed that AOAM is promising and solved task scheduling effectively compared with the other comparative methods.",IOTNET,https://www.semanticscholar.org/paper/1639b3057c0308ec483025e36cb969aaef5a28d4
Energy-Efficient Load Balancing Algorithm for Workflow Scheduling in Cloud Data Centers Using Queuing and Thresholds,"Cloud computing is a rapidly growing technology that has been implemented in various fields in recent years, such as business, research, industry, and computing. Cloud computing provides different services over the internet, thus eliminating the need for personalized hardware and other resources. Cloud computing environments face some challenges in terms of resource utilization, energy efficiency, heterogeneous resources, etc. Tasks scheduling and virtual machines (VMs) are used as consolidation techniques in order to tackle these issues. Tasks scheduling has been extensively studied in the literature. The problem has been studied with different parameters and objectives. In this article, we address the problem of energy consumption and efficient resource utilization in virtualized cloud data centers. The proposed algorithm is based on task classification and thresholds for efficient scheduling and better resource utilization. In the first phase, workflow tasks are pre-processed to avoid bottlenecks by placing tasks with more dependencies and long execution times in separate queues. In the next step, tasks are classified based on the intensities of the required resources. Finally, Particle Swarm Optimization (PSO) is used to select the best schedules. Experiments were performed to validate the proposed technique. Comparative results obtained on benchmark datasets are presented. The results show the effectiveness of the proposed algorithm over that of the other algorithms to which it was compared in terms of energy consumption, makespan, and load balancing.",CLD,https://www.semanticscholar.org/paper/5738e31f94799cb4cb988e6b439e6437b0a7b020
RETRACTED: Reinforcement learning-based controller for adaptive workflow scheduling in multi-tenant cloud computing,"Multi-tenancy is an essential feature in cloud computing and is a major component to achieve scalability and energy-efficient solution to gain high level of economic benefits. As the cloud, computing is gaining more audiences and high user base, the problem of scheduling the computational workflow for multi-tenant cloud scheduling is becoming a difficult task to achieve. In this study, we present a learning-based scheduler for catering heterogeneous software and hardware resources in the context of multi-tenant cloud computing. The experimentation has been carried out with the help of green cloud simulator and the results are compared with the state of the art techniques like minimum completion time, first come first serve and backfilling. The experimental results exhibit that the presented algorithm provides an effective means of utilizing cloud resources in addition with drastic reduction in cost of operation.",CLD,https://www.semanticscholar.org/paper/f0d75e6878fcb2a67534f4e6c74ef58c81a69b81
Multi-Objective Workflow Scheduling With Deep-Q-Network-Based Multi-Agent Reinforcement Learning,"Cloud Computing provides an effective platform for executing large-scale and complex workflow applications with a pay-as-you-go model. Nevertheless, various challenges, especially its optimal scheduling for multiple conflicting objectives, are yet to be addressed properly. The existing multi-objective workflow scheduling approaches are still limited in many ways, e.g., encoding is restricted by prior experts’ knowledge when handling a dynamic real-time problem, which strongly influences the performance of scheduling. In this paper, we apply a deep-Q-network model in a multi-agent reinforcement learning setting to guide the scheduling of multi-workflows over infrastructure-as-a-service clouds. To optimize multi-workflow completion time and user’s cost, we consider a Markov game model, which takes the number of workflow applications and heterogeneous virtual machines as state input and the maximum completion time and cost as rewards. The game model is capable of seeking for correlated equilibrium between make-span and cost criteria without prior experts’ knowledge and converges to the correlated equilibrium policy in a dynamic real-time environment. To validate our proposed approach, we conduct extensive case studies based on multiple well-known scientific workflow templates and Amazon EC2 cloud. The experimental results clearly suggest that our proposed approach outperforms traditional ones, e.g., non-dominated sorting genetic algorithm-II, multi-objective particle swarm optimization, and game-theoretic-based greedy algorithms, in terms of optimality of scheduling plans generated.",CLD,https://www.semanticscholar.org/paper/152f57e5660bc7fa80d365e86bc2296ccb52b9e9
Multiobjective Cloud Workflow Scheduling: A Multiple Populations Ant Colony System Approach,"Cloud workflow scheduling is significantly challenging due to not only the large scale of workflow but also the elasticity and heterogeneity of cloud resources. Moreover, the pricing model of clouds makes the execution time and execution cost two critical issues in the scheduling. This paper models the cloud workflow scheduling as a multiobjective optimization problem that optimizes both execution time and execution cost. A novel multiobjective ant colony system based on a co-evolutionary multiple populations for multiple objectives framework is proposed, which adopts two colonies to deal with these two objectives, respectively. Moreover, the proposed approach incorporates with the following three novel designs to efficiently deal with the multiobjective challenges: 1) a new pheromone update rule based on a set of nondominated solutions from a global archive to guide each colony to search its optimization objective sufficiently; 2) a complementary heuristic strategy to avoid a colony only focusing on its corresponding single optimization objective, cooperating with the pheromone update rule to balance the search of both objectives; and 3) an elite study strategy to improve the solution quality of the global archive to help further approach the global Pareto front. Experimental simulations are conducted on five types of real-world scientific workflows and consider the properties of Amazon EC2 cloud platform. The experimental results show that the proposed algorithm performs better than both some state-of-the-art multiobjective optimization approaches and the constrained optimization approaches.",CLD,https://www.semanticscholar.org/paper/127fc2aad95f6839796ccec2ed0c77c3731c91bb
A Heuristics-Based Cost Model for Scientific Workflow Scheduling in Cloud,": Scientific Workflow Applications (SWFAs) can deliver collaborative tools useful to researchers in executing large and complex scientific processes. Particularly, Scientific Workflow Scheduling (SWFS) accelerates the computational procedures between the available computational resources and the dependent workflow jobs based on the researchers’ requirements. However, cost optimization is one of the SWFS challenges in handling massive and complicated tasks and requires determining an approximate (near-optimal) solution within polynomial computational time. Motivated by this, current work proposes a novel SWFS cost optimization model effective in solving this challenge. The proposed model contains three main stages: (i) scientific workflow application, (ii) targeted computational environment, and (iii) cost optimization criteria. The model has been used to optimize completion time (makespan) and overall computational cost of SWFS in cloud computing for all considered scenarios in this research context. This will ultimately reduce the cost for service consumers. At the same time, reducing the cost data-intensiveness and computational-intensiveness of SWFAs. The results reveal that the proposed cost optimization model attained an optimal Job completion time (makespan) and total computational cost for small and large sizes of the considered dataset. In contrast, hybrid and hyper-based approaches consistently achieved better results for the medium-sized dataset.",CLD,https://www.semanticscholar.org/paper/26d211eec93c3d9dfd42a6b5af8cf60b75c09fdc
A Knowledge-Based Adaptive Discrete Water Wave Optimization for Solving Cloud Workflow Scheduling,"Workflow scheduling in cloud environments has become a significant topic in both commercial and industrial applications. However, it is still an extraordinarily challenge to generate effective and economical scheduling schemes under the deadline constraint especially for the large scale workflow applications. To address the issue, this article investigates the cloud workflow scheduling problem with the aim of minimizing the whole cost of workflow execution whereas maintaining its execution time under a predetermined deadline. A novel knowledge-based adaptive discrete water wave optimization (KADWWO) algorithm is developed based on the problem-specific knowledge of cloud workflow scheduling. In the proposed KADWWO, a discrete propagation operator is designed based on the idle time knowledge of hourly-based cost model to adaptively explore the huge search space. The adaptive refraction operator is employed to avoid stagnation and expand the available resource pool. Meanwhile, the dynamic grouping based breaking operator is designed to exploit the excellent block structure knowledge of task allocation scheme and corresponding resource to intensify the local region and accelerate convergence. Extensive simulation experiments on the well-known scientific workflow demonstrate that the KADWWO approach outperforms several recent state-of-the-art algorithms.",CLD,https://www.semanticscholar.org/paper/08e23999cb61978f286d6a82714cf70c845f11cc
Hybrid Workflow Scheduling on Edge Cloud Computing Systems,"Internet of Things applications can be represented as workflows in which stream and batch processing are combined to accomplish data analytics objectives in many application domains such as smart home, health care, bioinformatics, astronomy, and education. The main challenge of this combination is the differentiation of service quality constraints between batch and stream computations. Stream processing is highly latency-sensitive while batch processing is more likely resource-intensive. In this work, we propose an end-to-end hybrid workflow scheduling on an edge cloud system as a two-stage framework. In the first stage, we propose a resource estimation algorithm based on a linear optimization approach, gradient descent search (GDS), and in the second stage, we propose a cluster-based provisioning and scheduling technique for hybrid workflows on heterogeneous edge cloud resources. We provide a multi-objective optimization model for execution time and monetary cost under constraints of deadline and throughput. Results demonstrate the framework performance in controlling the execution of hybrid workflows by efficiently tuning several parameters including stream arrival rate, processing throughput, and workflow complexity. In comparison to a meta-heuristics technique using Particle Swarm Optimization (PSO), the proposed scheduler provides significant improvement for large-scale hybrid workflows in terms of execution time and cost with an average of 8% and 35%, respectively.",CLD,https://www.semanticscholar.org/paper/12562bf4b685442f0deb2b392f8af9239bb32c78
Cybersecurity compliance in financial institutions: A comparative analysis of global standards and regulations,"Cybersecurity is a critical concern for financial institutions worldwide, given the increasing frequency and sophistication of cyberattacks. This paper conducts a comparative analysis of global standards and regulations governing cybersecurity compliance in financial institutions. By examining the regulatory frameworks of key jurisdictions, including the United States, the European Union, and Asia-Pacific countries, this study aims to identify common trends, differences, and best practices in cybersecurity compliance. The analysis begins by outlining the regulatory landscape for cybersecurity in financial institutions, highlighting the key objectives and principles underlying these regulations. It then compares the regulatory frameworks of different regions, focusing on areas such as data protection, incident response, and risk management. By examining the specific requirements and guidelines set forth by each jurisdiction, this study identifies the strengths and weaknesses of current cybersecurity regulations and offers recommendations for enhancing compliance and resilience. One of the key findings of this study is the increasing convergence of global cybersecurity standards, driven by the interconnected nature of the financial sector and the need for harmonized regulatory approaches. While differences in regulatory frameworks still exist, particularly in areas such as data protection and breach notification, there is a growing recognition of the need for international cooperation and information sharing to combat cyber threats effectively. The study also highlights the challenges faced by financial institutions in achieving cybersecurity compliance, including resource constraints, evolving cyber threats, and the complexity of regulatory requirements. It underscores the importance of implementing robust cybersecurity measures, such as encryption, multi-factor authentication, and regular security audits, to mitigate these challenges. In conclusion, this comparative analysis provides valuable insights into the global landscape of cybersecurity compliance in financial institutions. By identifying common trends and best practices, this study aims to assist policymakers, regulators, and financial institutions in enhancing their cybersecurity posture and effectively addressing the evolving cyber threat landscape.",SEC,https://www.semanticscholar.org/paper/193f54879601e76884d6562a5e5e01117fed3ded
From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy,"Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it’s critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.",SEC,https://www.semanticscholar.org/paper/317ad53bea6fb603c20f692bb2f1a01e2dc86161
CYBERSECURITY IN BANKING: A GLOBAL PERSPECTIVE WITH A FOCUS ON NIGERIAN PRACTICES,"The paper review cybersecurity practices in banking, with a specific focus on Nigerian banks. Cybersecurity has become a paramount concern in the banking industry worldwide, given the escalating frequency and sophistication of cyber threats. This study provides an overview of the global landscape of cybersecurity in banking, with a specific focus on practices observed in Nigeria. The global banking sector is witnessing a surge in digital transformation, marked by the adoption of advanced technologies and online financial services. However, this digitization brings with it unprecedented cybersecurity challenges, ranging from data breaches and ransomware attacks to sophisticated financial fraud. Financial institutions globally are compelled to fortify their cybersecurity frameworks to protect sensitive customer information, ensure the integrity of financial transactions, and maintain trust in the digital financial ecosystem. Nigeria, as a key player in the African banking landscape, faces unique cybersecurity challenges and has developed distinct strategies to safeguard its financial institutions. This study explores the cybersecurity practices employed by Nigerian banks, considering regulatory frameworks, incident response mechanisms, and collaborative efforts with international cybersecurity entities. The analysis encompasses case studies illustrating real-world cyber threats and incidents in both global and Nigerian banking contexts. It investigates the effectiveness of cybersecurity measures implemented by Nigerian banks, shedding light on the nation's response to evolving cyber risks. Furthermore, the study delves into collaborative initiatives between Nigerian banks, regulatory bodies, and international cybersecurity organizations to enhance information sharing, threat intelligence, and collective defense mechanisms. It also explores the role of public awareness campaigns in fostering a cyber-resilient banking environment. The insights provided aim to contribute to the collective understanding of cybersecurity challenges in the global banking sector while offering a nuanced perspective on Nigerian practices. As digital financial ecosystems continue to evolve, the findings underscore the importance of ongoing adaptation, collaboration, and innovation in safeguarding the integrity and trustworthiness of banking systems, both on a global scale and within the Nigerian context. Keywords: Cybersecurity, Banking, Global Perspective, Nigerian, Cybersecurity, Artificial Intelligence.",SEC,https://www.semanticscholar.org/paper/a5747b67b3213c7eccba6183b9bffb4828e8a299
The integration of artificial intelligence in cybersecurity measures for sustainable finance platforms: An analysis,"This study delves into the integration of Artificial Intelligence (AI) in cybersecurity measures within smart cities, aiming to uncover both the challenges and opportunities this fusion presents. With the burgeoning reliance on interconnected digital infrastructures and the vast data ecosystems within urban environments, smart cities are increasingly susceptible to sophisticated cyber threats. Through a systematic literature review and content analysis, this research identifies the unique cybersecurity vulnerabilities faced by smart cities and evaluates how AI technologies can fortify urban cybersecurity frameworks. The methodology encompasses a comprehensive review of recent scholarly articles, industry reports, and case studies to assess the role of AI in enhancing threat detection, response, and prevention mechanisms. Key findings reveal that AI-driven cybersecurity solutions significantly enhance the resilience of smart cities against cyber threats by providing advanced analytical capabilities and real-time threat intelligence. However, the study also highlights the critical need for robust ethical and privacy considerations in the deployment of AI technologies. Strategic recommendations are provided for policymakers, urban planners, and technology leaders, emphasizing the importance of integrating secure AI-enabled infrastructure and fostering public-private partnerships. The study concludes with suggestions for future research directions, focusing on the ethical implications of AI in cybersecurity and the development of scalable AI solutions for diverse urban contexts. Keywords: Artificial Intelligence, Cybersecurity, Smart Cities, Urban Resilience.",SEC,https://www.semanticscholar.org/paper/c81aa1a53904bd359aa014d6576fbb54809bd779
Cybersecurity Threats Detection in Intelligent Networks using Predictive Analytics Approaches,"The modern scenario of network vulnerabilities necessitates the adoption of sophisticated detection and mitigation strategies. Predictive analytics is surfaced to be a powerful tool in the fight against cybercrime, offering unparalleled capabilities for automating tasks, analyzing vast amounts of data, and identifying complex patterns that might elude human analysts. This paper presents a comprehensive overview of how AI is transforming the field of cybersecurity. Machine intelligence can bring revolution to cybersecurity by providing advanced defense capabilities. Addressing ethical concerns, ensuring model explainability, and fostering collaboration between researchers and developers are crucial for maximizing the positive impact of AI in this critical domain.",SEC,https://www.semanticscholar.org/paper/a253daee62951a067b63ef06d119632bb81d32d7
Developing comprehensive cybersecurity frameworks for protecting green infrastructure: Conceptual models and practical applications,"This study investigates the critical intersection of cybersecurity and green infrastructure (GI), aiming to elucidate the challenges, opportunities, and strategic approaches necessary for safeguarding these essential systems against cyber threats. Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, industry reports, and regulatory publications from 2014 to 2024. The methodology focuses on identifying prevalent cybersecurity vulnerabilities within GI, the evolution of protective practices, the impact of regulatory frameworks, and the strategic implications for diverse stakeholders. Key findings reveal a complex landscape where the integration of digital technologies in GI introduces both innovative solutions and new vulnerabilities. The study highlights the pivotal role of international standards and regulatory bodies in shaping cybersecurity strategies, underscoring the necessity for a holistic approach that encompasses technological, regulatory, and human factors. Strategic recommendations advocate for interdisciplinary collaboration, enhanced regulatory frameworks, and stakeholder engagement to fortify the cybersecurity of GI. The research underscores the imperative of embedding cybersecurity into the fabric of GI planning and management. It calls for future research to explore predictive models and proactive measures, ensuring the resilience and sustainability of green infrastructure in an increasingly digitalized urban environment. This study contributes to the burgeoning discourse on securing sustainable urban systems against cyber threats, offering a foundation for further exploration and development in the field.",SEC,https://www.semanticscholar.org/paper/6a77366ff3d934a27d158708353c1875602c0d37
CYBERSECURITY CHALLENGES IN THE AGE OF AI: THEORETICAL APPROACHES AND PRACTICAL SOLUTIONS,"In the ever-evolving landscape of cybersecurity, the proliferation of artificial intelligence (AI) technologies introduces both promising advancements and daunting challenges. This paper explores the theoretical underpinnings and practical implications of addressing cybersecurity challenges in the age of AI. With the integration of AI into various facets of digital infrastructure, including threat detection, authentication, and response mechanisms, cyber threats have become increasingly sophisticated and difficult to mitigate. Theoretical approaches delve into understanding the intricate interplay between AI algorithms, human behavior, and adversarial tactics, elucidating the underlying mechanisms of cyber attacks and defense strategies. However, this complexity also engenders novel vulnerabilities, as AI-driven attacks leverage machine learning algorithms to evade traditional security measures, posing formidable challenges to organizations across sectors. As such, practical solutions necessitate a multifaceted approach, encompassing robust threat intelligence, adaptive defense mechanisms, and ethical considerations to safeguard against AI-driven cyber threats effectively. Leveraging AI for cybersecurity defense holds promise in enhancing detection capabilities, automating response actions, and augmenting human analysts' capabilities. Yet, inherent limitations, such as algorithmic biases, data privacy concerns, and the potential for AI-enabled attacks, underscore the need for a comprehensive risk management framework. Regulatory frameworks and industry standards play a crucial role in shaping the development and deployment of AI-powered cybersecurity solutions, ensuring accountability, transparency, and compliance with ethical principles. Moreover, fostering interdisciplinary collaboration and investing in cybersecurity education and training are vital for cultivating a skilled workforce equipped to navigate the evolving threat landscape. By integrating theoretical insights with practical strategies, this paper elucidates key challenges and opportunities in securing AI-driven systems, offering insights for policymakers, researchers, and practitioners alike. Keywords: Cybersecurity; Artificial Intelligence; Threat Detection; Defense Strategies; Ethical Considerations; Regulatory Frameworks.",SEC,https://www.semanticscholar.org/paper/5ccd0970fbdb1e4a321434223ee3e13399ad0851
A Critical Cybersecurity Analysis and Future Research Directions for the Internet of Things: A Comprehensive Review,"The emergence of the Internet of Things (IoT) technology has brought about tremendous possibilities, but at the same time, it has opened up new vulnerabilities and attack vectors that could compromise the confidentiality, integrity, and availability of connected systems. Developing a secure IoT ecosystem is a daunting challenge that requires a systematic and holistic approach to identify and mitigate potential security threats. Cybersecurity research considerations play a critical role in this regard, as they provide the foundation for designing and implementing security measures that can address emerging risks. To achieve a secure IoT ecosystem, scientists and engineers must first define rigorous security specifications that serve as the foundation for developing secure devices, chipsets, and networks. Developing such specifications requires an interdisciplinary approach that involves multiple stakeholders, including cybersecurity experts, network architects, system designers, and domain experts. The primary challenge in IoT security is ensuring the system can defend against both known and unknown attacks. To date, the IoT research community has identified several key security concerns related to the architecture of IoT systems. These concerns include issues related to connectivity, communication, and management protocols. This research paper provides an all-inclusive and lucid review of the current state of anomalies and security concepts related to the IoT. We classify and analyze prevalent security distresses regarding IoT’s layered architecture, including connectivity, communication, and management protocols. We establish the foundation of IoT security by examining the current attacks, threats, and cutting-edge solutions. Furthermore, we set security goals that will serve as the benchmark for assessing whether a solution satisfies the specific IoT use cases.",SEC,https://www.semanticscholar.org/paper/0b2e4715c70c5f79eeaffad6a9774cb0392babd0
The Role of AI in Cybersecurity: Addressing Threats in the Digital Age,"In the contemporary digital landscape, cybersecurity stands as a paramount concern due to the increasing sophistication and frequency of cyber threats. Artificial Intelligence (AI) has emerged as a potent tool in fortifying defenses against these evolving threats. This paper examines the multifaceted role of AI in cybersecurity, elucidating its applications in threat detection, vulnerability assessment, incident response, and predictive analysis. By leveraging machine learning algorithms, AI systems can swiftly analyze vast troves of data to identify anomalous patterns indicative of potential security breaches. Moreover, AI-driven technologies enable proactive defense mechanisms, empowering organizations to preemptively mitigate risks and safeguard sensitive information. However, the deployment of AI in cybersecurity also raises pertinent ethical and privacy considerations, necessitating a balanced approach towards its implementation. Through a comprehensive analysis, this paper underscores the imperative of integrating AI into cybersecurity frameworks to effectively mitigate threats in the digital age.",SEC,https://www.semanticscholar.org/paper/49771cf58fc601e7592fec0cc0de71e0364c3ff2
Machine learning in cybersecurity: A review of threat detection and defense mechanisms,"The cybersecurity concerns get increasingly intricate as the digital world progresses. In light of the increasing complexity of cyber threats, it is imperative to develop and implement advanced and flexible security strategies. Machine Learning (ML) has become a potent tool in strengthening cybersecurity, providing the capacity to scrutinise extensive information, recognise trends, and improve threat detection and defence methods. This paper examines the significance of ML in the field of cybersecurity, with a special emphasis on the identification of threats and the implementation of protective measures. By incorporating ML algorithms into cybersecurity frameworks, organisations may automate decision-making processes, facilitating prompt responses to ever-changing threats. The initial segment explores the terrain of cyber threats, highlighting the necessity for dynamic and aggressive security methods. Conventional solutions that rely on signatures are frequently inadequate when it comes to handling sophisticated, shape-shifting attacks. ML algorithms, in contrast, have exceptional proficiency in identifying nuanced patterns and irregularities within extensive datasets, therefore offering a more efficient method of detecting potential threats. The second section delves into several ML methodologies utilised in cybersecurity, including supervised and unsupervised learning, deep learning, and reinforcement learning. Every approach is assessed based on its suitability for threat detection, demonstrating its advantages and constraints. Furthermore, the relevance of feature engineering and data pretreatment in improving machine learning models for cybersecurity applications. The versatility of ML algorithms allows them to grow with emerging threats, making them a useful tool in the ever-changing arena of cyber warfare. The final segment focuses on real-world applications of machine learning in cybersecurity, presenting successful use cases across sectors. From anomaly detection to behavior analysis, ML algorithms contribute to the discovery of dangerous activity, lowering false positives and strengthening the overall security posture. Lastly, the paper covers the obstacles and ethical issues related to the adoption of ML in cybersecurity. Issues like as adversarial assaults, skewed datasets, and the interpretability of ML models are examined, highlighting the necessity for a holistic strategy that integrates modern technology with ethical considerations. The fusion of human expertise and machine intelligence offers a formidable defense against evolving cyber threats, paving the way for a more resilient and secure digital future.",SEC,https://www.semanticscholar.org/paper/e1ba19c3a819a79b5979cd57289011ba905bad70
DATA CONFIDENTIALITY AND INTEGRITY: A REVIEW OF ACCOUNTING AND CYBERSECURITY CONTROLS IN SUPERANNUATION ORGANIZATIONS,"In an era dominated by digital transformation, superannuation organizations face unprecedented challenges in safeguarding the confidentiality and integrity of sensitive financial data. This review explores the intricate relationship between accounting practices and cybersecurity controls within the context of superannuation entities. By examining the existing literature, regulatory frameworks, and industry best practices, this paper synthesizes the key considerations essential for ensuring robust data protection. The study delves into the critical role of accounting systems in managing financial information and the subsequent implications for data confidentiality. It investigates how evolving accounting standards and practices intersect with cybersecurity protocols to fortify the integrity of financial records within superannuation organizations. The dynamic nature of cyber threats necessitates a comprehensive analysis of technological safeguards, risk management frameworks, and compliance measures to uphold data confidentiality. Furthermore, the review underscores the imperative for a multidimensional approach to cybersecurity in the superannuation sector. It discusses the integration of advanced technologies such as encryption, blockchain, and anomaly detection alongside traditional accounting controls to create a resilient defense against emerging threats. The exploration extends to the examination of employee training programs, incident response strategies, and third-party risk assessments as integral components of a comprehensive cybersecurity posture. As superannuation organizations navigate the complex landscape of data management, a holistic understanding of the interplay between accounting and cybersecurity controls becomes paramount. This review contributes to the existing body of knowledge by providing insights into the challenges and opportunities presented by the evolving technological landscape, offering practitioners and policymakers a foundation for enhancing data confidentiality and integrity in the superannuation sector. Keywords: Data Confidentiality; Accounting; Cybersecurity; Superannuation Organization; Data Integrity.",SEC,https://www.semanticscholar.org/paper/b1c09857fea0540334b7dd9e1f7b5d2b1247d86c
LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition,"The automated and timely conversion of cybersecurity information from unstructured online sources, such as blogs and articles to more formal representations has become a necessity for many applications in the domain nowadays. Named Entity Recognition (NER) is one of the early phases towards this goal. It involves the detection of the relevant domain entities, such as product, version, attack name, etc. in technical documents. Although generally considered a simple task in the information extraction field, it is quite challenging in some domains like cybersecurity because of the complex structure of its entities. The state of the art methods require time-consuming and labor intensive feature engineering that describes the properties of the entities, their context, domain knowledge, and linguistic characteristics. The model demonstrated in this paper is domain independent and does not rely on any features specific to the entities in the cybersecurity domain, hence does not require expert knowledge to perform feature engineering. The method used relies on a type of recurrent neural networks called Long Short-Term Memory (LSTM) and the Conditional Random Fields (CRFs) method. The results we obtained showed that this method outperforms the state of the art methods given an annotated corpus of a decent size.",SEC,https://www.semanticscholar.org/paper/f169931858410fe06af98967fc131669a8c81ac4
CYBERSECURITY DYNAMICS IN NIGERIAN BANKING: TRENDS AND STRATEGIES REVIEW,"This paper provides an in-depth review of the cybersecurity dynamics within the Nigerian banking sector, emphasizing recent trends and strategic approaches to address emerging challenges. As a review paper, it synthesizes existing literature, reports, and case studies to offer a comprehensive understanding of the current cybersecurity landscape in Nigerian banks. The focus is on identifying the predominant cyber threats, analyzing the sector's response strategies, and evaluating the effectiveness of these measures in the context of Nigeria's unique socio-economic and regulatory environment. Our analysis reveals a notable escalation in cyber threats, particularly phishing, ransomware, and insider attacks, which have been intensified by the rapid digital transformation in banking services. The review identifies key factors contributing to these challenges, such as the increasing sophistication of cybercriminals, the digital literacy gap among customers, and the evolving nature of cyber threats. It also examines the strategic responses of Nigerian banks, including the adoption of advanced security technologies, enhanced staff training, and collaboration with government and international cybersecurity bodies.The paper concludes that Nigerian banks have made significant strides in fortifying their cybersecurity defenses. However, it also highlights the need for more robust regulatory frameworks, increased customer awareness initiatives, and a shift towards more integrated and proactive cybersecurity strategies. The findings of this review underscore the critical need for continuous evolution and investment in cybersecurity measures to effectively counter the dynamic and complex nature of cyber threats in the Nigerian banking sector. Keywords: Cybersecurity, Cybersecurity Dynamics, Nigerian Banking Sector, Digital Transformation, Cyber Threats, Phishing Attacks, Ransomware, Insider Threats, Regulatory Framework, Central Bank of Nigeria (CBN), Compliance Challenges, Security Technologies, Cybersecurity Awareness, Artificial Intelligence (AI), Machine Learning (ML), Blockchain Technology.",SEC,https://www.semanticscholar.org/paper/be8940fd490e058cb851a21bd455ffb9b01d025a
Large Language Models in Cybersecurity: State-of-the-Art,"The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.",SEC,https://www.semanticscholar.org/paper/a88d5fb82358d19d0daed7d35d49f03ea6796b57
MASTERING COMPLIANCE: A COMPREHENSIVE REVIEW OF REGULATORY FRAMEWORKS IN ACCOUNTING AND CYBERSECURITY,"In the rapidly evolving landscape of business and technology, the intersection of accounting and cybersecurity has become a focal point for organizations striving to maintain integrity, security, and regulatory adherence. This paper presents a meticulous examination of regulatory frameworks governing both accounting and cybersecurity domains. The study aims to provide a comprehensive understanding of the intricate compliance landscape, offering valuable insights for practitioners, policymakers, and scholars. The investigation unfolds through a dual lens, meticulously dissecting the regulatory intricacies surrounding financial reporting in accounting and the safeguarding of digital assets in cybersecurity. A critical analysis of prominent global regulatory bodies, such as the Financial Accounting Standards Board (FASB), the International Financial Reporting Standards (IFRS), and cybersecurity standards like ISO 27001 and NIST Cybersecurity Framework, forms the cornerstone of this research. The paper delves into the historical evolution of accounting and cybersecurity regulations, identifying key milestones and paradigm shifts that have shaped the current regulatory environment. It explores the synergies and dissonances between these two critical domains, shedding light on how compliance efforts in one area may impact the other. Furthermore, the study investigates the challenges and opportunities presented by emerging technologies such as blockchain, artificial intelligence, and cloud computing in the context of regulatory compliance. By examining real-world case studies and industry best practices, this thesis provides practical insights for organizations seeking to navigate the complex terrain of compliance in an era of digital transformation. The paper offers a holistic and forward-looking perspective on the regulatory frameworks governing accounting and cybersecurity. Through its comprehensive analysis, the thesis aims to equip professionals and academics with the knowledge and tools necessary to navigate the intricate regulatory landscape, fostering a proactive and adaptive approach to compliance in the dynamic business environment. Keywords: Regulatory Frameworks, Accounting, Cybersecurity, Cloud Computing, Blockchain",SEC,https://www.semanticscholar.org/paper/754f3d75adbf243cda601bb815784a2d73bbb711
Harnessing adversarial machine learning for advanced threat detection: AI-driven strategies in cybersecurity risk assessment and fraud prevention,"The abstract is ""The rapid evolution of cyber threats necessitates innovative defenses, particularly in the domains of risk assessment and fraud detection. This paper explores the integration of Artificial Intelligence (AI) and Adversarial Machine Learning (ML) techniques as a formidable strategy against increasingly sophisticated cyber-attacks. We present a comprehensive framework that leverages AI to dynamically assess cybersecurity risks and detect fraudulent activities with unprecedented accuracy and speed. Firstly, we delve into the foundational principles of adversarial machine learning, outlining how these techniques can be employed to simulate potential cyber threats, thereby enabling the development of more resilient AI-driven cybersecurity systems. We highlight the dual role of adversarial ML in both enhancing security defenses and potentially serving as a vector for sophisticated attacks, underscoring the importance of developing robust, adversarial-resistant models. Subsequently, we introduce a novel adaptive risk assessment methodology that incorporates real-time data analysis, machine learning algorithms, and predictive modeling to accurately identify and prioritize threats. This method adapts to the evolving digital landscape, ensuring that cybersecurity measures are always one step ahead of potential attackers. In the context of fraud detection, we explore -how AI algorithms can sift through vast datasets to detect anomalies and patterns indicative of fraudulent behavior. Through case studies and empirical analysis, we demonstrate the effectiveness of AI in identifying fraud across various sectors, from financial transactions to online identity verification processes. Our research contributes to the cybersecurity field by providing a detailed examination of how AI and adversarial ML can be harnessed to fortify digital defenses, improve risk assessment techniques, and enhance fraud detection capabilities. The insights garnered from this study not only advance theoretical understanding but also offer practical guidance for organizations seeking to implement AI-driven security solutions. As cyber threats continue to evolve, the integration of AI and adversarial ML in cybersecurity strategies will be paramount in safeguarding digital assets and maintaining the integrity of online systems.""",SEC,https://www.semanticscholar.org/paper/3bc1d69fd6b8010e220f2dbb53f01f14bac3dfcc
Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.",SEC,https://www.semanticscholar.org/paper/aa1b6eb2563523b72d01d2570f71a81930a83320
"The future of Cybersecurity in renewable energy systems: A review, identifying challenges and proposing strategic solutions","This study provides a comprehensive examination of cybersecurity within renewable energy systems, highlighting the critical role of cybersecurity measures in ensuring the sustainability and reliability of these systems. With the increasing reliance on renewable energy sources, the need for robust cybersecurity frameworks to protect against evolving cyber threats has never been more pressing. Through a systematic literature review and content analysis, this research identifies the prevalent cyber threats and vulnerabilities specific to renewable energy infrastructures, evaluates the effectiveness of current cybersecurity measures, and explores cutting-edge technologies and practices in the field. The methodology encompasses a detailed analysis of peer-reviewed academic journals, conference proceedings, industry reports, and white papers published from 2010 to 2024. This approach facilitates the identification of gaps in current cybersecurity practices and the proposal of strategic solutions to address these challenges. Key insights reveal the significance of adopting advanced cybersecurity technologies, such as artificial intelligence and machine learning algorithms, to enhance threat detection and mitigation efforts. The study concludes with strategic recommendations for industry practitioners and policymakers, emphasizing the importance of a proactive cybersecurity posture, collaboration and information sharing, investment in cybersecurity training, and the development of specific cybersecurity standards and regulations for the renewable energy sector. Future research directions are suggested to further explore innovative cybersecurity solutions and assess their implications for renewable energy systems. This study underscores the necessity of integrating robust cybersecurity measures to safeguard the future of sustainable energy. Keywords: Cybersecurity, Renewable Energy, Cyber Threats, Vulnerabilities, Advanced Cybersecurity Technologies.",SEC,https://www.semanticscholar.org/paper/fdc3a3723513bd55fd9e828455cce0ad34a8287f
A Situation Based Predictive Approach for Cybersecurity Intrusion Detection and Prevention Using Machine Learning and Deep Learning Algorithms in Wireless Sensor Networks of Industry 4.0,"Industry 4.0 is fundamentally based on networked systems. Real-time communication between machines, sensors, devices, and people makes it easier to transmit the data needed to make decisions. Informed decision-making is empowered by the comprehensive insights and analytics made possible by this connectedness in conjunction with information transparency. Industry 4.0-based wireless sensor networks (WSNs) are an integral part of modern industrial operations however, these networks face escalating cybersecurity threats. These networks are always vulnerable to cyber-attacks as they continuously collect data and optimize processes. Increased connections make people more susceptible to cyberattacks, necessitating the use of strong cybersecurity measures to protect sensitive data. This study proposes a predictive framework intended to intelligently prioritize and prevent cybersecurity intrusions on WSNs in Industry 4.0. The proposed framework enhances the cybersecurity of WSNs in Industry 4.0 using a multi-criteria approach. It implements machine-learning and deep-learning algorithms for cybersecurity intrusion detection in WSNs of Industry 4.0 and provides prevention by assigning priorities to the threats based on the situation and nature of the attacks. We implemented three models, i.e., Decision Tree, MLP, and Autoencoder, as proposed algorithms in the framework. For multidimensional classification and detection of cybersecurity intrusions, we implemented Decision Tree and MLP models. For binary classification and detection of cybersecurity intrusions in WSNs of Industry 4.0, we implemented Autoencoder model. Simulation results show that the Decision Tree model provides an accuracy of 99.48%, precision of 99.49%, recall of 99.48%, and F1 score of 99.49% in the detection and classification of cybersecurity intrusions. The MLP model provides an accuracy of 99.52%, precision of 99.5%, recall of 99.5%, and F1 score of 99.5% in the detection and classification of cybersecurity intrusions. The implementation of Autoencoder with binary classification yields an accuracy of 91%, a precision of 92%, a recall of 91%, and an F1 score of 91%. The benchmark models, i.e., Random Forest (RF) for multidimensional classification and Logistic Regression (LR) for binary classification, have also been implemented. We compared the performance of the benchmark models with the models implemented in the proposed framework, revealing that the models in the proposed framework",SEC,https://www.semanticscholar.org/paper/6c34f93dbcb34e259114f3fcaaeb202c495ca3cb
Aligning sustainable development goals with cybersecurity strategies: Ensuring a secure and sustainable future,"This study explores the critical intersection between cybersecurity and sustainable development, aiming to understand how cybersecurity measures can support the achievement of the Sustainable Development Goals (SDGs). Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, conference proceedings, and reports from international organizations, focusing on literature published from 2010 to 2024. The inclusion criteria targeted works that directly address the role of cybersecurity in sustainable development, particularly those discussing emerging technologies and their potential to enhance digital security in support of the SDGs. The exclusion criteria filtered out non-peer-reviewed articles, opinion pieces, and studies not explicitly linking cybersecurity with sustainable development efforts. Key findings highlight the indispensable role of cybersecurity in safeguarding digital infrastructure essential for achieving SDGs, emphasizing the transformative potential of innovations such as blockchain technology and artificial intelligence in enhancing cybersecurity measures. The study identifies significant challenges at the intersection of cybersecurity and sustainability, including emerging threats and the need for a global framework to integrate cybersecurity within sustainable development efforts. Strategic recommendations for stakeholders encompass fostering international cooperation, investing in cybersecurity education, and promoting inclusive cybersecurity practices. Finally, the study underscores the necessity of integrating advanced cybersecurity measures with sustainable development initiatives. Enhanced cybersecurity is pivotal for creating a secure, resilient, and sustainable digital future, thereby supporting the global pursuit of the Sustainable Development Goals.",SEC,https://www.semanticscholar.org/paper/208c0148b8e4bad8a557be72e424a01092389dc9
Best practices in cybersecurity for green building management systems: Protecting sustainable infrastructure from cyber threats,"This study explores the critical intersection of cybersecurity and sustainable infrastructure, with a focus on Green Building Management Systems (GBMS). Recognizing the increasing sophistication of cyber threats and the integration of digital technologies in sustainable buildings, this research aims to understand the challenges and prospects of cybersecurity within this context. Employing a systematic literature review and content analysis, the study examines peer-reviewed articles, conference proceedings, and industry reports from 2010 to 2024. The methodology facilitates a comprehensive understanding of the evolution, current practices, and future directions of cybersecurity measures in sustainable infrastructure. Key findings reveal that robust cybersecurity measures are foundational to protecting the digital and physical assets underpinning sustainable infrastructure. The study identifies core principles of cybersecurity, such as resilience and the integration of cybersecurity with sustainability efforts, as crucial for enhancing the security posture of GBMS. Looking ahead, the research anticipates a future where cybersecurity measures are seamlessly integrated into sustainable buildings, ensuring resilience against cyber threats while advancing sustainability goals. Strategic recommendations include adopting international standards, fostering interdisciplinary collaboration, investing in cybersecurity education, and leveraging emerging technologies. The study concludes that advancing research in cybersecurity technologies tailored for sustainable infrastructure is essential for navigating the complexities of cybersecurity in green building management.",SEC,https://www.semanticscholar.org/paper/2c2accf9d8a80d996ee79049af202385b87bfd3f
Artificial intelligence in cybersecurity: Protecting national infrastructure: A USA review,"Artificial Intelligence (AI) has emerged as a transformative force in the field of cybersecurity, playing a pivotal role in safeguarding national infrastructure. This review focuses on the application of AI technologies within the context of the United States, examining their efficacy in fortifying critical systems against evolving cyber threats. The paper delves into various AI-driven cybersecurity strategies, ranging from anomaly detection and predictive analysis to threat intelligence and automated response mechanisms. The integration of AI in cybersecurity not only enhances the speed and accuracy of threat detection but also addresses the dynamic nature of cyber threats. The specific AI technologies employed in the United States, including machine learning, natural language processing, and neural networks, highlighting their contributions to bolstering the resilience of national infrastructure are also examined. Furthermore, the challenges and ethical considerations associated with the widespread adoption of AI in cybersecurity are assessed. It discusses the need for robust regulatory frameworks to govern the deployment of AI in sensitive domains and emphasizes the importance of collaboration between government agencies, private enterprises, and research institutions to foster innovation and address emerging threats. In conclusion, this review provides a comprehensive analysis of the role of AI in cybersecurity within the United States, emphasizing its significance in protecting critical national infrastructure. By exploring technological advancements, challenges, and ethical considerations, this paper contributes to the ongoing discourse on leveraging AI to safeguard against the ever-evolving landscape of cyber threats.",SEC,https://www.semanticscholar.org/paper/3139e0c7002df948519a66b5a57a43fab9e013b9
AI-Driven Cybersecurity: Balancing Advancements and Safeguards,"As Artificial Intelligence (AI) continues its rapid evolution, its profound influence on cybersecurity becomes increasingly evident. This study delves into the pivotal role of AI in fortifying cybersecurity measures, emphasizing its capacity for enhanced threat detection, automated response mechanisms, and the development of resilient security frameworks. However, alongside its promise, recognition of AI's susceptibility to exploitation in sophisticated cyber-attacks exists, underscoring the imperative for continual advancements in AI-driven security solutions. This research offers a nuanced perspective on AI's impact on cybersecurity, advocating for the proactive integration of AI strategies, sustained research efforts, and formulating ethical guidelines. Adopting supervised machine learning (ML) algorithms like decision trees, support vector machines, and neural networks aims to harness AI's potential to bolster cybersecurity while concurrently addressing associated risks, paving the way for a secure digital landscape. Regarding accuracy, the neural network outperforms other models by 98%.",SEC,https://www.semanticscholar.org/paper/767e710dd7036a4b01a9a2156b006b66bc4b2250
COMPREHENSIVE REVIEW ON CYBERSECURITY: MODERN THREATS AND ADVANCED DEFENSE STRATEGIES,"In the rapidly evolving landscape of cyberspace, the prevalence of sophisticated cyber threats has escalated, posing formidable challenges to individuals, organizations, and nations. This comprehensive review explores the contemporary panorama of cybersecurity, focusing on the latest threats and the advanced defense strategies employed to mitigate them. The analysis encompasses a wide spectrum of cyber threats, including malware, ransomware, phishing attacks, and advanced persistent threats (APTs), shedding light on their evolving tactics, techniques, and procedures. The review delves into the intricate world of cybercrime, emphasizing the motives behind attacks and the diverse range of threat actors involved, from individual hackers to state-sponsored entities. By examining recent case studies and real-world incidents, the review provides valuable insights into the dynamic nature of cyber threats, emphasizing the need for proactive and adaptive cybersecurity measures. Furthermore, the review critically evaluates cutting-edge defense mechanisms and strategies deployed to counteract these threats. It explores advancements in artificial intelligence, machine learning, and behavioral analytics, showcasing their pivotal roles in bolstering cybersecurity defenses. Additionally, the review discusses the importance of threat intelligence sharing, collaborative efforts, and international cooperation to fortify the global cyber defense ecosystem. As cybersecurity extends beyond technical measures, the review also addresses the human element, emphasizing the significance of cybersecurity awareness training and the role of employees in fortifying organizational resilience. It scrutinizes regulatory frameworks and compliance standards that play a crucial role in shaping cybersecurity policies and practices. By synthesizing the latest research, industry best practices, and expert insights, this comprehensive review aims to provide a holistic understanding of the current state of cybersecurity, offering practitioners, policymakers, and researchers a valuable resource to navigate the intricate challenges posed by modern cyber threats and to develop effective defense strategies for the digital age. Keywords: Cybersecurity, Threats, Defense Strategy, Cyber Threats, Review.",SEC,https://www.semanticscholar.org/paper/1098d3743083c4c6a39a124efb725c10d2b7423c
CYBERSECURITY IN THE FINANCIAL SECTOR: A COMPARATIVE ANALYSIS OF THE USA AND NIGERIA,"This paper provides a comprehensive review and comparative analysis of cybersecurity challenges and strategies within the financial sectors of the United States of America (USA) and Nigeria. It aims to elucidate the complexities and variances in cybersecurity practices, focusing on the different approaches taken by these nations to safeguard their financial data against increasing cyber threats. Through a detailed examination of existing literature, including academic journals, industry reports, and cybersecurity incident databases, this study identifies the unique and common cybersecurity vulnerabilities, regulatory environments, and defense mechanisms employed by the financial sectors in both countries. The review reveals that the USA's financial sector benefits from advanced cybersecurity technologies and a strong regulatory framework, yet faces challenges related to sophisticated cyber-attacks and the management of insider threats. Conversely, Nigeria's financial sector grapples with issues such as limited cybersecurity awareness, technological constraints, and evolving regulatory frameworks. Despite these disparities, both countries share the necessity of enhancing their cybersecurity posture to combat the evolving nature of cyber threats effectively. Conclusively, the paper argues that addressing cybersecurity in the financial sector necessitates a comprehensive approach that includes not only technological solutions but also the strengthening of regulatory policies, enhancement of cybersecurity awareness, and fostering international collaboration. The comparative analysis underscores the importance of adopting best practices from each country's experience, aiming to bolster the resilience of financial institutions against cyber threats in an increasingly interconnected world. Keywords: Cybersecurity, Financial Sector, United States, Nigeria, Digital Infrastructure, Technological Vulnerabilities, Regulatory Complexities, Human Factors, Advanced Detection, Prevention Technologies, Machine Learning, Anomaly Detection, Cybersecurity Frameworks, Awareness, Training, Culture, Public-Private Partnerships, Threat Intelligence, Best Practices, Innovation, Regulatory Foresight, Human Capital Development, Stability, Integrity, Collaboration",SEC,https://www.semanticscholar.org/paper/e30a06e161d7ef4612ae56e5e91f7c8c15fe6951
Digital Transformation and Cybersecurity Challenges for Businesses Resilience: Issues and Recommendations,"This systematic literature review explores the digital transformation (DT) and cybersecurity implications for achieving business resilience. DT involves transitioning organizational processes to IT solutions, which can result in significant changes across various aspects of an organization. However, emerging technologies such as artificial intelligence, big data and analytics, blockchain, and cloud computing drive digital transformation worldwide while increasing cybersecurity risks for businesses undergoing this process. This literature survey article highlights the importance of comprehensive knowledge of cybersecurity threats during DT implementation to prevent interruptions due to malicious activities or unauthorized access by attackers aiming at sensitive information alteration, destruction, or extortion from users. Cybersecurity is essential to DT as it protects digital assets from cyber threats. We conducted a systematic literature review using the PRISMA methodology in this research. Our literature review found that DT has increased efficiency and productivity but poses new challenges related to cybersecurity risks, such as data breaches and cyber-attacks. We conclude by discussing future vulnerabilities associated with DT implementation and provide recommendations on how organizations can mitigate these risks through effective cybersecurity measures. The paper recommends a staged cybersecurity readiness framework for business organizations to be prepared to pursue digital transformation.",SEC,https://www.semanticscholar.org/paper/e9d573b0a7f72883c76a8457c6fc62cec69769d1
A REVIEW OF CYBERSECURITY STRATEGIES IN MODERN ORGANIZATIONS: EXAMINING THE EVOLUTION AND EFFECTIVENESS OF CYBERSECURITY MEASURES FOR DATA PROTECTION,"In an era where digital threats are increasingly pervasive, understanding the evolution and efficacy of cybersecurity strategies in modern organizations is paramount. This study provides a comprehensive analysis of the dynamic landscape of cybersecurity, exploring its progression from traditional methods to innovative, technology-driven approaches. The digital age has ushered in complex cyber threats, necessitating robust cybersecurity measures. This study examines cybersecurity strategies' historical development, current trends, and future directions across different organizational contexts and industries. The primary aim is to assess the evolution and effectiveness of cybersecurity measures, identify existing gaps, and understand the interplay between human behavior, technology, and policy in cybersecurity. The paper encompasses a comprehensive methodological framework for cybersecurity analysis, exploring the effectiveness of traditional versus modern approaches, the role of AI and ML, and the impact of international policies. It also presents case studies to illustrate successes and failures in cybersecurity implementation. Key findings reveal a significant shift towards advanced technologies like AI and ML in cybersecurity, the critical role of human factors in shaping cybersecurity outcomes, and the influence of international policies in standardizing cybersecurity practices. The study concludes that effective cybersecurity strategies require a balanced approach, combining technological advancements with understanding human factors and adherence to international standards. Recommendations include continuous education and training, adopting holistic cybersecurity strategies, and aligning with international policies. Keywords: Cybersecurity, Artificial Intelligence, Machine Learning, International Policies, Human Factors, Cyber Threats.",SEC,https://www.semanticscholar.org/paper/983968c21137edc191dd6d8b1578858f98ca5a93
Cybersecurity risks in online banking: A detailed review and preventive strategies applicatio,"In an era where the digital transformation of the banking sector intersects with the escalating complexity of cyber threats, this paper endeavors to dissect the multifaceted realm of cybersecurity within the banking industry. With a backdrop of increasing online banking adoption and the concomitant rise in cybercrime, the study aims to illuminate the current cybersecurity landscape, evaluate the efficacy of existing frameworks and propose strategic enhancements to fortify digital defenses. Employing a methodological amalgam of literature review and analysis of recent cybersecurity incidents, this investigation delves into the intricacies of cyber threats, the financial repercussions of breaches and the robustness of current cybersecurity measures in banking. The scope of this paper encompasses a comprehensive examination of recent cyber incidents, an assessment of the financial impact of cyber-attacks, an evaluation of the effectiveness of existing cybersecurity frameworks and the formulation of strategic recommendations for bolstering cybersecurity measures. Through this scholarly inquiry, key findings emerge, highlighting the critical need for dynamic cybersecurity strategies that integrate advanced technologies, promote regulatory compliance and foster a culture of cybersecurity awareness. Conclusively, the study posits that the banking sector must embrace a holistic and adaptive approach to cybersecurity, underscored by strategic investments in technology, education, and collaboration. Recommendations advocate for the integration of Big Data analytics, artificial intelligence and continuous risk assessment methodologies to navigate the evolving cyber threat landscape effectively. This paper serves as a clarion call to banking institutions, urging a reinvigorated commitment to cybersecurity resilience in safeguarding financial assets and customer trust against the backdrop of digital transformation.",SEC,https://www.semanticscholar.org/paper/1d028098b8301aa66e6a575e2e92573c9da1e92a
CYBERSECURITY CHALLENGES IN SMART CITIES: A CASE REVIEW OF AFRICAN METROPOLISES,"The rapid urbanization and digital transformation of cities across Africa have given rise to the concept of Smart Cities, where advanced technologies are integrated to enhance efficiency, sustainability, and the overall quality of urban life. However, this paradigm shift towards interconnected and technology-driven urban environments brings forth a host of cybersecurity challenges that demand careful consideration. This paper explores the cybersecurity challenges in Smart Cities, focusing on a case review of African metropolises. African cities, emblematic of the global urbanization trend, are embracing Smart City initiatives to address urban challenges and foster economic development. While these initiatives promise improved services and enhanced connectivity, they concurrently expose cities to a myriad of cybersecurity threats. The interconnectedness of devices and systems in Smart Cities creates a vast attack surface, making them susceptible to cyber-attacks ranging from data breaches to infrastructure disruptions. This case review delves into specific instances of cybersecurity challenges faced by African metropolises in their quest for technological advancement. It analyzes the vulnerabilities in critical infrastructure, such as energy grids, transportation systems, and healthcare networks, highlighting the potential risks associated with inadequate cybersecurity measures. Moreover, the paper sheds light on the socio-economic implications of cyber threats in Smart Cities, emphasizing the importance of resilient cybersecurity frameworks in safeguarding citizen data and urban functionality. In conclusion, the paper underscores the urgent need for comprehensive cybersecurity strategies tailored to the unique challenges faced by Smart Cities in Africa. The findings aim to contribute to a better understanding of the intricate relationship between urbanization, technology, and cybersecurity, offering insights that can inform policy decisions, technological implementations, and collaborative efforts to build secure and resilient Smart Cities in the African context. Keywords: Cybersecurity, Smart Cities, Africa, Metropolis, Review.",SEC,https://www.semanticscholar.org/paper/929ed7d5bd4460f64fdc052e5de97a820bbde65b
CYBERSECURITY AWARENESS AND EDUCATION PROGRAMS: A REVIEW OF EMPLOYEE ENGAGEMENT AND ACCOUNTABILITY,"As organizations continue to grapple with the escalating threat landscape of cyber-attacks, the imperative to fortify their cybersecurity defenses becomes increasingly paramount. This review delves into the critical realm of cybersecurity awareness and education programs, focusing on the pivotal factors of employee engagement and accountability. The effectiveness of these programs in cultivating a cyber-resilient workforce is scrutinized through an extensive examination of existing literature, empirical studies, and industry practices. The review begins by exploring the foundational elements of cybersecurity awareness programs, elucidating the significance of imparting knowledge and instilling a culture of vigilance among employees. It examines the diverse methodologies employed in these programs, ranging from interactive workshops and simulated phishing exercises to online modules and gamified learning platforms. A comparative analysis of these approaches sheds light on their respective strengths and limitations. A central theme of this review revolves around the nexus between employee engagement and cybersecurity resilience. It delves into the psychological and behavioral aspects of engagement, assessing how motivational factors and tailored learning experiences contribute to heightened cybersecurity awareness. The impact of organizational culture and leadership support on fostering a sense of responsibility among employees is also explored, emphasizing the need for a holistic approach that transcends mere compliance. Furthermore, the review investigates the role of accountability in sustaining the efficacy of cybersecurity initiatives. It examines the mechanisms employed by organizations to enforce adherence to security policies and protocols, emphasizing the role of robust monitoring systems, clear communication channels, and consequence management. Case studies and real-world examples are integrated to illustrate instances of successful accountability frameworks and their influence on overall cybersecurity posture. This review synthesizes key findings and identifies emerging trends in cybersecurity awareness and education programs, with a particular focus on optimizing employee engagement and fostering a culture of accountability. The insights gleaned from this analysis provide a roadmap for organizations seeking to fortify their defenses against evolving cyber threats by cultivating a vigilant and proactive workforce. Keywords: Cybersecurity, Education, Cyber threat, Employee engagement, Accountability.",SEC,https://www.semanticscholar.org/paper/70824b5fdee902acc0e6baec71ca560e90764bc3
Cybersecurity’s Role in Environmental Protection and Sustainable Development: Bridging Technology and Sustainability Goals,"This study investigates the pivotal role of cybersecurity in bolstering environmental protection and sustainable development, a critical yet underexplored nexus in contemporary research. Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, conference proceedings, and industry reports from 2015 to 2023, sourced from databases such as IEEE Xplore, ScienceDirect, and Google Scholar. The methodology is anchored in a rigorous search strategy, leveraging keywords related to cybersecurity, sustainability, and communication technologies, and adheres to defined inclusion and exclusion criteria to ensure the relevance and quality of the literature reviewed. Key findings highlight cybersecurity as an indispensable enabler of sustainable development initiatives, safeguarding the technological infrastructure essential for environmental conservation efforts. The study identifies evolving cyber threats as a significant challenge, necessitating adaptive security measures that anticipate and mitigate potential vulnerabilities. Furthermore, it underscores the opportunities presented by advanced cybersecurity technologies, such as artificial intelligence and blockchain, in enhancing the security and efficiency of sustainable practices. Strategic recommendations emphasize the need for comprehensive cybersecurity frameworks, stakeholder collaboration, cybersecurity education, and alignment with regulatory standards to fortify the resilience of sustainability initiatives against cyber threats. The study concludes that integrating robust cybersecurity measures is paramount in the pursuit of sustainable development goals, calling for ongoing vigilance, innovation, and interdisciplinary collaboration to navigate the complex landscape of digital threats and opportunities. This research contributes valuable insights into the critical intersection of cybersecurity and sustainability, offering a foundation for future studies and strategic initiatives aimed at securing sustainable development in the digital age. Keywords: Cybersecurity, Sustainable Development, Environmental Protection, Advanced Security Technologies.",SEC,https://www.semanticscholar.org/paper/39762dd992057072ccbde2f866e0448dec5d4069
Investigate the multifaceted dynamics of cybersecurity practices and their impact on the quality of e-government services: evidence from the KSA,"Purpose The Kingdom of Saudi Arabia (KSA) is embracing digital transformation and e-government services, aiming to improve efficiency, accessibility and citizen-centricity. Nonetheless, the country faces challenges such as evolving cyber threats. The purpose of this study is to investigate the factors influencing cybersecurity practices to ensure the reliability and security of e-government services. Design/methodology/approach This paper investigates the multifaceted dynamics of cybersecurity practices and their impact on the quality and effectiveness of e-government services. Five key factors explored include organizational culture, technology infrastructure, adherence to standards and regulations, employee training and awareness and financial investment in cybersecurity. This study used a quantitative method to gather data from 320 participants. The researcher collected 285 completed questionnaires, excluding unusable or incomplete responses, and analyzed the final data set using partial least squares structural equation modeling. Findings The findings show that financial investment in cybersecurity, employee training and awareness and adherence to cybersecurity regulations significantly influence the adoption of robust cybersecurity practices. However, the relationship between organizational culture and cybersecurity practices is less straightforward. The research establishes a strong positive correlation between cybersecurity practices and e-government service quality, highlighting the role of security in fostering public trust and user satisfaction and meeting the evolving needs of citizens and businesses. Originality/value This research contributes valuable empirical evidence to the fields of e-government and cybersecurity, offering insights that can inform evidence-based policy decisions and resource allocation. By understanding the nuanced dynamics at play, Saudi Arabia is better poised to fortify its digital governance infrastructure and provide secure, high-quality e-government services to its constituents.",SEC,https://www.semanticscholar.org/paper/6a91e9782a94dfa4dba78d2c7418730511ab8627
Cyber Threat Intelligence Mining for Proactive Cybersecurity Defense: A Survey and New Perspectives,"Today’s cyber attacks have become more severe and frequent, which calls for a new line of security defenses to protect against them. The dynamic nature of new-generation threats, which are evasive, resilient, and complex, makes traditional security systems based on heuristics and signatures struggle to match. Organizations aim to gather and share real-time cyber threat information and then turn it into threat intelligence for preventing attacks or, at the very least, responding quickly in a proactive manner. Cyber Threat Intelligence (CTI) mining, which uncovers, processes, and analyzes valuable information about cyber threats, is booming. However, most organizations today mainly focus on basic use cases, such as integrating threat data feeds with existing network and firewall systems, intrusion prevention systems, and Security Information and Event Management systems (SIEMs), without taking advantage of the insights that such new intelligence can deliver. In order to make the most of CTI so as to significantly strengthen security postures, we present a comprehensive review of recent research efforts on CTI mining from multiple data sources in this article. Specifically, we provide and devise a taxonomy to summarize the studies on CTI mining based on the intended purposes (i.e., cybersecurity-related entities and events, cyber attack tactics, techniques and procedures, profiles of hackers, indicators of compromise, vulnerability exploits and malware implementation, and threat hunting), along with a comprehensive review of the current state-of-the-art. Lastly, we discuss research challenges and possible future research directions for CTI mining.",SEC,https://www.semanticscholar.org/paper/1dce1ebadb25d7466de773335e7965a3d4a328ef
Recent Advances on Federated Learning for Cybersecurity and Cybersecurity for Federated Learning for Internet of Things,"Decentralized paradigm in the field of cybersecurity and machine learning (ML) for the emerging Internet of Things (IoT) has gained a lot of attention from the government, academia, and industries in recent years. Federated cybersecurity (FC) is regarded as a revolutionary concept to make the IoT safer and more efficient in the future. This emerging concept has the potential of detecting security threats, taking countermeasures, and limiting the spreading of threats over the IoT network system efficiently. An objective of cybersecurity is achieved by forming the federation of the learned and shared model on top of various participants. Federated learning (FL), which is regarded as a privacy-aware ML model, is particularly useful to secure the vulnerable IoT environment. In this article, we start with background and comparison of centralized learning, distributed on-site learning, and FL, which is then followed by a survey of the application of FL to cybersecurity for IoT. This survey primarily focuses on the security aspect but it also discusses several approaches that address the performance issues (e.g., accuracy, latency, resource constraint, and others) associated with FL, which may impact the security and overall performance of the IoT. To anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts, challenges, and research trends in this area. With this article, readers can have a more thorough understanding of FL for cybersecurity as well as cybersecurity for FL, different security attacks, and countermeasures.",SEC,https://www.semanticscholar.org/paper/ef8f8dc9eff937d65a312ee619c16cf06eb3e456
Cybersecurity of Smart Inverters in the Smart Grid: A Survey,"The penetration of distributed energy resources (DERs) in smart grids significantly increases the number of field devices owned and controlled by consumers, aggregators, third parties, and utilities. As the interface between DER and power grids, DER inverters are becoming smarter with various grid-support functions and communication capabilities. Meanwhile, the cybersecurity risks of smart inverters are also on the rise due to the extensive utilization of information and communication technologies. The potential negative impacts of cyberattacks on smart inverters have attracted significant attention from scholars and organizations. To advance the research on smart inverter cybersecurity and provide insights into its technical achievements, barriers, and future directions, this article will give a comprehensive review of critical attacks and defense strategies for smart inverters and inverter-based systems like microgrids. We start this survey with an overview of the smart inverter introduction, including device- and grid-level architectures, grid-support functions, and communication protocols. We then review various cyberattacks and defense strategies in different categories and scenarios tailed with discussions including their feasibility and remaining gaps. Finally, we discuss the opportunities and challenges of emerging technologies that can secure smart inverters. We hope this survey can inspire efforts to close research gaps and develop more mature cybersecurity solutions for smart inverters in the smart grid.",SEC,https://www.semanticscholar.org/paper/c6874a0d173de76c1e79df43324c4d58d30a30bc
Towards Artificial Intelligence-Based Cybersecurity: The Practices and ChatGPT Generated Ways to Combat Cybercrime,"Today, cybersecurity is considered one of the most noteworthy topics that are circulated frequently among companies in order to protect their data from hacking operations. The emergence of cyberspace contributed to the growth of electronic systems. It is a virtual digital space through which interconnection is established between computers and smartphones connected within the Internet of Things environment. This space is critical in building a safe digital environment free of threats and cybercrime. It is only possible to make a digital environment with the presence of cyberspace, which contains modern technologies that make this environment safe and far from unauthorized individuals. Cybersecurity has a wide range of challenges and obstacles in performance, and it is difficult for companies to face them. In this report, the most significant practices, sound, and good strategies will be studied to stop cybercrime and make a digital environment that guarantees data transfers between electronic devices safely and without the presence of malicious software. This report concluded that the procedures provided by cybersecurity are required and must be taken care of and developed.",SEC,https://www.semanticscholar.org/paper/81e8396f2257952819dabd425fd6ac7c48d16fb8
"Cybersecurity for Sustainable Smart Healthcare: State of the Art, Taxonomy, Mechanisms, and Essential Roles","Cutting-edge technologies have been widely employed in healthcare delivery, resulting in transformative advances and promising enhanced patient care, operational efficiency, and resource usage. However, the proliferation of networked devices and data-driven systems has created new cybersecurity threats that jeopardize the integrity, confidentiality, and availability of critical healthcare data. This review paper offers a comprehensive evaluation of the current state of cybersecurity in the context of smart healthcare, presenting a structured taxonomy of its existing cyber threats, mechanisms and essential roles. This study explored cybersecurity and smart healthcare systems (SHSs). It identified and discussed the most pressing cyber threats and attacks that SHSs face, including fake base stations, medjacking, and Sybil attacks. This study examined the security measures deployed to combat cyber threats and attacks in SHSs. These measures include cryptographic-based techniques, digital watermarking, digital steganography, and many others. Patient data protection, the prevention of data breaches, and the maintenance of SHS integrity and availability are some of the roles of cybersecurity in ensuring sustainable smart healthcare. The long-term viability of smart healthcare depends on the constant assessment of cyber risks that harm healthcare providers, patients, and professionals. This review aims to inform policymakers, healthcare practitioners, and technology stakeholders about the critical imperatives and best practices for fostering a secure and resilient smart healthcare ecosystem by synthesizing insights from multidisciplinary perspectives, such as cybersecurity, healthcare management, and sustainability research. Understanding the most recent cybersecurity measures is critical for controlling escalating cyber threats and attacks on SHSs and networks and encouraging intelligent healthcare delivery.",SEC,https://www.semanticscholar.org/paper/154862cd1839ebeae9c0ed79e9d4eadefad5843b
Counterattacking Cyber Threats: A Framework for the Future of Cybersecurity,"Amidst the rapid advancements in the digital landscape, the convergence of digitization and cyber threats presents new challenges for organizational security. This article presents a comprehensive framework that aims to shape the future of cyber security. This framework responds to the complexities of modern cyber threats and provides guidance to organizations to enhance their resilience. The primary focus lies in the integration of capabilities with resilience. By combining these elements into cyber security practices, organizations can improve their ability to predict, mitigate, respond to, and recover from cyber disasters. This article emphasizes the importance of organizational leadership, accountability, and innovation in achieving cyber resilience. As cyber threat challenges continue to evolve, this framework offers strategic guidance to address the intricate dynamics between digitization and cyber security, moving towards a safer and more robust digital environment in the future.",SEC,https://www.semanticscholar.org/paper/b2319ad4828be04bc873a55762cee49583740fb3
Machine Learning in Cybersecurity: Techniques and Challenges,"In the computer world, data science is the force behind the recent dramatic changes in cybersecurity's operations and technologies. The secret to making a security system automated and intelligent is to extract patterns or insights related to security incidents from cybersecurity data and construct appropriate data-driven models. Data science, also known as diverse scientific approaches, machine learning techniques, processes, and systems, is the study of actual occurrences via the use of data. Due to its distinctive qualities, such as flexibility, scalability, and the capability to quickly adapt to new and unknowable obstacles, machine learning techniques have been used in many scientific fields. Due to notable advancements in social networks, cloud and web technologies, online banking, mobile environments, smart grids, etc., cyber security is a rapidly expanding sector that requires a lot of attention. Such a broad range of computer security issues have been effectively addressed by various machine learning techniques. This article covers several machine-learning applications in cyber security. Phishing detection, network intrusion detection, keystroke dynamics authentication, cryptography, human interaction proofs, spam detection in social networks, smart meter energy consumption profiling, and security concerns with machine learning techniques themselves are all covered in this study. The methodology involves collecting a large dataset of phishing and legitimate instances, extracting relevant features such as email headers, content, and URLs, and training a machine-learning model using supervised learning algorithms. Machine learning models can effectively identify phishing emails and websites with high accuracy and low false positive rates. To enhance phishing detection, it is recommended to continuously update the training dataset to include new phishing techniques and to employ ensemble methods that combine multiple machine learning models for better performance.",SEC,https://www.semanticscholar.org/paper/427a38f01ee3ea20a7c850550e444190ec98852d
The Significance of Machine Learning and Deep Learning Techniques in Cybersecurity: A Comprehensive Review,"People in the modern era spend most of their lives in virtual environments that offer a range of public and private services and social platforms. Therefore, these environments need to be protected from cyber attackers that can steal data or disrupt systems. Cybersecurity refers to a collection of technical, organizational, and executive means for preventing the unauthorized use or misuse of electronic information and communication systems to ensure the continuity of their work, guarantee the confidentiality and privacy of personal data, and protect consumers from threats and intrusions. Accordingly, this article explores the cybersecurity practices that protect computer systems from attacks, hacking, and data thefts and investigates the role of artificial intelligence in this domain. This article also summarizes the most significant literature that explore the roles and effects of machine learning and deep learning techniques in cybersecurity. Results show that machine learning and deep learning techniques play significant roles in protecting computer systems from unauthorized entry and in controlling system penetration by predicting and understanding the behaviour and traffic of malicious software.",SEC,https://www.semanticscholar.org/paper/a2fe496b4db6c3baf093463f2af7c5c3bdf9c0c5
ChatGPT: Exploring the Role of Cybersecurity in the Protection of Medical Information,"ChatGPT is a large language model developed by OpenAI. It is trained on a dataset of conversational text and can be used to generate human-like responses to prompts in a variety of languages and formats. It can be used for tasks such as chatbots, language translation, and text completion. The role of ChatGPT is to generate human-like text based on a given prompt or context. It can be used in a variety of applications such as chatbots, language translation, text completion, and question answering. Additionally, it can be fine-tuned for specific tasks such as generating product descriptions or summarizing articles. It can also be used to generate creative writing such as poetry and stories. It can be integrated into a wide range of industries from customer service to entertainment, to research.",SEC,https://www.semanticscholar.org/paper/807110054f8137a10ed5bb05a33e93bf596f897a
A comprehensive study on cybersecurity challenges and opportunities in the IoT world,"It has become possible to link anything and everything to the Internet in recent decades due to the expanding Internet of Things (IoT). As a result, our usage of technology has changed a lot, causing digital disruption in the real world. IoT allows drones, sensors, digital set‐top boxes, surveillance cameras, wearable technology, and medical equipment to be connected to the internet. Healthcare, manufacturing, utilities, transportation, and housing are among the various sectors that has become intelligent. Recently, we have seen a surge in cybersecurity challenges and opportunities for the improvement of various IoT applications. Although cybersecurity and the IoT are extensively researched, there is a dearth of studies that exclusively focus on the evolution of cybersecurity challenges in the area of AI and machine learning, blockchain and zero trust, lightweight security, integration of IoT with 5G networks, and many more in the IoT world. The availability of environment‐capturing sensors and internet‐connected tracking devices allows for private life surveillance and cloud data transmission. Therefore, a significant problem for researchers and developers is to ensure the CIA (Confidentiality, Integrity, and Availability) security triangle for people. This paper presents a comprehensive study of cybersecurity applications, challenges, and opportunities in the IoT world. The IoT architectural layer, attacks against the IoT layer, and related issues are highlighted. Furthermore, cybersecurity issues and challenges in IoT along with the strength and weaknesses of existing techniques are discussed in detail. Our study will provide insight into various current cybersecurity research trends in the IoT world.",SEC,https://www.semanticscholar.org/paper/1e8ccc3909baf44b45161f8ee10bd25a49fc01a1
Deep Learning Based Attack Detection for Cyber-Physical System Cybersecurity: A Survey,"With the booming of cyber attacks and cyber criminals against cyber-physical systems (CPSs), detecting these attacks remains challenging. It might be the worst of times, but it might be the best of times because of opportunities brought by machine learning (ML), in particular deep learning (DL). In general, DL delivers superior performance to ML because of its layered setting and its effective algorithm for extract useful information from training data. DL models are adopted quickly to cyber attacks against CPS systems. In this survey, a holistic view of recently proposed DL solutions is provided to cyber attack detection in the CPS context. A six-step DL driven methodology is provided to summarize and analyze the surveyed literature for applying DL methods to detect cyber attacks against CPS systems. The methodology includes CPS scenario analysis, cyber attack identification, ML problem formulation, DL model customization, data acquisition for training, and performance evaluation. The reviewed works indicate great potential to detect cyber attacks against CPS through DL modules. Moreover, excellent performance is achieved partly because of several high-quality datasets that are readily available for public use. Furthermore, challenges, opportunities, and research trends are pointed out for future research.",SEC,https://www.semanticscholar.org/paper/b2084a383f69fbcc3ab779f4b1aa425c5224229e
Current trends in AI and ML for cybersecurity: A state-of-the-art survey,"Abstract This paper provides a comprehensive survey of the state-of-the-art use of Artificial Intelligence (AI) and Machine Learning (ML) in the field of cybersecurity. The paper illuminates key applications of AI and ML in cybersecurity, while also addressing existing challenges and posing unresolved questions for future research. The paper also emphasizes the ethical and legal implications associated with their implementation. The researchers conducted a thorough survey by reviewing numerous papers and articles from respected sources such as IEEE, ACM, and Springer. Their focus centered on the latest AI and ML breakthroughs in cybersecurity, while also exploring current challenges and open research questions. The results indicate that integrating AI and ML into cybersecurity systems shows great potential for future research and development. Intrusion detection and response, malware detection, and network security are among the most promising applications identified. According to the survey, 45% of organizations have already implemented AI and ML in their cybersecurity systems, while an additional 35% plan to do so. However, 20% of organizations believe that it is not yet the right time for adopting these technologies. Overall, this paper serves as a reliable reference for researchers and practitioners in the field of cybersecurity, offering a comprehensive overview of the use of AI and ML. It not only highlights the potential applications but also addresses the challenges and research gaps. Additionally, the paper raises awareness about the ethical and legal considerations associated with leveraging AI and ML in the cybersecurity domain.",SEC,https://www.semanticscholar.org/paper/66fc6467aec08ebb6047a3429a5c9ff6a8cceb90
Cybersecurity Risk Analysis of Electric Vehicles Charging Stations,"The increasing availability of Electric Vehicles (EVs) is driving a shift away from traditional gasoline-powered vehicles. Subsequently, the demand for Electric Vehicle Charging Systems (EVCS) is rising, leading to the significant growth of EVCS as public and private charging infrastructure. The cybersecurity-related risks in EVCS have significantly increased due to the growing network of EVCS. In this context, this paper presents a cybersecurity risk analysis of the network of EVCS. Firstly, the recent advancements in the EVCS network, recent EV adaptation trends, and charging use cases are described as a background of the research area. Secondly, cybersecurity aspects in EVCS have been presented considering infrastructure and protocol-centric vulnerabilities with possible cyber-attack scenarios. Thirdly, threats in EVCS have been validated with real-time data-centric analysis of EV charging sessions. The paper also highlights potential open research issues in EV cyber research as new knowledge for domain researchers and practitioners.",SEC,https://www.semanticscholar.org/paper/4d39e473e298765ef0f11dcccd204ac0c2ee8ad5
"Cyberattacks in Smart Grids: Challenges and Solving the Multi-Criteria Decision-Making for Cybersecurity Options, Including Ones That Incorporate Artificial Intelligence, Using an Analytical Hierarchy Process","Smart grids have emerged as a transformative technology in the power sector, enabling efficient energy management. However, the increased reliance on digital technologies also exposes smart grids to various cybersecurity threats and attacks. This article provides a comprehensive exploration of cyberattacks and cybersecurity in smart grids, focusing on critical components and applications. It examines various cyberattack types and their implications on smart grids, backed by real-world case studies and quantitative models. To select optimal cybersecurity options, the study proposes a multi-criteria decision-making (MCDM) approach using the analytical hierarchy process (AHP). Additionally, the integration of artificial intelligence (AI) techniques in smart-grid security is examined, highlighting the potential benefits and challenges. Overall, the findings suggest that “security effectiveness” holds the highest importance, followed by “cost-effectiveness”, “scalability”, and “Integration and compatibility”, while other criteria (i.e., “performance impact”, “manageability and usability”, “compliance and regulatory requirements”, “resilience and redundancy”, “vendor support and collaboration”, and “future readiness”) contribute to the evaluation but have relatively lower weights. Alternatives such as “access control and authentication” and “security information and event management” with high weighted sums are crucial for enhancing cybersecurity in smart grids, while alternatives such as “compliance and regulatory requirements” and “encryption” have lower weighted sums but still provide value in their respective criteria. We also find that “deep learning” emerges as the most effective AI technique for enhancing cybersecurity in smart grids, followed by “hybrid approaches”, “Bayesian networks”, “swarm intelligence”, and “machine learning”, while “fuzzy logic”, “natural language processing”, “expert systems”, and “genetic algorithms” exhibit lower effectiveness in addressing smart-grid cybersecurity. The article discusses the benefits and drawbacks of MCDM-AHP, proposes enhancements for its use in smart-grid cybersecurity, and suggests exploring alternative MCDM techniques for evaluating security options in smart grids. The approach aids decision-makers in the smart-grid field to make informed cybersecurity choices and optimize resource allocation.",SEC,https://www.semanticscholar.org/paper/4f71d51b8e296c7b7350a79d668adf6416144a0c
Cybersecurity Challenges for Manufacturing Systems 4.0: Assessment of the Business Impact Level,"An ever-growing number of companies are moving toward the Industry 4.0 paradigm, adopting a range of advanced technologies (e.g., smart sensors, big data analytics, and cloud computing) and networking their manufacturing systems. This improves the efficiency and effectiveness of operations but also introduces new cybersecurity challenges. In this article, the impact assessment methodology is applied in the context of manufacturing systems 4.0 (also known as smart manufacturing systems, cyber manufacturing systems, or digital manufacturing systems), thus identifying the critical assets to be protected against cyber-attacks and assessing the business impacts in the case of subtractive and additive technologies. The research design of the single case study with multiple units of analysis is applied. In particular, a large company, a leader in the manufacturing of aeronautical components, is considered a representative case study, and its two main types of manufacturing cells that is, those based on networked computer numerical control machines and 3-D printers, are taken as applicative cases for the methodology. The application of the impact assessment methodology in the manufacturing context 4.0 of aeronautical components represents a useful guide for researchers in the field of cybersecurity and for companies intending to implement it in their smart manufacturing environments. In particular, based on this study, companies can define the critical manufacturing data to protect against cyber-attacks, isolate the business impacts in case of cybersecurity breaches, correlate the identified business impacts with the specific data category, and assess the level of business impacts.",SEC,https://www.semanticscholar.org/paper/b652b2ce62479f8524a4917126da760dae7a048c
More than malware: unmasking the hidden risk of cybersecurity regulations,"Cybersecurity investments are made within a complex and ever-evolving environment, where regulatory changes represent a significant risk factor. While cybersecurity regulations aim to minimize cyber risks and enhance protection, the uncertainty arising from frequent changes or new regulations can significantly impact organizational response strategies. This paper explores the determinants and implications of regulatory risks associated with cybersecurity, aiming to provide a deeper understanding of how these risks influence strategic decision-making. The study delves into the suggestion of preventive and mitigative controls that enable businesses to adapt to and mitigate potential disruptions caused by regulatory changes, thereby preserving their established cybersecurity practices. Another key contribution of this study is the introduction of a stochastic econometric model that illustrates how regulatory risks and uncertainties can affect investment behaviors, often prompting a “wait-and-see” stance. This model synthesizes the complex relationship among investment choices, regulatory changes, and cybersecurity risks, providing insights into the dynamic nature of cybersecurity investment strategies. The research findings offer valuable guidance for risk management and strategic planning in cybersecurity investments. By comprehensively understanding the drivers and impacts of regulatory risks, businesses and policymakers can develop more effective risk evaluation and management approaches. This is essential for sustaining a strong cybersecurity posture while navigating the changing regulatory environment.",SEC,https://www.semanticscholar.org/paper/7dd8bfd43b690c7311c1e1ae7274c6c642258d0e
"AI-Driven Cybersecurity: An Overview, Security Intelligence Modeling and Research Directions","Artificial intelligence (AI) is one of the key technologies of the Fourth Industrial Revolution (or Industry 4.0), which can be used for the protection of Internet-connected systems from cyber threats, attacks, damage, or unauthorized access. To intelligently solve today’s various cybersecurity issues, popular AI techniques involving machine learning and deep learning methods, the concept of natural language processing, knowledge representation and reasoning, as well as the concept of knowledge or rule-based expert systems modeling can be used. Based on these AI methods, in this paper, we present a comprehensive view on “AI-driven Cybersecurity” that can play an important role for intelligent cybersecurity services and management. The security intelligence modeling based on such AI methods can make the cybersecurity computing process automated and intelligent than the conventional security systems. We also highlight several research directions within the scope of our study, which can help researchers do future research in the area. Overall, this paper’s ultimate objective is to serve as a reference point and guidelines for cybersecurity researchers as well as industry professionals in the area, especially from an intelligent computing or AI-based technical point of view.",SEC,https://www.semanticscholar.org/paper/cc1697038a9fcca37977b3b0e1bd9d434d3d9a3e
Evaluating the adoption of cybersecurity and its influence on organizational performance,"Cyberattacks negatively impact the performance of enterprises all around the globe. While organizations invest more in cybersecurity to avoid cyberattacks, studies on the factors affecting their overall cybersecurity adoption and awareness are sparse. In this paper, by integrating the diffusion of innovation theory (DOI), technology acceptance model (TAM), and technology-organization-environment (TOE) with the balanced scorecard approach, we propose a comprehensive set of factors that influence cybersecurity adoption and assess the effects of these factors on organizational performance. Data are collected through a survey of IT experts in small and medium-sized enterprises (SMEs) in the United Kingdom, with 147 valid responses. Structural equation modeling based on a statistical package for the social sciences (SPSS) was used to assess the model. The findings identify and confirm the importance of eight factors affecting SMEs' cybersecurity adoption. Moreover, cybersecurity technology adoption is found to positively impacts organizational performance. The proposed framework depicts variables influencing cybersecurity technology adoption and assesses their importance. The outcomes of this study provide a basis for future research and can be adopted by IT and cybersecurity managers to identify the most appropriate cybersecurity technologies that positively impact their company's performance.",SEC,https://www.semanticscholar.org/paper/eebc1abfd4d5615b1562686b182488a0e9914c9c
Exploring the Top Five Evolving Threats in Cybersecurity: An In-Depth Overview,"The term cybersecurity refers to an environment capable of protecting digital devices, networks and information from unauthorized access and preventing data theft or alteration. It is composed of a collection of carefully crafted techniques, processes, and practices to protect sensitive information and deterring cyber-attacks. In the recent period, the domain of cybersecurity has undergone rapid growth in response to the increasing cyber threats. Cybersecurity includes important tactics that help protect the digital environment, which are firewalls, encryption, secure passwords, and threat detection and response systems. Employees must be trained on these tactics. This article will discuss the five most pressing challenges facing the cybersecurity industry today that must be taken into account by businesses, organizations, and individuals in order to secure their confidential data from cybercrime. The conclusion of the article highlighted the significance of growing awareness about cybersecurity risks in order to effectively handle digital environments and protect them from any electronic threats.",SEC,https://www.semanticscholar.org/paper/f70158eddb1b09b48672a08e074f68dd7d26d327
Review of strategic alignment: Accounting and cybersecurity for data confidentiality and financial security,"In the contemporary landscape of rapidly evolving technological advancements and the increasing prevalence of cyber threats, organizations face a critical imperative to align their accounting practices with robust cybersecurity measures. This review explores the symbiotic relationship between accounting and cybersecurity in safeguarding data confidentiality and ensuring financial security. Focusing on the intersection of these two domains, we examine the strategic alignment required to fortify organizations against the escalating challenges posed by cyber threats to sensitive financial information. The review begins by delving into the intricate connection between accounting processes and the protection of financial data, emphasizing the pivotal role of accurate financial reporting and transparent disclosure in maintaining stakeholder trust. Subsequently, it scrutinizes the evolving threat landscape, identifying cyber risks that specifically target financial systems and data. The analysis underscores the need for a comprehensive strategic approach that integrates accounting practices with cybersecurity protocols to effectively mitigate these risks. Furthermore, the review investigates the contemporary tools and technologies that facilitate the integration of accounting and cybersecurity, enhancing organizations' ability to detect, prevent, and respond to cyber threats. It explores the adoption of advanced encryption methods, intrusion detection systems, and artificial intelligence-driven analytics to bolster data confidentiality and financial security. In examining case studies and best practices, this review highlights successful instances of organizations aligning accounting and cybersecurity strategies to achieve a cohesive defense against financial cyber threats. Lessons learned from these cases offer valuable insights for practitioners and decision-makers seeking to implement effective measures within their own organizational contexts. Ultimately, this review contributes to the evolving discourse on strategic alignment by emphasizing the imperative of synergizing accounting practices with cybersecurity initiatives. As organizations navigate an increasingly complex and interconnected business environment, a holistic approach that unifies financial integrity and cyber resilience becomes paramount for ensuring sustained success and safeguarding against the multifaceted challenges of the digital age.",SEC,https://www.semanticscholar.org/paper/07ac8f33e82eec20dc3733eb825deae5d6404967
Cybersecurity of Autonomous Vehicles: A Systematic Literature Review of Adversarial Attacks and Defense Models,"Autonomous driving (AD) has developed tremendously in parallel with the ongoing development and improvement of deep learning (DL) technology. However, the uptake of artificial intelligence (AI) in AD as the core enabling technology raises serious cybersecurity issues. An enhanced attack surface has been spurred on by the rising digitization of vehicles and the integration of AI features. The performance of the autonomous vehicle (AV)-based applications is constrained by the DL models' susceptibility to adversarial attacks despite their great potential. Hence, AI-enabled AVs face numerous security threats, which prevent the large-scale adoption of AVs. Therefore, it becomes crucial to evolve existing cybersecurity practices to deal with risks associated with the increased uptake of AI. Furthermore, putting defense models into practice against adversarial attacks has grown in importance as a field of study amongst researchers. Therefore, this study seeks to provide an overview of the most recent adversarial defensive and attack models developed in the domain of AD.",SEC,https://www.semanticscholar.org/paper/1ef8c59d615f8416f7039428d4416bd5eafb3748
Cybersecurity knowledge graphs,"Cybersecurity knowledge graphs, which represent cyber-knowledge with a graph-based data model, provide holistic approaches for processing massive volumes of complex cybersecurity data derived from diverse sources. They can assist security analysts to obtain cyberthreat intelligence, achieve a high level of cyber-situational awareness, discover new cyber-knowledge, visualize networks, data flow, and attack paths, and understand data correlations by aggregating and fusing data. This paper reviews the most prominent graph-based data models used in this domain, along with knowledge organization systems that define concepts and properties utilized in formal cyber-knowledge representation for both background knowledge and specific expert knowledge about an actual system or attack. It is also discussed how cybersecurity knowledge graphs enable machine learning and facilitate automated reasoning over cyber-knowledge.",SEC,https://www.semanticscholar.org/paper/4cb2924d5f4a076724942d791076438679b384be
The elephant in the room: cybersecurity in healthcare,"Cybersecurity has seen an increasing frequency and impact of cyberattacks and exposure of Protected Health Information (PHI). The uptake of an Electronic Medical Record (EMR), the exponential adoption of Internet of Things (IoT) devices, and the impact of the COVID-19 pandemic has increased the threat surface presented for cyberattack by the healthcare sector. Within healthcare generally and, more specifically, within anaesthesia and Intensive Care, there has been an explosion in wired and wireless devices used daily in the care of almost every patient—the Internet of Medical Things (IoMT); ventilators, anaesthetic machines, infusion pumps, pacing devices, organ support and a plethora of monitoring modalities. All of these devices, once connected to a hospital network, present another opportunity for a malevolent party to access the hospital systems, either to gain PHI for financial, political or other gain or to attack the systems directly to cause erroneous monitoring, altered settings of any device and even to access the EMR via this IoMT window. This exponential increase in the IoMT and the increasing wireless connectivity of anaesthesia and ICU devices as well as implantable devices presents a real and present danger to patient safety. There has, at the same time, been a chronic underfunding of cybersecurity in healthcare. This lack of cybersecurity investment has left the sector exposed, and with the monetisation of PHI, the introduction of technically unsecure IoT devices for monitoring and direct patient care, the healthcare sector is presenting itself for further devastating cyberattacks or breaches of PHI. Coupled with the immense strain that the COVID-19 pandemic has placed on healthcare and the changes in working patterns of many caregivers, this has further amplified the exposure of the sector to cyberattacks.",SEC,https://www.semanticscholar.org/paper/ad98eed098edbc93040f9eb3054a1cd7f4b95795
Cybersecurity for Blockchain-Based IoT Systems: A Review,"The Internet of Things (IoT) has become a pervasive technology with various applications ranging from smart homes and cities to industrial automation and healthcare. However, the increasing adoption of IoT devices has also raised significant concerns about cybersecurity and privacy. Blockchain, as a distributed and immutable ledger technology, has been proposed as a potential solution to enhance the security and privacy of IoT systems. Blockchain-based IoT systems offer several benefits, such as decentralization, transparency, and data integrity. However, they also pose unique cybersecurity challenges that need to be addressed for their secure and reliable deployment. In this paper, we review the existing literature and highlight the key challenges in cybersecurity for blockchain-based IoT systems. We categorize these challenges into three main areas: (i) IoT device security, (ii) blockchain security, and (iii) integration of IoT devices with blockchain (network security). Through an in-depth analysis, we present the current state of research and discuss potential solutions for each challenge. Additionally, we contribute by identifying future research directions to address these challenges and enhance the cybersecurity of blockchain-based IoT systems.",SEC,https://www.semanticscholar.org/paper/76a6b3a8e9900675a829a981d89540454acbcf1b
Cybersecurity data science: an overview from machine learning perspective,"In a computing context, cybersecurity is undergoing massive shifts in technology and its operations in recent days, and data science is driving the change. Extracting security incident patterns or insights from cybersecurity data and building corresponding data-driven model, is the key to make a security system automated and intelligent. To understand and analyze the actual phenomena with data, various scientific methods, machine learning techniques, processes, and systems are used, which is commonly known as data science. In this paper, we focus and briefly discuss on cybersecurity data science, where the data is being gathered from relevant cybersecurity sources, and the analytics complement the latest data-driven patterns for providing more effective security solutions. The concept of cybersecurity data science allows making the computing process more actionable and intelligent as compared to traditional ones in the domain of cybersecurity. We then discuss and summarize a number of associated research issues and future directions. Furthermore, we provide a machine learning based multi-layered framework for the purpose of cybersecurity modeling. Overall, our goal is not only to discuss cybersecurity data science and relevant methods but also to focus the applicability towards data-driven intelligent decision making for protecting the systems from cyber-attacks.",SEC,https://www.semanticscholar.org/paper/ede0a8039a561905f40777ec2ae66c2010e3f2bc
Detecting Cybersecurity Attacks in Internet of Things Using Artificial Intelligence Methods: A Systematic Literature Review,"In recent years, technology has advanced to the fourth industrial revolution (Industry 4.0), where the Internet of things (IoTs), fog computing, computer security, and cyberattacks have evolved exponentially on a large scale. The rapid development of IoT devices and networks in various forms generate enormous amounts of data which in turn demand careful authentication and security. Artificial intelligence (AI) is considered one of the most promising methods for addressing cybersecurity threats and providing security. In this study, we present a systematic literature review (SLR) that categorize, map and survey the existing literature on AI methods used to detect cybersecurity attacks in the IoT environment. The scope of this SLR includes an in-depth investigation on most AI trending techniques in cybersecurity and state-of-art solutions. A systematic search was performed on various electronic databases (SCOPUS, Science Direct, IEEE Xplore, Web of Science, ACM, and MDPI). Out of the identified records, 80 studies published between 2016 and 2021 were selected, surveyed and carefully assessed. This review has explored deep learning (DL) and machine learning (ML) techniques used in IoT security, and their effectiveness in detecting attacks. However, several studies have proposed smart intrusion detection systems (IDS) with intelligent architectural frameworks using AI to overcome the existing security and privacy challenges. It is found that support vector machines (SVM) and random forest (RF) are among the most used methods, due to high accuracy detection another reason may be efficient memory. In addition, other methods also provide better performance such as extreme gradient boosting (XGBoost), neural networks (NN) and recurrent neural networks (RNN). This analysis also provides an insight into the AI roadmap to detect threats based on attack categories. Finally, we present recommendations for potential future investigations.",SEC,https://www.semanticscholar.org/paper/6f5b87e38cfb4a47e3ac7988cca5612ccf0a2946
"Federated Learning for Cybersecurity: Concepts, Challenges, and Future Directions","Federated learning (FL) is a recent development in artificial intelligence, which is typically based on the concept of decentralized data. As cyberattacks are frequently happening in the various applications deployed in real time, most industrialists are hesitating to move forward in adopting the technology of the Internet of Everything. This article aims to provide an extensive study on how FL could be utilized for providing better cybersecurity and prevent various cyberattacks in real time. We present an extensive survey of the various FL models currently developed by researchers for providing authentication, privacy, trust management, and attack detection. We also discuss few real-time use cases that have been deployed recently and how FL is adopted in them for preserving privacy of data and improving the performance of the system. Based on the study, we conclude this article with some prominent challenges and future directions on which the researchers can focus for adopting FL in real-time scenarios.",SEC,https://www.semanticscholar.org/paper/4737e7b19041092055203adc0cb8a2570381766e
The Role of Machine Learning in Cybersecurity,"Machine Learning (ML) represents a pivotal technology for current and future information systems, and many domains already leverage the capabilities of ML. However, deployment of ML in cybersecurity is still at an early stage, revealing a significant discrepancy between research and practice. Such a discrepancy has its root cause in the current state of the art, which does not allow us to identify the role of ML in cybersecurity. The full potential of ML will never be unleashed unless its pros and cons are understood by a broad audience. This article is the first attempt to provide a holistic understanding of the role of ML in the entire cybersecurity domain—to any potential reader with an interest in this topic. We highlight the advantages of ML with respect to human-driven detection methods, as well as the additional tasks that can be addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic problems affecting real ML deployments in cybersecurity. Finally, we present how various stakeholders can contribute to future developments of ML in cybersecurity, which is essential for further progress in this field. Our contributions are complemented with two real case studies describing industrial applications of ML as defense against cyber-threats.",SEC,https://www.semanticscholar.org/paper/1814944434626fa98d79e17b4732c2a5d5bb1151
Cyber risk and cybersecurity: a systematic review of data availability,"Cybercrime is estimated to have cost the global economy just under USD 1 trillion in 2020, indicating an increase of more than 50% since 2018. With the average cyber insurance claim rising from USD 145,000 in 2019 to USD 359,000 in 2020, there is a growing necessity for better cyber information sources, standardised databases, mandatory reporting and public awareness. This research analyses the extant academic and industry literature on cybersecurity and cyber risk management with a particular focus on data availability. From a preliminary search resulting in 5219 cyber peer-reviewed studies, the application of the systematic methodology resulted in 79 unique datasets. We posit that the lack of available data on cyber risk poses a serious problem for stakeholders seeking to tackle this issue. In particular, we identify a lacuna in open databases that undermine collective endeavours to better manage this set of risks. The resulting data evaluation and categorisation will support cybersecurity researchers and the insurance industry in their efforts to comprehend, metricise and manage cyber risks.",SEC,https://www.semanticscholar.org/paper/8e08ba2cdb657f6b2986b92457dd09845094fab9
Cybersecurity in precision agriculture: Protecting data integrity and privacy,"Precision agriculture, an innovative approach to farming that leverages data-driven technologies, has revolutionized the agricultural sector by enhancing productivity, resource efficiency, and sustainability. However, the increasing reliance on digital tools and connected devices has introduced significant cybersecurity challenges, particularly concerning data integrity and privacy. This paper explores the critical importance of cybersecurity in precision agriculture, focusing on protecting sensitive agricultural data from breaches, unauthorized access, and potential manipulation. As precision agriculture systems collect vast amounts of data through sensors, drones, and GPS-enabled devices, the risk of cyber threats has escalated. These threats can compromise the integrity of critical data, leading to inaccurate decision-making, financial losses, and disruption of agricultural operations. Moreover, the interconnected nature of precision agriculture systems makes them vulnerable to cyberattacks that could have widespread implications across the agricultural supply chain. This study examines the key cybersecurity challenges in precision agriculture, including the protection of data at rest and in transit, the safeguarding of privacy in data sharing among stakeholders, and the implementation of robust encryption and authentication mechanisms. It also highlights the importance of developing industry-specific cybersecurity standards and best practices tailored to the unique needs of the agricultural sector. Furthermore, the paper discusses the ethical considerations of data privacy in precision agriculture, emphasizing the need to balance technological advancement with the protection of farmers' and consumers' rights. The potential consequences of inadequate cybersecurity measures, such as the loss of trust in digital farming technologies and the erosion of competitive advantage, are also addressed. In conclusion, as precision agriculture continues to evolve, ensuring the integrity and privacy of agricultural data through effective cybersecurity measures is paramount. This will not only protect the agricultural sector from emerging cyber threats but also foster the sustainable growth and adoption of precision agriculture technologies. Keywords: Cybersecurity, Precision Agriculture, Data Privacy, Data Integrity, Protecting.",SEC,https://www.semanticscholar.org/paper/0710c3ae074e484193e4da25e61d3f99d12b9855
Regulatory cybersecurity governance in the making: the formation of ENISA and its struggle for epistemic authority,"ABSTRACT Over the last decades, cybersecurity has become a top priority for the European Union (EU). As a contribution to scholarship on the ‘regulatory security state’, we analyze how the European Union Agency for Cybersecurity (ENISA), emerged and stabilized as the EU's key agency for cybersecurity. We use data from policy documents, secondary sources, and semi-structured interviews to show how ENISA struggled to become a relevant actor by carving out a specific role for itself. In particular, we show how challenging it was for the agency to acquire epistemic authority. Although the trajectory of ENISA supports attempts to govern through regulation, it also shows that its role was never a given, only functions as part of a larger whole, and continues to be subject to change. Our article indicates that the study of security governance must remain ontologically flexible to capture hybrid forms and political struggles.",SEC,https://www.semanticscholar.org/paper/fc3a36ccb3ab7ecd365592d52089d57a870b36bc
A Dynamic and Adaptive Cybersecurity Governance Framework,"Cybersecurity protects cyberspace from a wide range of cyber threats to reduce overall business risk, ensure business continuity, and maximize business opportunities and return on investments. Cybersecurity is well achieved by using appropriate sets of security governance frameworks. To this end, various Information Technology (IT) and cybersecurity governance frameworks have been reviewed along with their benefits and limitations. The major limitations of the reviewed frameworks are; they are complex and have complicated structures to implement, they are expensive and require high skill IT and security professionals. Moreover, the frameworks require many requirement checklists for implementation and auditing purposes and a lot of time and resources. To fill the limitations mentioned above, a simple, dynamic, and adaptive cybersecurity governance framework is proposed that provides security related strategic direction, ensures that security risks are managed appropriately, and ensures that organizations’ resources are utilized optimally. The framework incorporated different components not considered in the existing frameworks, such as research and development, public-private collaboration framework, regional and international cooperation framework, incident management, business continuity, disaster recovery frameworks, and compliance with laws and regulations. Moreover, the proposed framework identifies and includes some of the existing frameworks’ missed and overlapped components, processes, and activities. It has nine components, five activities, four outcomes, and seven processes. Performance metrics, evaluation, and monitoring techniques are also proposed. Moreover, it follows a risk based approach to address the current and future technology and threat landscapes. The design science research method was used in this research study to solve the problem mentioned. Using the design science research method, the problem was identified. Based on the problem, research objectives were articulated; the objective of this research was solved by developing a security governance framework considering different factors which were not addressed in the current works. Finally, performance metrics were proposed to evaluate the implementation of the governance framework.",SEC,https://www.semanticscholar.org/paper/bd2c8ab62b3713e22552b97bdc16bb454556eae3
"Exploring Cybersecurity Education and Training Techniques: A Comprehensive Review of Traditional, Virtual Reality, and Augmented Reality Approaches","Considering the alarming increase in cyberattacks and their potential financial implications, the importance of cybersecurity education and training cannot be overstated. This paper presents a systematic literature review that examines different cybersecurity education and training techniques with a focus on symmetry. It primarily focuses on traditional cybersecurity education techniques and emerging technologies, such as virtual reality (VR) and augmented reality (AR), through the lens of symmetry. The main objective of this study is to explore the existing cybersecurity training techniques, identify the challenges involved, and assess the effectiveness of cybersecurity training based on VR and AR while emphasizing the concept of symmetry. Through careful selection criteria, 66 primary studies were selected from a total of 150 pertinent research studies. This article offers valuable insights into the pros and cons of conventional training approaches, explores the use of VR and AR in cybersecurity education concerning symmetry, and thoroughly discusses the challenges associated with these technologies. The findings of this review contribute significantly to the continuing efforts in cybersecurity education by offering recommendations for improving employees’ knowledge, engagement, and motivation in cybersecurity training programs while maintaining symmetry in the learning process.",SEC,https://www.semanticscholar.org/paper/d87d4e0c38c07259036b540cc6e0976469c704f5
Exploring the Frontiers of Cybersecurity Behavior: A Systematic Review of Studies and Theories,"Cybersecurity procedures and policies are prevalent countermeasures for protecting organizations from cybercrimes and security incidents. Without considering human behaviors, implementing these countermeasures will remain useless. Cybersecurity behavior has gained much attention in recent years. However, a systematic review that provides extensive insights into cybersecurity behavior through different technologies and services and covers various directions in large-scale research remains lacking. Therefore, this study retrieved and analyzed 2210 articles published on cybersecurity behavior. The retrieved articles were then thoroughly examined to meet the inclusion and exclusion criteria, in which 39 studies published between 2012 and 2021 were ultimately picked for further in-depth analysis. The main findings showed that the protection motivation theory (PMT) dominated the list of theories and models examining cybersecurity behavior. Cybersecurity behavior and intention behavior counted for the highest purpose for most studies, with fewer studies focusing on cybersecurity awareness and compliance behavior. Most examined studies were conducted in individualistic contexts with limited exposure to collectivistic societies. A total of 56% of the analyzed studies focused on the organizational level, indicating that the individual level is still in its infancy stage. To address the research gaps in cybersecurity behavior at the individual level, this review proposes a number of research agendas that can be considered in future research. This review is believed to improve our understanding by revealing the full potential of cybersecurity behavior and opening the door for further research opportunities.",SEC,https://www.semanticscholar.org/paper/631c9955109d6ced0c1daa0b6aa46523a833856f
Prevention of Phishing Attacks Using AI-Based Cybersecurity Awareness Training,"Machine learning has been described as an effective measure in avoiding most cyberattacks. The development of AI has therefore promoted increased security for most computer attacks. Phishing attacks are risky and can be prevented through AI-based solutions. This factor suggests the need for increased awareness of cybersecurity through AI. Developing awareness for most people will prevent these types of attacks. The research paper describes how the awareness of AI-based cybersecurity could ensure a reduction of phishing attacks. The paper, therefore, showcases the effectiveness of AI-based cybersecurity awareness training and how it may influence cyber-attacks.",SEC,https://www.semanticscholar.org/paper/17859c19e41a29b657aa3323ed396f733086c47d
Organizational Learning from Cybersecurity Performance: Effects on Cybersecurity Investment Decisions,"IS literature has identified various economic, performance, and environmental factors affecting cybersecurity investment decisions. However, economic modeling approaches dominate, and research on cybersecurity performance as an antecedent to investments has taken a backseat. Neglecting the role of performance indicators ignores real-world concerns driving actual cybersecurity investment decision-making. We investigate two critical aspects of cybersecurity performance: breach costs and breach identification source, as antecedents to cybersecurity investment decisions. We use organizational learning to theorize how performance feedback from these two aspects of cybersecurity breaches influences subsequent investment decisions. Using firm-level data on 722 firms in the UK, we find that higher breach costs are more likely to elicit increases in cybersecurity investments. This relationship is further strengthened if a third party identifies the breach instead of the focal firm. We contribute to the literature on cybersecurity investments and incident response. The findings stress the need for firms to analyze aspects of their cybersecurity performance and use them as feedback for investment decisions, making these decisions data-driven and based on firm-specific needs.",SEC,https://www.semanticscholar.org/paper/edbe03bb332961060e20f4a01be5a87941886c55
"Cybercompetitions: A survey of competitions, tools, and systems to support cybersecurity education","Over the last decade, industry and academia have worked towards raising students’ interests in cybersecurity through game-like competitions to fill a shortfall of cybersecurity professionals. Rising interest in video games in combination with gamification techniques make learning fun, easy, and addictive. It is crucial that cybersecurity curricula enhance and expose cybersecurity education to a diversified student body to meet workforce demands. Gamification through cybercompetitions is one method to achieve that. With a vast list of options for competition type, focus areas, learning outcomes, and participant experience levels we need to systematize knowledge of attributes that ameliorate cybercompetitions. In the wake of the COVID-19 pandemic and global lock-downs, competition hosts scrambled to move platforms from local to online infrastructure due to poor interoperability between competition software. We derive a list of takeaways including the lack of interoperability between state-of-the-art competition systems, breaking the high knowledge barrier to participate, addressing competition type diversity, then suggest potential solutions and research questions moving forward. Our paper aims to systematize cybersecurity, access control, and programming competitions by surveying the history of these events. We explore the types of competitions that have been hosted and categorize them based on focus areas related to the InfoSEC Color Wheel. We then explore state-of-the-art technologies that enable these types of competitions, and finally, present our takeaways.",SEC,https://www.semanticscholar.org/paper/0802e689e8bc7791fea45311ec6203642c32fb7e
The role of data science in transforming business operations: Case studies from enterprises,"Data science has emerged as a pivotal force in transforming business operations across various industries, driving innovation, operational efficiency, and strategic decision-making. This review paper explores the multifaceted role of data science in business, examining key concepts, historical integration, and strategic advantages. It discusses the application of data science in diverse business domains, highlighting techniques such as predictive analytics, sentiment analysis, and optimization that have revolutionized marketing, supply chain management, finance, and customer service. The paper further analyzes data science's tangible and intangible benefits, including cost reduction, improved customer experience, and enhanced productivity, which collectively contribute to a competitive edge in the market. The paper reflects on emerging trends like artificial intelligence, machine learning, ethical data practices, and the integration of blockchain and IoT, which are set to shape the future of data-driven business operations. It offers recommendations for businesses to prepare for this evolving landscape, emphasizing the importance of a data-centric culture, robust infrastructure, and collaboration. The paper concludes by underscoring the critical role of data science in fostering sustained business success in an increasingly data-driven world. Keywords: Data Science, Business Transformation, Predictive Analytics, Operational Efficiency, Artificial Intelligence, Strategic Decision-Making.",CLD,https://www.semanticscholar.org/paper/9195017d7d80c29443a318b778dac72b91aebbe8
Disruptive Technologies and Operations Management in the Industry 4.0 Era and Beyond,"In the Industry 4.0 era, automation and data analytics emerge as the major forces to enhance efficiency in operations management (OM). Disruptive technologies, such as artificial intelligence, robotics, blockchain, 3D printing, 5G, Internet‐of‐Thing, digital twins, and augmented reality, are widely applied. They potentially will bring a radical change to real world operations. In this study, we first explore several major disruptive technologies, examine the corresponding OM studies, and highlight their current applications in the industry. Then, we discuss the pros and cons associated with the use of these technologies and uncover the potential human–machine conflicting areas. After that, we propose measures which may be able to achieve human–machine reconciles in the coming Industry 5.0 era. A concept of “sustainable social welfare” which includes worker welfare, privacy, etc. is proposed and the roles played by policy makers are also discussed. Finally, a future research agenda, which covers topics in both the Industry 4.0 and Industry 5.0 eras, is established.",SEC,https://www.semanticscholar.org/paper/1fce756bdc59d60e09b036e6d755feb4538a59bc
OM Forum - Distributed Ledgers and Operations: What Operations Management Researchers Should Know About Blockchain Technology,"Problem definition: Blockchain is a form of distributed ledger technology. While it has grown in prominence, its full potential and possible downsides are not fully understood yet, especially with respect to operations management (OM). Academic/practical relevance: This article fills this gap. Methodology: After briefly reviewing the technical foundations, we explore multiple business and policy aspects. Results: We identify five key strengths, the corresponding five main weaknesses, and three research themes of applying blockchain technology to OM. The key strengths are (1) visibility, (2) aggregation, (3) validation, (4) automation, and (5) resiliency. The corresponding weaknesses are (1) lack of privacy, (2) lack of standardization, (3) garbage in, garbage out, (4) black box effect, and (5) inefficiency. The three research themes are (1) information, (2) automation, and (3) tokenization. Managerial implications: We illustrate these research themes with multiple promising research problems, ranging from classical inventory management, to new areas of ethical OM, and to questions of industrial organization.",SEC,https://www.semanticscholar.org/paper/480510dc9c3e5ced37f55d0a970ee453bcd81906
"REVIEW ON THE EVOLUTION AND IMPACT OF IOT-DRIVEN PREDICTIVE MAINTENANCE: ASSESSING ADVANCEMENTS, THEIR ROLE IN ENHANCING SYSTEM LONGEVITY, AND SUSTAINABLE OPERATIONS IN BOTH MECHANICAL AND ELECTRICAL REALMS","This study provides a comprehensive review of the evolution and impact of Internet of Things (IoT)-driven predictive maintenance, focusing on advancements in technology, their role in enhancing system longevity, and promoting sustainable operations in mechanical and electrical systems. The primary objective was to assess how IoT integration has transformed traditional maintenance approaches, leading to improved system durability and reliability. Utilizing a systematic literature review methodology, the study involved sourcing data from peer-reviewed journals, conference proceedings, and industry reports. A content analysis approach was employed to analyze the data, focusing on themes such as technological advancements, sustainability considerations, and industry-specific applications of IoT in predictive maintenance. Key findings reveal significant advancements in IoT applications, particularly the integration of advanced data analytics, artificial intelligence, and machine learning in predictive maintenance strategies. These advancements have led to more accurate and timely maintenance interventions, contributing to enhanced system longevity and operational efficiency. The study also highlights the emergence of green IoT practices and the challenges and opportunities in the future landscape of IoT in predictive maintenance. The study concludes that IoT-driven predictive maintenance is pivotal for sustainable industrial operations, with opportunities lying in addressing challenges through innovative solutions and robust regulatory frameworks. Recommendations for industry and policy include fostering sustainable IoT practices and prioritizing energy efficiency. Future research directions involve exploring the integration of IoT with emerging technologies and investigating the long-term environmental impacts of IoT deployments. Keywords: Predictive Maintenance, System Longevity, Sustainable Operations, Internet of Things.",OPS,https://www.semanticscholar.org/paper/15bbd7b2ca393b85914da70a27d52a99d2f28f67
Blockchain technology: implications for operations and supply chain management,"PurposeThis paper aims to encourage the study of blockchain technology from an operations and supply chain management (OSCM) perspective, identifying potential areas of application, and to provide an agenda for future research.Design/methodology/approachAn explanation and analysis of blockchain technology is provided to identify implications for the field of OSCM.FindingsThe hype around the opportunities that digital ledger technologies offer is high. For OSCM, a myriad of ways in which blockchain could transform practice are identified, including enhancing product safety and security; improving quality management; reducing illegal counterfeiting; improving sustainable supply chain management; advancing inventory management and replenishment; reducing the need for intermediaries; impacting new product design and development; and reducing the cost of supply chain transactions. The immature state of practice and research surrounding blockchain means there is an opportunity for OSCM researchers to study the technology in its early stages and shape its adoption.Research limitations/implicationsThe paper provides a platform for new research that addresses gaps in knowledge and advances the field of OSCM. A research agenda is developed around six key themes.Practical implicationsThere are many opportunities for organisations to obtain an advantage by making use of blockchain technology ahead of the competition, enabling them to enhance their market position. But it is important that managers examine the characteristics of their products, services and supply chains to determine whether they need or would benefit sufficiently from the adoption of blockchain. Moreover, it is important that organisations build human capital expertise that allows them to develop, implement and exploit applications of this technology to maximum reward.Originality/valueThis is one of the first papers in a leading international OSCM journal to analyse blockchain technology, thereby complementing a recent article on digital supply chains that omitted blockchain.",CLD,https://www.semanticscholar.org/paper/abdb4a5bbdcab60344497edd3e48599b64f0c00f
Developing advanced predictive modeling techniques for optimizing business operations and reducing costs,"In today's competitive business landscape, organizations are increasingly turning to predictive modeling techniques to enhance operational efficiency and reduce costs. By leveraging data analytics, machine learning, and statistical methods, predictive models enable businesses to anticipate market trends, optimize resource allocation, and make data-driven decisions. This review explores the development of advanced predictive modeling techniques to optimize various business processes, from inventory management and supply chain optimization to customer relationship management and financial forecasting. The integration of predictive analytics into business operations can significantly reduce costs by automating workflows, minimizing waste, and enhancing accuracy in demand forecasting, key components of successful predictive modeling include robust data collection, preprocessing, and feature engineering, followed by the selection of appropriate algorithms and model evaluation. Challenges such as data quality, scalability, and ethical concerns are addressed, highlighting the need for transparency and explainable Artificial Intelligence in predictive applications. Furthermore, this review examines real-world case studies where businesses have successfully implemented predictive models to improve profitability and streamline operations. By identifying patterns and trends from historical data, these models support proactive decision-making ultimately leading to improved performance and cost savings. This concludes with insights into emerging technologies, such as deep learning and IoT, which are poised to further enhance predictive capabilities. As businesses continue to embrace digital transformation, predictive modeling will play a critical role in driving sustainable growth and competitive advantage. Keywords: Advanced Predictive Modeling, Optimizing Business Operations, Statistical Methods, Review.",OPS,https://www.semanticscholar.org/paper/427f6b3eaa9ddc9309be8425225a1e157d154156
Industry 4.0: Opportunities and Challenges for Operations Management,"Industry 4.0 connotes a new industrial revolution centered around cyber-physical systems. It posits that the real-time connection of physical and digital systems, along with new enabling technologies, will change the way that work is done and therefore, how work should be managed. It has the potential to break, or at least change, the traditional operations trade-offs among the competitive priorities of cost, flexibility, speed, and quality. This article describes the technologies inherent in Industry 4.0 and the opportunities and challenges for research in this area. The focus is on goods-producing industries, which includes both the manufacturing and agricultural sectors. Specific technologies discussed include additive manufacturing, the internet of things, blockchain, advanced robotics, and artificial intelligence.",IOTNET,https://www.semanticscholar.org/paper/12ca6ac4829605f4bc3206887759cddbb8c1ef6d
How will artificial intelligence and Industry 4.0 emerging technologies transform operations management?,"Emerging technologies such as artificial intelligence, blockchain, additive manufacturing, advanced robotics, autonomous vehicles, and the Internet of Things are frequently mentioned as part of “Industry 4.0.” As such, how will they influence operations and supply chain management? We answer this question by providing a brief review of the evolution of technologies and operations management (OM) over time. Because terms such as “Industry 4.0” do not have a precise definition, we focus on more fundamental issues raised by Industry 4.0 emerging technologies for research in OM. We propose a theory of disruptive debottlenecking and the SACE framework by classifying emerging technologies in terms of the functionalities they enable: sense, analyze, collaborate, and execute. Subsequently, we review the nascent but rapidly growing literature at the interface between digital technologies and OM. Our review suggests that one way to assess the value of Industry 4.0 technologies can be via their influence on adding revenues, differentiating, reducing costs, optimizing risks, innovating, and transforming business models and processes. Finally, we conclude by proposing an agenda for further research.",IOTNET,https://www.semanticscholar.org/paper/254fb0acc9e351173ec6570cbae538d0a00b4bc9
Unit Operations Of Chemical Engineering,"chemical engineering chemical engineering essentials for, home american journal of chemical engineering, bachelor of science in chemical engineering american, specialty polymers high performance polymers solvay, chemical engineering degrees top universities, chemical process wikipedia, perry s chemical engineers handbook eighth edition, wolfram and mathematica solutions for chemical engineering, aquatherm engineering consultants india pvt ltd edit, naval reserve officers training corps scholarship, college of engineering california state university long, unit and door heaters armstrong international, chemical engineering cput, water treatment products and services h2o engineering, journal of chemical amp engineering data acs publications, wbdg wbdg whole building design guide, chemical engineering for non chemical engineers aiche, proposed syllabus for b tech program in chemical engineering, phase out of the national diploma in chemical engineering, chemical engineering free books at ebd, operations council sgs, chemical plants india caustic soda plants chemical plants, martindale s calculators on line center mathematics, kraft recovery operations course tappi org, patent technology centers management uspto, chemical recycling makes waste plastic a resource, faculty of engineering amp technology vaal university of, chemical engineer wikipedia, index chemical engineering conferences asia events, european training network for chemical engineering, diploma of engineering curtin college, search unit standards south african qualifications authority, bcit chemical and environmental technology process, perry s chemical engineers handbook 9th edition, electrical amp systems engineering washington university, csulb chemical engineering california state university, visual encyclopedia of chemical engineeringabout gold membership gold level membership allows you full access to the chemical engineering archives dating back to 1986 quickly search and retrieve all articles and back issues, in chemical engineering process design is the design of processes for desired physical and or chemical transformation of materials process design is central to chemical engineering and it can be considered to be the summit of that field bringing together all of the fields components, at the department of chemical engineering we provide you with a challenging and contemporary chemical engineering degree program enhanced by the research accomplishments of our faculty members we value excellence in teaching quality research and service along with the intellectual development of students in a challenging rewarding academic environment, the largest selection of the highest performing polymers solvay is the industry leader in specialty polymers offering the broadest selection of high performance thermoplastic resins fluoroelastomers and fluorinated fluids, what is chemical engineering so what is chemical engineering chemical engineering is a multi disciplinary branch of engineering that combines natural and experimental sciences such as chemistry and physics along with life sciences such as biology microbiology and biochemistry plus mathematics and economics to design develop produce transform transport operate and manage the, in a scientific sense a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds such a chemical process can occur by itself or be caused by an outside force and involves a chemical reaction",OPS,https://www.semanticscholar.org/paper/7ab84ca9b72ae064e1c1a23ed439bb1a717c5185
Agricultural Robotics for Field Operations,"Modern agriculture is related to a revolution that occurred in a large group of technologies (e.g., informatics, sensors, navigation) within the last decades. In crop production systems, there are field operations that are quite labour-intensive either due to their complexity or because of the fact that they are connected to sensitive plants/edible product interaction, or because of the repetitiveness they require throughout a crop production cycle. These are the key factors for the development of agricultural robots. In this paper, a systematic review of the literature has been conducted on research and commercial agricultural robotics used in crop field operations. This study underlined that the most explored robotic systems were related to harvesting and weeding, while the less studied were the disease detection and seeding robots. The optimization and further development of agricultural robotics are vital, and should be evolved by producing faster processing algorithms, better communication between the robotic platforms and the implements, and advanced sensing systems.",CLD,https://www.semanticscholar.org/paper/70d86ae71102bf004fc85496ba4a0b35493b6eff
A hybrid meta-heuristic scheduler algorithm for optimization of workflow scheduling in cloud heterogeneous computing environment,"Purpose Improvement of workflow scheduling in distributed engineering systems Design/methodology/approach The authors proposed a hybrid meta heuristic optimization algorithm. Findings The authors have made improvement in hybrid approach by exploiting of genetic algorithm and simulated annealing plus points. Originality/value To the best of the authors’ knowledge, this paper presents a novel theorem and novel hybrid approach.",OPS,https://www.semanticscholar.org/paper/d06a229a41e9686f44198238365e867afdde3b63
IFFO: An Improved Fruit Fly Optimization Algorithm for Multiple Workflow Scheduling Minimizing Cost and Makespan in Cloud Computing Environments,"Cloud computing platforms have been extensively using scientific workflows to execute large-scale applications. However, multiobjective workflow scheduling with scientific standards to optimize QoS parameters is a challenging task. Various metaheuristic scheduling techniques have been proposed to satisfy the QoS parameters like makespan, cost, and resource utilization. Still, traditional metaheuristic approaches are incompetent to maintain agreeable equilibrium between exploration and exploitation of the search space because of their limitations like getting trapped in local optimum value at later evolution stages and higher-dimensional nonlinear optimization problem. This paper proposes an improved Fruit Fly Optimization (IFFO) algorithm to minimize makespan and cost for scheduling multiple workflows in the cloud computing environment. The proposed algorithm is evaluated using CloudSim for scheduling multiple workflows. The comparative results depict that the proposed algorithm IFFO outperforms FFO, PSO, and GA.",OPS,https://www.semanticscholar.org/paper/9e8ac27604132010014f94039bd6210a3b774bcf
Makespan-Driven Workflow Scheduling in Clouds Using Immune-Based PSO Algorithm,"Cloud Computing is becoming more and more popular for solving problems that need high concurrency and a lot of resources. Many traditional areas of research choose to solve their problems through the cloud, and workflow scheduling is one of them. Cloud computing brings many benefits, meanwhile, due to the almost “infinite” amount of resources for users, it also brings new challenges for scheduling and optimization, in which cost and makespan are the most concerned issues for workflow scheduling. Users want to obtain a low cost and fast makespan solution. This paper focuses on how to find an optimized solution to achieve better cost-makespan at the same time under the constraint of deadline. In order to solve this problem, an immune particle swarm optimization algorithm (IMPSO) is proposed, which effectively improves the quality and speed of the optimization. The proposed IMPSO overcomes the problem of slow convergence of PSO, which is easy to fall into local optimization. Experiments show the efficiency and effectiveness of the proposed approach.",OPS,https://www.semanticscholar.org/paper/742b24159c1df7929133d936b4a94ec350d45cf3
A Survey on QoS Requirements Based on Particle Swarm Optimization Scheduling Techniques for Workflow Scheduling in Cloud Computing,"Cloud computing is an innovative technology that deploys networks of servers, located in wide remote areas, for performing operations on a large amount of data. In cloud computing, a workflow model is used to represent different scientific and web applications. One of the main issues in this context is scheduling large workflows of tasks with scientific standards on the heterogeneous cloud environment. Other issues are particular to public cloud computing. These include the need for the user to be satisfied with the quality of service (QoS) parameters, such as scalability and reliability, as well as maximize the end-users resource utilization rate. This paper surveys scheduling algorithms based on particle swarm optimization (PSO). This is aimed at assisting users to decide on the most suitable QoS consideration for large workflows in infrastructure as a service (IaaS) cloud applications and mapping tasks to resources. Besides, the scheduling schemes are categorized according to the variant of the PSO algorithm implemented. Their objectives, characteristics, limitations and testing tools have also been highlighted. Finally, further directions for future research are identified.",OPS,https://www.semanticscholar.org/paper/1f94f532fdf78f677ea00efaa631f077e78ba380
Dynamic resource provisioning for workflow scheduling under uncertainty in edge computing environment,"Edge computing, an extension of cloud computing, is introduced to provide sufficient computing and storage resources for mobile devices. Moreover, a series of computing tasks in a mobile device are set as structured computing processes and flows to achieve effective management by the workflow. However, the execution uncertainty caused by performance degradation, service failure, and new service additions remains a huge challenge to the user's service experience. In order to address the uncertainty, a software‐defined network (SDN)‐based edge computing framework and a dynamic resource provisioning (UARP) method are proposed in this paper. The UARP method is implemented in the proposed framework and addresses the uncertainty through the advantages of SDN. In addition, the nondominated sorting genetic algorithm‐III is employed to optimize two goals, that is, the energy consumption and the completion time, to obtain balanced scheduling strategies. The comparative experiments are performed and the results show that the UARP method is superior to other methods in addressing the uncertainty, while reducing energy consumption and shortening the completion time.",OPS,https://www.semanticscholar.org/paper/90710586e7c244b2409887a2a4b91cf0c9df0d77
A New Double Rank-based Multi-workflow Scheduling with Multi-objective Optimization in Cloud Environments,"Workflow scheduling in clouds has been extensively researched. Many workflows from different users could be submitted to clouds at the same time and cloud providers should handle them simultaneously. So, it is necessary to consider the problem of scheduling multi-workflow. In addition, cloud computing systems can offer some special features, like Pay-Per-Use and Quality of Service (QoS) over the Internet. The scheduler has to consider the tradeoffs between different QoS parameters in order to satisfy the QoS requirements. Hence, how to schedule multiple heterogeneous workflows in the meanwhile to balance multiple objectives is a big challenge. The majority of the existing multi-workflow scheduling algorithms are based on QoS constrained approaches and attempt to optimize one objective while taking other QoS factors as constraints. Meanwhile, most of the multi-objective optimization scheduling works aim to deal with single-workflow. Conversely, this paper focuses on QoS optimization approaches by finding trade-off schedules to execute multi-workflow on cloud computing resources so as to balance multi-objective. To this end, a new double rank-based task sequencing method is proposed and integrated with a multi-objective heuristic algorithm for multi-workflow scheduling. Different algorithms are evaluated using various well-known real-world workflows and simulated workflows. The performance evaluation results demonstrate that the proposed approach is capable of generating efficient schedules with high quality in terms of meeting multi-objective for multiple workflows.",OPS,https://www.semanticscholar.org/paper/4525c0410656bcd57bcc68b444b0a26f71950473
Self adaptive fruit fly algorithm for multiple workflow scheduling in cloud computing environment,"Purpose In general, cloud computing is a model of on-demand business computing that grants a convenient access to shared configurable resources on the internet. With the increment of workload and difficulty of tasks that are submitted by cloud consumers; “how to complete these tasks effectively and rapidly with limited cloud resources?” is becoming a challenging question. The major point of a task scheduling approach is to identify a trade-off among user needs and resource utilization. However, tasks that are submitted by varied users might have diverse needs of computing time, memory space, data traffic, response time, etc. This paper aims to proposes a new way of task scheduling. Design/methodology/approach To make the workflow completion in an efficient way and to reduce the cost and flow time, this paper proposes a new way of task scheduling. Here, a self-adaptive fruit fly optimization algorithm (SA-FFOA) is used for scheduling the workflow. The proposed multiple workflow scheduling model compares its efficiency over conventional methods in terms of analysis such as performance analysis, convergence analysis and statistical analysis. From the outcome of the analysis, the betterment of the proposed approach is proven with effective workflow scheduling. Findings The proposed algorithm is more superior regarding flow time with the minimum value, and the proposed model is enhanced over FFOA by 0.23%, differential evolution by 2.48%, artificial bee colony (ABC) by 2.85%, particle swarm optimization (PSO) by 2.46%, genetic algorithm (GA) by 2.33% and expected time to compute (ETC) by 2.56%. While analyzing the make span case, the proposed algorithm is 0.28%, 0.15%, 0.38%, 0.20%, 0.21% and 0.29% better than the conventional methods such as FFOA, DE, ABC, PSO, GA and ETC, respectively. Moreover, the proposed model has attained less cost, which is 2.14% better than FFOA, 2.32% better than DE, 3.53% better than ABC, 2.43% better than PSO, 2.07% better than GA and 2.90% better than ETC, respectively. Originality/value This paper presents a new way of task scheduling for making the workflow completion in an efficient way and for reducing the cost and flow time. This is the first paper uses SA-FFOA for scheduling the workflow.",OPS,https://www.semanticscholar.org/paper/c92a2200df06a37a03fc9b4788a2c516ca8ceb96
Performance Modeling and Workflow Scheduling of Microservice-Based Applications in Clouds,"Microservice has been increasingly recognized as a promising architectural style for constructing large-scale cloud-based applications within and across organizational boundaries. This microservice-based architecture greatly increases application scalability, but meanwhile incurs an expensive performance overhead, which calls for a careful design of performance modeling and task scheduling. However, these problems have thus far remained largely unexplored. In this paper, we develop a performance modeling and prediction method for independent microservices, design a three-layer performance model for microservice-based applications, formulate a Microservice-based Application Workflow Scheduling problem for minimum end-to-end delay under a user-specified Budget Constraint (MAWS-BC), and propose a heuristic microservice scheduling algorithm. The performance modeling and prediction method are validated and justified by experimental results generated through a well-known microservice benchmark on disparate computing nodes, and the performance superiority of the proposed scheduling solution is illustrated by extensive simulation results in comparison with existing algorithms.",OPS,https://www.semanticscholar.org/paper/5b414cbbbe86fd15fc67e05bb3a355a11d46741b
"Leveraging Network Data Analytics Function and Machine Learning for Data Collection, Resource Optimization, Security and Privacy in 6G Networks","The full deployment of sixth-generation (6G) networks is inextricably connected with a holistic network redesign able to deal with various emerging challenges, such as integration of heterogeneous technologies and devices, as well as support of latency and bandwidth demanding applications. In such a complex environment, resource optimization, and security and privacy enhancement can be quite demanding, due to the vast and diverse data generation endpoints and associated hardware elements. Therefore, efficient data collection mechanisms are needed that can be deployed at any network infrastructure. In this context, the network data analytics function (NWDAF) has already been defined in the fifth-generation (5G) architecture from Release 15 of 3GPP, that can perform data collection from various network functions (NFs). When combined with advanced machine learning (ML) techniques, a full-scale network optimization can be supported, according to traffic demands and service requirements. In addition, the collected data from NWDAF can be used for anomaly detection and thus, security and privacy enhancement. Therefore, the main goal of this paper is to present the current state-of-the-art on the role of the NWDAF towards data collection, resource optimization and security enhancement in next generation broadband networks. Furthermore, various key enabling technologies for data collection and threat mitigation in the 6G framework are identified and categorized, along with advanced ML approaches. Finally, a high level architectural approach is presented and discussed, based on the NWDAF, for efficient data collection and ML model training in large scale heterogeneous environments.",OPS,https://www.semanticscholar.org/paper/e3a7c2d839cc31ea2b3547559ff0af74fdc4a896
Enterprise cloud resource optimization and management based on cloud operations,"The so-called automated operation and maintenance refers to a large number of repetitive tasks in daily IT operations (from simple daily checks, configuration changes and software installation to organizational scheduling of the entire change process) from manual execution in the past to standardized, streamlined and automated operations. This article delves into the realm of enterprise cloud resource optimization and management, leveraging automated operations (autoOps) as a fundamental strategy. As industries like banking witness exponential growth and innovation in IT systems, the complexity of managing resources escalates. Automated operations have emerged as a critical component, transitioning from manual interventions to encompass standardization, workflow optimization, and architectural enhancements. Through real-world deployments and theoretical frameworks, it elucidates effective strategies for optimizing and governing enterprise cloud resources, thereby enhancing efficiency, security, and resilience in IT operations.",OPS,https://www.semanticscholar.org/paper/7db264b007f7139fbec92dcb2c5507a32111fa3e
Smart Load-Based Resource Optimization Model to Enhance the Performance of Device-to-Device Communication in 5G-WPAN,"In wireless personal area networks (WPANs), devices can communicate with each other without relying on a central router or access point. They can improve performance and efficiency by allowing devices to share resources directly; however, managing resource allocation and optimizing communication between devices can be challenging. This paper proposes an intelligent load-based resource optimization model to enhance the performance of device-to-device communication in 5G-WPAN. Intelligent load-based resource optimization in device-to-device communication is a strategy used to maximize the efficiency and effectiveness of resource usage in device-to-device (D2D) communications. This optimization strategy is based on optimizing the network’s resource load by managing resource utilization and ensuring that the network is not overloaded. It is achieved by monitoring the current load on the network and then adjusting the usage of resources, such as bandwidth and power, to optimize the overall performance. This type of optimization is essential in D2D communication since it can help reduce costs and improve the system’s performance. The proposed model has achieved 86.00% network efficiency, 93.74% throughput, 91.94% reduced latency, and 92.85% scalability.",OPS,https://www.semanticscholar.org/paper/9aa334ff06c5cb3fc0806e0069933b66e2f95b7d
"Toward Efficient 6G IoT Networks: A Perspective on Resource Optimization Strategies, Challenges, and Future Directions","The next generation (6G) wireless communication technology has super advantages in high transmission rates scenarios. Internet of Things (IoT) has been applied in recent years due to its wide connection. However, effective resource optimization methods must be analyzed to meet the high requirements of key performance indicators in 6G wireless IoT networks. This paper discusses a general investigation of the resource optimization strategies in the 6G IoT system. The study aims to find the main solutions to optimize 6G IoT network performance. First, an overall summary of current research is preferred resource optimization in latency, reliability, Energy Efficiency (EE), Spectrum Efficiency (SE), bandwidth utilization efficiency, rate, and power efficiency. Second, we propose the multi-indicator tradeoff strategies associated with the latest resource optimization approaches and investigate optimal strategies. Furthermore, we show the limitations of the current resource optimization methods and discuss the future works of resource optimization for IoT devices in 6G communication. Our survey aims to help researchers optimize 6G IoT network performance using advanced techniques.",OPS,https://www.semanticscholar.org/paper/5b433ef43f24d9c90b5ddda05e6851dd5814280d
Importance of Circular Economy for Resource Optimization in Various Industry Sectors – A Review-based Opportunity Analysis,"Purpose: The circular economy concept is of significant importance across various industry sectors, including the primary, secondary, tertiary, and quaternary sectors. This concept has a profound impact across all industry sectors by promoting sustainable practices, resource efficiency, waste reduction, and innovation. By embracing circular principles, industries can contribute to the transition towards a more sustainable and resilient economy while creating economic value and minimizing environmental impact. Methodology: The importance of the circular economy is discussed in various industry sectors by means of a systematic review to know the current status and SWOC and ABCD analysis to know the Opportunity of exploring this field. Results: Based on analysis, comparison, evaluation, and interpretation of the circular economy in all four industry sectors, the importance of Circular Economy for Resource Optimization are suggested. Outcome/Values/Novelty: The importance of the Circular Economy in primary, secondary, tertiary, and quaternary industries are evaluated by knowing the current status and the opportunity are analyzed using SWOC and ABCD analysis frameworks. Type of Paper: Exploratory Analysis",OPS,https://www.semanticscholar.org/paper/fd6ce040f2d3a14b8b56cafff09979320928d05a
Fair Energy-Efficient Resource Optimization for Green Multi-NOMA-UAV Assisted Internet of Things,"Owing to the advantages of better air-ground channel and higher flexibility, unmanned aerial vehicle (UAV) has been widely applied in the field of Internet of Things (IoT) to increase the communication coverage. However, the UAV with limited energy is facing severe energy shortage when serving more and more IoT devices. In this paper, we propose a non-orthogonal multiple access (NOMA) based green multi-UAV assisted IoT system to increase user capacity while improving the energy utilization of each UAV. Considering the limited energy budget of each UAV, we formulate a fair energy-efficient resource optimization problem under the constraints of maximum transmit power of each UAV, minimum communication rate requirement of each user and UAV mobility. By alternately optimizing the communication scheduling, transmit power allocation and UAV trajectory with the Dinkelbach method and the successive convex approximation (SCA), the energy-efficient fairness can be achieved between the UAVs by maximizing the minimum energy efficiency of the UAVs. The simulation results indicate the fair energy efficiency between the UAVs can be obtained through the alternative resource optimization, and the proposed multi-NOMA-UAV assisted IoT can get higher energy efficiency than the traditional orthogonal multiple access (OMA)-UAV assisted IoT.",OPS,https://www.semanticscholar.org/paper/4bdae1f7159bf93bdcddcbfe09e7814873b7f650
Fair Energy-Efficient Resource Optimization for Multi-UAV Enabled Internet of Things,"Unmanned aerial vehicle (UAV) enabled Internet of Things (IoT) can keep network connectivity when the ground infrastructures are paralyzed. However, its transmission perform will be restricted due to the limited energy of the UAV. In this paper, a multi-UAV enabled IoT is proposed, where the UAVs as base stations send information to the ground IoT nodes via downlink within the flight time. And a fair energy-efficient resource optimization scheme for the IoT is studied to ensure fair energy consumption of multiple UAVs. The optimization problem seeks to maximize the minimum energy efficiency of each UAV by jointly optimizing communication scheduling, power allocations and trajectories of the UAVs. We decompose the non-convex optimization problem into three sub-optimization problems and solve them by Dlinkelbach method and successive convex approximation (SCA). Then a joint optimization algorithm is put forward to obtain the global optimal solutions by iteratively optimizing the three sub-optimization problems. The simulations results show that the multi-UAV enabled IoT can achieve significant performance improvement, and the energy efficiency between UAVs can achieve relative fairness by the fair resource optimization.",OPS,https://www.semanticscholar.org/paper/d57a0714375421a8e8864c9b3d999981d1f65c10
Resource Optimization Tool,"Logistics Management Institute (LMI) is a consulting firm that innovates technological solutions for its clients spanning across the defense, space, and healthcare industries. The company is interested in expanding their workforce analytics toolset by digitizing the project assignment process. Current processes are vulnerable to inaccurate estimates of employee skills, project requirements, and project priority. Manually crafting assignments from complex inputs can result in schedules with unequal employee utilization, long project time horizons, and suboptimal project sequencing. LMI has digital tools that quantify employee skills and project requirements and now requires a tool to produce an optimized project schedule. The team developed several mathematical models that are optimized using different solvers, heuristics, and software that converts the mathematical model output into human-readable schedules and performance metrics. This project has provided LMI with a product they can utilize internally or sell to customers to efficiently allocate resources. The optimized assignment approach produces solutions that show several improvements compared to an algorithmic approximation of the current state, with increases in average employee utilization of up to 41% and decreases in makespan of up to 60%.",OPS,https://www.semanticscholar.org/paper/71dd4517e978d8d30cc9da3c37ca7d451b52df0f
Eigen: End-to-end Resource Optimization for Large-Scale Databases on the Cloud,"Increasingly, cloud database vendors host large-scale geographically distributed clusters to provide cloud database services. When managing the clusters, we observe that it is challenging to simultaneously maximizing the resource allocation ratio and resource availability. This problem becomes more severe in modern cloud database clusters, where resource allocations occur more frequently and on a greater scale. To improve the resource allocation ratio without hurting resource availability, we introduce Eigen, a large-scale cloud-native cluster management system for large-scale databases on the cloud. Based on a resource flow model, we propose a hierarchical resource management system and three resource optimization algorithms that enable end-to-end resource optimization. Furthermore, we demonstrate the system optimization that promotes user experience by reducing scheduling latencies and improving scheduling throughput. Eigen has been launched in a large-scale public-cloud production environment for 30+ months and served more than 30+ regions (100+ available zones) globally. Based on the evaluation of real-world clusters and simulated experiments, Eigen can improve the allocation ratio by over 27% (from 60% to 87.0%) on average, while the ratio of delayed resource provisions is under 0.1%.",OPS,https://www.semanticscholar.org/paper/8c4afb83ef566eec8cc9729285650c7df557de5b
Vehicle Selection and Resource Optimization for Federated Learning in Vehicular Edge Computing,"As a distributed deep learning paradigm, federated learning (FL) provides a powerful tool for the accurate and efficient processing of on-board data in vehicular edge computing (VEC). However, FL involves the training and transmission of model parameters, which consumes the vehicles’ precious energy resources and takes up much time. It is a departure from many applications with severe real-time requirements in VEC. And the capabilities and data quality of each vehicle are distinct that will affect the performance of training the model. Therefore, it is crucial to select the appropriate vehicles to participate in learning tasks and optimize resource allocation under learning time and energy consumption constraints. In this paper, taking the vehicle position and velocity into consideration, we formulate a min-max optimization problem to jointly optimize the on-board computation capability, transmission power, and local model accuracy to achieve the minimum cost in the worst case of FL. Specifically, we propose a greedy algorithm to select vehicles with higher image quality dynamically, and it keeps the system’s overall cost to a minimum in FL. The formulated optimization problem is a nonlinear programming problem, so we decompose it into two subproblems. For the resource allocation problem, we use the Lagrangian dual problem and the subgradient projection method to approximate the optimal value iteratively. For the local model accuracy problem, we develop an adaptive harmony algorithm for heuristic search. The simulation results show that our proposed algorithms have well convergence and effectiveness and achieve a tradeoff between cost and fairness.",OPS,https://www.semanticscholar.org/paper/269789e0921d4b266bdc898d15876852ce3d5027
Joint Computation and Communication Resource Optimization for Beyond Diagonal UAV-IRS Empowered MEC Networks,"Recent advancements in 6G systems signal a leap towards universal connectivity and ultra-reliable, low-latency communications for real-time data devices. Yet, these advancements encounter obstacles such as limited device battery life and computational power, along with urban signal blockages. To counter these, Intelligent Reconfigurable Surfaces (IRS) within Mobile Edge Cloud (MEC) infrastructures offer enhanced computing to overcome device limitations and create alternative communication paths. Despite these improvements, connectivity issues remain for remote areas. Our paper presents the Beyond Diagonal IRS (BD-IRS or IRS 2.0), integrated with UAVs in MEC networks (BD-IRS-UAV), providing on-demand links for remote users to offload tasks, tackling resource and battery limitations. We propose a joint optimization strategy to reduce system's worst-case latency and UAV hovering time by optimizing BD-IRS-UAV deployment and resource allocation. This challenge is approached by dividing it into two sub-problems: BD-IRS-UAV Placement and Computational Resource Optimization, and Communication Resource Optimization, each solved iteratively. This design significantly enhances system performance, showing a $17.75\%$ increase over traditional diagonal IRS and a $25.43\%$ improvement over IRS on buildings, with a $13.44\%$ enhancement in worst-case latency compared to binary offloading schemes.",OPS,https://www.semanticscholar.org/paper/6c42d4bbc9346a52de55740995f120736c048e74
Online Trajectory and Resource Optimization for Stochastic UAV-enabled MEC System,"The recent development of unmanned aerial vehicle (UAV) and mobile edge computing (MEC) technologies provides flexible and resilient computation services to mobile users out of the terrestrial computing service coverage. In this paper, we consider a UAV-enabled MEC platform that serves multiple mobile ground users with random movements and task arrivals. We aim to minimize the average weighted energy consumption of all users subject to the average UAV energy consumption and data queue stability constraints. We formulate the problem as a multi-stage stochastic optimization, and adopt Lyapunov optimization to convert it into per-slot deterministic problems with fewer optimizing variables. We design two reduced-complexity methods that solve the resource allocation and the UAV movement either in two sequential steps or jointly in one step. Both methods can guarantee to satisfy the average UAV energy and queue stability constraints, meanwhile achieving a tradeoff between the user energy consumption and the length of queue backlog. Simulation results show that the two methods significantly outperform the other benchmark methods including a learning-based method in reducing the energy consumption of ground users. In between, the proposed joint optimization method achieves better performance than the two-stage method at the cost of higher computational complexity.",OPS,https://www.semanticscholar.org/paper/5c1daccf83fb2a2d9e25760a109d83d56b8101c1
Joint Trajectory and Resource Optimization for UAV-Aided Two-Way Relay Networks,"Unmanned aerial vehicle (UAV)-aided two-way relay networks with multiple terrestrial user pairs is investigated, where a fix-wing UAV is served as a two-way relay to assist information transmission. We aim to maximize the total rate of the two-way relay networks while the quality of service (QoS) being guaranteed. A novel relay strategy, namely time-slots pairing, is proposed by exploiting physical-layer network coding (PNC) protocol. Specifically, the UAV receives and buffers the signals from the scheduled user pair when the UAV approaches one of them, then forwards the modulated signal when the UAV is close to the other. As a result, a non-convex total rate maximization problem is formulated by joint trajectory design and resource optimization. To solve this problem, we first decompose it into three sub-problems, and then a three-step iterative algorithm that can effectively handle the non-convex problem with no worse than a polynomial complexity is developed leveraging the block coordinate descent (BCD) technique and successive convex approximation (SCA) method. Simulation results finally illustrate that: 1) Compared with amplify-and-forward (AF) or decode-and-forward (DF) protocols, the proposed design based on PNC protocol can significantly improve the total rate of the two-way relay networks; 2) the time-slots pairing relay strategy always achieves a significant enhancement on performance than the traditional non-time-slots pairing relay strategy; 3) the time-slots pairing relay strategy is more favorable for the mobile PNC relay networks since it can well resist the degradation of the entire networks caused by users’ increased service quality requirements.",OPS,https://www.semanticscholar.org/paper/b5b4e6ec05414dba0da282f1ca42256dbb087e96
Joint Online Route Planning and Resource Optimization for Multitarget Tracking in Airborne Radar Systems,"Reasonable route planning and resource allocation strategy in the airborne radar systems (ARS), can sufficiently utilize the limited resources and promote the multitarget tracking (MTT) performance. However, using separately the route planning and resource optimization method cannot take full advantage of the airborne platform. Considering this issue, we propose a joint online route planning and resource optimization strategy in the ARS to improve the system capability for MTT. First, a kinematic model of the ARS, including the mathematical expression between the radar states and the system control parameters, is introduced, which integrates the route planning to the radar scheduling scheme. Next, the posterior Cramér–Rao lower bound about route planning and resource optimization variables for the tracking targets is derived. Then, a scaled-based utility function is established to quantify the MTT performance. Hereafter, a nonconvex problem is formulated by minimizing the utility function with route and resource constraints, and then a efficient three-stage partition-based solution is proposed. Finally, simulation experiments demonstrate the effectiveness of the proposed algorithm. Furthermore, over the traditional benchmark algorithm, the tracking performance of the proposed approach improves 30.44%.",OPS,https://www.semanticscholar.org/paper/11a5e4d62d79c34122ac2f930b607030d2abd67a
GADGET: Online Resource Optimization for Scheduling Ring-All-Reduce Learning Jobs,"Fueled by advances in distributed deep learning (DDL), recent years have witnessed a rapidly growing demand for resource-intensive distributed/parallel computing to process DDL computing jobs. To resolve network communication bottleneck and load balancing issues in distributed computing, the so-called ""ring-all-reduce"" decentralized architecture has been increasingly adopted to remove the need for dedicated parameter servers. To date, however, there remains a lack of theoretical understanding on how to design resource optimization algorithms for efficiently scheduling ring-all-reduce DDL jobs in computing clusters. This motivates us to fill this gap by proposing a series of new resource scheduling designs for ring-all-reduce DDL jobs. Our contributions in this paper are threefold: i) We propose a new resource scheduling analytical model for ring-all-reduce deep learning, which covers a wide range of objectives in DDL performance optimization (e.g., excessive training avoidance, energy efficiency, fairness); ii) Based on the proposed performance analytical model, we develop an efficient resource scheduling algorithm called GADGET (greedy ring-all-reduce distributed graph embedding technique), which enjoys a provable strong performance guarantee; iii) We conduct extensive trace-driven experiments to demonstrate the effectiveness of the GADGET approach and its superiority over the state of the art.",OPS,https://www.semanticscholar.org/paper/578f04926177f2f63f1560aced52757678655344
Composed Resource Optimization for Multitarget Tracking in Active and Passive Radar Network,"In this article, a composed resource optimization (CRO) scheme is developed for an active and passive radar network engaged in multiple target tracking (MTT). The motivation of the CRO scheme is to collaboratively optimize the transmit resources of active radars, as well as the receiving processing resources of passive radars, to improve the overall MTT performance. We utilize the predicted conditional Cramér–Rao lower bound to evaluate the impact of allocation strategies on tracking performance and formulate the CRO as a mixed-integer nonlinear program problem since the adaptable parameters w.r.t. the target selection process are in binary form. To solve the problem, we propose an alternating direction method of multiplier-based algorithm. This algorithm transforms the original problem into an equality constrained problem by introducing two auxiliary vectors. In such a case, the CRO problem can be tackled by alternately solving several simple subproblems. Specifically, the subproblem w.r.t. the resource vector is convex, and the subproblems w.r.t. the auxiliary vectors are separable. Simulation results demonstrate that the proposed CRO scheme outperforms the traditional allocation schemes in terms of MTT performance. In addition, the performance of the CRO scheme is close to the optimal performance provided by the exhaustive method, but the computation load of the CRO scheme is lower than that of the exhaustive method. Finally, physical interpretations are presented to support our conclusions.",OPS,https://www.semanticscholar.org/paper/4c79686f23bf5b23b59cc1bd74eb71f47c2249da
The Effect of Big Data Analytics Capability on Competitive Performance: The Mediating Role of Resource Optimization and Resource Bricolage,"Although big data analytics capability (BDAC) leads to competitive performance, the mechanism of the relationship is still unclear. To narrow the research gap, this paper investigates the mediating roles of two forms of resource integration (resource optimization and resource bricolage) in the relationship between two forms of BDAC [big data analytics (BDA) management capability and BDA technology capability] and competitive performance. Supported by Partial Least Squares-Structural Equation Modeling (PLS-SEM) and the cross-sectional survey data from 219 Chinese enterprises, the results show that the resource bricolage plays a significantly mediating role in the relationships between BDA management capability and competitive performance as well as in the relationship between BDA technology capability and competitive performance. Furthermore, the mediating effect in the former relationship is stronger than that in the latter relationship. Additionally, BDA technology capability only has a direct effect on resource bricolage, while BDA management capability has a stronger effect on resource optimization than that on resource bricolage. Finally, resource bricolage has a stronger impact on competitive performance than resource optimization. These findings contribute to understanding how enterprises could apply different forms of BDAC to other kinds of resource integration to achieve outstanding competitive performance.",OPS,https://www.semanticscholar.org/paper/6cff9fbace1bee45a68efaa06b4d40377babbbe1
LBRO: Load Balancing for Resource Optimization in Edge Computing,"Mobile cloud computing and edge computing-based solutions provide means to offload tasks for resource-limited mobile devices. Mobile cloud computing provides remote cloud solutions while edge computing provides closer proximity-based solutions. Remote cloud solutions suffer from network latency and limited bandwidth challenges due to distance and dependency on the Internet. However, these challenges are addressed by edge-based solutions since the edge node is available in the same network. The use of Internet of Things-based solutions considering future Information Communication Technology infrastructure is on the rise resulting in the massive growth of digital equipment increasing the load at edge devices. Hence, some load balancing mechanism is required at the edge level to avoid resource congestion. The load balancing at the edge must consider the user’s preferences about edge resources such as personal computers or mobile devices. A user must declare which resources can be spared for other devices to avoid overprovisioning essential resources. We present Load Balancing for Resource Optimization (LBRO), a collaborative cloudlet platform to address load balancing challenges in edge computing considering users’ preferences. A comparative analysis of the proposed approach with the conventional edge-based approach yields that the proposed approach provides significantly improved results in terms of CPU, memory, and disk utilization.",OPS,https://www.semanticscholar.org/paper/3cf5b639d58c1b95179dd549a18fbdd34edc3637
Distributed Resource Optimization for NOMA Transmission in Beamforming SATCOM,"This work studies the application of nonorthogonal transmission in beamforming (BF) based forward links for next-generation satellite communication (SATCOM) with multiple gateways. With the aim of enhancing the throughput of BF SATCOM systems, the state-of-the-art nonorthogonal multiple access (NOMA) technique is exploited by serving multiple users per beam in the same time slot. In this regard, the feeder link limitations and multibeam satellite payload constraints must be considered for BF design and power allocation (PA) optimization in nonorthogonal SATCOM. To address these challenges, distributed resource optimization strategies are investigated for BF and flexible payload power resource allocation in multigateway (multi-GW) nonorthogonal SATCOM systems. Specifically, a per-feed available power-constrained BF strategy via maximization of the worst-user signal-to-leakage-and-noise ratio (SLNR) is explored with local channel state information (CSI) for a distributed operation of GWs. As an upper-bound performance limit, a centralized multilayer BF strategy is processed in a central unit with full global CSI and data sharing. After the BF direction optimization, a weighted sum-rate maximization-based (WSRM-based) power resource optimization strategy is locally applied at each GW to efficiently use the power resources for higher performance increment. The nonconvex WSRM problem, under the constraints of the practical satellite payload power budget, successful successive interference cancellation (SIC) decoding, and minimum data rate, is recast into an equivalent weighted sum-MSE minimization (WMMSE) counterpart for a tractable solution. Finally, an efficient user scheduling is designed to enable the operator to capture a substantial system-throughput gain. Accurate simulations are conducted with the near-to-real coverage area (footprints), the random distributions of users, and interference, relying on geographical locations of users. The results over a realistic simulation environment show the efficiency of our strategies.",OPS,https://www.semanticscholar.org/paper/a9005a312103784df21f3508e7701d678ed0abca
CroApp: A CNN-Based Resource Optimization Approach in Edge Computing Environment,"With the emergence of various convolutional neural network (CNN)-based applications and the rapid growth of CNN model scale, the resource-constricted end devices can hardly deploy CNN-based applications. Current work optimizes the CNN model on edge servers and deploys the optimized model on devices in an edge computing environment. However, most of them only optimize the resource consumption within or across models solely, whereas neglecting the other side. In this article, we propose a novel CNN-based resource optimization approach (CroApp) that not only optimizes the resource consumption within the CNN model but also pays attention to resource optimization across the applications. Specifically, we adopt model compression as the “inner-model” optimization method, as well as computation sharing as the “intermodel” optimization method. First, during “inner-model” optimization, the CroApp prunes unnecessary parameters within the model on edge servers to reduce the scale of the model. Then, during “intermodel” optimization, the CroApp trains a set of shareable models based on the pruned model and sends these shareable models to end devices. Finally, the CroApp adaptively adjusts the shared models to reduce resource consumption. The experimental results show that the CroApp outperforms the state-of-the-art approaches in terms of resource reduction, scalability, and application performance.",OPS,https://www.semanticscholar.org/paper/12efc9065b23bfe0fc02b5d413e79d66c310c5e1
Target Capacity Based Resource Optimization for Multiple Target Tracking in Radar Network,"In this paper, a target capacity based resource optimization (TC-RO) scheme is developed for multiple target tracking (MTT) application in radar networks. The key idea of this scheme is to coordinate the transmit power and dell time resource usage of multiple radars in order to increase the number of the targets that can be tracked with predetermined accuracy requirements. We adopt the Bayesian Cramér-Rao lower bound as a metric function to quantify the MTT accuracies, and build the TC-RO scheme as a non-smooth and non-convex optimization problem. To deal with this problem, we design an efficient three-step solution technique which incorporates relaxation and fine-tuning process. Specifically, we first relax the resulting optimization problem as a smooth one by applying sigmoid-type transformation to its objective, and then develop an appropriate method to find a local minimum to the relaxed non-convex problem with guaranteed convergence. After that, the local minimum of the relaxed problem is used as an initial point and a fine-tuning process is performed to search for a reasonable feasible solution to the original non-smooth optimization problem. Simulation results demonstrate that the proposed TC-RO scheme can greatly increase the target capacity of the radar network when compared with the traditional uniform allocation scheme.",OPS,https://www.semanticscholar.org/paper/73a73116de93e6531ecfcb91db82f2842330742a
Flexible Resource Optimization for GEO Multibeam Satellite Communication System,"Conventional GEO satellite communication systems rely on a multibeam foot-print with a uniform resource allocation to provide connectivity to users. However, applying uniform resource allocation is inefficient in presence of non-uniform demand distribution. To overcome this limitation, the next generation of broadband GEO satellite systems will enable flexibility in terms of power and bandwidth assignment, enabling on-demand resource allocation. In this paper, we propose a novel satellite resource assignment design whose goal is to satisfy the beam traffic demand by making use of the minimum transmit power and utilized bandwidth. The motivation behind the proposed design is to maximize the satellite spectrum utilization by pushing the spectrum reuse to affordable limits in terms of tolerable interference. The proposed problem formulation results in a non-convex optimization structure, for which we propose an efficient tractable solution. We validate the proposed method with extensive numerical results, which demonstrate the efficiency of the proposed approach with respect to benchmark schemes.",OPS,https://www.semanticscholar.org/paper/ca8a888b19070f2f24ca77023f247ed589ecc249
A Lightweight Authentication Protocol for UAV Networks Based on Security and Computational Resource Optimization,"The widespread use of Unmanned Aerial Vehicles (UAV) has made the security and computing resource application efficiency of UAV a hot topic in the security field of the Internet of Things. In this paper, an optimized lightweight identity security authentication protocol, Optimized Identity Authentication Protocol (ODIAP) is proposed for Internet of Drones (IoD) networks. The protocol is targeted to the security risks faced by IoD networks, and proposes the security authentication mechanism consisting of 3 phases and 7 authentication processes, which enables the protocol has both forward and backward security, and can resist mainstream network attacks. Meanwhile, this paper fully considers the computational load and proposes the identity information generation and verification method based on the Chinese residual theorem, which reduces the computational load of resource-constrained nodes and shifts the complex computational process to server nodes with abundant computational resources. Moreover, after security protocol analysis and tool verification based on the automated security verification tool Proverif, the protocol in this paper has complete security. At the same time, the performance analysis and comparison with other mainstream protocols shows that this protocol effectively optimizes the use of computing resources without compromising security.",OPS,https://www.semanticscholar.org/paper/cb1038974a7ff996f0742572ade50cc2430bdeef
Joint Waveform Control and Resource Optimization for Maneuvering Targets Tracking in Netted Colocated MIMO Radar Systems,"In netted colocated multiple-input multiple-output (MIMO) radar system (NCMRS), constrained to the limited resource, one must decide when to run the NCMRS, which nodes are to be activated, how much energy should be consumed, and which waveform type as well as parameter should be transmitted by each activated node. To this end, in this article, we consider the problem of joint waveform control and space-time resource management for maneuvering targets tracking in NCMRS. First, the joint waveform type as well as parameter selection and space-time resource management optimization model is proposed, where both the network system resource consumption and the overall tracking performance of maneuvering targets are considered and employed as the network comprehensive cost. Then, to solve the established optimization problem efficiently, a modified particle swarm optimization (MPSO)-based joint waveform control and space-time resource optimization (JWCSTRO) algorithm is put forward. In the MPSO-based JWCSTRO algorithm, the system sampling period, the activated nodes, the node-target assignment, and the subarray number, transmit energy and transmit waveform type, as well as parameter of each activated node can be adjusted jointly and adaptively, where the former five implement the space-time resource optimization and the last one achieves the waveform control in NCMRS. Finally, numerical simulation results demonstrate the effectiveness of the proposed MPSO-based JWCSTRO algorithm. Furthermore, compared with the five other benchmark algorithms, the maximum and minimum improvement ratios of the network comprehensive cost of the proposed MPSO-based JWCSTRO algorithm are 44.89% and 21.68%, respectively.",OPS,https://www.semanticscholar.org/paper/f34aa6120a6880214764c9932a329344c4ae86e6
Resource Optimization and Delay Guarantee Virtual Network Function Placement for Mapping SFC Requests in Cloud Networks,"Since the advent of network function virtualization (NFV), cloud service providers (CSPs) can implement traditional dedicated network devices as software and flexibly instantiate network functions (NFs) on common off-the-shelf servers. NFV technology enables CSPs to deploy their NFs to a cloud data center in the form of virtual network functions (VNFs) without costly capital expenditures and operating expenses. However, it is an essential but intractable issue for CSPs to devise a suitable VNF placement scheme to optimize network resource consumption and improve network performance. In this article, we focus on the VNF placement problem for mapping users’ service function chain requests (SFCRs) in cloud networks. To enhance network resource utilization, we consider the fundamental resource overheads and implementation method of VNFs. The VNF placement problem is formulated as an integer linear programming model with the aim of minimizing the total network resource consumption while guaranteeing the delay requirements of SFCRs. We devise a two-phase optimization solution (TPOS) to solve the problem. TPOS contains a mapping phase to map SFCRs on servers and an adjustment phase to optimize the placement of VNFs and VNF requests. Evaluation results demonstrate that TPOS can derive near-optimal server resource consumption and significantly enhance network resource utilization. TPOS can guarantee the delay requirements of SFCRs and outperform contrastive schemes in terms of activated servers, SFCR acceptance ratio, and average VNF utilization.",OPS,https://www.semanticscholar.org/paper/e674fc0a149c5aebbfa634c9f8ef4043eca4a9c3
Joint Trajectory-Resource Optimization in UAV-Enabled Edge-Cloud System With Virtualized Mobile Clone,"This article studies an unmanned aerial vehicle (UAV)-enabled edge-cloud system, where UAV acts as a mobile edge computing (MEC) server interplaying with remote central cloud to provide computation services to ground terminals (GTs). The UAV-enabled edge-cloud system implements a virtualized network function, namely, mobile clone (MC), for each GT to help execute their offloaded tasks. Through such network function virtualization (NFV) implemented on top of the UAV-enabled edge-cloud system, GTs can have extended computation capability and prolonged battery lifetime. We aim to jointly optimize the allocation of resource and the UAV trajectory in the 3-D spaces to minimize the overall energy consumption of the UAV. The proposed solution, therefore, can extend the endurance of the UAV and support reliable MC functions for GTs. This article solves the complicated optimization problem through a block coordinate descent algorithm in an iterative way. In each iteration, the allocation of resource is modeled as a multiple constrained optimization problem given predefined UAV trajectory, which can be reformulated into a more tractable convex form and solved by successive convex optimization and Lagrange duality. Second, given the allocated resource, the optimization of the trajectory of rotary-wing/fixed-wing UAV can be formulated into a series of convex quadratically constrained quadratically program (QCQP) problems and solved by the standard convex optimization techniques. After the block coordinate descent algorithm converges to a prescribed accuracy, a high-quality suboptimal solution can be found. According to the simulation, the numerical results verify the effectiveness of our proposed solution in contrast to the baseline solutions.",OPS,https://www.semanticscholar.org/paper/9909ae623e11a612ed7ef3805ae401664238dea7
Cognitive Carrier Resource Optimization for Internet-of-Vehicles in 5G-Enhanced Smart Cities,"Internet-of-Vehicles (IoV), an important part of Intelligent Transportation Systems, is one of the most strategic applications in smart cities initiatives. The mMTC and URLLC functions of 5G are especially crucial for ensuring the connectivity and communication needs of rapidly moving IoVs. In this backdrop, network virtualization, cognitive computing along with smart spectrum resource management to the virtual networks will play a key role in solving the spectrum resource challenge. In this article, we propose a dynamic carrier resource allocation scheme for supporting IoV systems in smart cities enabled by cloud radio access networks (CRAN)-based 5G carriers. In CRAN-based 5G networks, the carrier resource allocated to the virtual networks can be centrally managed and shared to meet the dynamic demand of cell capacities caused by the rapid movement of IoVs, and the response to this dynamic allocation will become more time critical. The proposed cognitive carrier resource optimization is achieved by enhancing the ability to predict movement of IoVs, hence the dynamically changing demand for carrier resources. As an enhancement of the traditional Markov Model, our prediction model introduces vehicles' mobility analysis in order to allow the construction of a more precise flow transition matrix to improve the prediction result. Numerical results are provided to show the performance improvement of the proposed method.",OPS,https://www.semanticscholar.org/paper/f552ef2a44b31e3743940adac5a3e76ba1533059
Ant colony resource optimization for Industrial IoT and CPS,"Internet‐of‐Things (IoT) enabled cyber‐physical systems (CPS) is a system in which communication between the physical devices and the cyber environment runs independently without any user interaction. Several optimization algorithms have been used for determining the optimal solutions that can reduce the production cost and/or enhance the production efficiency with in limited time‐periods. However, existing optimization approaches have failed to solve the issues in the complex manufacturing process. To overcome this issue, a novel technique called directed acyclic graph theory based multiobjective oppositional learnt artificial ant colony resource optimization (DAGT‐MOLAACRO) technique has been introduced in this study for solving the complex manufacturing process in the industry. Initially, IoT devices are used in the industrial sector for sensing and collecting data. Then the collected data is sent to the cyberspace of the CPS system with the least latency. Then, the CPS system collects the data generated from the industrial IoT devices that is stored in cyberspace with lesser memory consumption. MOLAACRO is applied to find the optimal solution among the population that satisfies the resource constraints by constructing the directed acyclic graph. In this way, the DAGT‐MOLAACRO technique reduces the time complexity with minimal latency and computation overhead. For verification purposes, our experimental work has been carried out using different performance metrics such as data latency, time complexity, and computation overhead with respect to the number of IoT devices and the amount of data collected. The results show that the DAGT‐MOLAACRO technique has better performance with reductions in terms of time complexity by 10%, latency by 17%, and the computation overhead by 11% against the existing works in literature.",OPS,https://www.semanticscholar.org/paper/5fd242a5240975d419841c00ae80db2293c0345d
Fuzzy-multi-mode Resource-constrained Discrete Time-cost-resource Optimization in Project Scheduling Using ENSCBO,"Construction companies are required to employ effective methods of project planning and scheduling in today's competitive environment. Time and cost are critical factors in project success, and they can vary based on the type and amount of resources used for activities, such as labor, tools, and materials. In addition, resource leveling strategies that are used to limit fluctuations in a project's resource consumption also affect project time and cost. The multi-mode resource-constrained discrete-time–cost-resource optimization (MRC-DTCRO) is an optimization tool that is developed for scheduling of a set of activities involving multiple execution modes with the aim of minimizing time, cost, and resource moment. Moreover, uncertainty in cost should be accounted for in project planning because activities are exposed to risks that can cause delays and budget overruns. This paper presents a fuzzy-multi-mode resource-constrained discrete-time–cost-resource optimization (F-MRC-DTCRO) model for the time-cost-resource moment tradeoff in a fuzzy environment while satisfying all the project constraints. In the proposed model, fuzzy numbers are used to characterize the uncertainty of direct cost of activities. Using this model, different risk acceptance levels of the decision maker can be addressed in the optimization process. A newly developed multi-objective optimization algorithm called ENSCBO is used to search non-dominated solutions to the fuzzy multi-objective model. Finally, the developed model is applied to solve a benchmark test problem. The results indicate that incorporating the fuzzy structure of uncertainty in costs to previously developed MRC-DTCRO models facilitates the decision-making process and provides more realistic solutions.",OPS,https://www.semanticscholar.org/paper/50d8ef8b79ac244946867dba3daf056bdde7d299
Resource optimization for the quantum Internet,"The quantum Internet enables networking based on the fundamentals of quantum mechanics. Here, we define methods and procedures of resource prioritization and resource balancing for the quantum Internet. The aim of the proposed solutions is to optimize the resource allocation mechanisms and to reduce the resource consumptions of the network entities.",OPS,https://www.semanticscholar.org/paper/d11c2ad6d18205465c3901fc8282ff8dce08500b
Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism,"Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this article, we present the primary design aspects for enabling federated learning at the network edge. We model the incentive- based interaction between a global server and participating devices for federated learning via a Stackelberg game to motivate the participation of the devices in the federated learning process. We present several open research challenges with their possible solutions. Finally, we provide an outlook on future research.",OPS,https://www.semanticscholar.org/paper/2a3d09bbdfe21418ce75d6973f71028fa9192b89
Transfer Learning-powered Resource Optimization for Green Computing in 5G-Aided Industrial Internet of Things,"Objective: Green computing meets the needs of a low-carbon society and it is an important aspect of promoting social sustainable development and technological progress. In the investigation, green computing for resource management and allocation issues is only discussed. Therefore, in the context of the 5G communication network, the investigation of the data classification and resource optimization of the Internet of Things are conducted. Method: The virtualization architecture of the heterogeneous wireless network resource based on 5G technology is designed. The related investigation is conducted based on 5G network and Internet of Things technology. Under the traditional method, the transfer learning is introduced to improve the AdaBoost (Adaptive Boosting) algorithm to classify the data. The investigated complete resource reuse method is used to optimize resources. A method that a sub-channel can be reused by a cellular link and any number of D2D links at the same time is proposed to conduct resource optimization investigation. Results: The investigation indicates that the classification accuracy of the algorithm is excellent for the data classification of the Internet of Things and has different advantages in various aspects compared with other algorithms. The designed algorithm can find a larger set of resource reuse and have a significant increase in spectrum utilization efficiency. Conclusion: The investigation can contribute to the boom in the Internet of Things in terms of data classification and resource optimization based on 5G.",OPS,https://www.semanticscholar.org/paper/39bf9f87a8e31b70f6cb08fb80d351eb8a610885
Stochastic Resource Optimization of Random Access for Transmitters With Correlated Activation,"For a range of scenarios arising in sensor networks, control and edge computing, communication is event-triggered; that is, in response to the environment of the communicating devices. A key feature of device activity in this setting is correlation, which is particularly relevant for sensing of physical phenomena such as earthquakes or flooding. Such correlation introduces a new challenge in the design of resource allocation and scheduling for random access that aim to maximize throughput or expected sum-rate, which do not admit a closed-form expression. In this letter, we develop stochastic resource optimization algorithms to design a random access scheme that provably converge with probability one to locally optimal solutions of the throughput and the sum-rate. A key feature of the stochastic optimization algorithm is that the number of parameters that need to be estimated grows at most linearly in the number of devices. We show via simulations that our algorithms outperform existing approaches in terms of the expected sum-rate by up to 30% for a moderate number of available slots.",OPS,https://www.semanticscholar.org/paper/ff9c34e2c980d71a58fe30a73b422da1f0a28d4b
Resource Optimization for Signal Recognition in Satellite MEC with Federated Learning,"Currently, limited resources and data privacy have influenced the development of signal modulation recognition in satellite communications. To solve the above issues, mobile edge computing (MEC) and federated learning (FL) are considered to signal modulation recognition on satellite communications in this paper. FL enables distributed learning with local datasets and allows model parameters instead of the whole training datasets being shared during learning process. MEC technology reduces transmission delay and energy consumption via setting up edge servers on Medium Earth Orbit (MEO) satellites close to Low Earth Orbit (LEO) satellites, High Attitude Platforms (HAPs) and Unmanned Aerial Vehicles (UAVs) replacing central server on Geostationary Earth Orbit (GEO) satellites. To accelerate the learning process and improve the resource allocation strategy simultaneously, we formulate a joint delay ratio and energy efficiency optimization problem. We introduce a Q-learning based algorithm to solve the problem. The experimental results indicate that the accuracy of the proposed scheme is almost the same as the scheme without resource optimization while the required resource is 10% less than the classical algorithms. Meanwhile, the computation complexity of the Q-learning based algorithm is $O(n^{2})$, much lower than the ergodic scheme $(O(n^{3}))$.",OPS,https://www.semanticscholar.org/paper/43969cb92e864046fbd60b99f47ebdb7d77b3676
Multilevel Task Offloading and Resource Optimization of Edge Computing Networks Considering UAV Relay and Green Energy,"Unmanned aerial vehicle (UAV)-assisted relay mobile edge computing (MEC) network is a prominent concept, where network deployment is flexible and network coverage is wide. In scenarios such as emergency communications and low-cost coverage, optimization of offloading methods and resource utilization are important ways to improve system effectiveness due to limited terminal and UAV energy and hardware equipment. A multilevel edge computing network resource optimization model on the basis of UAV fusion that provides relay forwarding and offload services is established by considering the initial energy state of the UAV, the green energy charging function, and the reliability of computing offload. With normalized system utility function maximization as the goal, a Markov decision process algorithm meets the needs of the practical application scene and provides a flexible and effective unloading mode. This algorithm is adopted to solve the optimal offloading mode and the optimal resource utilization scheme. Simulations verify the effectiveness and reliability of the proposed multilevel offloading model. The proposed model can optimize system resource allocation and effectively improve the utility function and user experience of computing offloading systems.",OPS,https://www.semanticscholar.org/paper/7d4d8c8793f15e27b32d19a273a5194f6bad0293
Joint Availability Guarantee and Resource Optimization of Virtual Network Function Placement in Data Center Networks,"Network Function Virtualization (NFV) is a promising technology that decouples network functions from the physical device on which they deployed. Network service in NFV is deployed as Service Function Chain (SFC) that consists of an ordered set of Virtual Network Functions (VNFs). In this paper, we focus on the VNF placement problem in data center networks considering availability guarantee and resource optimization. Firstly, we define an availability model that takes both physical device failures and VNF failures into consideration when evaluating the availability of SFC. Secondly, we propose a novel Joint Path-VNF (JPV) backup model that combines path backup and VNF backup in a joint way. In the JPV backup model, resource consumption can be effectively reduced. Finally, we design an Affinity-Based Algorithm (ABA) to reduce physical link consumption when map VNFs. The evaluation results show that ABA and JPV can achieve better availability improvement (99.99%) with less resource (reduce 40% PL consumption).",OPS,https://www.semanticscholar.org/paper/459c963874cf874b55f096ba863cbdcd45b535a4
"Artificial Intelligence for Predictive Maintenance Applications: Key Components, Trustworthiness, and Future Trends","Predictive maintenance (PdM) is a policy applying data and analytics to predict when one of the components in a real system has been destroyed, and some anomalies appear so that maintenance can be performed before a breakdown takes place. Using cutting-edge technologies like data analytics and artificial intelligence (AI) enhances the performance and accuracy of predictive maintenance systems and increases their autonomy and adaptability in complex and dynamic working environments. This paper reviews the recent developments in AI-based PdM, focusing on key components, trustworthiness, and future trends. The state-of-the-art (SOTA) techniques, challenges, and opportunities associated with AI-based PdM are first analyzed. The integration of AI technologies into PdM in real-world applications, the human–robot interaction, the ethical issues emerging from using AI, and the testing and validation abilities of the developed policies are later discussed. This study exhibits the potential working areas for future research, such as digital twin, metaverse, generative AI, collaborative robots (cobots), blockchain technology, trustworthy AI, and Industrial Internet of Things (IIoT), utilizing a comprehensive survey of the current SOTA techniques, opportunities, and challenges allied with AI-based PdM.",OPS,https://www.semanticscholar.org/paper/402303ecf9fe73d152543a4e78d4014f1c7e2df2
Marine Propulsion Health Monitoring: Integrating Neural Networks and IoT Sensor Fusion in Predictive Maintenance,"The maritime sector is shifting towards predictive maintenance to improve marine propulsion system dependability and efficiency. This research introduces neural networks and IoT sensor fusion for marine propulsion health monitoring. Real-time operational data is collected by a sophisticated sensor array spanning crucial propulsion system components. Fusing sensor data using modern IoT algorithms gives a comprehensive overview of system health. The suggested technique uses neural networks for predictive maintenance. A deep learning model analyses sensor-fused data to detect flaws or performance deterioration. Training the neural network on past data from various operating situations allows it to adapt and forecast faults. The model's capacity to learn and develop improves its vessel operating state adaptation. Neural networks and IoT sensor fusion offer early defect identification and dynamic maintenance schedules. Low downtime, operating expenses, and marine propulsion system longevity are achieved using this strategy. Case studies and simulations indicate that the suggested system can predict and avoid significant failures, making it suitable for marine use.",OPS,https://www.semanticscholar.org/paper/322a433c4c5d532f32edd16d5a488d12c8d9a51a
AI in renewable energy: A review of predictive maintenance and energy optimization,"In the dynamic landscape of the burgeoning renewable energy sector, optimizing energy output, ensuring robust infrastructure maintenance, and seamless integration into the grid present formidable challenges. This paper delves into the transformative potential of artificial intelligence (AI) as a solution to these critical issues. The focus of this study is on the current state of AI applications within the renewable energy domain, particularly honing in on its profound impact on predictive maintenance and energy optimization across diverse sources such as solar, wind, and hydro. By examining the underlying AI techniques employed in this context, the research seeks to unravel the intricacies of how AI contributes to enhancing the efficiency and sustainability of renewable energy systems. A critical component of this exploration involves the analysis of successful case studies, illustrating real-world applications where AI has made substantial strides in predictive maintenance and energy optimization. These cases provide tangible evidence of the practical implications of incorporating AI into renewable energy practices. The research explores AI’s role in renewable energy, focusing on emerging trends and future directions. It aims to understand AI’s transformative influence on optimization, sustainability, and energy efficiency, fostering a more resilient and efficient energy landscape. AI is revolutionizing the renewable energy sector, transforming infrastructure maintenance, energy generation optimization, and integrating renewable sources into the grid. Its advanced analytics, predictive capabilities, and optimization are crucial in achieving global renewable energy targets. As AI technology evolves, its impact on the renewable energy landscape will deepen, paving the way for a cleaner, more sustainable future. By harnessing AI’s power, we can accelerate the transition towards a renewable energy future, ensuring a thriving planet for future generations.",OPS,https://www.semanticscholar.org/paper/91ac58f3877e6fb04049373e792bd44918a65380
Artificial intelligence (AI) in renewable energy: A review of predictive maintenance and energy optimization,"The integration of Artificial Intelligence (AI) in the renewable energy sector has emerged as a transformative force, enhancing the efficiency and sustainability of energy systems. This paper provides a comprehensive review of the application of AI in two critical aspects of renewable energy in relation to predictive maintenance and energy optimization. Predictive maintenance, enabled by AI, has revolutionized the renewable energy landscape by predicting and preventing equipment failures before they occur. Utilizing machine learning algorithms, AI analyzes vast amounts of data from sensors and historical performance to identify patterns indicative of potential faults. This proactive approach not only minimizes downtime but also extends the lifespan of renewable energy infrastructure, resulting in substantial cost savings and improved reliability. Furthermore, AI plays a pivotal role in optimizing the energy output of renewable sources. Through advanced data analytics and real-time monitoring, AI algorithms can adapt to changing environmental conditions, predicting energy production patterns and optimizing resource allocation. This ensures maximum energy yield from renewable sources, making them more competitive with traditional energy sources. The paper delves into specific AI techniques such as deep learning, neural networks, and predictive analytics employed for predictive maintenance and energy optimization in various renewable energy systems like solar, wind, and hydropower. Challenges and opportunities associated with implementing AI in renewable energy are discussed, including data security, interoperability, and the need for standardized frameworks. The synthesis of AI technologies with renewable energy not only addresses operational challenges but also contributes to the global transition towards sustainable and clean energy solutions. This review serves as a valuable resource for researchers, practitioners, and policymakers seeking insights into the evolving landscape of AI applications in the renewable energy sector. As technology continues to advance, the synergies between AI and renewable energy are poised to shape the future of the global energy paradigm.",OPS,https://www.semanticscholar.org/paper/b9609f23d937b3524e9a2acfe2a501ffc2606003
Transforming equipment management in oil and gas with AI-Driven predictive maintenance,"The oil and gas industry faces significant challenges in managing equipment maintenance due to the complexity and criticality of its assets. Traditional maintenance approaches are often reactive and inefficient, leading to costly downtime and safety risks. However, the emergence of artificial intelligence (AI) and predictive maintenance technologies offers a transformative solution to these challenges. This paper explores the role of AI-driven predictive maintenance in revolutionizing equipment management in the oil and gas sector. AI-driven predictive maintenance leverages machine learning algorithms to analyze equipment data and predict when maintenance is required before a breakdown occurs. By monitoring equipment performance in real-time, AI can identify potential issues early, allowing operators to take proactive maintenance actions. This approach helps minimize downtime, reduce maintenance costs, and improve overall equipment reliability and safety. The implementation of AI-driven predictive maintenance requires a comprehensive strategy that includes data collection, analysis, and integration with existing maintenance practices. Successful adoption of AI-driven predictive maintenance can lead to significant benefits for oil and gas companies, including increased equipment uptime, extended asset lifespan, and enhanced operational efficiency. This paper reviews the current landscape of equipment management in the oil and gas industry, highlighting the limitations of traditional maintenance practices and the need for a more proactive approach. It then examines the principles and benefits of AI-driven predictive maintenance, showcasing real-world examples of its successful implementation. Finally, the paper discusses the challenges and considerations for implementing AI-driven predictive maintenance and provides recommendations for oil and gas companies looking to transform their equipment management practices. Keywords: Transforming Equipment; Management; Oil and Gas; AI-Driven; Predictive Maintenance.",OPS,https://www.semanticscholar.org/paper/df8bb43acbf82f63802f7cb2acc5ebc49e8e64ba
Toward Physics-Informed Machine-Learning-Based Predictive Maintenance for Power Converters—A Review,"Predictive maintenance for power electronic converters has emerged as a critical area of research and development. With the rapid advancements in deep-learning techniques, new possibilities have emerged for enhancing the performance and reliability of power converters. However, addressing challenges related to data resources, physical consistency, and generalizability has become crucial in achieving optimal strategies. This comprehensive review article presents an insightful overview of the recent advancements in the field of predictive maintenance for power converters. It explores three paradigms: model-based approaches, data-driven techniques, and the emerging concept of physics-informed machine learning (PIML). By leveraging the integration of physical knowledge into machine-learning architectures, PIML holds great promise for overcoming the aforementioned concerns. Drawing upon the current state-of-art, this review identifies common trends, practical challenges, and significant research opportunities in the domain of predictive maintenance for power converters. The analysis covers a broad spectrum of approaches used for parameter identification, feature engineering, fault detection, and remaining useful life estimation. This article not only provides a comprehensive survey of recent methodologies but also highlights future trends, serving as a resource for researchers and practitioners involved in the development of predictive maintenance strategies for power converters.",OPS,https://www.semanticscholar.org/paper/72b9253b8d462f33bd92f99a81bb2a0253eb67f8
"Explainable Predictive Maintenance of Rotating Machines Using LIME, SHAP, PDP, ICE","Artificial Intelligence (AI) is a key component in Industry 4.0. Rotating machines are critical components in manufacturing industries. In the vast world of Industry 4.0, where an IoT network acts as a monitoring and decision-making system, predictive maintenance is quickly gaining importance. Predictive maintenance is a method that uses AI to handle potential problems before they cause breakdowns in operations, processes or systems. However, there is a significant issue with the AI models’ (also known as “black boxes”) inability to explain their decisions. This interpretability is vital for making maintenance decisions and validating the model’s reliability, leading to improved trust and acceptance of AI-driven predictive maintenance strategies. Explainable AI is the solution because it provides human-understandable insights into how the AI model arrives at its predictions. In this regard, the paper presents Explainable AI-based predictive maintenance of Industrial rotating machines. The proposed approach unfolds in four comprehensive stages: 1) Multi-sensor based multi-fault (5 different fault classes) data acquisition; 2) frequency-domain statistical feature extraction; and c) comparison of results for multiple AI algorithms, and d) XAI integration using “Local Interpretable Model Agnostic Explanation (LIME)”, “SHapley Additive exPlanation (SHAP)”, “Partial Dependence Plot (PDP)” and “Individual Conditional Expectation (ICE)” to interpret the results.",OPS,https://www.semanticscholar.org/paper/38350b2b3969040f0e7f153c6c143792f9a2ad65
Advancements in predictive maintenance for aging oil and gas infrastructure,"The oil and gas industry relies heavily on aging infrastructure to extract, transport, and process hydrocarbons. As these assets age, the risk of failures and downtime increases, leading to safety hazards and costly repairs. Predictive maintenance has emerged as a valuable strategy to mitigate these risks by using data-driven insights to predict equipment failures and schedule maintenance proactively. This review highlights advancements in predictive maintenance technologies for aging oil and gas infrastructure, focusing on the benefits and challenges of implementation. Advancements in sensor technology and data analytics have significantly improved the effectiveness of predictive maintenance in the oil and gas industry. Sensors installed on critical equipment collect real-time data on temperature, pressure, vibration, and other key parameters, providing insights into equipment health and performance. Data analytics tools analyze this data to identify patterns and trends indicative of potential failures, enabling operators to take preventive action before a breakdown occurs. Machine learning algorithms have also played a crucial role in enhancing predictive maintenance capabilities. These algorithms can process large volumes of data and learn from past equipment failures to predict future issues accurately. By continuously learning from new data, machine learning algorithms can improve their predictive accuracy over time, leading to more effective maintenance strategies. Despite these advancements, implementing predictive maintenance in aging oil and gas infrastructure poses several challenges. One major challenge is integrating new sensor technology with existing equipment, which may require retrofitting or upgrading existing assets. Another challenge is managing the vast amounts of data generated by sensors and analytics tools, which can strain existing IT infrastructure and require specialized expertise to analyze effectively. In conclusion, advancements in predictive maintenance technologies offer significant benefits for aging oil and gas infrastructure. By leveraging sensor technology, data analytics, and machine learning, operators can predict equipment failures, reduce downtime, and extend the life of critical assets. However, implementing these technologies requires careful planning and investment to overcome challenges related to integration, data management, and expertise.",OPS,https://www.semanticscholar.org/paper/3d9f58fbecbc9d275c127ea2babedbdc7dfa93e2
The internet of things,"When the Internet emerged more than two decades ago, it changed everything. But the Internet of Everything makes that pale in comparison. After two decades of networking and communication, 99% of things are still not networked. The Internet of Everything will disrupt several industries. That means new opportunities, businesses, experiences, and services, and big opportunities for people, companies, and countries. The Internet of Everything demands an intelligent network — a distributed, application-centric networking, computing and storage platform that connects things together, connects things to the Network and connects people and things to the cloud in ways that just weren't possible, or even imaginable, before.",IOTNET,https://www.semanticscholar.org/paper/4ce13dcc8e5497755bbedcea39c4b9b7e14fe64f
Internet of Things for Smart Cities,"The Internet of Things (IoT) shall be able to incorporate transparently and seamlessly a large number of different and heterogeneous end systems, while providing open access to selected subsets of data for the development of a plethora of digital services. Building a general architecture for the IoT is hence a very complex task, mainly because of the extremely large variety of devices, link layer technologies, and services that may be involved in such a system. In this paper, we focus specifically to an urban IoT system that, while still being quite a broad category, are characterized by their specific application domain. Urban IoTs, in fact, are designed to support the Smart City vision, which aims at exploiting the most advanced communication technologies to support added-value services for the administration of the city and for the citizens. This paper hence provides a comprehensive survey of the enabling technologies, protocols, and architecture for an urban IoT. Furthermore, the paper will present and discuss the technical solutions and best-practice guidelines adopted in the Padova Smart City project, a proof-of-concept deployment of an IoT island in the city of Padova, Italy, performed in collaboration with the city municipality.",IOTNET,https://www.semanticscholar.org/paper/52f168c6c4f42294c4c9f9305bc88b6d25ffec9a
Blockchains and Smart Contracts for the Internet of Things,"Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts-scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.",IOTNET,https://www.semanticscholar.org/paper/c998aeb12b78122ec4143b608b517aef0aa2c821
Context Aware Computing for The Internet of Things: A Survey,"As we are moving towards the Internet of Things (IoT), the number of sensors deployed around the world is growing at a rapid pace. Market research has shown a significant growth of sensor deployments over the past decade and has predicted a significant increment of the growth rate in the future. These sensors continuously generate enormous amounts of data. However, in order to add value to raw sensor data we need to understand it. Collection, modelling, reasoning, and distribution of context in relation to sensor data plays critical role in this challenge. Context-aware computing has proven to be successful in understanding sensor data. In this paper, we survey context awareness from an IoT perspective. We present the necessary background by introducing the IoT paradigm and context-aware fundamentals at the beginning. Then we provide an in-depth analysis of context life cycle. We evaluate a subset of projects (50) which represent the majority of research and commercial solutions proposed in the field of context-aware computing conducted over the last decade (2001-2011) based on our own taxonomy. Finally, based on our evaluation, we highlight the lessons to be learnt from the past and some possible directions for future research. The survey addresses a broad range of techniques, methods, models, functionalities, systems, applications, and middleware solutions related to context awareness and IoT. Our goal is not only to analyse, compare and consolidate past research work but also to appreciate their findings and discuss their applicability towards the IoT.",IOTNET,https://www.semanticscholar.org/paper/a24e67caf71c73bfd6507a3cb4e234b019e27685
"Industrial Internet of Things: Challenges, Opportunities, and Directions","Internet of Things (IoT) is an emerging domain that promises ubiquitous connection to the Internet, turning common objects into connected devices. The IoT paradigm is changing the way people interact with things around them. It paves the way for creating pervasively connected infrastructures to support innovative services and promises better flexibility and efficiency. Such advantages are attractive not only for consumer applications, but also for the industrial domain. Over the last few years, we have been witnessing the IoT paradigm making its way into the industry marketplace with purposely designed solutions. In this paper, we clarify the concepts of IoT, Industrial IoT, and Industry 4.0. We highlight the opportunities brought in by this paradigm shift as well as the challenges for its realization. In particular, we focus on the challenges associated with the need of energy efficiency, real-time performance, coexistence, interoperability, and security and privacy. We also provide a systematic overview of the state-of-the-art research efforts and potential research directions to solve Industrial IoT challenges.",IOTNET,https://www.semanticscholar.org/paper/2c311060ccd61c7d0f2ad19054c729986964ec1d
The Internet of Things for Health Care: A Comprehensive Survey,"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",IOTNET,https://www.semanticscholar.org/paper/cddb22908f28a1636cbbdeb3a4f0e00f9cef05a9
Federated Learning for Internet of Things: A Comprehensive Survey,"The Internet of Things (IoT) is penetrating many facets of our daily life with the proliferation of intelligent services and applications empowered by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may not be feasible in realistic application scenarios due to the high scalability of modern IoT networks and growing data privacy concerns. Federated Learning (FL) has emerged as a distributed collaborative AI approach that can enable many intelligent IoT applications, by allowing for AI training at distributed IoT devices without the need for data sharing. In this article, we provide a comprehensive survey of the emerging applications of FL in IoT networks, beginning from an introduction to the recent advances in FL and IoT to a discussion of their integration. Particularly, we explore and analyze the potential of FL for enabling a wide range of IoT services, including IoT data sharing, data offloading and caching, attack detection, localization, mobile crowdsensing, and IoT privacy and security. We then provide an extensive survey of the use of FL in various key IoT applications such as smart healthcare, smart transportation, Unmanned Aerial Vehicles (UAVs), smart cities, and smart industry. The important lessons learned from this review of the FL-IoT services and applications are also highlighted. We complete this survey by highlighting the current challenges and possible directions for future research in this booming area.",IOTNET,https://www.semanticscholar.org/paper/f03333b06c1b2e356dcc03f6bd3ef3d849d531e5
A Comprehensive Survey on Internet of Things (IoT) Toward 5G Wireless Systems,"Recently, wireless technologies have been growing actively all around the world. In the context of wireless technology, fifth-generation (5G) technology has become a most challenging and interesting topic in wireless research. This article provides an overview of the Internet of Things (IoT) in 5G wireless systems. IoT in the 5G system will be a game changer in the future generation. It will open a door for new wireless architecture and smart services. Recent cellular network LTE (4G) will not be sufficient and efficient to meet the demands of multiple device connectivity and high data rate, more bandwidth, low-latency quality of service (QoS), and low interference. To address these challenges, we consider 5G as the most promising technology. We provide a detailed overview of challenges and vision of various communication industries in 5G IoT systems. The different layers in 5G IoT systems are discussed in detail. This article provides a comprehensive review on emerging and enabling technologies related to the 5G system that enables IoT. We consider the technology drivers for 5G wireless technology, such as 5G new radio (NR), multiple-input–multiple-output antenna with the beamformation technology, mm-wave commutation technology, heterogeneous networks (HetNets), the role of augmented reality (AR) in IoT, which are discussed in detail. We also provide a review on low-power wide-area networks (LPWANs), security challenges, and its control measure in the 5G IoT scenario. This article introduces the role of AR in the 5G IoT scenario. This article also discusses the research gaps and future directions. The focus is also on application areas of IoT in 5G systems. We, therefore, outline some of the important research directions in 5G IoT.",IOTNET,https://www.semanticscholar.org/paper/58df8ecc89886a3d0fa63a847f64f80f4898f639
A Study of LoRa: Long Range & Low Power Networks for the Internet of Things,"LoRa is a long-range, low-power, low-bitrate, wireless telecommunications system, promoted as an infrastructure solution for the Internet of Things: end-devices use LoRa across a single wireless hop to communicate to gateway(s), connected to the Internet and which act as transparent bridges and relay messages between these end-devices and a central network server. This paper provides an overview of LoRa and an in-depth analysis of its functional components. The physical and data link layer performance is evaluated by field tests and simulations. Based on the analysis and evaluations, some possible solutions for performance enhancements are proposed.",IOTNET,https://www.semanticscholar.org/paper/7e78f4d96a9c27d0ae6f3685999c3c4470cab1f1
"Internet of Things: Architectures, Protocols, and Applications","The Internet of Things (IoT) is defined as a paradigm in which objects equipped with sensors, actuators, and processors communicate with each other to serve a meaningful purpose. In this paper, we survey state-of-the-art methods, protocols, and applications in this new emerging area. This survey paper proposes a novel taxonomy for IoT technologies, highlights some of the most important technologies, and profiles some applications that have the potential to make a striking difference in human life, especially for the differently abled and the elderly. As compared to similar survey papers in the area, this paper is far more comprehensive in its coverage and exhaustively covers most major technologies spanning from sensors to applications.",IOTNET,https://www.semanticscholar.org/paper/95b6f66798c968bff6ff35119934243a7eeab6c7
"iFogSim: A toolkit for modeling and simulation of resource management techniques in the Internet of Things, Edge and Fog computing environments","Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real‐time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit of RAM consumption and execution time is verified under different circumstances.",IOTNET,https://www.semanticscholar.org/paper/0c06d0d41ed1a946a5927157312f5936349f62bf
A Survey on 5G Networks for the Internet of Things: Communication Technologies and Challenges,"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for machine-type communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like long-term evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. In addition, the third generation partnership project cellular-based low-power wide area solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including extended coverage global system for mobile communications for the Internet of Things, enhanced machine-type communications, and narrowband-Internet of Things. Furthermore, 5G new radio enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of massive to critical IoT applications are also presented in coming up with an efficient context-aware congestion control mechanism.",IOTNET,https://www.semanticscholar.org/paper/a966699e318c9ce1859dfce6671b7cfa60c0b955
Internet of Things (IoT): A Literature Review,"One of the buzzwords in the Information Technology is Internet of Things (IoT). The future is Internet of Things, which will transform the real world objects into intelligent virtual objects. The IoT aims to unify everything in our world under a common infrastructure, giving us not only control of things around us, but also keeping us informed of the state of the things. In Light of this, present study addresses IoT concepts through systematic review of scholarly research papers, corporate white papers, professional discussions with experts and online databases. Moreover this research article focuses on definitions, geneses, basic requirements, characteristics and aliases of Internet of Things. The main objective of this paper is to provide an overview of Internet of Things, architectures, and vital technologies and their usages in our daily life. However, this manuscript will give good comprehension for the new researchers, who want to do research in this field of Internet of Things (Technological GOD) and facilitate knowledge accumulation in efficiently.",IOTNET,https://www.semanticscholar.org/paper/ac53687795aea4a0f22dc28096b686ab2d225baf
Smart Farming: Internet of Things (IoT)-Based Sustainable Agriculture,"Smart farming is a development that has emphasized information and communication technology used in machinery, equipment, and sensors in network-based hi-tech farm supervision cycles. Innovative technologies, the Internet of Things (IoT), and cloud computing are anticipated to inspire growth and initiate the use of robots and artificial intelligence in farming. Such ground-breaking deviations are unsettling current agriculture approaches, while also presenting a range of challenges. This paper investigates the tools and equipment used in applications of wireless sensors in IoT agriculture, and the anticipated challenges faced when merging technology with conventional farming activities. Furthermore, this technical knowledge is helpful to growers during crop periods from sowing to harvest; and applications in both packing and transport are also investigated.",IOTNET,https://www.semanticscholar.org/paper/ce5d99cfd5f5dd826e0432c285ac91590fc94d41
Blockchain for Internet of Things: A Survey,"Internet of Things (IoT) is reshaping the incumbent industry to smart industry featured with data-driven decision-making. However, intrinsic features of IoT result in a number of challenges, such as decentralization, poor interoperability, privacy, and security vulnerabilities. Blockchain technology brings the opportunities in addressing the challenges of IoT. In this paper, we investigate the integration of blockchain technology with IoT. We name such synthesis of blockchain and IoT as blockchain of things (BCoT). This paper presents an in-depth survey of BCoT and discusses the insights of this new paradigm. In particular, we first briefly introduce IoT and discuss the challenges of IoT. Then, we give an overview of blockchain technology. We next concentrate on introducing the convergence of blockchain and IoT and presenting the proposal of BCoT architecture. We further discuss the issues about using blockchain for fifth generation beyond in IoT as well as industrial applications of BCoT. Finally, we outline the open research directions in this promising area.",IOTNET,https://www.semanticscholar.org/paper/7fe4bbce603abe533f688b888a51d597db600609
Mobile Unmanned Aerial Vehicles (UAVs) for Energy-Efficient Internet of Things Communications,"In this paper, the efficient deployment and mobility of multiple unmanned aerial vehicles (UAVs), used as aerial base stations to collect data from ground Internet of Things (IoT) devices, are investigated. In particular, to enable reliable uplink communications for the IoT devices with a minimum total transmit power, a novel framework is proposed for jointly optimizing the 3D placement and the mobility of the UAVs, device-UAV association, and uplink power control. First, given the locations of active IoT devices at each time instant, the optimal UAVs’ locations and associations are determined. Next, to dynamically serve the IoT devices in a time-varying network, the optimal mobility patterns of the UAVs are analyzed. To this end, based on the activation process of the IoT devices, the time instances at which the UAVs must update their locations are derived. Moreover, the optimal 3D trajectory of each UAV is obtained in a way that the total energy used for the mobility of the UAVs is minimized while serving the IoT devices. Simulation results show that, using the proposed approach, the total-transmit power of the IoT devices is reduced by 45% compared with a case, in which stationary aerial base stations are deployed. In addition, the proposed approach can yield a maximum of 28% enhanced system reliability compared with the stationary case. The results also reveal an inherent tradeoff between the number of update times, the mobility of the UAVs, and the transmit power of the IoT devices. In essence, a higher number of updates can lead to lower transmit powers for the IoT devices at the cost of an increased mobility for the UAVs.",IOTNET,https://www.semanticscholar.org/paper/4bb7d334bfb9cf71262f691d656d5b7a058b8fca
IoT Solutions in Agriculture: Enhancing Efficiency and Productivity,"The agricultural sector is on the brink of a transformative era with the emergence of Internet of Things (IoT) technologies. This paper delves into integrating IoT solutions in agriculture, focusing on how these technologies can significantly enhance efficiency, productivity, and sustainability. It explores various IoT applications, including precision farming, automated irrigation, soil monitoring, and pest control, and discusses their benefits and challenges. The study underlines the immense potential of IoT in shaping the future of agriculture by harnessing real-time data, advanced analytics, and intelligent decision-making systems.",IOTNET,https://www.semanticscholar.org/paper/410f5df8ff2625eba69e61355c340d1ff1052ade
CICIoT2023: A Real-Time Dataset and Benchmark for Large-Scale Attacks in IoT Environment,"Nowadays, the Internet of Things (IoT) concept plays a pivotal role in society and brings new capabilities to different industries. The number of IoT solutions in areas such as transportation and healthcare is increasing and new services are under development. In the last decade, society has experienced a drastic increase in IoT connections. In fact, IoT connections will increase in the next few years across different areas. Conversely, several challenges still need to be faced to enable efficient and secure operations (e.g., interoperability, security, and standards). Furthermore, although efforts have been made to produce datasets composed of attacks against IoT devices, several possible attacks are not considered. Most existing efforts do not consider an extensive network topology with real IoT devices. The main goal of this research is to propose a novel and extensive IoT attack dataset to foster the development of security analytics applications in real IoT operations. To accomplish this, 33 attacks are executed in an IoT topology composed of 105 devices. These attacks are classified into seven categories, namely DDoS, DoS, Recon, Web-based, brute force, spoofing, and Mirai. Finally, all attacks are executed by malicious IoT devices targeting other IoT devices. The dataset is available on the CIC Dataset website.",IOTNET,https://www.semanticscholar.org/paper/4618d69b16ad46ecb59a1c693bb42fa7d3502a95
Integration of IoT-Enabled Technologies and Artificial Intelligence (AI) for Smart City Scenario: Recent Advancements and Future Trends,"As the global population grows, and urbanization becomes more prevalent, cities often struggle to provide convenient, secure, and sustainable lifestyles due to the lack of necessary smart technologies. Fortunately, the Internet of Things (IoT) has emerged as a solution to this challenge by connecting physical objects using electronics, sensors, software, and communication networks. This has transformed smart city infrastructures, introducing various technologies that enhance sustainability, productivity, and comfort for urban dwellers. By leveraging Artificial Intelligence (AI) to analyze the vast amount of IoT data available, new opportunities are emerging to design and manage futuristic smart cities. In this review article, we provide an overview of smart cities, defining their characteristics and exploring the architecture of IoT. A detailed analysis of various wireless communication technologies employed in smart city applications is presented, with extensive research conducted to determine the most appropriate communication technologies for specific use cases. The article also sheds light on different AI algorithms and their suitability for smart city applications. Furthermore, the integration of IoT and AI in smart city scenarios is discussed, emphasizing the potential contributions of 5G networks coupled with AI in advancing modern urban environments. This article contributes to the existing literature by highlighting the tremendous opportunities presented by integrating IoT and AI, paving the way for the development of smart cities that significantly enhance the quality of life for urban dwellers while promoting sustainability and productivity. By exploring the potential of IoT, AI, and their integration, this review article provides valuable insights into the future of smart cities, demonstrating how these technologies can positively impact urban environments and the well-being of their inhabitants.",IOTNET,https://www.semanticscholar.org/paper/608a698276123a3a3747cdcbbabe6343892f12e0
Exploring the Full Potentials of IoT for Better Financial Growth and Stability: A Comprehensive Survey,"Cutting-edge technologies, with a special emphasis on the Internet of Things (IoT), tend to operate as game changers, generating enormous alterations in both traditional and modern enterprises. Understanding multiple uses of IoT has become vital for effective financial management, given the ever-changing nature of organizations and the technological disruptions that come with this paradigm change. IoT has proven to be a powerful tool for improving operational efficiency, decision-making processes, overall productivity, and data management. As a result of the continuously expanding data volume, there is an increasing demand for a robust IT system capable of adeptly handling all enterprise processes. Consequently, businesses must develop suitable IoT architectures that can efficiently address these continually evolving requirements. This research adopts an incremental explanatory approach, guided by the principles of the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA). A rigorous examination of 84 research papers has allowed us to delve deeply into the current landscape of IoT research. This research aims to provide a complete and cohesive overview of the existing body of knowledge on IoT. This is accomplished by combining a rigorous empirical approach to categorization with ideas from specialized literature in the IoT sector. This study actively contributes to the ongoing conversation around IoT by recognizing and critically examining current difficulties. This, consequently, opens new research possibilities and promotes future developments in this ever-changing sector.",IOTNET,https://www.semanticscholar.org/paper/7079280c437b99b2cd8608f9f84dd77815e32425
Edge-IIoTset: A New Comprehensive Realistic Cyber Security Dataset of IoT and IIoT Applications for Centralized and Federated Learning,"In this paper, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the dataset has been generated using a purpose-built IoT/IIoT testbed with a large representative set of devices, sensors, protocols and cloud/edge configurations. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, etc.). Furthermore, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. In addition, we extract features obtained from different sources, including alerts, system resources, logs, network traffic, and propose new 61 features with high correlations from 1176 found features. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches (i.e., traditional machine learning as well as deep learning) in both centralized and federated learning modes. The Edge-IIoTset dataset can be publicly accessed from [1].",IOTNET,https://www.semanticscholar.org/paper/d827388ec32e4da0fce0beefe21fd97da83d4485
A Novel Deep Learning-Based Intrusion Detection System for IoT Networks,"The impressive growth rate of the Internet of Things (IoT) has drawn the attention of cybercriminals more than ever. The growing number of cyber-attacks on IoT devices and intermediate communication media backs the claim. Attacks on IoT, if they remain undetected for an extended period, cause severe service interruption resulting in financial loss. It also imposes the threat of identity protection. Detecting intrusion on IoT devices in real-time is essential to make IoT-enabled services reliable, secure, and profitable. This paper presents a novel Deep Learning (DL)-based intrusion detection system for IoT devices. This intelligent system uses a four-layer deep Fully Connected (FC) network architecture to detect malicious traffic that may initiate attacks on connected IoT devices. The proposed system has been developed as a communication protocol-independent system to reduce deployment complexities. The proposed system demonstrates reliable performance for simulated and real intrusions during the experimental performance analysis. It detects the Blackhole, Distributed Denial of Service, Opportunistic Service, Sinkhole, and Workhole attacks with an average accuracy of 93.74%. The proposed intrusion detection system’s precision, recall, and F1-score are 93.71%, 93.82%, and 93.47%, respectively, on average. This innovative deep learning-based IDS maintains a 93.21% average detection rate which is satisfactory for improving the security of IoT networks.",IOTNET,https://www.semanticscholar.org/paper/e6d2a223b292aed8c126608178c69b8f3ae7e1f7
AI for UAV-Assisted IoT Applications: A Comprehensive Review,"With the rapid development of the Internet of Things (IoT), there are a dramatically increasing number of devices, leading to the fact that only using terrestrial infrastructure can hardly provide high-quality services to all devices. Due to their flexibility, maneuverability, and economy, unmanned aerial vehicles (UAVs) are widely used to improve the performance of IoT networks. UAVs can not only provide wireless access to IoT devices in the absence of a terrestrial network but can also perform rich IoT services and applications, such as video surveillance, cargo transportation, pesticide spraying, and so forth. However, due to the high complexity, dynamics, and heterogeneity of the UAV-assisted IoT networks, growing attention has focused on using artificial intelligence (AI)-based methods to optimize, schedule, and orchestrate UAV-assisted IoT networks. In this article, we comprehensively analyze the impact of applying advanced AI architectures, models, and methods to different aspects of UAV-assisted IoT networks, including key IoT technologies, tasks, and applications. In addition, this article also explores challenges and discusses potential research directions of AI-enabled UAV-assisted IoT networks.",IOTNET,https://www.semanticscholar.org/paper/2531e68c03ecb9c90b0533a0813fdf9d87004207
"Unleashing the Power of IoT: A Comprehensive Review of IoT Applications and Future Prospects in Healthcare, Agriculture, Smart Homes, Smart Cities, and Industry 4.0","The Internet of Things (IoT) technology and devices represent an exciting field in computer science that is rapidly emerging worldwide. The demand for automation and efficiency has also been a contributing factor to the advancements in this technology. The proliferation of IoT devices coincides with advancements in wireless networking technologies, driven by the enhanced connectivity of the internet. Today, nearly any everyday object can be connected to the network, reflecting the growing demand for automation and efficiency. This paper reviews the emergence of IoT devices, analyzed their common applications, and explored the future prospects in this promising field of computer science. The examined applications encompass healthcare, agriculture, and smart cities. Although IoT technology exhibits similar deployment trends, this paper will explore different fields to discern the subtle nuances that exist among them. To comprehend the future of IoT, it is essential to comprehend the driving forces behind its advancements in various industries. By gaining a better understanding of the emergence of IoT devices, readers will develop insights into the factors that have propelled their growth and the conditions that led to technological advancements. Given the rapid pace at which IoT technology is advancing, this paper provides researchers with a deeper understanding of the factors that have brought us to this point and the ongoing efforts that are actively shaping the future of IoT. By offering a comprehensive analysis of the current landscape and potential future developments, this paper serves as a valuable resource to researchers seeking to contribute to and navigate the ever-evolving IoT ecosystem.",IOTNET,https://www.semanticscholar.org/paper/5a6cb7ead99d868a16224dc62de423cb847e1985
"IoT-Enabled Smart Agriculture: Architecture, Applications, and Challenges","The growth of the global population coupled with a decline in natural resources, farmland, and the increase in unpredictable environmental conditions leads to food security is becoming a major concern for all nations worldwide. These problems are motivators that are driving the agricultural industry to transition to smart agriculture with the application of the Internet of Things (IoT) and big data solutions to improve operational efficiency and productivity. The IoT integrates a series of existing state-of-the-art solutions and technologies, such as wireless sensor networks, cognitive radio ad hoc networks, cloud computing, big data, and end-user applications. This study presents a survey of IoT solutions and demonstrates how IoT can be integrated into the smart agriculture sector. To achieve this objective, we discuss the vision of IoT-enabled smart agriculture ecosystems by evaluating their architecture (IoT devices, communication technologies, big data storage, and processing), their applications, and research timeline. In addition, we discuss trends and opportunities of IoT applications for smart agriculture and also indicate the open issues and challenges of IoT application in smart agriculture. We hope that the findings of this study will constitute important guidelines in research and promotion of IoT solutions aiming to improve the productivity and quality of the agriculture sector as well as facilitating the transition towards a future sustainable environment with an agroecological approach.",IOTNET,https://www.semanticscholar.org/paper/534a20bd80cacaf8a4edc467f26be2bebc0ef488
A Survey on Indoor Positioning Systems for IoT-Based Applications,"The Internet of Things (IoT), as a pervasive paradigm, is becoming an integral part of the tech industry and academic research in recent years. It forms a ubiquitous heterogeneous network connecting humans and things. The basic premise is acquiring data from the environment with sensors and remote intelligent management via actuators. For IoT service providers, time and place are functional parameters. Whereas most IoT scenarios are in indoor spaces and GPS cannot fully cover them, applying an indoor positioning system (IPS) is necessary. Besides, indoor enabling technologies can leverage the capability of IoT in context-aware services. In this article, we aim to provide a panoramic view of IPSs and localization services with the centrality of IoT. First, we explain the main concepts and review the latest positioning methods, techniques, and technologies with IoT remarks. Then, we discuss technical implementation challenges and open issues with feasible solutions. Finally, we mentioned location-based services (LBSs), real IoT applications, and active vendors in the realm of positioning services. This article provides a real insight into LBSs in IoT for future research.",IOTNET,https://www.semanticscholar.org/paper/f44647694f22e070c0d32170d2a4e83f36b11a85
Design and Implementation of ESP32-Based IoT Devices,"The Internet of Things (IoT) has become a transformative technology with great potential in various sectors, including home automation, industrial control, environmental monitoring, agriculture, wearables, health monitoring, and others. The growing presence of IoT devices stimulates schools and academic institutions to integrate IoT into the educational process, since IoT skills are in demand in the labor market. This paper presents educational IoT tools and technologies that simplify the design, implementation, and testing of IoT applications. The article presents the introductory IoT course that students perform initially and then presents some of the projects that they develop and implement on their own later in the project.",IOTNET,https://www.semanticscholar.org/paper/5d17cd00cd818b5f7d705cef144da45e129f4b90
"Internet of Things (IoT) Security Intelligence: A Comprehensive Overview, Machine Learning Solutions and Research Directions","The Internet of Things (IoT) is one of the most widely used technologies today, and it has a significant effect on our lives in a variety of ways, including social, commercial, and economic aspects. In terms of automation, productivity, and comfort for consumers across a wide range of application areas, from education to smart cities, the present and future IoT technologies hold great promise for improving the overall quality of human life. However, cyber-attacks and threats greatly affect smart applications in the environment of IoT. The traditional IoT security techniques are insufficient with the recent security challenges considering the advanced booming of different kinds of attacks and threats. Utilizing artificial intelligence (AI) expertise, especially machine and deep learning solutions , is the key to delivering a dynamically enhanced and up-to-date security system for the next-generation IoT system. Throughout this article, we present a comprehensive picture on IoT security intelligence , which is built on machine and deep learning technologies that extract insights from raw data to intelligently protect IoT devices against a variety of cyber-attacks. Finally, based on our study, we highlight the associated research issues and future directions within the scope of our study. Overall, this article aspires to serve as a reference point and guide, particularly from a technical standpoint, for cybersecurity experts and researchers working in the context of IoT.",IOTNET,https://www.semanticscholar.org/paper/9b622cb37b2d7e7a5de12cdfade79066b61baea2
"IoT-Enabled Smart Cities: A Review of Concepts, Frameworks and Key Technologies","In recent years, smart cities have been significantly developed and have greatly expanded their potential. In fact, novel advancements to the Internet of things (IoT) have paved the way for new possibilities, representing a set of key enabling technologies for smart cities and allowing the production and automation of innovative services and advanced applications for the different city stakeholders. This paper presents a review of the research literature on IoT-enabled smart cities, with the aim of highlighting the main trends and open challenges of adopting IoT technologies for the development of sustainable and efficient smart cities. This work first provides a survey on the key technologies proposed in the literature for the implementation of IoT frameworks, and then a review of the main smart city approaches and frameworks, based on classification into eight domains, which extends the traditional six domain classification that is typically adopted in most of the related works.",IOTNET,https://www.semanticscholar.org/paper/84803788ace001524475b8382972c8ba143eea6d
N-BaIoT—Network-Based Detection of IoT Botnet Attacks Using Deep Autoencoders,"The proliferation of IoT devices that can be more easily compromised than desktop computers has led to an increase in IoT-based botnet attacks. To mitigate this threat, there is a need for new methods that detect attacks launched from compromised IoT devices and that differentiate between hours- and milliseconds-long IoT-based attacks. In this article, we propose a novel network-based anomaly detection method for the IoT called N-BaIoT that extracts behavior snapshots of the network and uses deep autoencoders to detect anomalous network traffic from compromised IoT devices. To evaluate our method, we infected nine commercial IoT devices in our lab with two widely known IoT-based botnets, Mirai and BASHLITE. The evaluation results demonstrated our proposed methods ability to accurately and instantly detect the attacks as they were being launched from the compromised IoT devices that were part of a botnet.",IOTNET,https://www.semanticscholar.org/paper/6b8aec2a36d28f32b85163c7b1107767405d678a
TON_IoT Telemetry Dataset: A New Generation Dataset of IoT and IIoT for Data-Driven Intrusion Detection Systems,"Although the Internet of Things (IoT) can increase efficiency and productivity through intelligent and remote management, it also increases the risk of cyber-attacks. The potential threats to IoT applications and the need to reduce risk have recently become an interesting research topic. It is crucial that effective Intrusion Detection Systems (IDSs) tailored to IoT applications be developed. Such IDSs require an updated and representative IoT dataset for training and evaluation. However, there is a lack of benchmark IoT and IIoT datasets for assessing IDSs-enabled IoT systems. This paper addresses this issue and proposes a new data-driven IoT/IIoT dataset with the ground truth that incorporates a label feature indicating normal and attack classes, as well as a type feature indicating the sub-classes of attacks targeting IoT/IIoT applications for multi-classification problems. The proposed dataset, which is named TON_IoT, includes Telemetry data of IoT/IIoT services, as well as Operating Systems logs and Network traffic of IoT network, collected from a realistic representation of a medium-scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). This paper also describes the proposed dataset of the Telemetry data of IoT/IIoT services and their characteristics. TON_IoT has various advantages that are currently lacking in the state-of-the-art datasets: i) it has various normal and attack events for different IoT/IIoT services, and ii) it includes heterogeneous data sources. We evaluated the performance of several popular Machine Learning (ML) methods and a Deep Learning model in both binary and multi-class classification problems for intrusion detection purposes using the proposed Telemetry dataset.",IOTNET,https://www.semanticscholar.org/paper/d7598dc4da0e93768068ed3b640935ad77ac7841
"Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Toward 6G","The next wave of wireless technologies is proliferating in connecting things among themselves as well as to humans. In the era of the Internet of Things (IoT), billions of sensors, machines, vehicles, drones, and robots will be connected, making the world around us smarter. The IoT will encompass devices that must wirelessly communicate a diverse set of data gathered from the environment for myriad new applications. The ultimate goal is to extract insights from this data and develop solutions that improve quality of life and generate new revenue. Providing large-scale, long-lasting, reliable, and near real-time connectivity is the major challenge in enabling a smart connected world. This paper provides a comprehensive survey on existing and emerging communication solutions for serving IoT applications in the context of cellular, wide-area, as well as non-terrestrial networks. Specifically, wireless technology enhancements for providing IoT access in the fifth-generation (5G) and beyond cellular networks, and communication networks over the unlicensed spectrum are presented. Aligned with the main key performance indicators of 5G and beyond 5G networks, we investigate solutions and standards that enable energy efficiency, reliability, low latency, and scalability (connection density) of current and future IoT networks. The solutions include grant-free access and channel coding for short-packet communications, non-orthogonal multiple access, and on-device intelligence. Further, a vision of new paradigm shifts in communication networks in the 2030s is provided, and the integration of the associated new technologies like artificial intelligence, non-terrestrial networks, and new spectra is elaborated. In particular, the potential of using emerging deep learning and federated learning techniques for enhancing the efficiency and security of IoT communication are discussed, and their promises and challenges are introduced. Finally, future research directions toward beyond 5G IoT networks are pointed out.",IOTNET,https://www.semanticscholar.org/paper/55476893ab00a8f5846695673aa11a40e4372408
A Survey on Federated Learning for Resource-Constrained IoT Devices,"Federated learning (FL) is a distributed machine learning strategy that generates a global model by learning from multiple decentralized edge clients. FL enables on-device training, keeping the client’s local data private, and further, updating the global model based on the local model updates. While FL methods offer several advantages, including scalability and data privacy, they assume there are available computational resources at each edge-device/client. However, the Internet-of-Things (IoT)-enabled devices, e.g., robots, drone swarms, and low-cost computing devices (e.g., Raspberry Pi), may have limited processing ability, low bandwidth and power, or limited storage capacity. In this survey article, we propose to answer this question: how to train distributed machine learning models for resource-constrained IoT devices? To this end, we first explore the existing studies on FL, relative assumptions for distributed implementation using IoT devices, and explore their drawbacks. We then discuss the implementation challenges and issues when applying FL to an IoT environment. We highlight an overview of FL and provide a comprehensive survey of the problem statements and emerging challenges, particularly during applying FL within heterogeneous IoT environments. Finally, we point out the future research directions for scientists and researchers who are interested in working at the intersection of FL and resource-constrained IoT environments.",IOTNET,https://www.semanticscholar.org/paper/195b88ba2ab7bc56ed24ea5b5d3f28622d2c6166
Enabling Massive IoT Toward 6G: A Comprehensive Survey,"Nowadays, many disruptive Internet-of-Things (IoT) applications emerge, such as augmented/virtual reality online games, autonomous driving, and smart everything, which are massive in number, data intensive, computation intensive, and delay sensitive. Due to the mismatch between the fifth generation (5G) and the requirements of such massive IoT-enabled applications, there is a need for technological advancements and evolutions for wireless communications and networking toward the sixth-generation (6G) networks. 6G is expected to deliver extended 5G capabilities at a very high level, such as Tbps data rate, sub-ms latency, cm-level localization, and so on, which will play a significant role in supporting massive IoT devices to operate seamlessly with highly diverse service requirements. Motivated by the aforementioned facts, in this article, we present a comprehensive survey on 6G-enabled massive IoT. First, we present the drivers and requirements by summarizing the emerging IoT-enabled applications and the corresponding requirements, along with the limitations of 5G. Second, visions of 6G are provided in terms of core technical requirements, use cases, and trends. Third, a new network architecture provided by 6G to enable massive IoT is introduced, i.e., space–air–ground–underwater/sea networks enhanced by edge computing. Fourth, some breakthrough technologies, such as machine learning and blockchain, in 6G are introduced, where the motivations, applications, and open issues of these technologies for massive IoT are summarized. Finally, a use case of fully autonomous driving is presented to show 6G supports massive IoT.",IOTNET,https://www.semanticscholar.org/paper/331dfb64e347d3ea700848657236ec38009de824
"A Survey on IoT Security: Application Areas, Security Threats, and Solution Architectures","The Internet of Things (IoT) is the next era of communication. Using the IoT, physical objects can be empowered to create, receive, and exchange data in a seamless manner. Various IoT applications focus on automating different tasks and are trying to empower the inanimate physical objects to act without any human intervention. The existing and upcoming IoT applications are highly promising to increase the level of comfort, efficiency, and automation for the users. To be able to implement such a world in an ever-growing fashion requires high security, privacy, authentication, and recovery from attacks. In this regard, it is imperative to make the required changes in the architecture of the IoT applications for achieving end-to-end secure IoT environments. In this paper, a detailed review of the security-related challenges and sources of threat in the IoT applications is presented. After discussing the security issues, various emerging and existing technologies focused on achieving a high degree of trust in the IoT applications are discussed. Four different technologies, blockchain, fog computing, edge computing, and machine learning, to increase the level of security in IoT are discussed.",IOTNET,https://www.semanticscholar.org/paper/8d208c08c9fea2793085a6da625b8e4f5ebe7695
"IoT in Smart Cities: A Survey of Technologies, Practices and Challenges","Internet of Things (IoT) is a system that integrates different devices and technologies, removing the necessity of human intervention. This enables the capacity of having smart (or smarter) cities around the world. By hosting different technologies and allowing interactions between them, the internet of things has spearheaded the development of smart city systems for sustainable living, increased comfort and productivity for citizens. The IoT for Smart Cities has many different domains and draws upon various underlying systems for its operation. In this paper, we provide a holistic coverage of the Internet of Things in Smart Cities. We start by discussing the fundamental components that make up the IoT based Smart City landscape followed by the technologies that enable these domains to exist in terms of architectures utilized, networking technologies used as well as the Artificial Algorithms deployed in IoT based Smart City systems. This is then followed up by a review of the most prevalent practices and applications in various Smart City domains. Lastly, the challenges that deployment of IoT systems for smart cities encounter along with mitigation measures.",IOTNET,https://www.semanticscholar.org/paper/ee3407727084e3498f4fae13f574d868eda5b458
IOT BASED SMART AGRICULTURE,"Agriculture plays a vital role in the development of an agricultural country. Around 70% of people depend on farming in our country, and one-third of the nation's income comes from agriculture. Issues concerning agriculture have been continually impeding the improvement of the nation. The main answer for this issue is smart agriculture by modernizing the current customary strategies for farming. Henceforth the undertaking targets making agribusiness smart using computerization and IoT advances. The featuring highlights of this project incorporate smart GPSbased remote-controlled robots to perform errands like weeding, sensing moisture, animal and bird scaring, spraying, and so on. Also, it contains smart irrigation with savvy control and canny dynamic dependent on precise ongoing field information. Thirdly, smart warehouse management includes temperature support, humidity maintenance, and burglary discovery in the stockroom. Controlling of every one of these tasks will be through any far-off shrewd gadget or PC associated with the Web, and the tasks will be performed by interfacing sensors, Wi-Fi or ZigBee modules, camera and actuators with miniature regulator and raspberry pi",IOTNET,https://www.semanticscholar.org/paper/c77d1965f44f2aa55eef4220251841e9551ed1b2
Federated-Learning-Based Anomaly Detection for IoT Security Attacks,"The Internet of Things (IoT) is made up of billions of physical devices connected to the Internet via networks that perform tasks independently with less human intervention. Such brilliant automation of mundane tasks requires a considerable amount of user data in digital format, which, in turn, makes IoT networks an open source of personally identifiable information data for malicious attackers to steal, manipulate, and perform nefarious activities. A huge interest has been developed over the past years in applying machine learning (ML)-assisted approaches in the IoT security space. However, the assumption in many current works is that big training data are widely available and transferable to the main server because data are born at the edge and are generated continuously by IoT devices. This is to say that classic ML works on the legacy set of entire data located on a central server, which makes it the least preferred option for domains with privacy concerns on user data. To address this issue, we propose the federated-learning (FL)-based anomaly detection approach to proactively recognize intrusion in IoT networks using decentralized on-device data. Our approach uses federated training rounds on gated recurrent units (GRUs) models and keeps the data intact on local IoT devices by sharing only the learned weights with the central server of FL. Also, the approach’s ensembler part aggregates the updates from multiple sources to optimize the global ML model’s accuracy. Our experimental results demonstrate that our approach outperforms the classic/centralized machine learning (non-FL) versions in securing the privacy of user data and provides an optimal accuracy rate in attack detection.",IOTNET,https://www.semanticscholar.org/paper/795308ca0a281865b42b612045e5074076a82a75
Classifying IoT Devices in Smart Environments Using Network Traffic Characteristics,"The Internet of Things (IoT) is being hailed as the next wave revolutionizing our society, and smart homes, enterprises, and cities are increasingly being equipped with a plethora of IoT devices. Yet, operators of such smart environments may not even be fully aware of their IoT assets, let alone whether each IoT device is functioning properly safe from cyber-attacks. In this paper, we address this challenge by developing a robust framework for IoT device classification using traffic characteristics obtained at the network level. Our contributions are fourfold. First, we instrument a smart environment with 28 different IoT devices spanning cameras, lights, plugs, motion sensors, appliances, and health-monitors. We collect and synthesize traffic traces from this infrastructure for a period of six months, a subset of which we release as open data for the community to use. Second, we present insights into the underlying network traffic characteristics using statistical attributes such as activity cycles, port numbers, signalling patterns, and cipher suites. Third, we develop a multi-stage machine learning based classification algorithm and demonstrate its ability to identify specific IoT devices with over 99 percent accuracy based on their network activity. Finally, we discuss the trade-offs between cost, speed, and performance involved in deploying the classification framework in real-time. Our study paves the way for operators of smart environments to monitor their IoT assets for presence, functionality, and cyber-security without requiring any specialized devices or protocols.",IOTNET,https://www.semanticscholar.org/paper/56a57b99b100694f74fa38b9a18b9269d6c9f937
"Internet of Things (IoT) for Next-Generation Smart Systems: A Review of Current Challenges, Future Trends and Prospects for Emerging 5G-IoT Scenarios","The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird’s eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing.",IOTNET,https://www.semanticscholar.org/paper/05fe0a3ae371dc1e2b0f90ee29cb73063c6c43bf
"Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities","The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of data protection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.",IOTNET,https://www.semanticscholar.org/paper/f1d92f97c01d49ef0ccc422d7a0f6eb4eeb73b20
Hierarchical Adversarial Attacks Against Graph-Neural-Network-Based IoT Network Intrusion Detection System,"The advancement of Internet of Things (IoT) technologies leads to a wide penetration and large-scale deployment of IoT systems across an entire city or even country. While IoT systems are capable of providing intelligent services, the large amount of data collected and processed in IoT systems also raises serious security concerns. Many research efforts have been devoted to design intelligent network intrusion detection system (NIDS) to prevent misuse of IoT data across smart applications. However, existing approaches may suffer from the issue of limited and imbalanced attack data when training the detection model, which make the system vulnerable especially for those unknown type attacks. In this study, a novel hierarchical adversarial attack (HAA) generation method is introduced to realize the level-aware black-box adversarial attack strategy, targeting the graph neural network (GNN)-based intrusion detection in IoT systems with a limited budget. By constructing a shadow GNN model, an intelligent mechanism based on a saliency map technique is designed to generate adversarial examples by effectively identifying and modifying the critical feature elements with minimal perturbations. A hierarchical node selection algorithm based on random walk with restart (RWR) is developed to select a set of more vulnerable nodes with high attack priority, considering their structural features, and overall loss changes within the targeted IoT network. The proposed HAA generation method is evaluated using the open-source data set UNSW-SOSR2019 with three baseline methods. Comparison results demonstrate its ability in degrading the classification precision by more than 30% in the two state-of-the-art GNN models, GCN and JK-Net, respectively, for NIDS in IoT environments.",IOTNET,https://www.semanticscholar.org/paper/6aa8d38e271bea8f962a386e08bc41c2e86e6e66
Anomaly Detection for IoT Time-Series Data: A Survey,"Anomaly detection is a problem with applications for a wide variety of domains; it involves the identification of novel or unexpected observations or sequences within the data being captured. The majority of current anomaly detection methods are highly specific to the individual use case, requiring expert knowledge of the method as well as the situation to which it is being applied. The Internet of Things (IoT) as a rapidly expanding field offers many opportunities for this type of data analysis to be implemented, however, due to the nature of the IoT, this may be difficult. This review provides a background on the challenges which may be encountered when applying anomaly detection techniques to IoT data, with examples of applications for the IoT anomaly detection taken from the literature. We discuss a range of approaches that have been developed across a variety of domains, not limited to IoT due to the relative novelty of this application. Finally, we summarize the current challenges being faced in the anomaly detection domain with a view to identifying potential research opportunities for the future.",IOTNET,https://www.semanticscholar.org/paper/30d65fbc8d71ac202ee8d7d5f0e58c63a6c6a957
