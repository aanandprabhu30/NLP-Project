{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47bf978d-760c-4772-9a5b-3c683e54fe62",
   "metadata": {},
   "source": [
    "# ðŸ§  Discipline Classifier: SciBERT + LoRA (PyTorch Loop)\n",
    "\n",
    "This notebook fine-tunes **`allenai/scibert_scivocab_uncased`** with **LoRA** on a dataset of 5,402 computing research abstracts labeled as **CS**, **IS**, or **IT**.  \n",
    "We have already:\n",
    "1. Loaded and preprocessed the combined dataset (Title + Abstract â†’ `text`, `label`).  \n",
    "2. Tokenized it with `AutoTokenizer`.  \n",
    "3. Wrapped SciBERT for sequence classification via PEFT (LoRA).  \n",
    "\n",
    "Below, instead of using `Trainer`, we implement a **pure PyTorch training loop** to avoid version mismatches between `transformers`/`accelerate`/`peft`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aeec8f9-0bf5-4c1c-aee3-bb6294382901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf68fc2-b61f-4333-8c05-1bbf1d4a2593",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset\n",
    "\n",
    "We load the dataset from `Expanded Discipline Dataset.csv`, encode labels (CS, IS, IT), combine Title + Abstract into a single `text` field, and prepare for Hugging Faceâ€™s `datasets.Dataset` format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd0feed-f6c0-4978-9100-9f45f184c11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VITA-Audio: Fast Interleaved Cross-Modal Token...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMO: Adaptive Motion Optimization for Hyper-De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FlexiAct: Towards Flexible Action Control in H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actor-Critics Can Achieve Optimal Sample Effic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Demonstrating ViSafe: Vision-enabled Safety fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  VITA-Audio: Fast Interleaved Cross-Modal Token...      0\n",
       "1  AMO: Adaptive Motion Optimization for Hyper-De...      0\n",
       "2  FlexiAct: Towards Flexible Action Control in H...      0\n",
       "3  Actor-Critics Can Achieve Optimal Sample Effic...      0\n",
       "4  Demonstrating ViSafe: Vision-enabled Safety fo...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1  Standard imports for DataFrame manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# 1.2  Read CSV into DataFrame\n",
    "df = pd.read_csv(\"Data/Expanded Discipline Dataset.csv\")\n",
    "\n",
    "# 1.3  Drop rows where Title, Abstract, or Discipline is missing\n",
    "df = df.dropna(subset=[\"Title\", \"Abstract\", \"Discipline\"])\n",
    "\n",
    "# 1.4  Combine Title + Abstract into one 'text' column\n",
    "df[\"text\"] = df[\"Title\"].str.strip() + \". \" + df[\"Abstract\"].str.strip()\n",
    "\n",
    "# 1.5  Encode discipline labels\n",
    "label2id = {\"CS\": 0, \"IS\": 1, \"IT\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "df[\"label\"] = df[\"Discipline\"].map(label2id)\n",
    "\n",
    "# 1.6  Keep only the 'text' and 'label' columns\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# 1.7  Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170b09e-66cb-4474-8813-02bdb0df00bb",
   "metadata": {},
   "source": [
    "# 2. Import & Load SciBERT Tokenizer\n",
    "\n",
    "Before tokenizing, we need to import and instantiate SciBERTâ€™s tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2fb164-9ae8-4b56-9484-fb8ba9540edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2.1  Load the SciBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb29f5-bf31-4431-9a72-308fdde8b28d",
   "metadata": {},
   "source": [
    "# 3. Tokenize Dataset\n",
    "\n",
    "Now that we have `tokenizer`, we will:\n",
    "1. Convert our pandas `df` into a Hugging Face `datasets.Dataset`.  \n",
    "2. Tokenize each `text` into `input_ids` + `attention_mask`.  \n",
    "3. Remove any extra columns (keeping only `input_ids`, `attention_mask`, and `label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42f4b50-0308-4276-a484-84917083cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277032e009dd451bbb073ddaa771695d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5402\n",
      "Features: {'label': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,  # SciBERT's max sequence length\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text column, keep only input_ids, attention_mask, and label\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6ae80c-2ac6-4b1d-a0b3-683db9da7c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4321\n",
      "Test samples: 1081\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6226b6-bba4-49be-ad68-5b8f806b2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 592,131 || all params: 110,512,902 || trainable%: 0.5358025979627248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp-bert/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the base SciBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    num_labels=3,  # CS, IS, IT\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Sequence Classification\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,  # LoRA scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]  # Apply LoRA to attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506e4666-f251-4ffd-811d-93e912c7bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7158434-0bf9-4c06-851e-4ce57dc697dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_training_steps)\n",
    "\n",
    "# Loss function (already included in the model, but we can define it explicitly)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036b5fc8-5aac-4623-b2a4-ce25bdc51766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate loss and predictions\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50d975-c24e-4d9d-9d44-eed238d212f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆ              | 48/271 [50:43<1:22:07, 22.10s/it, loss=1.0416]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_f1s = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Train F1-Score: {train_f1:.4f}\")\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bed12-c598-40c2-8916-93d26f7f5dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp-bert)",
   "language": "python",
   "name": "nlp-bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
