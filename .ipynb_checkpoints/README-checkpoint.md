# ðŸ§  NLP Project â€“ Identifying Research Methodologies in Computing

This project classifies computing research abstracts by:

- ðŸ§ **Discipline** â€“ Computer Science (CS), Information Systems (IS), Information Technology (IT)
- ðŸ§ **Subfield** â€“ AI, ML, CV, CYB, BSP, SEC, CLD, etc.
- ðŸ§ **Research Methodology** â€“ Qualitative, Quantitative, Mixed Methods

---

## ðŸ›  Built With

- Python
- Jupyter Notebook
- scikit-learn
- pandas
- seaborn
- matplotlib
- joblib

---

## ðŸ“ Current Phase

âœ… All classifiers finalized:  
**Discipline âžœ Subfield âžœ Methodology**

- Used:
  - `Logistic Regression + Bigram TF-IDF` for Discipline  
  - `SVM + SMOTE + Bigram TF-IDF` for Subfields  
  - `XGBoost + SMOTE + SciBERT (768-dim)` for Methodology  
- Evaluated using **5-fold stratified cross-validation**  
- Results include **accuracy, standard deviation, fold-wise breakdown, and version comparison**  
- Saved all trained models and vectorizers in `/Artefacts`  
- Documented full evaluation in Notion and `CrossValidation_AllModels.ipynb`  

> ðŸ” Final architecture:  
> - `Discipline`: Logistic Regression + bigram TF-IDF  
> - `Subfield`: SVM + SMOTE + bigram TF-IDF  
> - `Methodology`: XGBoost + SMOTE + SciBERT (Title + Abstract, 768-dim)

> ðŸ” v1.1 was skipped in versioning to standardize upgrades directly from v1.0 âž v1.2 âž v2.0 âž v2.2.1
---

## ðŸš€ Next Phase (Future Work)

- ðŸ”Ž Scrape and annotate additional abstracts, especially for **Mixed Methods**, to improve class balance  
- ðŸ§ª Re-run `v2.2.1` architecture on expanded dataset to evaluate generalizability gains  
- ðŸ¤– Experiment with **SciBERT fine-tuning** and other transformer models (e.g., BERT, RoBERTa via Hugging Face)  
- ðŸ§­ Explore **hierarchical modeling**: Discipline âž Subfield âž Methodology  
- ðŸ“¦ Package the final pipeline into an **inference-ready API** or lightweight UI prototype (e.g., Streamlit)  
- ðŸ§  Document best practices for model versioning, resampling, and semantic feature integration  

---

## ðŸ—‚ï¸ Repository Structure

| Folder/File | Description |
|-------------|-------------|
| `/Artefacts/` | Trained classifiers + vectorizers + evaluation visuals |
| `/Data/` | All labeled datasets used across classification tasks |
| `README.md` | This file |
| `TASKS.md` | To-do log and milestones |
| `CrossValidation_AllModels (v1.0).ipynb` | Cross-validation for original pipeline |
| `CrossValidation_AllModels (v1.2 and v2.0).ipynb` | Full CV for Subfield v1.2 and Methodology v2.0 |
| `Evaluate_DisciplineClassifier (v1.0).ipynb` | Manual test set evaluation (9 entries) |
| `methodology_classifier_(v2.1)_bert.ipynb` | MiniLM BERT embeddings + SVM/LogReg |
| `methodology_classifier_(v2.2)_scibert.ipynb` | âœ… SciBERT + XGBoost (+SMOTE) |
| `NLP_Classifier_DisciplineOnly (v1.1).ipynb` | Logistic Regression on Discipline |
| `NLP_Classifier_SubfieldOnly_CS (v1.2).ipynb` | SVM + SMOTE for CS subfields |
| `NLP_Classifier_SubfieldOnly_IS (v1.2).ipynb` | SVM + SMOTE for IS subfields |
| `NLP_Classifier_SubfieldOnly_IT (v1.2).ipynb` | SVM + SMOTE for IT subfields |
| `NLP_Methodology_Classifier (v1.2).ipynb` | SVM + SMOTE for Methodology |
| `NLP_Methodology_Classifier (v2.0).ipynb` | Methodology with Title + Abstract (TF-IDF) |
| `NLP_Pipeline_Prototype_15_Abstracts.ipynb` | Early 15-entry pipeline prototype |

---

## ðŸ“Š Data Files (`/Data/`)

| File | Description |
|------|-------------|
| `Evaluation Dataset - 9 entries.csv` | Held-out test set for Discipline model |
| `NLP_Abstract_Dataset (Discipline).csv` | Initial prototype dataset (15 entries) |
| `NLP_Abstract_Dataset (Discipline)(105).csv` | Final labeled Discipline set |
| `NLP_Abstract_Dataset (Method)(105).csv` | Final labeled Methodology set |
| `NLP_Abstract_Dataset (Subfield)(105).csv` | Final labeled Subfield set |
| `NLP_Dataset_Title_Abstract_Discipline_Subfield_Methodology.csv` | Combined dataset for v2.0 |

---

## ðŸ§  Model Artifacts (`/Artefacts/`)

| File | Description |
|------|-------------|
| `baseline_classifier_logreg.pkl` | Generic placeholder baseline |
| `discipline_classifier_logreg.pkl` | Logistic Regression for Discipline |
| `tfidf_vectorizer.pkl` | Shared TF-IDF for Discipline |
| `subfield_classifier_logreg_cs.pkl` | LogReg for CS subfields |
| `tfidf_vectorizer_cs.pkl` | TF-IDF for CS (LogReg) |
| `subfield_classifier_logreg_is.pkl` | LogReg for IS subfields |
| `tfidf_vectorizer_is.pkl` | TF-IDF for IS (LogReg) |
| `subfield_classifier_logreg_it.pkl` | LogReg for IT subfields |
| `tfidf_vectorizer_it.pkl` | TF-IDF for IT (LogReg) |
| `cs_subfield_classifier_svm_smote.pkl` | SVM + SMOTE for CS subfields |
| `cs_subfield_vectorizer_smote.pkl` | Bigram TF-IDF for CS (SVM) |
| `is_subfield_classifier_svm_smote.pkl` | SVM + SMOTE for IS subfields |
| `is_subfield_vectorizer_smote.pkl` | Bigram TF-IDF for IS (SVM) |
| `it_subfield_classifier_svm_smote.pkl` | SVM + SMOTE for IT subfields |
| `it_subfield_vectorizer_smote.pkl` | Bigram TF-IDF for IT (SVM) |
| `methodology_classifier_logreg.pkl` | Logistic Regression (v1.0) â€“ Abstract only |
| `methodology_classifier_svm.pkl` | SVM + SMOTE (v1.2) â€“ Abstract only |
| `methodology_classifier_v2_titleabstract.pkl` | SVM + SMOTE (v2.0) â€“ Title + Abstract |
| `tfidf_vectorizer_methodology.pkl` | Used in v1.0 (LogReg only) |
| `tfidf_vectorizer_methodology_smote.pkl` | Used in v1.2 (SVM + SMOTE) |
| `tfidf_vectorizer_methodology_v2_titleabstract.pkl` | Used in v2.0 |
| `methodology_confusion_matrix_v2.png` | Confusion matrix for Methodology v2.0 |
| `methodology_scibert_xgb_v2.2_model.pkl` | SciBERT + XGBoost (v2.2) baseline |
| `methodology_scibert_xgb_v2.2.1_smote_model.pkl` | âœ… Best-performing model (v2.2.1) |
| `methodology_scibert_xgb_v2.2_scaler.pkl` | StandardScaler used in v2.2 & v2.2.1 |
| `methodology_scibert_xgb_v2.2_label_encoder.pkl` | LabelEncoder for BERT-based classifiers |
---
## ðŸ” Methodology Version Map

| Version   | Model                                | Vectorizer / Embedding                          | Notes |
|-----------|---------------------------------------|--------------------------------------------------|-------|
| v1.0      | `methodology_classifier_logreg.pkl`   | `tfidf_vectorizer_methodology.pkl`              | Abstract-only baseline |
| v1.2      | `methodology_classifier_svm.pkl`      | `tfidf_vectorizer_methodology_smote.pkl`        | SVM + SMOTE |
| v2.0      | `methodology_classifier_v2_titleabstract.pkl` | `tfidf_vectorizer_methodology_v2_titleabstract.pkl` | Title + Abstract |
| v2.1.1    | Logistic Regression (Scaled)          | MiniLM (384-dim) via sentence-transformers      | CV only â€“ no model saved |
| v2.2      | `methodology_scibert_xgb_v2.2_model.pkl` | SciBERT (768-dim, Title + Abstract)             | XGBoost baseline |
| v2.2.1    | `methodology_scibert_xgb_v2.2.1_smote_model.pkl` | SciBERT + SMOTE (768-dim)                  | âœ… Best performance |
| v2.2.1-CV | N/A                                   | N/A                                              | CV-only evaluation (5-fold) |

> â„¹ï¸ Version 1.0 classifier notebooks were overwritten. Only `.pkl` artifacts retained for comparison and version history.

---

## ðŸ“Š Cross-Validation Summary (v1.1 / v1.2 / v2.0 / v2.1.1 / v2.2.1-CV)

### Fold-wise Cross-Validation Scores

| **Classifier**              | **Fold 1** | **Fold 2** | **Fold 3** | **Fold 4** | **Fold 5** | **Mean Accuracy** | **Std Dev** |
|-----------------------------|------------|------------|------------|------------|------------|-------------------|-------------|
| **Discipline (v1.1)**       | 0.7143     | 0.9048     | 0.7143     | 0.8571     | 0.6667     | **0.7429**        | **0.1151**  |
| **CS Subfield (v1.2)**      | 0.5714     | 0.4286     | 0.5714     | 0.2857     | 0.1429     | **0.4000**        | **0.1895**  |
| **IS Subfield (v1.2)**      | 0.4286     | 0.2857     | 0.5714     | 0.4286     | 0.2857     | **0.4571**        | **0.1069**  |
| **IT Subfield (v1.2)**      | 0.5714     | 0.5714     | 0.5714     | 0.5714     | 0.2857     | **0.5143**        | **0.1143**  |
| **Methodology (v2.0)**      | 0.5238     | 0.7143     | 0.7143     | 0.7143     | 0.6286     | **0.6381**        | **0.0883**  |
| **Methodology (v2.1.1)**    | 0.5238     | 0.5714     | 0.3333     | 0.2857     | 0.4762     | **0.4381**        | **0.1143**  |
| **Methodology (v2.2.1-CV)** | 0.7143     | 0.7619     | 0.4762     | 0.6190     | 0.7143     | **0.6571**        | **0.1017**  |

> âš ï¸ Mixed Methods class remained unclassified across all versions. BERT-based models improved semantic depth but still struggled with extreme class imbalance.
---

### ðŸ” Version Comparison Table

| **Task**         | **Version** | **Prev Accuracy** | **Final Accuracy** | **Î” Accuracy** | **Prev Std Dev** | **Final Std Dev** | **Î” Std Dev** |
|------------------|-------------|-------------------|---------------------|----------------|------------------|--------------------|----------------|
| Discipline       | v1.1        | 0.7714            | 0.7429              | â†“ -0.0285      | 0.0923           | 0.1151             | â†‘ +0.0228      |
| Subfield â€“ CS    | v1.2        | 0.2857            | 0.4000              | â†‘ +0.1143      | 0.1807           | 0.1895             | â†‘ +0.0088      |
| Subfield â€“ IS    | v1.2        | 0.4000            | 0.4571              | â†‘ +0.0571      | 0.1069           | 0.1069             | âž– No change   |
| Subfield â€“ IT    | v1.2        | 0.4286            | 0.5143              | â†‘ +0.0857      | 0.1565           | 0.1143             | â†“ -0.0422      |
| Methodology      | v2.0 â†’ v2.1.1 | 0.6381           | 0.4381              | â†“ -0.2000      | 0.0883           | 0.1143             | â†‘ +0.0260      |
| Methodology      | v2.1.1 â†’ v2.2.1-CV | 0.4381      | 0.6571              | â†‘ +0.2190      | 0.1143           | 0.1017             | â†“ -0.0126      |
---

## ðŸŽ¯ Project Goals

- âœ… Build a scalable, modular NLP pipeline with clear versioning and task separation  
- âœ… Handle small, imbalanced datasets using SMOTE, class weighting, and stratified cross-validation  
- âœ… Demonstrate baseline-to-semantic progression across TF-IDF and transformer-based features  
- âœ… Apply transformer embeddings (MiniLM and SciBERT) for contextual understanding of abstracts  
- âœ… Evaluate classical vs. semantic models on accuracy, macro F1, and classwise recall  
- ðŸ§  Establish generalizable architecture ready for future fine-tuning, domain adaptation, or ensemble modeling  

---

## ðŸ‘¨â€ðŸ’» Author

Aanand Prabhu  
[GitHub â†’ @aanandprabhu30](https://github.com/aanandprabhu30)

> _Submitted as part of my BSc Final Year Project in Computer Science â€“ University of London_